\section{Introduction}

%Multi-task learning is an inductive learning mechanism to improve generalization performance using related task data.
%Many state-of-the-art results in computer vision and natural language processing are obtained using multi-task learning.
Multi-task learning has recently found many applications in computer vision \cite{chexnet17,ZSSGM18}, natural language processing \cite{MTDNN19} and numerous other areas \cite{R17,ZY17}.
%In multi-task learning, having related task data is fundamental to its performance.
%Multi-task learning is particularly powerful when there is limited labeled data for a task to be solved, meanwhile more labeled data from different but related tasks is available.
By learning from related information sources, models that are trained using multi-task learning can share information across different data \cite{C97}.
%For example, many applications in , and many other areas have been achieved by learning from multiple tasks together.
At the same time, negative results have also been reported in the literature \cite{AP16,BS17}, where training a multi-task model performs worse for predicting a task compared to learning the task in isolation (single-task learning).
This phenomenon, known as \textit{negative transfer}, is fundamental to the understanding of multi-task learning.
%\todo{transition}
In this work, we provide a formal study to better understand when and why negative transfer happens for learning multiple high-dimensional linear regression tasks. % for learning from multiple linear regression tasks.
Based on the theory, we develop practical insights to detect negative transfer and guide multi-task training.
%We consider a setting where the target task has limited labeled data and show
%On the other hand, unless the structures across task data are well-understood, applying multi-task learning on several different datasets often result in suboptimal models (or negative transfer in more technical terms).

Comparing the performance of multi-task learning to single-task learning requires developing generalization bounds that scale tightly with the qualities of task data, such as the number of datapoints.
Previous work has shown the benefit of multi-task learning for learning certain half-spaces \cite{MPR16} and sparse regression where the parameter vectors have the same support \cite{LPVT11}. \todo{unpack}
Furthermore, classical Rademacher or VC based theory of multi-task learning, the generalization bounds are usually presented in a way so that the error goes down as more labeled data is added \cite{M06}.
On the other hand, we have observed that adding more labeled data does not always improve performance in multi-task learning.

%The technical challenge to develop a theory for multi-task learning is how to capture generalization performance that scales tightly with the amount of labeled data, in particular when the size of the training set is small.
%Prior generalization theory using uniform stability \cite{LTSM16}, Rademacher complexity \cite{BS03,BBCKP10} is unable to explain the phenomenon of negative transfer because there is no tight bound on test error that scales with the amount of labeled data.
%In particular in Figure \ref{fig_motivation}, we observe a shift from positive transfer to negative transfer as a parameter of task relatedness.
%The theory we develop will provide a precise explanation to such a phenomenon.

%To gain insight into the working of multi-task learning, we consider a simplified setting for learning multiple high-dimensional linear regression tasks.
%A typical process to do multi-task learning involves two steps:
%(i) Jointly learn a shared representation for all the tasks;
%(ii) Fine-tune the learnt model on a specific target task.
%We focus on a hard parameter sharing model proposed in \cite{R17,WZR20} and identify conditions on when multi-task and transfer learning works, and when it doesn't.
%The high-dimensional linear regression setting where the target task data size is limited captures the intuition that the target task only contains limited labeled data.


\todo{One paragraph summarize conceptual finding}

\textbf{Setup.} Concretely, our input consists of $k$ tasks $(X_1, Y_1)$, $(X_2, Y_2)$, $\dots$, $(X_k, Y_k)$.
We shall assume that each task data follows a linear model, i.e. $y_i = X_i \beta_i + \varepsilon_i$, $1\le i\le k$.
Here $\beta_i\in\real^p$ is the model parameter for the $i$-th task.
Each row of $X_i\in\real^{n_i\times p}$ is assumed to be drawn i.i.d. from a fixed distribution with covariance matrix $\Sigma_i$.
We use a shared body $B\in\real^{p\times r}$ for all tasks and a separate prediction head $\set{W_i \in \real^{r}}_{i=1}^k$ for each task.
%    \paragraph{Different covariates:}
This corresponds to minimizing the following optimization objective.
\begin{align}
	\label{eq_mtl}
	f(B; W_1, \dots, W_k) = \sum_{i=1}^k \norm{X_i B W_i - Y_i}^2.
\end{align}
Note that we consider the natural parameterization without reweighting the tasks above.
The shared body $B$ plays an important role because it allows information transfer between different task data.
This is known as the hard parameter sharing architecture in the literature, where we control the capacity $r$ of $B$, e.g. \cite{KD12,WZR20}.
We focus on comparing the test performance on a target task using estimators from doing multi-task training and transfer learning.
We compare the test performance of these estimators to the single-task baseline.
%The details are described in Algorithm \ref{alg_estimator}.


\begin{figure}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/model_distance_motivation.pdf}
	\caption{Positive to negative transfer as the source task rotates further away from the target task (left to right).}
	\label{fig_motivation}
\end{figure}


%We formulate the heterogeneity between different task data under covariate and model shifts .
%\begin{algorithm}[!t]
%	\caption{Multi-task learning using a hard-parameter sharing architecture}
%	\label{alg_estimator}
%	\begin{algorithmic}[1]
%		\Input Two regression tasks $(X_1, Y_2)$, $(X_2, Y_2)$.
%		\Param Shared body $B$, task-specific prediction heads $W_1, W_2$.
%		\State Training the shared body $B$.
%		\State Optimizing the task heads on the validation set

%		\begin{itemize}
%			\item Jointly optimizing both tasks: $\hat{\beta}_t^{\MTL}$
%			\item Optimizing the target task: $\hat{\beta}_t^{\TL}$
%			\item Single-task training baseline: $\hat{\beta}_t^{\STL}$
%		\end{itemize}
%		\State Problem statement: how can we compare the test error of the three estimators on the target task?
		%, $\te(\hat{\beta}_t^{\MTL})$, $\te(\hat{\beta}_t^{\STL})$ and $\te(\hat{\beta}_t^{\TL})$?
%	\end{algorithmic}
%\end{algorithm}

{\bf Main Results.}
	We show that the benefit from doing multi-task or transfer learning stems from reducing the variance of the estimator for the target task through newly added source task data.
	We derive this result for the setting of two tasks with general inputs and $k$ tasks with the same covariates for any $k \ge 2$.
	The latter setting is prevalent in applications of multi-task learning to image classification, where there are multiple prediction labels/tasks for every image \cite{EA20}.
%	First, we provide a precise analysis for performing multi-task learning over two tasks under covariate and model shifts.
	On the other hand, the difference between task models causes a negative effect that we call the \textit{model shift bias}.
	We show bounds on the trade-off between the amount of variance reduced and the amount of model shift bias incurred, which become tighter and tighter as the number of source task data points increases.

	A crucial technical tool that we develop is the asymptotic limit of the trace of the inverse of the sum of two independent sample covariance matrices. Our result not only extends the well-known Marchenkoâ€“Pastur law in random matrix theory to the sum of two independent sample covariance matrices with general covariance matrices, but also gives an almost optimal error bound, which may be of independent interest.

\textbf{Quantitative insights.}
	We identify three factors: task model similarities and their noise level, data size and covariate shift \cite{PY09,K18}.
	These are achieved through tight generalization bounds established in the high-dimensional regression setting.
	Using these tools, we can explain several phenomena that are not explained by the techniques of \cite{WZR20}.
	\begin{itemize}
		\item We provide a sharp transition to show that task model similarities can determine whether there is positive transfer.
		For settings where tasks are similar, we further show that the transfer effect depends on the single-task accuracy of the source task.
		\item We show how source task data size can also determine transfer by providing a sharp transition.
		We use our tools to explain the result of taskonomy \cite{ZSSGM18}, regarding the data efficiency of multi-task learning.
		\item Our result has implications on the following question.
	Is it better for two tasks to have the same covariance matrix or complementary covariance matrices?
	For our setting, we show that when the data ratio is large, having the same covariance matrix provably yields the lowest test performance on the target task.
	On the other hand, when data ratio is small, we find that there are cases when having complementary covariance matrices is better.
	The result provides insight into why the covariance alignment algorithm can help improve performance in \cite{WZR20}.
	\end{itemize}

Finally, we extend our reuslts to study the transfer functions used in taskonomy \cite{ZSSGM18}.
%	{\it Insight 1}: $\hat{\beta}_t^{\MTL}$ vs $\hat{\beta}_t^{\STL}$.
%	With multi-task training, since the training objective balances the losses from both the source and target tasks, the trained model can have worse performance for the target task.
%	In particular, if model shift is too large, we get negative transfer from multi-task training.

%	{\it Insight 2}: $\hat{\beta}_t^{\TL}$ vs $\hat{\beta}_t^{\MTL}$.
%	Finally, we show that the transfer learning estimator always improves over mutli-task training.
%	The amount of improvement becomes more significant as the model distance becomes larger.

%	We show the result for two settings: 2 tasks with different covariates, and $k$ tasks that all share the same covariates.
%	We show that our results and insights from the above can be extended to this setting as well.
	%{\bf Negative transfer from model shift:} To complement the above result, we establish a fundamental limit of multi-task learning compared to single-task learning in the presence of model shift.
	%The phenomenon of negative transfer persists despite changing the model capacity or reweighting the tasks. [\textbf{done}]

%\noindent{\bf Variance reduction from information transfer.}
%	Second, we apply our general theory to compare the performance of three different estimators for the target task: multi-task training, single-task training and transfer learning.
%	\begin{enumerate}

%\smallskip
%	\noindent\textbf{Covariate shift and data ratio.}
%	For the case of two tasks with general inputs, we further study the factors that determine the transfer rate.
%	We identify two factors, the covariate shift matrix and the ratio between number of task data.
%	Next we show that if we we only optimize the multi-task model on the target task, then we always obtain improved performance over single-task training.
%	We provide fine-grained understanding on the amount of improvement that depends on covariate shift and data ratio.

%	\textit{Insight 3}: $\hat{\beta}_t^{\TL}$ vs $\hat{\beta}_t^{\STL}$.
\textbf{Experimental results.} We provide practical implications to validate our theory.
\begin{itemize}
	\item We validate on text and image classification tasks that comparing single-task accuracies can help determine whether multi-task learning performs better than single-task learning.
	\item We show that when the number of source task datapoints is large compared to the target task, then aligning task covariances always improves performance.
	On the other hand, if the number of source task datapoints is comparable to the target task, aligning task covariances may hurt performance.
\end{itemize}

