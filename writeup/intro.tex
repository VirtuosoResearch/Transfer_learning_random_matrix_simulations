\section{Introduction}

The idea of transferring information across tasks has recently found applications in lots of work in deep learning.
In our own experience as well as discussions with collaborators, we have often heard that multi-task learning that is applied on heterogeneous data often result in suboptimal models (or negative transfer in more technical terms).

To gain insight into the working of these methods, we consider a principled study of how hard parameter sharing based multi-task and transfer learning works for learning multiple high-dimensional linear regression tasks.
We follow the model proposed in \cite{WZR20} that to identify conditions on when multi-task and transfer learning works, and when it doesn't.
We refine the task covariance part of \cite{WZR20} into three factors: model distance, covariate shift matrix and data ratio \cite{PY09,K18}.
We study a high-dimensional linear regression setting where the target task data size is limited.

\smallskip
\noindent\textbf{Setup.} We are given the input of $k$ tasks $(X_1, Y_1)$, $(X_2, Y_2)$, $\dots$, $(X_k, Y_k)$.
We shall assume that each task data follows a linear model.
For every $1\le i\le k$, we assume that
\[ Y_i = X_i \beta_i + \varepsilon_i, \]
where $\beta_i\in\real^p$ is the model parameter for the $i$-th task.
Each row of $X_i\in\real^{n_i\times p}$ is assumed to be drawn i.i.d. from a fixed distribution with covariance matrix $\Sigma_i$.
We assume that for every row $x$ of $X_i$, we have
\[ \ex{xx^{\top}} = \Sigma_i. \]
We also write $x = \Sigma_i^{1/2} z_i$, where $z_i$ is a random vector with mean $0$ and variance $1$.

We use a shared body (module) $B\in\real^{p\times r}$ for all tasks and a separate head (module) $\set{W_i \in \real^{r}}_{i=1}^k$ for each task.
%    \paragraph{Different covariates:}
This corresponds to minimizing the following optimization objective.
\begin{align}
	\label{eq_mtl}
	f(B; W_1, \dots, W_k) = \sum_{i=1}^k \norm{X_i B W_i - Y_i}^2.
\end{align}
Note that we consider the natural parameterization without reweighting the tasks above.
The shared body $B$ plays an important role because it allows information transfer between different task data.
This is known as the hard parameter sharing architecture in the literature, where we control the capacity $r$ of $B$, e.g. \cite{KD12,WZR20}.
Moreover, \cite{KD12} observed that controlling the capacity can outperform the implicit capacity control of adding regularization over $B$.


%We formulate the heterogeneity between different task data under covariate and model shifts .
By using random matrix theory, we can explain several phenomena that are not explained by the techniques of \cite{WZR20}.
\todo{list those here}
These are achieved through tight generalization bounds established in the high-dimensional regression setting.
Our main results are described as follows.

\smallskip
\noindent{\bf Variance reduction from information transfer.}
	First, we provide a precise analysis for performing multi-task learning over two tasks under covariate and model shifts.
	We show that multi-task learning helps improve performance over target tasks by \textit{reducing variance} through newly added source task data.
	On the other hand, the difference between task models causes a negative effect that we call the \textit{model shift bias}.
	We show upper and lower bounds on both the amount of variance reduced and the amount of model shift bias incurred, which become tighter and tighter as the number of source task data points increases.

	{\it Insight 1: MTL vs STL.} With multi-task training, since the training objective balances the losses from both the source and target tasks, the trained model can have worse performance for the target task.
	In particular, if model shift is too large, we get negative transfer from multi-task training.

	{\it Insight 2: TL vs MTL.} Finally, we show that the transfer learning estimator always improves over mutli-task training.
		The amount of improvement becomes more significant as the model distance becomes larger.

	We show the result for two settings: 2 tasks with different covariates, and $k$ tasks that all share the same covariates.
	The latter setting is prevalent in applications of multi-task learning to image classification, where there are multiple prediction labels/tasks for every image \cite{EA20}.
	We show that our results and insights from the above can be extended to this setting as well.

	%{\bf Negative transfer from model shift:} To complement the above result, we establish a fundamental limit of multi-task learning compared to single-task learning in the presence of model shift.
	%The phenomenon of negative transfer persists despite changing the model capacity or reweighting the tasks. [\textbf{done}]

%\noindent{\bf Variance reduction from information transfer.}
%	Second, we apply our general theory to compare the performance of three different estimators for the target task: multi-task training, single-task training and transfer learning.
%	\begin{enumerate}

\smallskip
	\noindent\textbf{Covariate shift and data ratio.}
	Since there is no covariate shift and the data ratio is always equal to one, the main factor is model distance.
	Next we show that if we we only optimize the multi-task model on the target task, then we always obtain improved performance over single-task training.
	We provide fine-grained understanding on the amount of improvement that depends on covariate shift and data ratio.

	\textit{Insight 3: TL vs STL.} When data ratio is small, having complementary covariance matrices is better. When data ratio is large, having the same covariance matrix is better.

\smallskip
	\noindent\textbf{Experimental results.}


