
\begin{abstract}
When does multi-task learning outperform single-task learning?
In this work, we address this question by studying the test performance of predicting a particular task given multiple tasks using a commonly used hard parameter sharing architecture.
In the setting of high-dimensional linear regression, we provide a sharp analysis of the bias-variance tradeoff for multi-task learning estimators.
A key technical tool that we develop is the trace of $(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}$ for two random matrices $X_1$ and $X_2$ in the setting of two tasks.
	Based on the theory, we provide more precise interpretations of many empirical phenomena in multi-task learning.
	For example, we theoretically analyze the benefit of multi-task learning for reducing the amount of labeled data needed to achieve comparable performance to single-task learning. %, which has been a key empirical finding in recent work.
	Finally, we show practical implications of our theory for detecting and mitigating negative effects in image and text classification tasks.
\end{abstract}

\section{Introduction}

%Multi-task learning is an inductive learning mechanism to improve generalization performance using related task data.
%Many state-of-the-art results in computer vision and natural language processing are obtained using multi-task learning.
Multi-task learning represents a powerful paradigm to solve complex prediction tasks in computer vision \cite{chexnet17,ZSSGM18}, natural language processing \cite{GLUE,superglue} and numerous other areas \cite{ZY17}.
%In multi-task learning, having related task data is fundamental to its performance.
%Multi-task learning is particularly powerful when there is limited labeled data for a task to be solved, meanwhile more labeled data from different but related tasks is available.
By combining multiple information sources, it is possible to share all the information in the same model.
%For example, many applications in , and many other areas have been achieved by learning from multiple tasks together.
The performance of multi-task learning depends on the relationship of the information sources \cite{C97}.
When the information sources are heterogeneous, negative transfer -- where multi-task learning performs worse than single-task learning -- has often been observed \cite{AP16,BS17}.
While numerous studies have sought to alleviate negative transfer \cite{ZY17}, a rigorous understanding of the contributing causes of negative transfer has remained elusive in the literature \cite{R17}.
%This phenomenon, known as \textit{negative transfer}, is fundamental to the understanding of multi-task learning.
In this work, we develop technical tools to compare multi-task learning to single-task learning for the setting of high-dimensional linear regression. % for learning from multiple linear regression tasks.
We use the tools to rigorously explain the phenomena of negative transfer.
Our theory also leads to practical implications: we propose a diagnostic metric and a training schedule to improve multi-task training.
%We consider a setting where the target task has limited labeled data and show
%On the other hand, unless the structures across task data are well-understood, applying multi-task learning on several different datasets often result in suboptimal models (or negative transfer in more technical terms).

Identifying negative transfer requires developing tight generalization bounds for both multi-task learning and single-task learning.
In classical Rademacher or VC based theory of multi-task learning \cite{B00,AZ05,M06}, the generalization bounds are usually presented so that the error reduces as the data sizes of all tasks increase.
For instance, the data sizes of all tasks are often assumed to be equal \cite{B00}.
On the other hand, uneven data sizes or dominating tasks have been empirically observed as a cause of negative transfer \cite{YKGLHF20}.
More recent work has shown the benefit of learning multi-task representations for certain half-spaces \cite{MPR16} and sparse regression \cite{LPTV09,LPVT11}.
To rigorously study negative transfer, the technical challenge is to develop generalization bounds that scale tightly with properties of the data.


In this work, we consider the setting of high-dimensional linear regression and focus on predicting a particular task whose amount of labeled data is limited.
Following Hastie et al. \cite{HMRT19} and Bartlett et al. \cite{BLLT20}, we assume that for every task $1\le i\le t$, its features are random vectors $x = \Sigma_i^{1/2}z$, where $z\in\real^p$ consists of i.i.d. entries with mean zero and unit variance, and $\Sigma_i\in\real^{p\times p}$ is a positive semidefinite matrix.
Let $n_i$ denote the data size and $X_i\in\real^{n_i\times p}$ denote the features of task $i$, for every $1\le i\le t$.
The label of task $i$ is given by $Y_i = X_i\beta_i + \varepsilon_i$, where $\beta_i\in\real^p$ denotes the ground truth parameters for task $i$ and $\varepsilon_i$ denotes i.i.d. random noise with mean zero and variance $\sigma^2$.
Without loss of generality, let the $t$-th task denote the target task.
We assume that the data size of task $t$ is a small constant $\rho_t > 1$ times $p$ to capture the need for more labeled data.
%We shall assume that each task data follows a linear model, i.e. $y_i = X_i \beta_i + \varepsilon_i$, $1\le i\le k$.
%Here $\beta_i\in\real^p$ is the model parameter for the $i$-th task.
%Each row of $X_i\in\real^{n_i\times p}$ is assumed to be drawn i.i.d. from a fixed
%distribution with covariance matrix $\Sigma_i$.

We combine all the labeled data using a hard parameter sharing architecture with a linear shared layer $B\in\real^{p\times r}$ for all tasks and a separate prediction head $\set{W_i \in \real^{r}}_{i=1}^t$ for each task \cite{R17,MTDNN19,WZR20}.
%This corresponds to minimizing $ \sum_{i=1}^t \norm{X_i B W_i - Y_i}^2$.
For a target task $t$, let $\hat{\beta}_t^{\MTL} = B W_t$ denote the optimal multi-task estimator (defined precisely in Section \ref{sec_setup}).
Let $\hat{\beta}_t^{\STL}$ denote the standard linear regression estimator for task $t$.
We say there is negative transfer if the prediction loss of $\hat{\beta}_t^{\MTL}$  is larger than that of $\hat{\beta}_t^{\STL}$, or positive transfer otherwise.

\textbf{Main results.}
We revisit the bias-variance tradeoff of the multi-task estimator.
Interestingly, we observe that the variance of $\hat{\beta}_t^{\MTL}$ is always smaller than that of $\hat{\beta}_t^{\STL}$, hence resulting in a positive effect of variance reduction.
The bias of $\hat{\beta}_t^{\MTL}$, which we term as \textit{model shift bias}, results in a negative effect caused by the difference between $\beta_t$ and the rest of $\set{\beta_i}_{i=1}^{t-1}$.
Hence, the tradeoff between the variance reduction effect and model shift bias determines whether the transfer is positive.

To quantify the tradeoff more precisely, we develop a new technical tool that provides a tight bound on the trace of $(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}$, which arises naturally from the setting of two tasks.
Our result extends a well-known result on the trace of $(X_1^{\top}X_1)^{-1}$ for a single random matrix \cite{S07}.
Furthermore, we provide a nearly optimal error bound, which may be of independent interest.
Using the result, we provide a sharp analysis of the bias-variance tradeoff of $\hat{\beta}_t^{\MTL}$ for two settings:
(i) two tasks with general covariance matrice; (ii) any number of tasks that have the same features (e.g. images).
Finally, we apply our technical tool to the related setting of transfer learning.
We pool learnt representations from source tasks into a shared body similar to $B$ in the hard parameter sharing architecture, which has also been used in Taskonomy by Zamir et al.'18 \cite{ZSSGM18}.
For this setting, we show that the model shift bias can be captured by the orthogonal projection of $\beta_t$ to the subspace spanned by $\set{\beta_i}_{i=1}^{t-1}$.
These results are presented more precisely in Section \ref{sec_main}.

The tradeoff that we develop depends on the properties of each task such as its data size and covariance matrix.
In Section \ref{sec_insight}, we use our technical tool to rigorously explain negative transfer in multi-task learning.
We identify three factors including task similarity, data size and covariate shift.
\squishlist
		\item First, we define task dissimilarity as the distance between $\beta_1$ and $\beta_2$ using a simplified isotropic setting of two tasks.
		Using our technical tool, we show a sharp transition from positive to negative transfer as task dissimilarity increases.
		Furthermore, we show that negative transfer is more likely to occur when the source task labels are particularly noisy.
%		In Section \ref{sec_validate}, we validate the observation on text and image classification tasks.
%	In , we provide the trade-off between $\norm{\beta_1 - \beta_2}^2$ and a certain function $\Phi(\rho_1, \rho_2)$ to determine the type of transfer.
		\item Second, we consider the effect of uneven data sizes.
	We begin by observing that adding more labeled data from the source task does not always improve performance.
	Then, we theoretically analyze the benefit of multit-task learning for reducing the amount of labeled data needed to achieve comparable performance to single-task learning, which is a key empirical finding of Taskonomy \cite{ZSSGM18}.
%	We define the \textit{data efficiency ratio} as the smallest $x$ such that if we only use an $$ fraction of labeled data, then the test error of $\hat{\beta}_t^{\MTL}$ matches that of the $\hat{\beta}_t^{\STL}$ on the entire set.
	%In Proposition \ref{prop_data_efficiency}, we show that the data efficiency ratio of an illustrative example is at most $\alert{\frac {1}{2\rho_t}}$.
	%In Section \ref{sec_validate}, we validate that performing multi-task learning can reduce the need for labeled data on 6 sentiment analysis tasks. %, we observe that just by using \alert{40\%} of the labeled data, the overall accuracy of multi-task learning matches that of learning every task in isolation.
		\item Finally, we show that covariate shift, measured by $\Sigma_1^{1/2}\Sigma_2^{-1/2}$, is another cause for suboptimal performance for $\hat{\beta}_t^{\MTL}$.
		We show that as $n_1 / n_2$ becomes large, the source task should have the same covariance matrix as the target task to ensure the best performance.
%		On the other hand, when $n_1 / n_2$ is small, there are counter examples where having the same covariance matrix is not necessarily the optimal choice.
\squishend
We validate the above phenomena on text and image classification tasks.
Based on the theory, we provide algorithmic implications for detecting and mitigating negative transfer.
First, we provide a metric to determine positive versus negative transfer by comparing the test accuracies of single-task models.
Second, \todo{}
Finally, we provide a new insight on the covariance alignment algorithm, proposed in \cite{WZR20}.
We show that as the size of the source task becomes larger compared to the target task, the covariance alignment algorithm produces more significant results.

