\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath, amsthm, mathabx,mathtools}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\usepackage{tikz}
\usetikzlibrary{shapes,snakes}
\usepackage{color, enumerate}
\usepackage{comment, authblk}
\usepackage{hyperref}
\usepackage{pgfplots}
\usepackage{bm}
\usepackage{stmaryrd}
%\usepackage[notref,notcite]{showkeys}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{a4wide}
\usepackage{titlesec}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{epstopdf}

\usepackage{sectsty}
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{algpseudocode}
\usepackage{pifont}
%\usepackage[normalem]{ulem}

%\usepackage{showlabels}

\titleformat*{\subsection}{\large\bfseries}
\numberwithin{equation}{section}
\renewcommand\Authands{ and }

\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\newlength\figureheight
\newlength\figurewidth

\def\RR{{\mathbb R}}
\def\ZZ{{\mathbb Z}}

\newcommand{\Hprod}[2]{\langle #1 , #2 \rangle_{\H}}
\newcommand{\Hnorm}[1]{\| #1 \|_{\H}}
\newcommand{\Rprod}[2]{\langle #1 , #2 \rangle}
\newcommand{\Lnorm}[2]{\left\| #2 \right\|_{{#1}}}
\newcommand{\Dnorm}[1]{\| #1 \|_{\DD}}
\DeclareMathOperator{\fc}{fc}
\newcommand{\Hpnorm}[2]{\| #2 \|_{\H,#1}}


\newcommand{\Ric}{{\text{Ric}}}
\DeclareMathOperator{\Ricci}{Ricci}



\def\cA{{\mathcal A}}
\def\cB{{\mathcal B}}
\def\cO{{\mathcal O}}
\def\cW{{\mathcal W}}


\newcommand{\bmu}{{\bm{u}}}

\newcommand{\bmq}{{\bm q}}

\DeclareMathOperator{\Prob}{\mathbb{P}}   %probability
\DeclareMathOperator{\cov}{cov}             %logarithmic integral
\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\Vol}{Vol}
\DeclareMathOperator{\cum}{cum}
\DeclareMathOperator{\A}{d A}
\DeclareMathOperator{\G}{G}
\newcommand{\1}{\mathds{1}}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{wrapfig}

\special{papersize=8.5in,11in}

\flushbottom

\usepackage{color}
\newcommand{\sidenote}[1]{\marginpar{\color{red}\footnotesize #1}}
\newcommand{\Id}{{\mathrm{Id}}}

\topmargin=-0.15in
\oddsidemargin=0in
\evensidemargin=0in
\textwidth=6.5in


\numberwithin{equation}{section}
\newcommand{\bla}{\bm{\lambda}}
\newcommand{\boeta}{\bm{\eta}}
\newcommand{\bzeta}{\bm{\zeta}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bov}{\bm{v}}
\newcommand{\bow}{\bm{w}}
%\newcommand{\bla}{\mbox{\boldmath $\lambda$}}Ass
\DeclareMathOperator{\bp}{\bold p}
\newcommand{\boldu}{\mbox{\boldmath $u$}}
\newcommand{\bnu}{\mbox{\boldmath $\nu$}}
%\newcommand{\boldeta}{\mbox{\boldmath $\eta$}}
\newcommand{\bbeta}{\mbox{\boldmath $\beta$}}
\def\cH{{\mathcal H}}

\newcommand{\const}{\mbox{const}}
\newcommand{\La}{\Lambda}
\newcommand{\Sig}{\Sigma}
\newcommand{\eps}{\varepsilon}
\newcommand{\pt}{\partial}
\newcommand{\rd}{{\rm d}}
\newcommand{\rdA}{{\rm d A}}
\newcommand{\bR}{{\mathbb R}}

\newcommand{\bZ}{{\mathbb Z}}
\newcommand{\bke}[1]{\left( #1 \right)}
\newcommand{\bkt}[1]{\left[ #1 \right]}
\newcommand{\bket}[1]{\left\{ #1 \right\}}

\newcommand{\bka}[1]{\left\langle #1 \right\rangle}
\newcommand{\vect}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\tp}[1]{{#1}^\dagger}
\newcommand{\pbb}[1]{\biggl({#1}\biggr)}
\newcommand{\pBB}[1]{\Biggl({#1}\Biggr)}

\newcommand{\fn}{{\mathfrak n}}


\newcommand{\ba}{{\bf{a}}}
\newcommand{\ri}{\mathrm{i}}
\newcommand{\rI}{\mathrm{I}}
\newcommand{\Ub}{\mathbf{U}}
\newcommand{\Db}{\mathbf{D}}

\newcommand{\beq}{\begin{equation}}
\newcommand{\bEq}{\end{equation}}
\newcommand{\del}{\partial}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\fd}{{\mathfrak d}}

\newcommand{\bx}{{\bf{x}}}
\newcommand{\by}{{\bf{y}}}
%\newcommand{\bu}{{\bf{u}}}
%\newcommand{\bv}{{\bf{v}}}
%\newcommand{\bw}{{\bf{w}}}

\newcommand{\bz} {{\bf {z}}}
\newcommand{\bt} {{\bf t }}

\newcommand{\bh}{{\bf{h}}}


\newcommand{\wG}{{\widehat G}}
\newcommand{\al}{\alpha}
\newcommand{\de}{\delta}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\newcommand{\hp}[1]{with $ #1 $-high probability}


\newcommand{\e}{{\varepsilon}}

\newcommand{\ga}{{\gamma}}
\newcommand{\Ga}{{\Gamma}}
\newcommand{\la}{\lambda}
\newcommand{\Om}{{\Omega}}
\newcommand{\om}{{\omega}}
\newcommand{\si}{\sigma}
\renewcommand{\th}{\theta}
\newcommand{\td}{\wt}
\newcommand{\ze}{\zeta}

\newcommand{\qqq}[1]{\llbracket{#1}\rrbracket}

\newcommand{\cL}{{\cal L}}
\newcommand{\cE}{{\cal E}}
\newcommand{\cG}{{\cal G}}
\newcommand{\cP}{{\cal P}}
\newcommand{\LL}{{\rm L}}
\newcommand{\PP}{{\rm P}}
\newcommand{\QQ}{{\rm Q}}
\newcommand{\cC}{{\cal C}}

\newcommand{\cQ}{{\cal Q}}
\newcommand{\cK}{{\cal K}}

\newcommand{\cN}{{\cal N}}

%\newcommand{\bC}{{\mathbb C}}
\newcommand{\pd}{{\partial}}
\newcommand{\nb}{{\nabla}}
\newcommand{\lec}{\lesssim}

\newcommand{\rU}{{\rm U}}

\newcommand{\ph}{{\varphi}}


\def\cA{{\mathcal A}}
\def\cO{{\mathcal O}}
\def\cW{{\mathcal W}}

\newcommand{\tD}{\widetilde{D}}
\newcommand{\tG}{\widetilde{G}}
\newcommand{\ctQ}{\widetilde{\mathcal{Q}}}
\newcommand{\CTG}{\widetilde{\mathcal{G}}}
\newcommand{\tsig}{\widetilde{\sigma}}

\newcommand{\fa}{{\mathfrak a}}
\newcommand{\fb}{{\mathfrak b}}
\newcommand{\fw}{{\mathfrak w}}

%\newcommand{\wb}{{\mathcal w}}
\newcommand{\Ai}{{\text{Ai} }}

\newcommand{\cS}{{\mathcal S}}


\newcommand{\br}{{\bf{r}}}
\newcommand{\cR}{{\mathcal R}}
\newcommand{\cU}{{\mathcal U}}
\newcommand{\cV}{{\mathcal V}}


\renewcommand{\div}{\mathop{\mathrm{div}}}
\newcommand{\curl}{\mathop{\mathrm{curl}}}
\newcommand{\spt}{\mathop{\mathrm{spt}}}
\newcommand{\wkto}{\rightharpoonup}
\newenvironment{pf}{{\bf Proof.}} {\hfill\qed}

\newcommand{\lv}{{\bar v}}
\newcommand{\lp}{{\bar p}}


\setcounter{tocdepth}{1}


\renewcommand{\S}{\mathbb S}
\newcommand{\T}{\mathbb T}
\newcommand{\V}{\mathbb V}
\newcommand{\bU}{ {\bf  U}}
\renewcommand{\S}{[\bf S]}
\newcommand{\bT}{\T}
\newcommand{\non}{\nonumber}
\newcommand{\wH}{K}






\newcommand{\ttau}{\vartheta}


\usepackage{amsmath} %[intlimits]
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{amsxtra}
%\usepackage{amscd}
%\usepackage{bbm}
%\usepackage{mathrsfs}
%\usepackage{bm}
%\def\bfdefault{b}


\renewcommand\labelenumi{(\roman{enumi})}

\setlength{\unitlength}{1cm}

\def\RR{{\mathbb R}}
\def\ZZ{{\mathbb Z}}



\renewcommand{\b}[1]{\bm{\mathrm{#1}}} %bold
\newcommand{\bb}{\mathbb} %blackboard bold
\renewcommand{\r}{\mathrm}
\renewcommand{\ss}{\mathsf} %sans serif
\renewcommand{\cal}{\mathcal}
\newcommand{\fra}{\mathfrak}
\newcommand{\ul}[1]{\underline{#1} \!\,} %underline
\newcommand{\ol}[1]{\overline{#1} \!\,} %overline
\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}
\newcommand{\mG}{\mathcal G}

\newcommand{\me}{\mathrm{e}} %\newcommand{\me}{\mathrm{e}}
\newcommand{\ii}{\mathrm{i}} %\newcommand{\mi}{\mathrm{i}}
\newcommand{\dd}{\mathrm{d}}

\newcommand{\s}{\mspace{-0.9mu}} 		%empty space for tensor indices
\newcommand{\col}{\mathrel{\mathop:}}
\newcommand{\st}{\,\col\,}
\newcommand{\deq}{\mathrel{\mathop:}=}
\newcommand{\eqd}{=\mathrel{\mathop:}}
\newcommand{\id}{\mspace{2mu}\mathrm{i}\mspace{-0.6mu}\mathrm{d}} %identity map
\newcommand{\umat}{\mathbbmss{1}} %unit matrix
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\newcommand{\floor}[1] {\lfloor {#1} \rfloor}
\newcommand{\ceil}[1]  {\lceil  {#1} \rceil}
\newcommand{\ind}[1]{\b 1 (#1)}%{\umat_{\{#1\}}}
\newcommand{\indb}[1]{\b 1 \pb{#1}}
\newcommand{\indB}[1]{\b 1 \pB{#1}}
\newcommand{\indbb}[1]{\b 1 \pbb{#1}}
\newcommand{\indBB}[1]{\b 1 \pBB{#1}}
\newcommand{\inda}[1]{\b 1 \pa{#1}}%{\umat_{\{#1\}}}

\renewcommand{\le}{\leq}
\renewcommand{\ge}{\geq}

%\renewcommand{\dagger}{*}

%%%%%%%%%%%%%%%%%%%   blackboard bold letters   %%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}

\newcommand{\IE}{\mathbb{I} \mathbb{E}}


%%%%%%%%%%%%%%%%%%%%%%%   Parantheses   %%%%%%%%%%%%%%%%%%%%%
\newcommand{\p}[1]{({#1})}
\newcommand{\pb}[1]{\bigl({#1}\bigr)}
\newcommand{\pB}[1]{\Bigl({#1}\Bigr)}
\newcommand{\pa}[1]{\left({#1}\right)}

\newcommand{\qb}[1]{\bigl[{#1}\bigr]}
\newcommand{\qB}[1]{\Bigl[{#1}\Bigr]}
\newcommand{\qbb}[1]{\biggl[{#1}\biggr]}
\newcommand{\qBB}[1]{\Biggl[{#1}\Biggr]}
\newcommand{\qa}[1]{\left[{#1}\right]}

\newcommand{\h}[1]{\{{#1}\}}
\newcommand{\hb}[1]{\bigl\{{#1}\bigr\}}
\newcommand{\hB}[1]{\Bigl\{{#1}\Bigr\}}
\newcommand{\hbb}[1]{\biggl\{{#1}\biggr\}}
\newcommand{\hBB}[1]{\Biggl\{{#1}\Biggr\}}
\newcommand{\ha}[1]{\left\{{#1}\right\}}

\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\absb}[1]{\bigl\lvert #1 \bigr\rvert}
\newcommand{\absB}[1]{\Bigl\lvert #1 \Bigr\rvert}
\newcommand{\absbb}[1]{\biggl\lvert #1 \biggr\rvert}
\newcommand{\absBB}[1]{\Biggl\lvert #1 \Biggr\rvert}
\newcommand{\absa}[1]{\left\lvert #1 \right\rvert}

\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\normb}[1]{\bigl\lVert #1 \bigr\rVert}
\newcommand{\normB}[1]{\Bigl\lVert #1 \Bigr\rVert}
\newcommand{\normbb}[1]{\biggl\lVert #1 \biggr\rVert}
\newcommand{\normBB}[1]{\Biggl\lVert #1 \Biggr\rVert}
\newcommand{\norma}[1]{\left\lVert #1 \right\rVert}

\newcommand{\avg}[1]{\langle #1 \rangle}
\newcommand{\avgb}[1]{\bigl\langle #1 \bigr\rangle}
\newcommand{\avgB}[1]{\Bigl\langle #1 \Bigr\rangle}
\newcommand{\avgbb}[1]{\biggl\langle #1 \biggr\rangle}
\newcommand{\avgBB}[1]{\Biggl\langle #1 \Biggr\rangle}
\newcommand{\avga}[1]{\left\langle #1 \right\rangle}

\newcommand{\scalar}[2]{\langle{#1} \mspace{2mu}, {#2}\rangle}
\newcommand{\scalarb}[2]{\bigl\langle{#1} \mspace{2mu}, {#2}\bigr\rangle}
\newcommand{\scalarB}[2]{\Bigl\langle{#1} \,\mspace{2mu},\, {#2}\Bigr\rangle}
\newcommand{\scalarbb}[2]{\biggl\langle{#1} \,\mspace{2mu},\, {#2}\biggr\rangle}
\newcommand{\scalarBB}[2]{\Biggl\langle{#1} \,\mspace{2mu},\, {#2}\Biggr\rangle}
\newcommand{\scalara}[2]{\left\langle{#1} \,\mspace{2mu},\, {#2}\right\rangle}

\newcommand{\com}[2]{[{#1} \mspace{2mu}, {#2}]}
\newcommand{\comb}[2]{\bigl[{#1} \mspace{2mu}, {#2}\bigr]}
\newcommand{\comB}[2]{\Bigl[{#1} \,\mspace{2mu},\, {#2}\Bigr]}
\newcommand{\combb}[2]{\biggl[{#1} \,\mspace{2mu},\, {#2}\biggr]}
\newcommand{\comBB}[2]{\Biggl[{#1} \,\mspace{2mu},\, {#2}\Biggr]}
\newcommand{\coma}[2]{\left[{#1} \,\mspace{2mu},\, {#2}\right]}

\newcommand{\poi}[2]{\{{#1} \mspace{2mu}, {#2}\}}
\newcommand{\poib}[2]{\bigl\{{#1} \mspace{2mu}, {#2}\bigr\}}
\newcommand{\poiB}[2]{\Bigl\{{#1} \,\mspace{2mu},\, {#2}\Bigr\}}
\newcommand{\poibb}[2]{\biggl\{{#1} \,\mspace{2mu},\, {#2}\biggr\}}
\newcommand{\poiBB}[2]{\Biggl{\{#1} \,\mspace{2mu},\, {#2}\Biggr\}}
\newcommand{\poia}[2]{\left\{{#1} \,\mspace{2mu},\, {#2}\right\}}


\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\brab}[1]{\bigl\langle #1 \bigr|}
\newcommand{\braB}[1]{\Bigl\langle #1 \Bigr|}
\newcommand{\brabb}[1]{\biggl\langle #1 \biggr|}
\newcommand{\braBB}[1]{\Biggl\langle #1 \Biggr|}

\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\ketb}[1]{\bigl| #1 \bigr\rangle}
\newcommand{\ketB}[1]{\Bigl| #1 \Bigr\rangle}
\newcommand{\ketbb}[1]{\biggl| #1 \biggr\rangle}
\newcommand{\ketBB}[1]{\Biggl| #1 \Biggr\rangle}

\newcommand{\dscalar}[2]{\langle #1 \,|\, #2 \rangle}
\newcommand{\dscalarb}[2]{\bigl\langle #1 \,\big|\, #2 \bigr\rangle}
\newcommand{\dscalarB}[2]{\Bigl\langle #1 \,\Big|\, #2 \Bigr\rangle}
\newcommand{\dscalarbb}[2]{\biggl\langle #1 \,\bigg|\, #2 \biggr\rangle}
\newcommand{\dscalarBB}[2]{\Biggl\langle #1 \,\Bigg|\, #2 \Biggr\rangle}

\newcommand{\dexp}[3]{\langle #1 \,|\, #2 \,|\, #3 \rangle}
\newcommand{\dexpb}[3]{\bigl\langle #1 \,\big|\, #2 \,\big|\, #3 \bigr\rangle}
\newcommand{\dexpB}[3]{\Bigl\langle #1 \,\Big|\, #2 \,\Big|\, #3 \Bigr\rangle}
\newcommand{\dexpbb}[3]{\biggl\langle #1 \,\bigg|\, #2 \,\bigg|\, #3 \biggr\rangle}
\newcommand{\dexpBB}[3]{\Biggl\langle #1 \,\Bigg|\, #2 \,\Bigg|\, #3 \Biggr\rangle}


\DeclareMathOperator*{\slim}{s-lim}
\DeclareMathOperator*{\wlim}{w-lim}
\DeclareMathOperator*{\wstarlim}{w*-lim}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\cotanh}{cotanh}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\SU}{SU}
\DeclareMathOperator{\So}{SO}
\DeclareMathOperator{\su}{su}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\dom}{\mathcal{D}}
\DeclareMathOperator{\domq}{\mathcal{Q}}
\DeclareMathOperator{\ran}{\mathcal{R}}
\DeclareMathOperator{\keroperator}{\mathcal{N}}
\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\sgn}{sgn}
\renewcommand{\ker}{\keroperator}
\DeclareMathOperator{\OO}{O}
\DeclareMathOperator{\oo}{o}
\DeclareMathOperator{\UU}{U}

\DeclareMathOperator{\bA}{\mathbf{A}}
\DeclareMathOperator{\bC}{\mathbf{C}}
\DeclareMathOperator{\bD}{\mathbf{D}}
\DeclareMathOperator{\bS}{\mathbf{S}}
\DeclareMathOperator{\bv}{\mathbf{v}}
\DeclareMathOperator{\bu}{\mathbf{u}}
\DeclareMathOperator{\bw}{\mathbf{w}}

\DeclareMathOperator{\bbC}{\mathbb{C}}
\DeclareMathOperator{\bbE}{\mathbb{E}}
\DeclareMathOperator{\bbN}{\mathbb{N}}
\DeclareMathOperator{\bbP}{\mathbb{P}}

\DeclareMathOperator{\sI}{\mathcal{I}}
\DeclareMathOperator{\sG}{\mathcal{G}}
\DeclareMathOperator{\sW}{\mathcal{W}}

\DeclareMathOperator{\one}{\mathbf{1}}
\DeclareMathOperator{\lc}{\lceil}
\DeclareMathOperator{\rc}{\rceil}
\DeclareMathOperator{\lf}{\lfloor}
\DeclareMathOperator{\rf}{\rfloor}
\DeclareMathOperator{\Var}{\mathbf{Var}}
%\DeclareMathOperator{\Re}{\textnormal{Re}}
%\DeclareMathOperator{\Im}{\im}

\newcommand{\inprod}[1]{\langle#1\rangle}
\newcommand*{\esup}[1]{\overline{#1}}
\newcommand*{\cl}[1]{\overline{#1}}
\newcommand*{\conj}[1]{\overline{#1}}


\theoremstyle{plain} %plain, definition, remark
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem*{lemma*}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{proposition*}{Proposition}
\newtheorem{claim}[theorem]{Claim}

\newtheorem{definition}[theorem]{Definition}
\newtheorem*{definition*}{Definition}
\theoremstyle{remark}
\newtheorem{example}[theorem]{Example}
\newtheorem*{example*}{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem*{remark*}{Remark}
\newtheorem*{remarks*}{Remarks}
\newtheorem*{notation}{Notation}


\setcounter{tocdepth}{1}

\let\cIm\Im
\let\cRe\Re
\renewcommand{\Im}{{\rm{Im}}}
\renewcommand{\Re}{{\rm{Re}}}

\newcommand{\cop}{\color{magenta}}
\newcommand{\cor}{\color{red}}
\newcommand{\cob}{\color{blue}}
\newcommand{\nc}{\normalcolor}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\wb}{\mathbf{w}}
\def\bR{{\mathbb R}}
\def\bZ{{\mathbb Z}}
%\def\bC{{\mathbb C}}
\def\bN{{\mathbb N}}
\newcommand\Si{S}
\newcommand{\Tr}{\mbox{Tr\,}}
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{1}
\newcommand{\bq}{{\bf q}}
%\newcommand{\fa}{{\mathfrak a}} 
%\newcommand{\fb}{{\mathfrak b}} 
\newcommand{\bk}{{\bf{k}}}
%\newcommand{\fw}{{\mathfrak w}} 
%\newcommand{\br}{{\bf r}}
\def\xip{{\xi'}}
\newcommand{\cM}{{\cal M}}
\newcommand{\f}[1]{\boldsymbol{\mathrm{#1}}} 
\newcommand{\for}{\qqquad \text{for} \quad}
\newcommand{\where}{\qqquad \text{where} \quad}




\allowdisplaybreaks

%\title{Edge eigenvalue statistics of separable covariance matrices: the edge universality}
\title{Spectrum for a random matrix model}
\author{ }

\begin{document}
%\maketitle



%\section{Definitions and main results}\label{main_result}

\section{The model}\label{sec_defspike}

%\subsection{The model}

We study the spectrum of the random matrix model: 
$$Q= \Sigma_1^{1/2}  X_1^T X_1 \Sigma_1^{1/2}  + \Sigma_2^{1/2}  X_2^T X_2 \Sigma_2^{1/2} ,$$
where $\Sigma_{1,2}$ are $p\times p$ deterministic covariance matrices, and $X_1=(x_{ij})_{1\le i \le n_1, 1\le j \le p}$ and $X_2=(x_{ij})_{n_1+1\le i \le n_1+n_2, 1\le j \le p}$ are $n_1\times p$ and $n_2 \times p$ random matrices, respectively, where the entries $x_{ij}$, $1 \leq i \leq n_1+n_2\equiv n$, $1 \leq j \leq p$, are real independent random variables satisfying
\begin{equation}\label{eq_12moment} %\label{assm1}
\mathbb{E} x_{ij} =0, \ \quad \ \mathbb{E} \vert x_{ij} \vert^2  = n^{-1}.
\end{equation} 
For now, we assume that the random variables $x_{ij}$ are i.i.d. Gaussian, but we know that universality holds for generally distributed entries. %have arbitrarily high moments, 
%in the sense that for any fixed $k\in \mathbb N$, there is a constant $\mu_k>0$ such that
%\begin{equation}\label{eq_highmoment} %\label{eqn:subgaus}
%\max_{i,j}\left(\mathbb E|x_{ij}|^k\right)^{1/k} \le \mu_k n^{-1/2},  %\var \left(h_{xy}\right)^{1/2}
%\end{equation}
%for all $n$. %For simplicity, we assume that $k$ is a finite fixed integer, the strengths $d_1 > d_2 > \cdots > d_k >0$ are fixed constants, and $ \bu_i$,  $\bv_i$ are deterministic unit vectors. 
We shall consider the high-dimensional setting such that
$$\gamma_n:= \frac{p} {n} \to \gamma,\quad c_n:= \frac{n_1}{n} \to c, \quad \text{as } \ n\to \infty, $$ 
for some constants $\gamma\in (0,\infty)$ and $c \in (0,1)$. 

We assume that $ \Sigma_1^{-1/2}\Sigma_2$ has eigendecomposition
\be \nonumber %\label{eigen}
\Sigma_1^{-1/2}\Sigma_2^{1/2} = ODO^T ,\quad D=\text{diag}(d_1, \cdots, d_p).
\ee
Then by the rotational invariance of Gaussian matrices, we have
$$\wt Q \overset{d}{=}\Sigma_1^{1/2} O \wt Q O^T \Sigma_1^{1/2},\quad \wt Q:=   X_1^T X_1  + D X_2^T X_2 D .$$
Thus we study the spectrum of $\wt Q$ instead. We define $\cal G(z):= (\wt Q-z)^{-1}$ for $z\in \C_+$. With some random matrix tools, we have that 
$$\cal G(z) \approx \diag\left( \frac{1}{-z\left( 1+ m_3(z) + d_i^2 m_4(z)\right)}\right)_{1\le i \le p}= \frac{1}{-z\left( 1+ m_3(z) + D^2 m_4(z)\right)} $$
{\cob in certain sense}. Here $m_{3,4}(z)$ satisfy the following self-consistent equations
%$$\frac{1}{G_{ii}} \approx -z \left( 1+m_3 + d_i^2m_4 \right), \quad \frac{1}{G_{\mu\mu}} = -z(1+m_1), \ \ \mu\in \cal I_1,\quad \frac{1}{G_{\nu\nu}} = -z(1+m_2), \ \ \nu\in \cal I_2,$$
%$$m_1= \frac1n\sum_{i}G_{ii}, \quad m_2= \frac1n\sum_{i}d_i^2 G_{ii}, \quad m_3 = \frac1n\sum_{\mu\in \cal I_1} G_{\mu\mu},\quad m_4 = \frac1n\sum_{\mu\in \cal I_2} G_{\mu\mu}.$$
\begin{align}\label{m34}
\frac{n_1}{n}\frac1{m_3} = - z +\frac1n\sum_{i=1}^p \frac1{  1+m_3 + d_i^2m_4  } ,\quad \frac{n_2}{n}\frac1{m_4} = - z +\frac1n\sum_{i=1}^p \frac{d_i^2 }{  1+m_3 + d_i^2m_4  } 
\end{align}
When $z\to 0$, we shall have
$$m_3(z)= - \frac{a_3}{z} + \OO(1), \quad m_4(z)= - \frac{a_4}{z} + \OO(1),\quad a_3,a_4>0.$$
Then for $z\to0$, the equations in \eqref{m34} are reduced to
\begin{align}\label{m34}
\frac{n_1}{n}\frac{1}{a_3} = 1 +\frac1n\sum_{i=1}^p \frac{1}{a_3 + d_i^2a_4  } ,\quad \frac{n_2}{n}\frac1{a_4} = 1 +\frac1n\sum_{i=1}^p \frac{d_i^2 }{  a_3 + d_i^2 a_4 }. 
\end{align}
First, it is easy to see that these equations are equivalent to
$$a_3 + a_4 = 1- \gamma_n, \quad a_3 +\frac1n\sum_{i=1}^p \frac{a_3}{a_3 + d_i^2[(1-\gamma_n)-a_3]  }=c_n  .$$
Furthermore, we have
\begin{align*}
\tr (Q^{-1}) &= \lim_{z\to 0}\tr \left[\Sigma_1^{-1/2} O \cal G(z)O^T \Sigma_1^{-1/2}\right]
=\tr \left[\Sigma_1^{-1/2} O  \left( \frac{1}{a_3 + D^2 a_4 }\right) O^T \Sigma_1^{-1/2}\right] \\
&=\tr \left[\Sigma_1^{-1/2}  \frac{1}{a_3+ \Sigma_1^{-1}\Sigma_2 a_4} \Sigma_1^{-1/2}\right]=\tr \left[ \frac{1}{a_3\Sigma_1 + a_4\Sigma_2 } \right].
\end{align*}






\end{document}
%where
%$$V^a= (\bv^a_1, \cdots, \bv^a_p),\quad V^b= (\bv^b_1, \cdots, \bv^b_n). $$ %\quad \si_1^a \ge \si_2^a \ge \ldots \ge \si_n^a \ge 0 \ , \quad  \si_1^b \ge \si_2^b \ge \ldots \ge  \si^b_N \ge 0 \ .$$
We denote the empirical spectral distributions (ESD) of $\mathbf C$ and $B:=SS^T $ by
\begin{equation}\label{sigma_ESD}
\pi_{\mathbf C} := \frac{1}{p} \sum_{i=1}^p \delta_{\si_i} \ ,\quad \pi_{B}:= \frac{1}{r} \sum_{i=1}^r \delta_{s_i}\ .
\end{equation}
For our purpose, we may need to assume some regularity assumptions on $\pi_{\mathbf C}$ and $\pi_B$ later.
Moreover, we assume that $\pi_{\mathbf C}$ and $\pi_B$ converge to certain probability distributions as $n\to \infty$. 

%We assume that there exists a small constant $0<\tau<1$ such that for all $n$ large enough,
%\begin{equation}\label{assm3}
%\max\{\si_1^a,  \sigma_1^b \} \le \tau^{-1}, \quad \max\left\{\pi_A^{(p)}([0,\tau]), \pi_B^{(n)}([0,\tau])\right\} \le 1 - \tau .
%\end{equation}
%Note the first condition means that the operator norms of $A$ and $B$ are bounded by $\tau^{-1}$, and the second condition means that the spectrums of $A$ and $B$ cannot concentrate at zero.
%
%
%In this paper, we study spiked separable sample covariance matrices, which can be realized through a low rank perturbation of the non-spiked version. We shall assume that $\mathcal Q_1$ is a separable sample covariance matrix without spikes (see Assumption \ref{ass:unper} below).
%To add spikes, we follow the setup in \cite{ding2017} and assume that there exist some fixed intergers $r, s\in \N$ and constants $d_i^a$, $1\le i \le r$, and $d_\mu^b$, $1\le \mu \le s$, such that
%%we follow the setup from the paper of the first author \cite{ding2017}  by defining
%\begin{equation}\label{eq_defnsigmaa}
%\wt A= V^a\wt \Sig^a (V^a)^*, \quad \wt B= V^b \wt \Sigma^b (V^b)^* ,\quad \wt\Sig^a=\text{diag}(\wt\si_1^a, \ldots, \wt\si_p^a), \quad \wt\Sig^b=\text{diag}( \wt\si_1^b, \ldots,  \wt\si_n^b),
%\end{equation}
%where 
%  \begin{equation}\label{eq_defnsigmab}
%\widetilde{\sigma}_i^a=
% \begin{cases}
% \sigma_i^a(1+d^a_i), \  & 1 \leq i \leq r \\
% \sigma_i^a, \ & \text{otherwise}
% \end{cases},\qquad \widetilde{\sigma}_\mu^b=
% \begin{cases}
% \sigma_\mu^b(1+d^b_\mu), \  & 1 \leq \mu \leq s \\
% \sigma_\mu^b \ & \text{otherwise}
% \end{cases}.
% \end{equation} 
% Without loss of generality, we assume that we have reordered indices such that 
% \be\label{reorder} \wt\si_1^a \ge \wt\si_2^a \ge \ldots \ge \wt\si_p^a \ge 0 \ , \quad  \wt\si_1^b \ge \wt\si_2^b \ge \ldots \ge  \wt\si^b_n \ge 0 \ .\ee
% Moreover, we assume that 
% \begin{equation}\label{assm33}
%\max\{\widetilde\si_1^a,  \widetilde\sigma_1^b \} \le \tau^{-1} .
%\end{equation}
% With (\ref{eq_defnsigmaa}) and (\ref{eq_defnsigmab}), we can write   
%\begin{equation}\label{AOBO}
%\widetilde A = A\Big(1+{V_o^a} {D}^{a} ({V_o}^a)^*\Big)=\Big(1+ {V_o}^{a} D^a({V_o}^a)^*\Big)A,\quad \widetilde B= B\Big(1+{V_o}^{b} {D}^{b} {(V_o^b)}^*\Big)=\Big(1+{V_o}^{b} {D}^{b} {(V_o^b)}^*\Big)B,
%\end{equation}
%where 
%$${D}^{a}=\text{diag}(d_1^{a},\cdots, d_r^a), \quad V_o^a=(\bv_1^a,\cdots, \bv^a_r),\quad {D}^{b}=\text{diag}(d_1^{b},\cdots, d_s^b), \quad V_o^b=(\bv_1^b,\cdots, \bv^b_s).$$
%Then we define the spiked separable sample covariance matrices as
%\begin{equation}\label{eq_sepamodel}
%\widetilde{\mathcal{Q}}_1=\widetilde A^{1/2} X \widetilde B X^* \widetilde A^{1/2}, \quad  \widetilde{\mathcal{Q}}_2=\widetilde B^{1/2} X^* \widetilde A X \widetilde B^{1/2}.
%\end{equation} 
%
%
%We summarize our basic assumptions here for future reference.
%\begin{assumption}\label{assm_big1}
%We assume that $X$ is a $p\times n$ random matrix with real entries satisfying (\ref{eq_12moment}) and \eqref{eq_highmoment}, $A$ and $B$ are deterministic non-negative definite symmetric matrices satisfying \eqref{eigen} and (\ref{assm3}), $\wt A$ and $\wt B$ are deterministic non-negative definite symmetric matrices satisfying \eqref{eq_defnsigmaa}, \eqref{eq_defnsigmab}, \eqref{reorder} and \eqref{assm33}, and $d_n$ satisfies \eqref{eq_ratio}.
% %and (\ref{assm2}). We assume that $T$ is an $M\times M$ deterministic diagonal matrix satisfying (\ref{simple_assumption}) and (\ref{assm3}).  
%\end{assumption}

  

\subsection{Resolvents and limiting laws}

We want to study the eigenvalue statistics of $\mathcal Q_{1,2}$ and $\wt {\mathcal Q}_{1,2}$ through their {\it{resolvents}} (or  {\it{Green's functions}}). 
Throughout the following, we shall denote the upper half complex plane and the right half real line by 
$$\mathbb C_+:=\{z\in \mathbb C: \im z>0\}, \quad \mathbb R_+:=[0,\infty).$$ %\quad  \mathbb R_*:=(0,\infty).$$

\begin{definition}[Resolvents]\label{defn_resolvent}
For $z = E+ \ii \eta \in \mathbb C_+,$ we define the following resolvents as %for $\wt {\mathcal Q}_{1,2}$ as
\begin{equation}\label{def_green}
\mathcal G_{1,2}(X,z):=\left({\mathcal Q}_{1,2}(X) -z\right)^{-1} , \ \ \ \wt{\mathcal G}_{1,2} (X,z):=\left(\wt{\mathcal Q}_{1.2}(X)-z\right)^{-1} .
\end{equation}
 We denote the ESD $\rho^{(p)}$ of ${\mathcal Q}_{1}$ and its Stieltjes transform as
\be\label{defn_m}
\rho\equiv \rho^{(p)} := \frac{1}{p} \sum_{i=1}^p \delta_{\lambda_i({\mathcal Q}_1)},\quad m(z)\equiv m^{(n)}(z):=\int \frac{1}{x-z}\rho^{(p)}(dx)=\frac{1}{p} \mathrm{Tr} \, \mathcal G_1(z).
\ee
We also introduce the following quantities:
\begin{equation}\label{defn_m1m2}
m_1(z)\equiv m_1^{(n)}(z):= \frac{1}n\tr \left(\mathbf C \mathcal G_1(z)\right) ,\quad m_2(z)\equiv m_2^{(n)}(z):=\frac{1}{n}\tr\left( B\mathcal G_2(z)\right). 
\end{equation}
\end{definition}


%It was shown in \cite{Separable} that if $d_n \to d \in (0,\infty)$ and $\pi_A^{(p)}$, $\pi_B^{(n)}$ converge to certain probability distributions, then almost surely $\rho^{(p)}$ converges to a deterministic distributions $ \rho_{\infty}$. 

We now describe the limiting laws of the density $\rho$ and its Stieltjes transform $m(z)$.
%$$m_{\infty}(z):=\int_{\mathbb R} \frac{\rho_{\infty}(dx)}{x-z}, \quad z \in \mathbb C_+.$$ For any finite $n$, $p=nd_n$ and $z\in \mathbb C_+$, 
We define $(m_{1c}(z),m_{2c}(z))\in \mathbb C_+^2$ as the unique solution to the system of self-consistent equations
\begin{equation}\label{separa_m12}
\begin{split}
& {m_{1c}(z)} = \frac1n\sum_{i = 1}^p \frac{\sigma_i}{-z(1+\sigma_i m_{2c})} = \gamma_n  \int\frac{x}{-z\left[1+xm_{2c}(z) \right]} \pi_{\mathbf C}(dx) ,\\
& {m_{2c}(z)} = \frac{1}{n} \sum_{\mu= 1}^r \frac{\sigma_\mu}{-z(1+\sigma_\mu m_{1c})} = \xi_n \int\frac{x}{-z\left[1+xm_{1c}(z) \right]} \pi_B (dx) .
%= \frac1n\sum_{i=1}^p \frac{\sigma_i^a}{-z\left[1+\sigma_i^a m^{(n)}_{2c}(z) \right]}, \\
%&  = \frac1n\sum_{\mu=1}^n \frac{\sigma_\mu^b}{-z\left[1+\sigma_\mu^b m^{(n)}_{1c}(z) \right]},.
\end{split}
\end{equation}
Then we define
\begin{equation}\label{def_mc}
m_c(z):= \frac{1}{p}\sum_{i = 1}^p \frac{1}{-z(1+\sigma_i m_{2c})}   =\int\frac{1}{-z\left[1+xm_{2c}(z) \right]} \pi_{\mathbf C}(dx).
\end{equation}
It is easy to verify that $m_c(z)\in \mathbb C_+$ for $z\in \mathbb C_+$. Letting $\eta \downarrow 0$, we can obtain a probability measure $\rho_{c}$ with the inverse formula
\begin{equation}\label{ST_inverse}
\rho_{c}(E) = \lim_{\eta\downarrow 0} \frac{1}{\pi}\Im\, m_{c}(E+\ii \eta).
\end{equation}
Suppose the bulk component of $\rho_c(E)$ has right edge at $\lambda_r$, which is also the classical location of the largest eigenvalues of $\cal Q_1$.

%If $d_n \to d \in (0,\infty)$ and $\pi_A^{(p)}$, $\pi_B^{(n)}$ converge to certain probability distributions, then $m_c^{(n)}$ also converges and we have
%$$m_{\infty}(z):=\lim_{n \to \infty} m_c^{(n)}(z), \ \ z \in \mathbb C_+.$$
%Letting $\eta \downarrow 0$, we can recover the asymptotic eigenvalue density $ \rho_{\infty}$ with
%\begin{equation}\label{ST_inverse}
%\rho_{\infty}(E) = \lim_{\eta\downarrow 0} \frac{1}{\pi}\Im\, m_{\infty}(E+\ii \eta).
%\end{equation}
%It is also easy to see that $\rho_\infty$ is the weak limit of $\rho_{c}^{(n)}$. 



We introduce a convenient self-adjoint linearization trick. Define the following $(p+r)\times (p+r)$ self-adjoint block matrix, which is a linear function of $X$:
 \begin{equation}\label{linearize_block}
   H \equiv H(X,z): = z^{1/2} \left( {\begin{array}{*{20}c}
   { 0 } & {Y}   \\
   {Y^*} & {0}  \\
   \end{array}} \right),  \quad z\in \mathbb C_+ .
 \end{equation}
where $Y=\mathbf C^{1/2}X S^T $ and $z^{1/2}$ is taken to be the branch cut with positive imaginary part. Then we define its resolvent (Green's function) as
 \begin{equation}\label{eq_gz} %\label{eqn_defG}
 G \equiv G (X,z):= \left(H(X,z)-z\right)^{-1} .
 \end{equation}
%It is easy to verify that the eigenvalues $\lambda_1(H)\ge \ldots \ge \lambda_{n+N}(H)$ of $H$ are related to the ones of $\mathcal Q_1$ through
%\begin{equation}\label{Heigen}
%\lambda_i(H)=-\lambda_{n+N-i+1}(H)=\sqrt{\lambda_i\left(\mathcal Q_2\right)}, \ \ 1\le i \le n\wedge N, \quad \text{and}\quad \lambda_i(H)=0, \ \ n\wedge N + 1 \le i \le n\vee N.
%\end{equation}
%and
%$$\lambda_i(H)=0, \ \ n\wedge N + 1 \le i \le n\vee N.$$
%where we used the notations $n\wedge N:=\min\{N,M\}$ and $n\vee N:=\max\{N,M\}$. 
By Schur complement formula, we can verify that (recall \eqref{def_green})
\begin{align} 
G(z) = \left( {\begin{array}{*{20}c}
   { \mathcal G_1} & z^{-1/2}\mathcal G_1 Y \\
   {z^{-1/2}Y^* \mathcal G_1} & { \mathcal G_2 }  \\
\end{array}} \right) = \left( {\begin{array}{*{20}c}
   { \mathcal G_1} & z^{-1/2}Y \mathcal G_2   \\
   {z^{-1/2} \mathcal G_2 Y^*} & { \mathcal G_2 }  \\ 
\end{array}} \right). \label{green2} 
\end{align}
%where $\mathcal G_{1,2}$ are defined in (\ref{def_green}). 
Thus a control of $G$ yields directly a control of the resolvents $\mathcal G_{1,2}$. Similarly, we can define $\wt H$ and $\wt G$  by replacing $Y$ with the spiked version $\wt Y$. 
%For simplicity of notations, we define the index sets
%\[\mathcal I_1:=\{1,...,p\}, \quad \mathcal I_2:=\{p+1,...,p+r\}, \quad \mathcal I:=\mathcal I_1\cup\mathcal I_2.\]
In this note, we shall label the indices of the matrices according to 
$$X= (X_{i\mu}), \quad \mathbf C=(A_{ij}),\quad S=(S_{\mu\nu}).$$  
%In the rest of this paper, %whenever referring to the entries of $H$ and $G$, 
%we will consistently use the latin letters $i,j\in\mathcal I_1$ and greek letters $\mu,\nu\in\mathcal I_2$. Note that for the index $1\le \mu \le n$ used in previous sections, it can be translated into an index in $\mathcal I_2$ by taking $\mu \to \mu +p$. 

We define the deterministic limit $\Pi$ of the resolvent $G$ in (\ref{eq_gz}) as
\begin{equation}\label{defn_pi}
\Pi (z):=\left( {\begin{array}{*{20}c}
   { \Pi_1} & 0  \\
   0 & { \Pi_2}  \\
\end{array}} \right), \quad \Pi_1:  =-z^{-1}\left(1+m_{2c}(z)\mathbf C\right)^{-1},\quad \Pi_2:=- z^{-1} (1+m_{1c}(z)B )^{-1}.
\end{equation}
Note that from (\ref{separa_m12}) we have
\be\label{mcPi}
\frac1{n}\tr \Pi_{1} =m_c, \quad  \frac1{n}\tr \left(\mathbf C \Pi_{1}\right) =m_{1c}, \quad \frac1{n}\tr \left(B\Pi_{2}\right) =m_{2c}.
\ee
%\be\label{mcPi}
%\frac1{n}\sum_{i\in \mathcal I_1} \Pi_{ii} =m_c, 
%\ee
%Define the control parameter
%\begin{equation}\label{eq_defpsi}
%\Psi (z):= \sqrt {\frac{\Im \, m_{2c}(z)}{{n\eta }} } + \frac{1}{n\eta}.
%\end{equation}
%Note that by (\ref{eq_estimm}) and (\ref{Piii}), we have
%\begin{equation}\label{psi12}
%\|\Pi\|=\OO(1), \quad \Psi \gtrsim n^{-1/2} , \quad \Psi^2 \lesssim (n\eta)^{-1}, \quad \Psi(z) \sim  \sqrt {\frac{\Im \, m_{1c}(z)}{{n\eta }} } + \frac{1}{n\eta},
%\end{equation}
%for $z\in S(\varsigma_1,\varsigma_2)$. Now we state the local laws for $G(z)$, which are the main tools for our proof. Given any constant $\epsilon>0$, we define the spectral domains
%\be \label{tildeS}
%S_0(\varsigma_1,\varsigma_2,\e):= S(\varsigma_1,\varsigma_2) \cap \left\{z = E+ \ii \eta: \eta\ge n^{-1+\epsilon}\right\},
%\ee
%and
%\be \label{tildeS}
%\wt S(\varsigma_1,\varsigma_2,\e):= S(\varsigma_1,\varsigma_2) \cap \left\{z = E+ \ii \eta: \eta\ge n^{-1+\epsilon}, \ n^{1/2} \Psi^2(z) \le n^{-\e/2}\right\}.
%\ee
%
%
%%For the purpose of proving Theorem \ref{main_thm}, we shall relax the condition \eqref{assm_3rdmoment} a little bit. 
%
%
%\begin{theorem} [Local laws]\label{LEM_SMALL} %[Results on covariance matrices with small support]
%Suppose that Assumptions \ref{assm_big1} and \ref{ass:unper} hold. Fix constants $\varsigma_1 $ and $\varsigma_2>0$ as in Lemma \ref{lem_mplaw}. Then for any fixed $\e>0$, the following estimates hold. 
%\begin{itemize}
%\item[(1)] {\bf Anisotropic local law}: For any $z\in \wt S(\varsigma_1,\varsigma_2,\e)$ and deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb C^{\mathcal I}$,
%\begin{equation}\label{aniso_law}
%\left| \langle \mathbf u, G(X,z) \mathbf v\rangle - \langle \mathbf u, \Pi (z)\mathbf v\rangle \right| \prec \Psi(z).
%\end{equation}
%
%\item[(2)] {\bf Averaged local law}: For any $z \in \wt S(\varsigma_1,\varsigma_2,\e)$,  we have
%\begin{equation}
% \vert m(z)-m_{c}(z) \vert + \vert m_1(z)-m_{1c}(z) \vert  + \vert m_2(z)-m_{2c}(z) \vert \prec  (n \eta)^{-1}, \label{aver_in1} %+ q^2 
%\end{equation}
%where $m$ is defined in \eqref{defn_m} and $m_{1,2}$ are defined in \eqref{defn_m1m2}. Moreover, outside of the spectrum we have the following stronger estimate
%\begin{equation}\label{aver_out1}
%  \vert m(z)-m_{c}(z) \vert + \vert m_1(z)-m_{1c}(z) \vert  + \vert m_2(z)-m_{2c}(z) \prec \frac{1}{n(\kappa +\eta)} + \frac{1}{(n\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%uniformly in $z\in \wt S(\varsigma_1,\varsigma_2,\e)\cap \{z=E+\ii\eta: E\ge \lambda_r, n\eta\sqrt{\kappa + \eta} \ge n^\epsilon\}$, where $\kappa$ is defined in \eqref{KAPPA}. 
%
%\item[(3)] If we have (a) \eqref{assm_3rdmoment} holds, or (b) either $A$ or $B$ is diagonal, then the estimate \eqref{aniso_law}-\eqref{aver_out1} hold for $z\in S_0(\varsigma_1,\varsigma_2,\e)$. 
%\end{itemize}
%The above estimates are uniform in $z$ and any set of deterministic vectors of cardinality $N^{\OO(1)}$. 
%\end{theorem}
We have proved that the following local laws holds away from the support of $\rho_{c}$.


\begin{theorem}[Anisotropic local law outside of the spectrum]\label{lem_localout}  
Fix any small constant $\epsilon>0$ and large constants $C_0>0$. For any 
\begin{equation}\label{eq_paraout}
z\in S_{out}(\e,C_0):=\left\{ E+ \ii\eta: \lambda_r + n^{-2/3+\e}\le E \le C_0,  \eta\in [0,C_0]\right\},
\end{equation}
and any deterministic unit vectors $\bu, \bv \in \mathbb{C}^{p+r}$, we have the anisotropic local law 
\begin{equation}\label{aniso_outstrong}
\left| \langle \mathbf u, G(X,z) \mathbf v\rangle - \langle \mathbf u, \Pi (z)\mathbf v\rangle \right|  \prec n^{-1/2}|z-\lambda_r|^{-1/4} .
\end{equation}
\end{theorem}

Here we have used the following notation of stochastic domination. Let
\[\xi=\left(\xi^{(n)}(u):n\in\bbN, u\in U^{(n)}\right),\hskip 10pt \zeta=\left(\zeta^{(n)}(u):n\in\bbN, u\in U^{(n)}\right)\]
be two families of nonnegative random variables, where $U^{(n)}$ is a possibly $n$-dependent parameter set. We say $\xi$ is stochastically dominated by $\zeta$, uniformly in $u$, if for any fixed (small) $\epsilon>0$ and (large) $D>0$, 
\[\sup_{u\in U^{(n)}}\bbP\left(\xi^{(n)}(u)>n^\epsilon\zeta^{(n)}(u)\right)\le n^{-D}\]
for large enough $n \ge n_0(\epsilon, D)$, and we shall use the notation $\xi\prec\zeta$. 

\subsubsection{The spiked eigenvalues and eigenvectors}

With the anisotropic local law, we can derive the master equation for the outlier eigenvalues and eigenvectors.
We write the signal matrix as
$$\sum_{i=1}^k d_i \bu_i \bv_i^*=UDV^*, \quad D=\text{diag}(d_1,\cdots, d_k), \quad U=(\bu_1,\cdots, \bu_k), \quad V= (\bv_1,\cdots, \bv_k),$$
and the linearized form as
$$ \Delta H:= z^{1/2}\left( {\begin{array}{*{20}c}
   { 0 } & {UDV^*}   \\
   {VDU^*} & {0}  \\
   \end{array}} \right) =z^{1/2} \bU\cal D\bU^*, \quad \bU:=  \left( {\begin{array}{*{20}c}
   { U } & {0}   \\
   {0} & {V}  \\
   \end{array}} \right) , \quad \cal D:= \left( {\begin{array}{*{20}c}
   { 0 } & {D}   \\
   {D} & {0}  \\
   \end{array}} \right) .$$

\begin{lemma}\label{lem_pertubation} 
If $x>\lambda_r$ is not an eigenvalue of $\mathcal Q_1$, then it is an eigenvalue of $\widetilde{Q}_1$ if and only if
\begin{equation}\label{masterx}
\det\left(\mathcal{D}^{-1}+x^{1/2} \mathbf U^* G(x) \mathbf U\right)=0.
\end{equation} 
\end{lemma}
\begin{proof} 
Note that the non-zero eigenvalues of $z^{-1/2}\wt H$ is given by 
$$\pm \sqrt{\lambda_1(\wt Q_1)}, \ \pm \sqrt{\lambda_2(\ctQ_1)}, \ \cdots \ , \ \pm \sqrt{\lambda_{p\wedge n}(\ctQ_1)}.$$
Hence it is easy to see that $x>0$ is an eigenvalue of $\ctQ_1$ if and only if
\be\label{detH1}
\det\left(\wt H(X,x) - x\right)=0,
\ee
from which we obtain that %if $x \ne 0$ is an eigenvalue of $\wt H=PHP$ if and only if    
\begin{align*}
0&=\det(H+\Delta H -x)=\det(H-x) \det\Big(1 + G(x) \Delta H \Big)=\det(H-x) \det(1+ x^{1/2}\bU^* G(x) \bU \cal D)\\
&=\det(\cal D)\det(H-x) \det(\cal D^{-1}+ x^{1/2}\bU^* G(x) \bU ),
\end{align*}
where in the third step we used $\det(1+AB)=\det(1+BA)$. The claim then follows if $x$ is not an eigenvalue of $\cal Q_1$.
\end{proof}

Using the local law Theorem \ref{lem_localout}, up to some small error of order $\OO_\prec(n^{-1/2})$, the equation \eqref{masterx} gives approximately 
\be\label{master_evalue}
\det \left( {\begin{array}{*{20}c}
   {  -x^{-1/2}U^* \left(1+m_{2c}(x)\mathbf C\right)^{-1}U } & {D^{-1}}   \\
   {D^{-1}} & {- x^{-1/2} V^* (1+m_{1c}(x)B )^{-1} V}  \\
   \end{array}} \right) = 0.
\ee
To get some explicit results, we will consider some special cases in next section. Right now we assume that we the outlier $\wt\lambda_i$ lies around a classical location $\theta_i$. We want to study the overlap between the sample eigenvector and the population eigenvector $\bu_i$. 
%Next we introduce the spectral decomposition of $G$. 
Let
$$\wt Y  = \sum_{k = 1}^{p \wedge r} {\sqrt {\wt\lambda_k} \wt{\bm \xi}_k } \wt{\bm\zeta} _{k}^* ,$$
be a singular value decomposition, where
$$\wt\lambda_1\ge \wt\lambda_2 \ge \ldots \ge \wt\lambda_{p\wedge r} \ge 0 = \wt\lambda_{p\wedge r+1} = \ldots = \wt\lambda_{p\vee n}$$
are the eigenvalues of $\ctQ_1,$
$\{\wt{\bm \xi}_{k}\}_{k=1}^{p}$ and $\{\wt{\bm \zeta}_{k}\}_{k=1}^{r}$ are the left and right singular vectors of $\wt Y,$ respectively. 
%orthonormal bases of $\mathbb R^{\mathcal I_1}$ and $\mathbb R^{\mathcal I_2}$, respectively. 
Then using (\ref{green2}) for $\wt G$, we can get that for $i,j\in \mathcal I_1$ and $\mu,\nu\in \mathcal I_2$,
\begin{align}
\wt G_{ij} = \sum_{k = 1}^{p} \frac{\wt{\bm \xi}_k(i) \wt{\bm \xi}_k^*(j)}{\wt\lambda_k-z},\ \quad \ &\wt G_{\mu\nu} = \sum_{k = 1}^{r} \frac{\wt{\bm \zeta}_k(\mu) \wt{\bm \zeta}_k^*(\nu)}{\wt\lambda_k-z}, \label{spectral1}\\
\wt G_{i\mu} = z^{-1/2}\sum_{k = 1}^{p\wedge r} \frac{\sqrt{\wt\lambda_k}\wt{\bm \xi}_k(i) \wt{\bm \zeta}_k^*(\mu)}{\wt\lambda_k-z}, \ \quad \ &\wt G_{\mu i} =  z^{-1/2}\sum_{k = 1}^{p\wedge r} \frac{\sqrt{\wt\lambda_k} \wt{\bm \zeta}_k(\mu) \wt{\bm \xi}_k^*(i)}{\wt\lambda_k-z}.\label{spectral2}
\end{align}
We record the following lemma for matrix perturbation, which follows from a simple algebraic calculation.

\begin{lemma} [Woodbury matrix identity] \label{lem_woodbury} For $\mathcal{A},S,\mathcal{B},T$ of conformable dimensions, we have 
\begin{equation}\label{Woodbury}
(\mathcal A+S\mathcal BT)^{-1}=\mathcal A^{-1}-\mathcal A^{-1}S(\mathcal B^{-1}+T\mathcal A^{-1}S)^{-1}T\mathcal A^{-1}.
\end{equation}
as long as all the operations are legitimate. As a special case, we have the following Hua's identity:
\begin{equation}\label{Huaineq}
\mathcal A-\mathcal A(\mathcal A+\mathcal B)^{-1}\mathcal A=\mathcal B-\mathcal B(\mathcal A+\mathcal B)^{-1}\mathcal B
\end{equation}
if $\mathcal A+\mathcal B$ is non-singular.
\end{lemma}

With \eqref{Woodbury}, we obtain that
\begin{align*}
\bU^* \wt G(z) \bU = \bU^* \frac{1}{H - z + z^{1/2} \bU\cal D\bU^*} \bU =\bU^* \left(G(z) - G(z) \bU \frac{1}{z^{-1/2}\cal D^{-1}  + \bU^*G(z)\bU} \bU^*G(z)\right) \bU
\end{align*}
Our goal is to study $|\langle \mathbf u_j, \wt{\bxi}_i\rangle|^2 $ for some spiked eigenvector $\wt{\bxi}_i$. We consider a small contour $\Gamma_i$ around $\theta_i$, which only encloses $\wt\lambda_i$ but no other eigenvalues. Then using Cauchy's Theorem, we obtain that 
\begin{align} \label{ev_origin}
|\langle \mathbf u_j, \wt{\bxi}_i\rangle|^2 = \frac{-1}{2\pi \ii} \oint_{\Gamma_i}\mathbf e_j^* \bU^* \wt G(z) \bU \mathbf e_j\dd z = \frac{1}{2\pi \ii \sqrt{\wt\lambda_i}} \left( \oint_{\Gamma_i}\mathbf e_j^*\cal D^{-1}\frac{1}{\cal D^{-1}  + z^{1/2}\bU^*G(z)\bU} \cal D^{-1}\mathbf e_j  \dd z\right) .
\end{align}
Again we need to study the matrix $\left(D^{-1}  + z^{1/2}\bU^*G(z)\bU\right)^{-1}$.




 \section{Sketching in PCA}

In this section, we apply the formalism in last section to several special cases. In the following, we assume that $\mathbf C=I$, $\{\bu_i\}$ are orthonormal, and $\bv_i = \bw_i S^T$. Denote
$$W= (\bw_1,\cdots, \bw_k).$$
Then we consider the following choices of $S$.
 
 \subsection{Uniform random projection}

In this section, we consider a general case where $\{\bw_i\}$ are arbitrary orthonormal vectors. We take $S$ to be $r\times n$ partial orthonormal. Then $S S^T=I$ and equation \eqref{separa_m12} becomes
\begin{equation}
\begin{split}
& {m_{1c}(z)} = \gamma_n \frac{1}{-z\left[1+m_{2c}(z) \right]} ,\quad {m_{2c}(z)} =  \xi_n  \frac{1}{-z\left[1+m_{1c}(z) \right]}  .
%= \frac1n\sum_{i=1}^p \frac{\sigma_i^a}{-z\left[1+\sigma_i^a m^{(n)}_{2c}(z) \right]}, \\
%&  = \frac1n\sum_{\mu=1}^n \frac{\sigma_\mu^b}{-z\left[1+\sigma_\mu^b m^{(n)}_{1c}(z) \right]},.
\end{split}
\end{equation}
Then we can obtain the equation for $m_{2c}$:
\begin{equation}\label{g_{2c}}
\begin{split}
& \frac{\xi_n}{m_{2c}(z)} =  -z+ \frac{\gamma_n }{1+m_{2c}(z)}   \Rightarrow zm_{2x}^2 + (z-\gamma_n +\xi_n)m_{2c} + \xi_n =0.
\end{split}
\end{equation}
We can solve $m_{2c}$ as
\be\label{solv m2c}
m_{2c}(z)=\frac{-(z-\gamma_n +\xi_n) + \sqrt{(z-\lambda_+)(z-\lambda_-)} }{2z}, \quad \lambda_{\pm}= (\sqrt{\gamma_n} \pm \sqrt{\xi_n})^2,
\ee
and the inverse function $g_{2c}$ of $m_{2c}$:
\be\label{solv g2c}
g_{2c}(m)= \frac{\gamma_n }{1+m} -  \frac{\xi_n}{m}.
\ee

We can write $W=\mathbb W \begin{pmatrix}I_k \\ 0 \end{pmatrix}$, where $\mathbf W$ is an $n\times n$ orthogonal matrix. Since the distribution of $S$ is rotational invariant, we have 
$$V^* V= \begin{pmatrix}I_k & 0 \end{pmatrix} \hat S^* \hat S \begin{pmatrix}I_k \\ 0 \end{pmatrix} ,$$
where $\hat V= S W$ is also a uniformly distributed partial orthogonal matrix. By calculating the moments, one can see that $V^* V = \xi_n I_k + \oo(1)$ with high probability (\cor FY: I am not familiar with Haar distributed random matrices, but I believe this is true\nc). Thus we have
$$- x^{-1/2} V^* (1+m_{1c}(x)B )^{-1} V \approx - x^{-1/2} \frac{\xi_n}{1+m_{1c}} = x^{1/2}m_{2c}(x).$$
%Since $\bw_i$ are Gaussian vectors, we have that $\bv_i$ are also independent $r$-dimensional Gaussian vectors with $i.i.d.$ entries with mean zero and variance $n^{-1}$. Then by CLT, $V^\star V$ is roughly a $k\times k$ scalar matrix $\xi_n I$ up to some small error of order $n^{-1/2}$. 
Now equation \eqref{master_evalue} becomes
\be\label{master_evalue1}
\det \left( {\begin{array}{*{20}c}
   {  -x^{-1/2}\left(1+m_{2c}(x)\right)^{-1} } & {D^{-1}}   \\
   {D^{-1}} & {x^{1/2} m_{2c}(x)}  \\
   \end{array}} \right) = 0.
\ee
The matrix in the above equation consists of the following $2\times 2$ blocks along the diagonal:
$$ \det\begin{pmatrix} -x^{-1/2}\left(1+m_{2c}(x)\right)^{-1}  & d_i^{-1} \\  d_i^{-1} & x^{1/2} m_{2c}(x)\end{pmatrix} =0 \Rightarrow m_{2c}(x) = -\frac{1}{1+d_i^2} .$$
In order to have an outlier, we need to have that 
$$ -\frac{1}{1+d_i^2} > m_{2c}(\lambda_+) = -\frac{\sqrt{\xi_n}}{\sqrt{\xi_n} + \sqrt{\gamma_n}} \Rightarrow d_i^2 > \sqrt{\frac{\gamma_n}{\xi_n}} .$$
Using \eqref{solv g2c}, we obtain that the classical location for the outlier caused by $d_i$ is
$$ \theta_i = \left( 1+d_i^2\right) \left(\frac{\gamma_n}{d_i^2} + \xi_n \right). $$
Then using \eqref{ev_origin}, we obtain that $|\langle \mathbf u_j, \wt{\bxi}_i\rangle|^2 =\oo(1)$ if $j\ne i$. If $j=i$, we have 
\begin{align*} 
|\langle \mathbf u_i, \wt{\bxi}_i\rangle|^2 &\approx \frac{1}{2\pi \ii \sqrt{\theta_i}} \oint_{\Gamma_i}(0,d_i^{-1})\left( {\begin{array}{*{20}c}
   {  -z^{-1/2}\left(1+m_{2c}(z)\right)^{-1} } & {D^{-1}}   \\
   {D^{-1}} & {z^{1/2} m_{2c}(z)}  \\
   \end{array}} \right)^{-1}  \begin{pmatrix} 0  \\ d_i^{-1}\end{pmatrix} \dd z \\
 &=  \frac{1}{2\pi \ii \theta_i \left(1+d_i^2\right)} \oint_{\Gamma_i} \frac1{  {m_{2c}(z)}  + (1+d_i^{2})^{-1}}\dd z = \frac{1}{\theta_i \left(1+d_i^2\right) m_{2c}'(\theta_i)}    \\
 &= \frac{g_{2c}'(-(1+d_i^2)^{-1})}{\theta_i \left(1+d_i^2\right) }  = \frac{\xi_n - \frac{\gamma_n}{d_i^4}}{\xi_n + \frac{\gamma_n}{d_i^2}}.
\end{align*}

\subsection{Uniform random sampling}
We take $S$ to be an $n\times n$ diagonal matrix, with $i.i.d.$ $Bernoulli(r/n)$ diagonal entries. In this section, we define the random variable 
$$\hat \xi_n := \frac{1}{n}\sum_{i=1}^n S_{ii}$$
to be the portion of non-zero diagonal entries of $S$. But we know that for large $r$ and $n$, $\hat\xi_n$ is concentrated around $\xi_n$. We fix a realization of $S$. Then equation \eqref{separa_m12} becomes
\begin{equation}
\begin{split}
& {m_{1c}(z)} = \gamma_n \frac{1}{-z\left[1+m_{2c}(z) \right]} ,\quad {m_{2c}(z)} =  \hat \xi_n  \frac{1}{-z\left[1+m_{1c}(z) \right]}  .
%= \frac1n\sum_{i=1}^p \frac{\sigma_i^a}{-z\left[1+\sigma_i^a m^{(n)}_{2c}(z) \right]}, \\
%&  = \frac1n\sum_{\mu=1}^n \frac{\sigma_\mu^b}{-z\left[1+\sigma_\mu^b m^{(n)}_{1c}(z) \right]},.
\end{split}
\end{equation}
Then \eqref{solv m2c} and \eqref{solv g2c} hold since $\xi_n$ concentrates around $\hat \xi_n$.

We can calculate that 
$$- x^{-1/2} W^* S^T (1+m_{1c}(x)SS^T )^{-1} SW =\frac{- 1}{x^{1/2}(1+m_{1c}(x))} W^* SW .$$
To further simplify the expression, we assume that the entries of $W$ are independent with mean zero and variance $n^{-1}$. In this case, we have $W^* SW=\xi_n I_k +\oo(1)$ with high probability. Then it is easy to see that the results in this case should be the same as the ones in the Haar case.

\subsection{Gaussian projection}

Now we pick $S$ to be a $r\times n$ Gaussian random matrix with $i.i.d.$ normal entries with mean zero and variance $n^{-1}$. Then \eqref{separa_m12} gives
$${m_{1c}(z)} = \gamma_n \frac{1}{-z\left[1+m_{2c}(z) \right]} $$
and
\begin{equation} \nonumber
\begin{split}
 {m_{2c}(z)} & =  \xi_n \int\frac{x}{-z\left[1+xm_{1c}(z) \right]} \pi_B (dx) = \frac{\xi_n}{-zm_{1c}(z)}\left( 1- \frac{1}{m_{1c}(z)} \int\frac{1}{x + m_{1c}^{-1}(z)} \pi_B (dx) \right) \\
&=\frac{\xi_n}{-zm_{1c}(z)}\left( 1- \frac{1}{m_{1c}(z)}m^{S}_{1c}(-m_{1c}^{-1}(z)) \right).
%= \frac1n\sum_{i=1}^p \frac{\sigma_i^a}{-z\left[1+\sigma_i^a m^{(n)}_{2c}(z) \right]}, \\
%&  = \frac1n\sum_{\mu=1}^n \frac{\sigma_\mu^b}{-z\left[1+\sigma_\mu^b m^{(n)}_{1c}(z) \right]},.
\end{split}
\end{equation}
Then we obtain the following self-consistent equation satisfying by $m_{1c}$:
\begin{equation} \nonumber
\begin{split}
 \frac{\gamma_n}{m_{1c}(z)} &= -z + \frac{\xi_n}{m_{1c}(z)}\left( 1- \frac{1}{m_{1c}(z)}m^{S}_{1c}(-m_{1c}^{-1}(z)) \right).
%= \frac1n\sum_{i=1}^p \frac{\sigma_i^a}{-z\left[1+\sigma_i^a m^{(n)}_{2c}(z) \right]}, \\
%&  = \frac1n\sum_{\mu=1}^n \frac{\sigma_\mu^b}{-z\left[1+\sigma_\mu^b m^{(n)}_{1c}(z) \right]},.
\end{split}
\end{equation}
Now it is easy to obtain the inverse functions $g_{1c}$ of $m_{1c}$:
\be\label{solv g1c}
g_{1c}(m)=- \frac{\gamma_n}{m}   + \frac{\xi_n}{m}\left( 1- \frac{1}{m}m^{S}_{1c}(-m^{-1}) \right).
\ee
Here $m_{1c}^S$ is the limiting Stieltjes transform of $SS^T$, which is determined by the following self-consistent equations:
\begin{equation}
\begin{split}
& m_{1c}^S(z) =  \frac{1}{-z\left[1+m_{2c}^S(z) \right]} ,\quad m_{2c}^S(z) =   \frac{1}{-z\left[1+\xi_n m_{1c}^S(z) \right]}  .
%= \frac1n\sum_{i=1}^p \frac{\sigma_i^a}{-z\left[1+\sigma_i^a m^{(n)}_{2c}(z) \right]}, \\
%&  = \frac1n\sum_{\mu=1}^n \frac{\sigma_\mu^b}{-z\left[1+\sigma_\mu^b m^{(n)}_{1c}(z) \right]},.
\end{split}
\end{equation}
Denoting the inverse functions of $m_{1,2c}^S$ by $g_{1,2c}^S$, then we obtain from the above equations that 
$$g_{1c}^S(m)= \frac{1}{1+\xi_n m}-\frac{1}{m}, \quad g_{2c}^S(m)= \frac{\xi_n}{1+m}-\frac{1}{m}.$$
Using this function, we can obtain that $m_{1c}$ satisfy the following cubic equation:
\begin{equation} \nonumber
\begin{split}
& -\frac{1}{m_{1c}} =g_{1c}^S\left(-\frac{\gamma_n - \xi_n }{\xi_n} m_{1c}- \frac{z}{\xi_n}m_{1c}^2\right) = \frac{1}{1-(\gamma_n - \xi_n)m_{1c} - zm_{1c}^2 } + \frac{\xi_n}{(\gamma_n - \xi_n)m_{1c}+zm_{1c}^2} \\
&\Rightarrow \frac{m_{1c}}{1-(\gamma_n - \xi_n)m_{1c} - zm_{1c}^2 } + \frac{\xi_n}{(\gamma_n - \xi_n)+zm_{1c}}  + 1=0 \\
&\Rightarrow z^2m_{1c}^3 - z(1+\xi_n-2\gamma_n)m_{1c}^2 - \left( z + (1-\gamma_n)(\gamma_n-\xi_n)\right) m_{1c} - \gamma_n =0.
\end{split}
\end{equation}

To study the spiked eigenvalues and eigenvectors, we again need to study
\be\label{needto}
- x^{-1/2} V^* (1+m_{1c}(x)B )^{-1} V = - x^{-1/2} W^* S^T\frac{1}{1+m_{1c}(x)SS^T} SW.
\ee
With \eqref{Woodbury}, we obtain that
$$\eqref{needto} = - \frac{1}{x^{1/2}m_{1c}} W^*\left( {1} - \frac{1}{1 +m_{1c} S^T S}\right)W . $$
Using the isotropic local law for $S^T S$, we obtain that with high probability,
$$\eqref{needto} = - \frac{1}{x^{1/2}m_{1c}} \left( {1} - \frac{1}{m_{1c}}m_{2c}^S (-m_{1c}^{-1})\right) +\oo(1) . $$

Now equation \eqref{master_evalue} becomes
\be\label{master_evalue2}
\det \left( {\begin{array}{*{20}c}
   {  x^{1/2}\gamma_n^{-1} m_{1c}} & {D^{-1}}   \\
   {D^{-1}} & { - \frac{1}{x^{1/2}m_{1c}} \left( {1} - \frac{1}{m_{1c}}m_{2c}^S (-m_{1c}^{-1})\right)}  \\
   \end{array}} \right) = 0,
\ee
which gives the equations
\begin{align*}
-\gamma_n^{-1}\left( {1} - \frac{1}{m_{1c}}m_{2c}^S (-m_{1c}^{-1})\right)=d_i^{-2} & \Rightarrow -m_{1c}^{-1}(x) = g_{2c}^S\left( (1+\gamma_n d_i^{-2})m_{1c}\right) \\
&\Rightarrow m_{1c}(x)=-\frac{\gamma_n d_i^{-2}}{\left(1+\gamma_n d_i^{-2}\right)\left(\xi_n+\gamma_n d_i^{-2}\right)} .
\end{align*}
In order to have an outlier, we need to have that 
$$ -\frac{\gamma_n d_i^{-2}}{\left(1+\gamma_n d_i^{-2}\right)\left(\xi_n+\gamma_n d_i^{-2}\right)}> m_{1c}(\lambda_+) .$$
Using \eqref{solv g1c}, we can obtain that the classical location $\theta_i$ for the outlier caused by $d_i$ (which will be explicit, but we do not write it down right now).
%$$ \theta_i = \left( 1+d_i^2\right) \left(\frac{\gamma_n}{d_i^2} + \xi_n \right). $$
Finally using \eqref{ev_origin}, we obtain that $|\langle \mathbf u_j, \wt{\bxi}_i\rangle|^2 =\oo(1)$ if $j\ne i$. If $j=i$, we have 
\be\nonumber
\begin{split}
|\langle \mathbf u_i, \wt{\bxi}_i\rangle|^2 &\approx \frac{1}{2\pi \ii \sqrt{\theta_i}d_i^{2}} \oint_{\Gamma_i}\frac{ z^{1/2}\gamma_n^{-1} m_{1c}(z)}{-\gamma_n^{-1}\left( {1} - \frac{1}{m_{1c}(z)}m_{2c}^S (-m_{1c}^{-1}(z))\right)-d_i^{-2}} \dd z \\
&=- \frac{m_{1c}^2(\theta_i)}{2\pi \ii d_i^{2}} \oint_{\Gamma_i}\frac{ 1}{(1+\gamma_n d_i^{-2})m_{1c}(z) - m_{2c}^S (-m_{1c}^{-1}(z))} \dd z \\
&=- \frac{m_{1c}^2(\theta_i)}{2\pi \ii d_i^{2}} \oint_{g_{1c}(\Gamma_i)}\frac{g_{1c}'(\zeta)}{(1+\gamma_n d_i^{-2})\zeta - m_{2c}^S (-\zeta^{-1})} \dd \zeta \\
&= \frac{\al_i^2}{d_i^{2}} \frac{g_{1c}'(\al_i)}{\al_i^{-2}(m_{2c}^S)' (-\al_i^{-1})-(1+\gamma_n d_i^{-2}) },
\end{split}
\ee
where
$$\al_i:=m_{1c}(\theta_i)=-\frac{\gamma_n d_i^{-2}}{\left(1+\gamma_n d_i^{-2}\right)\left(\xi_n+\gamma_n d_i^{-2}\right)} .$$
Again the expression can be made explicit, but we ignore that right now.

\subsection{Optimal singular value shrinkage}
Denoting $c_i^2:=|\langle \mathbf u_i, \wt{\bxi}_i\rangle|^2$. We can study the optimal shrinkage for $\wt Q_1$. 
We can use the Frobenius norm for example. \cor continue \nc
%{\cor We can use the above results to study the optimal singular value shrinkage.}


\section{Another spiked model: deterministic data}


%-----------------------------------------------------------------
\bibliographystyle{abbrv}
\bibliographystyle{plain}
\bibliography{references}



\end{document}
