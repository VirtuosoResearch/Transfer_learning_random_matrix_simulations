\section{Results for Two Tasks}\label{sec_defspike}


We would like to get insight on how covariate and model shifts result in slower rates of transfer.
We denote task 2 as the target, i.e. $\beta_2 = \beta_t$.
We denote task 1 as the source, i.e. $\beta_1 = \beta_s$.

\subsection{Same Model $\beta$ with Covariate Shift}

First, we show that if $\beta_s$ and $\beta_t$ are equal, then combining the source and target task always helps.
The key quantity is to look at:
\begin{align}\label{eq_two_task}
  \tr[(X_1^{\top}X_1 + X_2^{\top} X_2)^{-1}],
\end{align}
and compare it to
\begin{align}\label{eq_target_task}
	\tr[(X_2^{\top} X_2)^{-1}].
\end{align}

\begin{proposition}
  Show that adding tasks always helps or reduces estimation error.
\end{proposition}

\begin{proof}
	\todo{straightforward calculation}
\end{proof}

Next, we calculate the amount of improvement by comparing equation \eqref{eq_two_task} to equation \eqref{eq_target_task}.
We study the spectrum of the random matrix model: 
$$Q= \Sigma_1^{1/2}  X_1^T X_1 \Sigma_1^{1/2}  + \Sigma_2^{1/2}  X_2^T X_2 \Sigma_2^{1/2} ,$$
where $\Sigma_{1,2}$ are $p\times p$ deterministic covariance matrices, and $X_1=(x_{ij})_{1\le i \le n_1, 1\le j \le p}$ and $X_2=(x_{ij})_{n_1+1\le i \le n_1+n_2, 1\le j \le p}$ are $n_1\times p$ and $n_2 \times p$ random matrices, respectively, where the entries $x_{ij}$, $1 \leq i \leq n_1+n_2\equiv n$, $1 \leq j \leq p$, are real independent random variables satisfying
\begin{equation}\label{eq_12moment} %\label{assm1}
\mathbb{E} x_{ij} =0, \ \quad \ \mathbb{E} \vert x_{ij} \vert^2  = n^{-1}.
\end{equation} 
For now, we assume that the random variables $x_{ij}$ are i.i.d. Gaussian, but we know that universality holds for generally distributed entries. %have arbitrarily high moments, 
%in the sense that for any fixed $k\in \mathbb N$, there is a constant $\mu_k>0$ such that
%\begin{equation}\label{eq_highmoment} %\label{eqn:subgaus}
%\max_{i,j}\left(\mathbb E|x_{ij}|^k\right)^{1/k} \le \mu_k n^{-1/2},  %\var \left(h_{xy}\right)^{1/2}
%\end{equation}
%for all $n$. %For simplicity, we assume that $k$ is a finite fixed integer, the strengths $d_1 > d_2 > \cdots > d_k >0$ are fixed constants, and $ \bu_i$,  $\bv_i$ are deterministic unit vectors. 
We shall consider the high-dimensional setting such that
$$\gamma_n:= \frac{p} {n} \to \gamma,\quad c_n:= \frac{n_1}{n} \to c, \quad \text{as } \ n\to \infty, $$ 
for some constants $\gamma\in (0,\infty)$ and $c \in (0,1)$. 

We assume that $ \Sigma_1^{-1/2}\Sigma_2$ has eigendecomposition
\be \nonumber %\label{eigen}
\Sigma_1^{-1/2}\Sigma_2^{1/2} = ODO^T ,\quad D=\text{diag}(d_1, \cdots, d_p).
\ee
Then by the rotational invariance of Gaussian matrices, we have
$$\wt Q \overset{d}{=}\Sigma_1^{1/2} O \wt Q O^T \Sigma_1^{1/2},\quad \wt Q:=   X_1^T X_1  + D X_2^T X_2 D .$$
Thus we study the spectrum of $\wt Q$ instead. We define $\cal G(z):= (\wt Q-z)^{-1}$ for $z\in \C_+$. With some random matrix tools, we have that 
$$\cal G(z) \approx \diag\left( \frac{1}{-z\left( 1+ m_3(z) + d_i^2 m_4(z)\right)}\right)_{1\le i \le p}= \frac{1}{-z\left( 1+ m_3(z) + D^2 m_4(z)\right)} $$
{\cob in certain sense}. Here $m_{3,4}(z)$ satisfy the following self-consistent equations
%$$\frac{1}{G_{ii}} \approx -z \left( 1+m_3 + d_i^2m_4 \right), \quad \frac{1}{G_{\mu\mu}} = -z(1+m_1), \ \ \mu\in \cal I_1,\quad \frac{1}{G_{\nu\nu}} = -z(1+m_2), \ \ \nu\in \cal I_2,$$
%$$m_1= \frac1n\sum_{i}G_{ii}, \quad m_2= \frac1n\sum_{i}d_i^2 G_{ii}, \quad m_3 = \frac1n\sum_{\mu\in \cal I_1} G_{\mu\mu},\quad m_4 = \frac1n\sum_{\mu\in \cal I_2} G_{\mu\mu}.$$
\begin{align}\label{m34}
\frac{n_1}{n}\frac1{m_3} = - z +\frac1n\sum_{i=1}^p \frac1{  1+m_3 + d_i^2m_4  } ,\quad \frac{n_2}{n}\frac1{m_4} = - z +\frac1n\sum_{i=1}^p \frac{d_i^2 }{  1+m_3 + d_i^2m_4  } 
\end{align}
When $z\to 0$, we shall have
$$m_3(z)= - \frac{a_3}{z} + \OO(1), \quad m_4(z)= - \frac{a_4}{z} + \OO(1),\quad a_3,a_4>0.$$
Then for $z\to0$, the equations in \eqref{m34} are reduced to
\begin{align}\label{m35}
\frac{n_1}{n}\frac{1}{a_3} = 1 +\frac1n\sum_{i=1}^p \frac{1}{a_3 + d_i^2a_4  } ,\quad \frac{n_2}{n}\frac1{a_4} = 1 +\frac1n\sum_{i=1}^p \frac{d_i^2 }{  a_3 + d_i^2 a_4 }. 
\end{align}
First, it is easy to see that these equations are equivalent to
$$a_3 + a_4 = 1- \gamma_n, \quad a_3 +\frac1n\sum_{i=1}^p \frac{a_3}{a_3 + d_i^2[(1-\gamma_n)-a_3]  }=c_n  .$$
Furthermore, we have
\begin{align*}
\tr (Q^{-1}) &= \lim_{z\to 0}\tr \left[\Sigma_1^{-1/2} O \cal G(z)O^T \Sigma_1^{-1/2}\right]
=\tr \left[\Sigma_1^{-1/2} O  \left( \frac{1}{a_3 + D^2 a_4 }\right) O^T \Sigma_1^{-1/2}\right] \\
&=\tr \left[\Sigma_1^{-1/2}  \frac{1}{a_3+ \Sigma_1^{-1}\Sigma_2 a_4} \Sigma_1^{-1/2}\right]=\tr \left[ \frac{1}{a_3\Sigma_1 + a_4\Sigma_2 } \right].
\end{align*}

\subsection{Model Shift}

In this case, $\beta_s$ and $\beta_t$ are different.
If we put the two tasks together, we get
\begin{align}
	\hat{\beta}_{\MTL} &= (X_1^{\top}X_1 + X_2^{\top}X_2)^{-1} (X_1^{\top}Y_1 + X_2^{\top}Y_2) \\
	&= (X_1^{\top}X_1 + X_2^{\top}X_2)^{-1} \bigbrace{(X_1^{\top}X_1\beta_s + X_2^{\top}X_2\beta_t) + (X_1^{\top}\varepsilon_1 + X_2^{\top}\varepsilon_2)}
\end{align}
Hence
\begin{align}
	\exarg{\varepsilon}{\bignorm{\hat{\beta}_{\MTL} - \beta_t}}
	= \bignorm{(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1} X_1^{\top}X_1 (\beta_s - \beta_t)}^2
	+ \sigma^2 \bigtr{(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}}
\end{align}
