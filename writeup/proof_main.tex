\subsection{Proof of Two Tasks with General Covariance}\label{app_proof_main}

%To illustrate the idea, we observe that by using Lemma \ref{lem_minv}, we have that
%\[ \te(\hat{\beta}_t^{\STL}) = \frac{\sigma^2}{n_2 - p}\bigtr{\Sigma_2^{-1}}. \]
%We shall also derive the limit of $\te(\hat{\beta}_t^{\MTL})$.
%\todo{write a brief technical overview}

In this section, we state and prove the formal version of Theorem \ref{thm_main_informal}. First, we need to introduce several quantities that will be used in our statement.

Given the optimal ratio $\hat v$, let $\hat{M} = \hat{v} \Sigma_1^{1/2}\Sigma_2^{-1/2}$ denote the weighted covariate shift matrix. Denote by ${\hat\lambda}_1\ge {\hat\lambda}_2 \ge \dots \ge {\hat\lambda}_p$ the eigenvalues of $\hat{M}^{\top}\hat{M}$. Let $(\hat a_1, \hat a_2)$ be the solutions to the following deterministic equations
	\be
		 \hat a_1 +  \hat a_2 = 1- \frac{1}{\rho_1 + \rho_2},\quad  \hat a_1 + \frac1{\rho_1 + \rho_2}\cdot \frac1p\sum_{i=1}^p \frac{ \hat \lambda_i^2 \hat a_1}{ \hat \lambda_i^2\hat a_1 +  \hat a_2} = \frac{\rho_1}{\rho_1 + \rho_2}.\label{eq_a2} \\
		 \ee
		 After obtaining $(\hat a_1,\hat a_2)$, we can solve the following linear equations to get $(\hat a_3,\hat a_4)$:
\begin{gather}
		\left(\rho_2 \hat a_2^{-2}- \hat b_0\right)\cdot \hat  a_3 - \hat b_1 \cdot \hat a_4
		=\hat b_0, \quad \left(\rho_1\hat a_1^{-2} - \hat b_2  \right)\cdot \hat a_4 - \hat b_1 \cdot \hat a_3 =\hat b_1 .\label{eq_a3} 
%		\left(\frac{n_1}{\hat a_1^2} -  \sum_{i=1}^p \frac{\hat \lambda_i^4   }{  (\hat a_2 + \hat \lambda_i^2\hat a_1)^2  }\right)\hat a_4 -\left(\sum_{i=1}^p \frac{\hat \lambda_i^2  }{  (\hat a_2 + \hat \lambda_i^2\hat a_1)^2  }\right)\hat a_3
%		= \sum_{i=1}^p \frac{\hat \lambda_i^2 }{  (\hat a_2 + \hat \lambda_i^2\hat a_1)^2  }. \label{eq_a4}
	\end{gather}
where we denoted
$$\hat b_k:= \frac1{p}\sum_{i=1}^p \frac{\hat \lambda_i^{2k}}{ (\hat a_2 +\hat  \lambda_i^2\hat a_1)^2  },\quad k=0,1,2.$$
Then we introduce the following matrix
$$\Pi = \frac{\rho_1^2}{(\rho_1 + \rho_2)^2}\cdot{\hat M} \frac{(1 + \hat a_3)\id +\hat  a_4 \hat {M}^{\top}\hat {M}}{(\hat a_2 +\hat  a_1 \hat {M}^{\top}\hat {M})^2} \hat {M}^{\top},$$
which is defined in a way such that {\it in certain sense} it is the asymptotic limit of the random matrix 
$$\hat v\Sigma_1^{1/2} (\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1} \Sigma_2 (\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_1^{1/2}.$$ 
We introduce two factors that will appear often in our statements and discussions:
$$\al_-:=(1- \rho_1^{-1/2})^2,\quad \al_+:=(1 + \rho_1^{-1/2})^2.$$ 
In fact, $\al_-^2$ and $\al_+^2$ correspond to the largest and smallest singular values of $Z_1/\sqrt{n_1}$, respectively, as given by the famous Mar{\v c}enko-Pastur law \cite{MP}. In particular, as $\rho_1$ increases, both $\al_-$ and $\al_+$ will converge to 1 and $Z_1/\sqrt{n_1}$ will be more close to an isometry. Finally, we introduce the error term  
$$\delta:=\frac{\al_+^2 - 1 }{\al_-^{2}} \cdot \kappa^4(\hat M)\cdot \norm{\Sigma_1^{1/2}(\beta_s - \hat{w}\beta_t)}^2,$$
where we recall that $\kappa(\hat M)$ is the condition number of $\hat M$. Note that this factor converges to 0 as $\rho_1\to \infty$.

%$$\delta:=\left[\frac{n_1 \lambda_1}{(\sqrt{n_1}-\sqrt{p})^2\lambda_p  +  (\sqrt{n_2}-\sqrt{p})^2}\right]^2\cdot \norm{\Sigma_1^{1/2}(\beta_s - \hat{w}\beta_t)}^2.$$
%{\cor may be able to get a better bound, but the statement will be long}

Now we are ready to state our main result for two tasks with both covariate and model shift. It shows that the information transfer is solely determined by two deterministic quantities $\Delta_{\beta}$ and $\Delta_{\vari}$, which give the change of model shift bias and the change of variance, respectively.



\begin{theorem}\label{thm_model_shift}
For $i=1,2$, let $Y_i = X_i\beta_i + \varepsilon_i$ be two independent data models, where $X_i=Z_i\Sigma_i^{1/2}\in \R^{n_i\times p}$ satisfy Assumption \ref{assm_secA1} with $\rho_i:=n_i/p>1$ being fixed constants, and $\e_i\in \R^{n_i}$ are  random vectors with i.i.d. entries with mean zero, variance $\sigma^2$ and all moments as in \eqref{assmAhigh}.  
	Then with high probability, we have
	\begin{align}
	 	\te(\hat{\beta}_{t}^{\MTL}) \le \te(\hat{\beta}_t^{\STL}) \quad \text{ when: } \ \ &\Delta_{\vari} - \Delta_{\beta} \ge  \delta \label{upper}\\
		\te(\hat{\beta}_t^{\MTL}) \ge \te(\hat{\beta}_t^{\STL}) \quad \text{ when: } \ \ &\Delta_{\vari} - \Delta_{\beta} \le -\delta, \label{lower}
	\end{align}
	where
	\begin{align*} %\bigtr{{\Sigma_2^{-1}}}
		\Delta_{\vari} &\define {\sigma^2}\bigbrace{\frac{1}{\rho_2 - 1} -  \frac{1}{\rho_1 + \rho_2}\cdot \frac1p \bigtr{(\hat a_1 \hat M^{\top}\hat M + \hat a_2\id)^{-1}} } \\
		\Delta_{\beta} &\define (\beta_s - \hat{v}\beta_t)^{\top} \Sigma_1^{1/2} \Pi \Sigma_1^{1/2} (\beta_s - \hat{v}\beta_t).
	\end{align*}
\end{theorem}

Under the simplifying setting in Section \ref{sec_insight}, we actually have an easier and sharper bound than Theorem \ref{thm_model_shift} as follows. 
\begin{lemma}\label{prop_model_shift_tight}
		In the setting of Theorem \ref{thm_model_shift}, assume that $\Sigma_1 =\id$,
		$\beta_t$ is a random vector with i.i.d. entries with mean $0$, variance $\kappa^2$ and all moments, and $\beta_2$ is a random vector such that $\beta_s - \beta_t$ is a random vector with i.i.d. entries with mean $0$, variance $d^2$ and all moments. Denote
		$\wt \Delta_{\beta} := \bigbrace{(1 - \hat{v})^2 \kappa^2 + d^2)} \bigtr{\Pi}.$
	Then we have
		\begin{align*}
			\te(\hat{\beta}_t^{\MTL}) \le \te(\hat{\beta}_t^{\STL})\quad \text{ when: }\ \  & \Delta_{\vari} \ge  (\al_+^2+\oo(1))\cdot \wt \Delta_{\beta}, \\
			\te(\hat{\beta}_t^{\MTL}) \ge \te(\hat{\beta}_t^{\STL})\quad \text{ when: }\ \  & \Delta_{\vari} \le (\al_-^2-\oo(1))\cdot \wt\Delta_{\beta}.
		\end{align*}
\end{lemma}

Now we first give a proof of Theorem \ref{thm_model_shift} based on Lemma \ref{lem_cov_shift}. 

\begin{proof}[Proof of Theorem \ref{thm_model_shift}]
%\noindent
%To prove Theorem \ref{thm_cov_shift}, we study the spectrum of the random matrix model:
%$$Q= \Sigma_1^{1/2}  Z_1^{\top} Z_1 \Sigma_1^{1/2}  + \Sigma_2^{1/2}  Z_2^{\top} Z_2 \Sigma_2^{1/2} ,$$
%where $\Sigma_{1,2}$ are $p\times p$ deterministic covariance matrices, and $X_1=(x_{ij})_{1\le i \le n_1, 1\le j \le p}$ and $X_2=(x_{ij})_{n_1+1\le i \le n_1+n_2, 1\le j \le p}$ are $n_1\times p$ and $n_2 \times p$ random matrices, respectively, where the entries $x_{ij}$, $1 \leq i \leq n_1+n_2\equiv n$, $1 \leq j \leq p$, are real independent random variables satisfying
%\begin{equation}\label{eq_12moment} %\label{assm1}
%\mathbb{E} z_{ij} =0, \ \quad \ \mathbb{E} \vert z_{ij} \vert^2  = 1.
%\end{equation}
\todo{A proof outline; including the following key lemma.}

The proof is divided into four parts.

\paragraph{Part I: Bounding the bias from model shift.}
We relate the first term in equation \eqref{eq_te_model_shift} to $\Delta_{\beta}$.
\begin{proposition}\label{prop_model_shift}
	In the setting of Theorem \ref{thm_model_shift},
	denote by $K = (\hat{w}^2X_1^{\top}X_1 + X_2^{\top}X_1)^{-1}$, and
	\begin{align*}
		\delta_1 &= \hat{w}^2 \bignorm{\Sigma_2^{1/2} K X_1^{\top}X_1(\beta_s - \hat{w}\beta_t)}^2, \\
		\delta_2 &= n_1^2\cdot \hat{w}^2 \bignorm{\Sigma_2^{1/2}K\Sigma_1(\beta_s - \hat{w}\beta_t)}, \\
		\delta_3 &= n_1^2\cdot \hat{w}^2 \bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \cdot \bignorm{\Sigma_1^{1/2} (\beta_s - \hat{w}\beta_t)}^2.
	\end{align*}
	We have that
	\begin{align*}
		-2n_1^2\bigbrace{{2\sqrt{\frac{p}{n_1}}} + {\frac{p}{n_1}}} \delta_3
		\le  \delta_1 - \delta_2
		\le n_1^2\bigbrace{2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}}\bigbrace{2 + 2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}}\delta_3.
	\end{align*}
	For the special case when $\Sigma_1 = \id$ and $\beta_s - \beta_t$ is i.i.d. with mean $0$ and variance $d^2$, we further have
	\begin{align*}
		\bigbrace{1 - \sqrt{\frac{p}{n_1}}}^4 \Delta_{\beta}
		\le \bignorm{\Sigma_2^{1/2} (X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}X_1^{\top}X_1(\beta_s - \beta_t)}^2.
	\end{align*}
\end{proposition}

\begin{proof}
	The proof follows by applying equation \eqref{eq_isometric}.
	Recall that $X_1^{\top}X_1 = \Sigma_1^{1/2}Z_1^{\top}Z_1\Sigma_1^{1/2}$.
	Denote by $\cE = Z_1^{\top}Z_1 - {n_1}\id$.
	Let
%	Let $\alpha = \bignorm{\Sigma_2^{1/2} K \Sigma_1 (\beta_s - \hat{w}\beta_t)}^2$.
	We have
	\begin{align}
%		& \bignorm{\Sigma_2^{1/2}(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}X_1^{\top}X_1(\beta_s - \hat{w}\beta_t)}^2 \nonumber \\
		\delta_1 = \delta_2 + {2\hat{w}^2n_1}(\beta_s - \hat{w}\beta_t)^{\top}\Sigma_1^{1/2}\cE\Sigma_1^{1/2}K \Sigma_2 K \Sigma_1 (\beta_s - \hat{w}\beta_t)
		+ \hat{w}^2\bignorm{\Sigma_2^{1/2} K \Sigma_1^{1/2}\cE \Sigma_1^{1/2}(\beta_s - \hat{w}\beta_t)}^2 \label{eq_lem_model_shift_1}
%		\le& n_1\bigbrace{{n_1^2}{} + \frac{2n_1}p(p + 2\sqrt{{n_1}p}) + (p + 2\sqrt{{n_1}p})^2} \alpha = n_1^2\bigbrace{1 + \sqrt{\frac{p}{n_1}}}^4 \alpha. \nonumber
	\end{align}
	Here we use the following on the second term in equation \eqref{eq_lem_model_shift_1}
	\begin{align*}
		& \bigabs{(\beta_s - \hat{w}\beta_t)^{\top} \Sigma_1^{1/2} \cE \Sigma_1^{1/2} K \Sigma_2 K \Sigma_1 (\beta_s - \hat{w}\beta_t)} \\
		= & \bigabs{\bigtr{\cE \Sigma_1^{1/2}K\Sigma_2 K \Sigma_1(\beta_s - \hat{w}\beta_t)(\beta_s - \hat{w}\beta_t)^{\top} \Sigma_1^{1/2}}} \\
		\le & \norm{\cE} \cdot \bignormNuclear{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1 (\beta_s - \hat{w}\beta_t) (\beta_s - \hat{w}\beta_t)^{\top} \Sigma_1^{1/2}} \\
		\le & n_1 \bigbrace{2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}} \cdot \bignormNuclear{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1 (\beta_s - \hat{w}\beta_t)(\beta_s - \hat{w}\beta_t)^{\top} \Sigma_1^{1/2}} \tag{by equation \eqref{eq_isometric}} \\
		\le   & n_1 \bigbrace{2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}} \bignorm{\Sigma_1^{1/2}K \Sigma_2 K \Sigma_1^{1/2}} \cdot \bignorm{\Sigma_1^{1/2}(\beta_s - \hat{w}\beta_t)}^2 \tag{since the matrix inside is rank 1}
	\end{align*}
	The third term in equation \eqref{eq_lem_model_shift_1} can be bounded with
	\begin{align*}
		\bignorm{\Sigma_2^{1/2}K\Sigma_1^{1/2}\cE\Sigma_1^{1/2}(\beta_s - \hat{w}\beta_t)}^2
		\le n_1^2 \bigbrace{2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}}^2 \bignorm{\Sigma_1^{1/2}K\Sigma_{2}K\Sigma_1^{1/2}} \cdot \bignorm{\Sigma_1^{1/2}(\beta_s - \hat{w}\beta_t)}^2.
	\end{align*}
	Combined together we have shown the right direction for $\delta_1 - \delta_2$.
	For the left direction, we simply note that the third term in equation \eqref{eq_lem_model_shift_1} is positive.
	And the second term is bigger than $-2n_1^2(2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}) \alpha$ using equation \eqref{eq_isometric}.
\end{proof}




\paragraph{Part II: The limit of $\bignorm{\Sigma_2^{1/2} (\hat{w}^2X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_1 (\beta_s - \hat{w}\beta_t)}^2$ using random matrix theory.}
We consider the same setting as in previous subsection:
$$ X_1^{\top}X_1:=\Sigma_1^{1/2}  Z_1^T Z_1 \Sigma_1^{1/2} ,\quad X_2^{\top}X_2= \Sigma_2^{1/2}  Z_2^T Z_2 \Sigma_2^{1/2},$$
where $z_{ij}$, $1 \leq i \leq n_1+n_2\equiv n$, $1 \leq j \leq p$, are real independent random variables satisfying \eqref{eq_12moment}. For now, we assume that the random variables $z_{ij}$ are i.i.d. Gaussian, but we know that universality holds for generally distributed entries. Assume that $p/n_1$ is a small number such that $Z_1^TZ_1$ is roughly an isometry, that is, under \eqref{eq_12moment},
%\todo{
%\begin{align}\left\| Z_1^T Z_1 -  \frac{n_1}{n} I \right\| \le 2\sqrt{\frac{p}{n}} + {\frac{p}{n}} .
%\end{align}}
{\color{blue}
If we assume the variances of the entries of $Z_1$ are $1$, then we have
\begin{align}
- {n_1} \left(2\sqrt{\frac{p}{n_1}} - {\frac{p}{n_1}}\right)  \le {Z_1^T Z_1 -  n_1 \id}  \le {n_1} \left(2\sqrt{\frac{p}{n_1}} + {\frac{p}{n_1}}\right) . \label{eq_isometric}
\end{align}
}

\begin{lemma}\label{lem_cov_derivative}
	In the setting of Theorem \ref{thm_model_shift}, we have with high probability $1-{\rm o}(1)$,
\begin{equation}\label{lem_cov_derv_eq}
\begin{split}
&\wh w^{2}(n_1+n_2)^2\bignorm{\Sigma_2^{1/2} (\hat{w}X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_1 (\beta_s - {w}\beta_t)}^2 \\
&= (\beta_s - {w}\beta_t)^{\top} \Sigma_1^{1/2} {M} \frac{(1 + a_3)\id + a_4 {M}^{\top}{M}}{(a_2 + a_1 {M}^{\top}{M})^2} {M}^{\top} \Sigma_1^{1/2} (\beta_s - \hat{w}\beta_t) +\OO(n^{-1/2+\e}),
\end{split}
\end{equation}
	for any constant $\epsilon>0$. %, where $a_{3,4}$ are found using equations in  \eqref{m35reduced}
\end{lemma}
We will give the proof of this lemma in Section \ref{sec_maintools}.

\paragraph{Part III: An $\e$-net argument.} 

{\cor add some arguments with $\e$-net.}
\end{proof}

{\cor add some remarks about the difficulty in random matrix theory}

By applying a tighter bound to Part I of the above proof, we can conclude the proof Lemma \ref{prop_model_shift_tight}.

\begin{proof}[Proof of Lemma \ref{prop_model_shift_tight}]
the proof for tighter bound ........... 
\end{proof}



\subsection{Proof of Many Tasks with the Same Covariates}\label{app_proof_many_tasks}

\begin{theorem}\label{thm_many_tasks}
	Let $n = c \cdot p$.
	Let $X\in\real^{n\times p}$ and $Y_i = X\beta_i + \varepsilon_i$, for $i = 1,\dots,k$.
	Let $U_r U_r^{\top}$ denote the best rank-$r$ approximation subspace of $B^{\star}\Sigma B^{\star}$, where $U_r\in\real^{k\times r}$.
	Let $U_r(i)$ denote the $i$-th row vector of $U_r$.
	We have the following
	\begin{itemize}
		\item If $(1 - \norm{U_r(i)}^2)\cdot \frac{\sigma^2}{c - 1} \ge \norm{\Sigma (B^{\star} U_r U_r(i) - \beta_i)}^2$, then whp $\te(\hat{\beta}_t^{\MTL}) < \te(\hat{\beta}_t^{\STL})$.
		\item If $(1 - \norm{U_r(i)}^2)\cdot \frac{\sigma^2}{c - 1} < \norm{\Sigma(B^{\star} U_r U_r(i) - \beta_i)}^2$, then whp $\te(\hat{\beta}_t^{\MTL}) > \te(\hat{\beta}_t^{\STL})$.
	\end{itemize}
\end{theorem}
As a remark, since the spectral norm of $U_r$ is less than $1$, we have that $\norm{U_r(i)} < 1$, for any $1 \le i \le k$. Compared to Theorem \ref{thm_main_informal}, we can get a simple expression for the two functions $\Delta_{vari}$ and $\Delta_{\beta}$. The proof of Theorem \ref{thm_many_tasks} can be found in Appendix \ref{app_proof_many_tasks}.



%In this section we consider the setting with $k$ many that have the same covariates.
%Since every task has the same number of data points as well as the same covariance, the only differences between different tasks are their models $\set{\beta_i}_{i=1}^k$.
%For this setting, we derive solutions for the multi-task training and the transfer learning setting that match our insights qualitatively from Section \ref{sec_denoise}.

For this setting, the problem reduces to the following.
\begin{align}
	f(B; W_1, \dots, W_k) = \sum_{i=1}^k \bignorm{X B W_i - Y_i}^2. \label{eq_mtl_same_cov}
\end{align}
In order to prove Theorem \ref{thm_many_tasks}, we will derive a closed form solution for equation \eqref{eq_mtl_same_cov}.

\begin{proof}[Proof of Theorem \ref{thm_many_tasks}]
	By fixing $W_1, W_2, \dots, W_k$, we can derive a closed form solution for $B$ as
	\begin{align*}
		\hat{B}(W_1, \dots, W_k) &= (X^{\top}X)^{-1} X^{\top} \bigbrace{\sum_{i=1}^k Y_i W_i^{\top}} (Z Z^{\top})^{-1} \\
		&= \sum_{i=1}^k \bigbrace{\beta_i W_i^{\top}} (ZZ^{\top})^{-1} + (X^{\top}X)^{-1}X^{\top} \bigbrace{\sum_{i=1}^k \varepsilon_i W_i^{\top}} (ZZ^{\top})^{-1}
	\end{align*}
	where we denote $Z\in\real^{r\times k}$ as the $k$ vectors $W_1, W_2, \dots, W_k$ stacked together.
	%Now we switch $\hat{B}$ back into equation \eqref{eq_mtl_same_cov} to
	Similar to Section \ref{sec_setup}, we consider minimizing the validation loss over $W_1, W_2, \dots, W_k$ provided with $\hat{B}$.

	Denote by $\varepsilon(W) = \sum_{i=1}^k \varepsilon_i W_i^{\top}$.
	We shall decompose the validation loss $\val(\hat{B}; W_1, \dots, W_k)$ into two parts.
	The first part is the model shift bias, which is equal to
	\begin{align*}
		\sum_{j=1}^k \bigbrace{\bignorm{\Sigma^{1/2}\bigbrace{\sum_{i=1}^k(\beta_i W_i^{\top}) (ZZ^{\top})^{-1} W_j - \beta_j}}^2}
	\end{align*}
	The second part is the variance, which is equal to
	\begin{align*}
		& \sum_{j=1}^k \exarg{\varepsilon_i, \forall i}{\bigbrace{\bigbrace{\sum_{i=1}^k \varepsilon_i W_i^{\top}} (ZZ^{\top})^{-1} W_j}^2} \cdot {\bigtr{\Sigma(X^{\top}X)^{-1}}} \\
		=& \sigma^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}.
	\end{align*}
	Therefore we shall focus on the minimizer for the model shift bias since the variance part does not depend the weights.
	Let us denote $Q = Z^{\top} (ZZ^{\top})^{-1} Z \in\real^{k\times k}$ where the $(i,j)$-th entry is equal to $W_i^{\top} (ZZ^{\top})^{-1} W_j$, for any $1\le i, j\le k$.
	Let $B^{\star} = [\beta_1, \beta_2, \dots, \beta_k] \in\real^{p \times k}$ denote the true model parameters.
	We can now write the validation loss succinctly as follows.
	\begin{align*}
		\val(\hat{B}; W_1, \dots, W_k) = \bignormFro{\Sigma^{1/2} \bigbrace{B^{\star} Q - B^{\star}}}^2 + \sigma^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}
	\end{align*}
	From the above we can solve for $Q$ optimally as $U_{r}U_r^{\top}$.
	Furthermore, we can solve $\hat{\beta}_i^{\MTL}$ as $B^{\star} U_r U_r(i)$.
	Now we get that
	\begin{align*}
		\te(\hat{\beta}_t^{\MTL}) &= \bignorm{\Sigma^{1/2}\bigbrace{\sum_{i=1}^k W_i^{\top} (ZZ^{\top})^{-1}W_j \beta_i - \beta_j}}^2
		+ \sigma^2  W_j^{\top} (ZZ^{\top})^{-1} W_j \cdot \bigtr{\Sigma (X^{\top}X)^{-1}} \\
		&= \bignorm{\Sigma^{1/2} \bigbrace{B^{\star} U_r U_r(i)}}^2 + \sigma^2\norm{U_r(i)}^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}.
	\end{align*}
	By using Lemma \ref{lem_minv}, we conclude the proof.
\end{proof}
%From the above we can obtain three conceptual insights that are consistent with Section \ref{sec_denoise} and \ref{sec_insight}.
%\begin{itemize}
%	\item The de-noising effect of multi-task learning.
%	\item Multi-task training vs single-task training can be either positive or negative.
%	\item Transfer learning is better than the other two. And the improvement over multi-task training increases as the model distances become larger.
%\end{itemize}

