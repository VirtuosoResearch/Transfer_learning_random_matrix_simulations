\section{Technical Tools: Quantifying Model Shift Bias versus Variance Trade-off}\label{sec_main}

%In this part, we consider the case of two tasks to show  establish the intuition that adding more data helps in multi-task learning by reducing the variance of the estimator.
%We achieve this through tight generalization bounds obtained from random matrix theory.
%For the case of two tasks, we identify three factors that determine the type of transfer between tasks: model distance, covariate shift matrix, and data ratio.
We begin by observing that the test error of $\hat{\beta}_t^{\MTL}$ consists of two parts.
One part captures how similar the task models are and the other part captures the variance of $\hat{\beta}_t^{\MTL}$.
Compared with $\hat{\beta}_t^{\STL}$, we observe that the variance part of $\hat{\beta}_t^{\MTL}$ gets reduced, since more data is added from source tasks.
The bias part of $\hat{\beta}_t^{\MTL}$, which we term as \textit{model shift bias}, affects performance negatively.
We derive the asympotic limit of $\te(\hat{\beta}_t^{\MTL})$ as $p$ approaches infinity.
We compare it with the asympotic limit of $\te(\hat{\beta}_t{\STL})$, for settings where the target data size is limited.
We show sharp generalization bounds for two settings: i) two tasks with general covaraites; ii) many tasks with the same covariates.

\subsection{Two Tasks with General Covariance Matrices}

For the case of two tasks, we decompose the test error of $\hat{\beta}_{t}^{\MTL}$ on the targe task into two parts
\begin{align}
	\te(\hat{\beta}_t^{\MTL}) =& \hat{v}^2 \bignorm{\Sigma_2^{1/2} (\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}X_1^{\top}X_1 (\beta_s - \hat{v}\beta_t)}^2 \nonumber \\
	&+ \sigma^2\cdot \bigtr{(\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_2}, \label{eq_te_model_shift}
\end{align}
where $\hat{v}$ denotes the ratio of the output layer weights (to be defined more precisely in Appendix \ref{app_proof_main}).
It is not hard to show that the variance of $\hat{\beta}_t^{\MTL}$ is reduced compared to $\hat{\beta}_t^{\STL}$, i.e. %(following the argument of Proposition \ref{prop_monotone}), i.e.
\[ \bigtr{(\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_2} \le \bigtr{(X_2^{\top}X_2)^{-1}\Sigma_2}. \]
Because of model shift bias, we can no longer guarantee that $\te(\hat{\beta}_{t}^{\MTL}) \le \te(\hat{\beta}_t^{\STL})$.
%The main result of this section show deterministic conditions under which we get positive or negative transfer.
%And the conditions depend only on the covariate shift matrix $M$, the difference of the task models, and the number of per-task data points.
The technical crux of our approach is to derive the asymptotic limit of $\te(\hat{\beta}_t^{\MTL})$ in the high-dimensional setting, when $p$ approaches infinity.
A key highlight of our approach implies a precise limit on $\bigtr{(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}}$, which only depends on $\Sigma_1, \Sigma_2$ and $n_1, n_2$ (see Lemma \ref{lem_cov_shift} in Appendix \ref{app_proof_main} for the result).

\begin{theorem}\label{thm_main_informal}
	Let $X_i \in\real^{n_i \times p}$ and $Y_i = X_i\beta_i + \varepsilon_i$, for $i = 1, 2$.
	Suppose that $n_1 = c_1 p$ and $n_2 = c_2 p$, where $c_1 > ??$ and $c_2 > 3$ are fixed constants.
	There exists two deterministic functions $\Delta_{\beta}$ and $\Delta_{\vari}$ that only depends on $\set{\hat{v}, M, n_1, n_2, \beta_1, \beta_2}$ such that
	\begin{itemize}
		\item If $\Delta_{\vari} - \Delta_{\beta} \ge \delta$, then whp $\te{\hat{\beta}_t^{\MTL}} < \te(\hat{\beta}_t^{\STL})$.
		\item If $\Delta_{\vari} - \Delta_{\beta} \le \delta$, then whp $\te(\hat{\beta}_t^{\MTL}) > \te(\hat{\beta}_t^{\STL})$.
	\end{itemize}
\end{theorem}


Theorem \ref{thm_main_informal} shows upper and lower bounds that guarantee positive transfer, which is determined by the change of variance $\Delta_{\vari}$ and a certain model shift bias parameter $\Delta_{\beta}$ determined by the covariate shift matrix and the model shift.
The bounds get tighter and tighter as $n_1 / p$ increases.


\subsection{Many Tasks with the Same Covariates}

In this section we consider the setting with $k$ many that have the same covariates.
Since every task has the same number of data points as well as the same covariance, the only differences between different tasks are their models $\set{\beta_i}_{i=1}^k$.
For this setting, we derive solutions for the multi-task training and the transfer learning setting that match our insights qualitatively from Section \ref{sec_denoise}.


\section{Extension to Transfer Learning}

We study the transfer function of taskonomy \cite{ZSSGM18}.
The algorithm is as follows.
First, we obtain the single-task estimator $\hat{\beta}_i$ from every task, for $1\le i \le k$.
This forms the shared representation $B$ in Algorithm \ref{alg_estimator}.
Then, we learn the output layer on the target task.
We use our tools to analyze this setting as follows.