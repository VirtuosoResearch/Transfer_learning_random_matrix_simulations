\section{Introduction}

Multi-task learning that is applied on heterogenous data can often result in suboptimal models (or negative transfer in more technical terms).
In a previous work \cite{WZR20}, we identified three factors that help determine when multi-task learning works, and when it doesn't.
In this work, we zoom in to the task covariance part of \cite{WZR20} to further understand when multi-task learning works under covariate and model shifts.
By using random matrix theory, we can explain several phenomena that are not explained by the techniques of \cite{WZR20}.
These are achieved through tight generalization bounds established in the high-dimensional linear regression setting.
\begin{enumerate}
	\item {\bf Negative transfer:} we establish a fundamental limit that is caused by model shift.
	Moreover, this cannot be corrected via changing the model capacity or reweighting the tasks.
	\item {\bf De-noising effect of MTL:} we show that multi-task transfer has a de-noising effect of reducing the variance of the estimator.
	When this variance reduction effect is bigger than the bias caused by model shift, we get positive transfer.
	\item {\bf Covariate shift affects the rate of transfer:} we further show that covariate shift can control the rate in which transfer occurs.
\end{enumerate}


\section{Problem Setup}\label{sec_setup}

In the multi-task learning (MTL) problem, we are given the input of $k$ tasks $(X_1, Y_1), (X_2, Y_2), \dots, (X_k, Y_k)$.
We use a shared body (module) $B\in\real^{p\times r}$ for all tasks and a separate head (module) $\set{A_i \in \real^{r}}_{i=1}^k$ for each task.
%    \paragraph{Different covariates:}
This corresponds to minimizing the following optimization objective.
\begin{align}
	\label{eq_mtl}
	f(B; A_1, \dots, A_k) = \sum_{i=1}^k \norm{X_i B A_i - Y_i}^2.
\end{align}
Note that we consider the natural parameterization without reweighting the tasks above.
The shared body $B$ plays an important role because it allows information transfer between different task data.
There are two ways to ensure the sharing of information between tasks.
\begin{itemize}
	\item Adding a regularization over $B$, e.g. \cite{LPTV09,LPVT11}.
	\item Controlling the capacity $r$ of $B$, e.g. \cite{KD12,WZR20}. Moreover, \cite{KD12} observed that controlling the capacity can outperform the implicit capacity control of adding regularization over $B$.
\end{itemize}


\paragraph{Data generation.} We shall assume that each task data follows a linear model.
For every $1\le i\le k$, we assume that
\[ Y_i = X_i \beta_i + \varepsilon_i, \]
where $\beta_i\in\real^p$ is the model parameter for the $i$-th task.
Each row of $X_i\in\real^{n_i\times p}$ is assumed to be drawn i.i.d. from a fixed distribution with covariance matrix $\Sigma_i$.
We assume that for every row $x$ of $X_i$, we have
\[ \ex{xx^{\top}} = \Sigma_i. \]
We also write $x = \Sigma_i^{1/2} z_i$, where $z_i$ is a random vector with mean $0$ and variance $1$.


\paragraph{Objectives.}
We will designate the $k$-th task as the target.
Our goal is to come up with an estimator $\hat{\beta}$ to provide accurate predictions for the target task, provided with the other auxiliary task data.
Concretely, we focus on two objectives.
\begin{itemize}
	\item Estimation error for the target model $\beta_t$: we consider their distance
		\[ \err(\hat{\beta}) \define \exarg{\varepsilon_i, \forall 1\le i\le k}{\bignorm{\hat{\beta} - \beta_t}^2}. \]
	\item Test error for the target task:
		\begin{align*}
			\te(\hat{\beta}) &\define \exarg{x \sim \Sigma_k}{\exarg{{\varepsilon_i, \forall 1\le i\le k}}{(x^{\top}\hat{\beta} - x^{\top}\beta_t)^2}} \\
			&= \exarg{\varepsilon_i, \forall 1\le i\le k}{(\hat{\beta} - \beta_t)^{\top}\Sigma_k(\hat{\beta} - \beta_t)}.
		\end{align*}
\end{itemize}
%\todo{may also consider test error}
%Several relevant directions on this setting.
%\begin{itemize}
%  \item {\bf Low-rank space of $\set{\theta_i}_{i=1}^k$.} We may assume that the task models themselves form a low-rank space. This imposes that the tasks should be related to each other in some way.
%  \item {\bf PCA-based averaging for distributed regression.} This also leads to a natural heuristic for the distributed learning problem.
%    After receiving $\hat{\theta_i}$, for $i = 1, 2,\dots, k$, we can apply PCA to find a low-rank space of the estimates.
%  \item {\bf The hypothesis testing view.} A practical question that often arises in MTL is, if we can access a new task data (say $k+1$-th), should we add the task to the existing set of tasks or not?
%    One hypothesis is to find the projecion of $\hat{\theta}_{k+1}$ to the low-rank space of the estimates.
%  \item {\bf MTL and matrix factorization.} Could we use the results from MF to solve MTL? Does the landscape of MTL connect to MF?
%\end{itemize}


\subsection{Hypothesis}

Our hypothesis is that the heterogeneity among the multiple tasks can be categorized into two classes, \textit{covariate shift} and \textit{model shift}. %\todo{Add some references to add spice on these takes.}
We consider two natural questions within each category.
\begin{itemize}
	\item How does covariate shift affect the rate of information transfer? For example, is it better to have the same covariance matrix or not?
	\item Under model shift, when do we get positive vs. negative transfer? How does the type of transfer depend on the number of data points, the distance of the task models etc?
\end{itemize}



\subsubsection{Covariate Shift}

A natural setting is when the covariance matrices $\Sigma_i$ are different across tasks, i.e. having different spectrum or singular vectors.
This is also known as covariate shift in the literature.
\todo{Our hypothesis is that the covariate shift can slow down the convergence of learning the true $\theta$ as a function of the number of data points.}
A special case of this setting is that the single-task models are the same across all the tasks, i.e. $\beta_i = \beta$, for all $1\le i\le k$.
%\begin{align}
%	Y_i = X_i \beta + \varepsilon_i. %, \mbox{ with } \frac 1 {n_i} X_i^{\top}X_i \sim
%\Sigma_i
%\end{align}

Since all tasks share the same underlying model $\beta$, we use a simplified objective as follows.
\begin{align}
	\label{eq_mtl_basic}
	f(w) = \sum_{i=1}^k \norm{X_i w - Y_i}^2.
\end{align}
%{\bf The distributed learning problem.}
%    \begin{align}
%     f(w) = \sum_{i=1}^k \normFro{X_i w - Y_i}^2. \label{eq_dist}
%   \end{align}
Equation \eqref{eq_mtl_basic} is simplified from equation \eqref{eq_mtl} by setting $A_i$ to be 1 for all tasks. %can select a model from the subspace of $B$ to fit $(X_i, Y_i)$.

\begin{proposition}\label{prop_monotone}
	Suppose that $n > p$.
  When there is no model shift, adding the source task data always reduces the estimation error and the test error for the target task, i.e.
	\begin{align}
		\err(\hat{\beta}_{s,t})  &\le \err(\hat{\beta}_t), \text{ and} \label{eq_mono_e}\\
		\te(\hat{\beta}_{s,t}) &\le \te(\hat{\beta}_t) \label{eq_mono_te}
	\end{align}
\end{proposition}

\begin{proof}
	Equation \eqref{eq_mono_e} is simply because
		\[ \err(\hat{\beta}_{s,t}) = \bigtr{(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}} \le \bigtr{(X_1^{\top}X_1)^{-1}} = \err(\hat{\beta}_t). \]
	Equation \eqref{eq_mono_te} follows because
	\begin{align*}
		\te(\hat{\beta}_{s,t}) = \bigtr{(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_2} &= \bigtr{\bigbrace{\Sigma_2^{-1/2}X_1^{\top}X_1\Sigma_2^{-1/2} + \Sigma_2^{-1/2}X_2^{\top}X_2\Sigma^{-1/2}}^{-1}} \\
		&\le \bigtr{\bigbrace{\Sigma_2^{-1/2}X_2^{\top}X_2\Sigma_2^{-1/2}}^{-1}}
			= \bigtr{(X_2^{\top}X_2)^{-1} \Sigma_2} = \te(\hat{\beta}_t).
	\end{align*}
\end{proof}


\subsubsection{Model Shift}

In addition to covariate shift, in general the single-task models can also be different across different tasks.
\todo{Here the hypothesis is that the optimal $B$ is captured by a low-rank approximation of the single-task models.}
%We remove the assumption that the models are the same.
%The $i$-th task data can be viewed as generated by a separate model $\beta_i\in\real^p$.
%\begin{align}
%	Y_i = X_i \beta_i + \varepsilon_i.
%\end{align}


\subsection{The High-Dimensional Setting}

\todo{setup motivation and notations}
We would like to get insight on how covariate and model shifts affect the rate of transfer.
For the case of two tasks, we can get precise rates using random matrix theory.
For the sake of clarity, we call task 1 the source task and task 2 the target task,
i.e. $\beta_1 = \beta_s$ and $\beta_2 = \beta_t$.