\section{Problem Setup}\label{sec_setup}

In the multi-task learning (MTL) problem, we are given the input of $k$ tasks $(X_1, Y_1), (X_2, Y_2), \dots, (X_k, Y_k)$.
We use a shared body (module) $B\in\real^{p\times r}$ for all tasks and a separate head (module) $\set{A_i \in \real^{r}}_{i=1}^k$ for each task.
%    \paragraph{Different covariates:}
This corresponds to minimizing the following optimization objective.
\begin{align}
	\label{eq_mtl}
	f(B; A_1, \dots, A_k) = \sum_{i=1}^k \norm{X_i B A_i - Y_i}^2.
\end{align}
Note that we consider the natural parameterization without reweighting the tasks above.
The shared body $B$ plays an important role because it allows information transfer between different task data.
There are two ways to ensure the sharing of information between tasks.
\begin{itemize}
	\item Adding a regularization over $B$, e.g. \cite{LPTV09,LPVT11}.
	\item Controlling the capacity $r$ of $B$.
\end{itemize}

\subsection{Hypothesis}

Our hypothesis is that the heterogeneity among the multiple tasks can be categorized into two classes, \textit{covariate shift} and \textit{model shift}. %\todo{Add some references to add spice on these takes.}
We consider two natural questions within each category.
\begin{itemize}
	\item How does covariate shift affect the rate of information transfer? For example, is it better to have the same covariance matrix or not?
	\item Under model shift, when do we get positive vs. negative transfer? How does the type of transfer depend on the number of data points, the distance of the task models etc?
\end{itemize}

\paragraph{Objectives.}
We will designate the $k$-th task as the target.
Our goal is to come up with an estimator $\hat{\beta}$ to provide accurate predictions for the target task, provided with the other auxiliary task data.
Concretely, we focus on two objectives.
\begin{itemize}
	\item Estimation error for the target model $\beta_t$: we consider their distance
		\[ e(\hat{\beta}) \define \exarg{\varepsilon_i, \forall 1\le i\le k}{\bignorm{\hat{\beta} - \beta_t}^2}. \]
	\item Test error for the target task:
		\begin{align*}
			te(\hat{\beta}) &\define \exarg{x \sim \Sigma_k}{\exarg{{\varepsilon_i, \forall 1\le i\le k}}{(x^{\top}\hat{\beta} - x^{\top}\beta_t)^2}} \\
			&= \exarg{\varepsilon_i, \forall 1\le i\le k}{(\hat{\beta} - \beta_t)^{\top}\Sigma_k(\hat{\beta} - \beta_t)}.
		\end{align*}
\end{itemize}
%\todo{may also consider test error}
%Several relevant directions on this setting.
%\begin{itemize}
%  \item {\bf Low-rank space of $\set{\theta_i}_{i=1}^k$.} We may assume that the task models themselves form a low-rank space. This imposes that the tasks should be related to each other in some way.
%  \item {\bf PCA-based averaging for distributed regression.} This also leads to a natural heuristic for the distributed learning problem.
%    After receiving $\hat{\theta_i}$, for $i = 1, 2,\dots, k$, we can apply PCA to find a low-rank space of the estimates.
%  \item {\bf The hypothesis testing view.} A practical question that often arises in MTL is, if we can access a new task data (say $k+1$-th), should we add the task to the existing set of tasks or not?
%    One hypothesis is to find the projecion of $\hat{\theta}_{k+1}$ to the low-rank space of the estimates.
%  \item {\bf MTL and matrix factorization.} Could we use the results from MF to solve MTL? Does the landscape of MTL connect to MF?
%\end{itemize}


\subsubsection{Covariate Shift}

A natural setting is when the covariance matrices $\frac{1}{n_i}\cdot X_i^{\top} X_i$ are different, i.e. having different spectrum or singular vectors.
This is also known as covariate shift in the literature.
\todo{Our hypothesis is that the covariate shift can slow down the convergence of learning the true $\theta$ as a function of the number of data points.}


In this setting, we assume that the covariates of task $i$ are drawn from $\Sigma_i \in \real^{d \times d}$, but the models are the same across all the tasks, i.e.
\begin{align}
	Y_i = X_i \beta + \varepsilon_i. %, \mbox{ with } \frac 1 {n_i} X_i^{\top}X_i \sim \Sigma_i
\end{align}

When the models are the same for all tasks, we use a simplified objective as follows.
\begin{align}
	\label{eq_mtl_basic}
	f(w) = \sum_{i=1}^k \norm{X_i w - Y_i}^2.
\end{align}
%{\bf The distributed learning problem.}
%    \begin{align}
%     f(w) = \sum_{i=1}^k \normFro{X_i w - Y_i}^2. \label{eq_dist}
%   \end{align}
Equation \eqref{eq_mtl_basic} is simplified from Equation \eqref{eq_mtl} by setting $A_i$ to be 1 for all tasks. %can select a model from the subspace of $B$ to fit $(X_i, Y_i)$.

\begin{proposition}\label{prop_monotone}
	Suppose that $n > p$.
  When there is no model shift, adding the source task data always reduces the estimation error, i.e.
	\[ e(\hat{\beta}_{s,t}) \le e(\hat{\beta}_t). \]
\end{proposition}

\begin{proof}
	This is simply because $\bigtr{(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}} \le \bigtr{(X_1^{\top}X_1)^{-1}}$.
\end{proof}


\subsubsection{Model Shift}

In addition to covariate shift, in general the single-task models can also be different across different tasks.
\todo{Here the hypothesis is that the optimal $B$ is captured by a low-rank approximation of the single-task models.}
%We remove the assumption that the models are the same.
The $i$-th task data can be viewed as generated by a separate model $\beta_i\in\real^p$.
\begin{align}
	Y_i = X_i \beta_i + \varepsilon_i.
\end{align}

\paragraph{Sparse models.}
One important case is the sparse case, where we assume that the $\beta_i$'s are sparse in the sense 
that each $\beta_i$ is in a subspace of dimension $k_i\ll p$. In other words,
$$\beta_i = \sum_{j=1}^{k_i} a^{(i)}_j v^{(i)}_j,$$
where $\{v^{(i)}_j\}$ is a set of $k_i$ orthonormal vectors. For $X_i= Z_i \Sigma_i^{1/2}$, we have 
$$ X_i \beta_i = Z_i \Sigma_i^{1/2}\beta_i=  Z_i \Sigma_i^{1/2}P_i\beta_i,\quad P_i:= \sum_{j=1}^{k_i}v_j^{(i)}(v_j^{(i)})^\top.$$
Then we have 
$$\frac{1}{n_i}\mathbb E\left[P_i \Sigma_i^{1/2} Z_i^T Z_i \Sigma_i^{1/2}P_i\right]=P_i \Sigma_i P_i.$$
Hence our analysis will be valid with projected covariance matrices $P_i \Sigma_i P_i$. Moreover, due to the sparsity assumption, it is natural to have restricted isometry property: 
$$\frac{1}{n_i} P_i \Sigma_i^{1/2} Z_i^T Z_i \Sigma_i^{1/2}P_i \approx P_i \Sigma_i P_i.$$
Finally, for our analysis it is not necessary to assume that $\beta_T$ is sparse for the target. 


