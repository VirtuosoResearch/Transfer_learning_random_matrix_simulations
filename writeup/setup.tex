\section{Problem Setup}\label{sec_setup}

In the multi-task learning (MTL) problem, we are given the input of $k$ tasks $(X_1, Y_1), (X_2, Y_2), \dots, (X_k, Y_k)$.
We use a shared body (module) $B$ for all tasks and a separate head (module) $\set{A_i}_{i=1}^k$ for each task.
%    \paragraph{Different covariates:}
This corresponds to minimizing the following optimization objective.
\begin{align}
	\label{eq_mtl}
	f(B; A_1, \dots, A_k) = \sum_{i=1}^k \norm{X_i B A_i - Y_i}^2.
\end{align}
Note that we consider the natural parameterization without reweighting the tasks above.


\subsection{Hypothesis}

Our hypothesis is that the heterogeneity among the multiple tasks can be categorized into two classes, \textit{covariate shift} and \textit{model shift}. %\todo{Add some references to add spice on these takes.}

\subsubsection{Covariate Shift}

A natural setting is when the covariance matrices $X_i^{\top} X_i$ are different, i.e. having different spectrum.
This is also known as covariate shift in the literature.
Our hypothesis is that the covariate shift can slow down the convergence of learning the true $\theta$ as a function of the number of data points.


In this setting, we assume that the covariates of task $i$ are drawn from $\Sigma_i \in \real^{d \times d}$, but the models are the same across all the tasks, i.e.
\begin{align}
	Y_i = X_i \beta + \varepsilon_i. %, \mbox{ with } \frac 1 {n_i} X_i^{\top}X_i \sim \Sigma_i
\end{align}

When the models are the same for all tasks, we use a simplified objective as follows.
\begin{align}
	\label{eq_mtl_basic}
	f(w) = \sum_{i=1}^k \norm{X_i w - Y_i}^2.
\end{align}
%{\bf The distributed learning problem.}
%    \begin{align}
%     f(w) = \sum_{i=1}^k \normFro{X_i w - Y_i}^2. \label{eq_dist}
%   \end{align}

Equation \eqref{eq_mtl} is a slightly more complicated parametrization of equation \eqref{eq_mtl_basic}, in that each $A_i$ can select a model from the subspace of $B$ to fit $(X_i, Y_i)$.



\subsubsection{Model Shift}

The most general version is when the covariates are different between different tasks and the single-task models are also different.
Here the hypothesis is that the optimal $B$ is captured by a low-rank approximation of the single-task models.


We remove the assumption that the models are the same.
The $i$-th task data can be viewed as generated by a separate model $\beta_i\in\real^d$.
\begin{align}
	y_i = X_i \beta_i + \varepsilon_i.
\end{align}

\paragraph{Sparse models.}
One important case is the sparse case, where we assume that the $\beta_i$'s are sparse in the sense 
that each $\beta_i$ is in a subspace of dimension $k_i\ll p$. In other words,
$$\beta_i = \sum_{j=1}^{k_i} a^{(i)}_j v^{(i)}_j,$$
where $\{v^{(i)}_j\}$ is a set of $k_i$ orthonormal vectors. For $X_i= Z_i \Sigma_i^{1/2}$, we have 
$$ X_i \beta_i = Z_i \Sigma_i^{1/2}\beta_i=  Z_i \Sigma_i^{1/2}P_i\beta_i,\quad P_i:= \sum_{j=1}^{k_i}v_j^{(i)}(v_j^{(i)})^\top.$$
Then we have 
$$\frac{1}{n_i}\mathbb E\left[P_i \Sigma_i^{1/2} Z_i^T Z_i \Sigma_i^{1/2}P_i\right]=P_i \Sigma_i P_i.$$
Hence our analysis will be valid with projected covariance matrices $P_i \Sigma_i P_i$. Moreover, due to the sparsity assumption, it is natural to have restricted isometry property: 
$$\frac{1}{n_i} P_i \Sigma_i^{1/2} Z_i^T Z_i \Sigma_i^{1/2}P_i \approx P_i \Sigma_i P_i.$$
Finally, for our analysis it is not necessary to assume that $\beta_T$ is sparse for the target. 

\subsection{Objectives}

We will designate the $k$-th task as the target.
Our goal is to come up with an estimator $\hat{\beta}$ to provide accurate predictions for the target task.
Concretely, we focus on two objectives.
\begin{itemize}
	\item Estimation error for the target model $\beta_t$: we consider their distance
		\[ \exarg{\varepsilon}{\bignorm{\hat{\beta} - \beta_t}^2}. \]
	\item Test error for the target task:
		\[ \exarg{x \sim \Sigma_k}{(x^{\top}\hat{\beta} - x^{\top}\beta_t)^2}. \]
\end{itemize}
%\todo{may also consider test error}
%Several relevant directions on this setting.
%\begin{itemize}
%  \item {\bf Low-rank space of $\set{\theta_i}_{i=1}^k$.} We may assume that the task models themselves form a low-rank space. This imposes that the tasks should be related to each other in some way.
%  \item {\bf PCA-based averaging for distributed regression.} This also leads to a natural heuristic for the distributed learning problem.
%    After receiving $\hat{\theta_i}$, for $i = 1, 2,\dots, k$, we can apply PCA to find a low-rank space of the estimates.
%  \item {\bf The hypothesis testing view.} A practical question that often arises in MTL is, if we can access a new task data (say $k+1$-th), should we add the task to the existing set of tasks or not?
%    One hypothesis is to find the projecion of $\hat{\theta}_{k+1}$ to the low-rank space of the estimates.
%  \item {\bf MTL and matrix factorization.} Could we use the results from MF to solve MTL? Does the landscape of MTL connect to MF?
%\end{itemize}

