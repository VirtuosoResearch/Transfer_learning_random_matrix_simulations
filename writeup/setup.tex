\section{Problem Formulation for Multi-Task Learning}
\label{sec_prelim}

We begin by defining our problem setup including the multi-task estimator we study.
Then, we describe the bias-variance tradeoff of the multi-task estimator, and connect the bias and variance of the estimator to \textit{task similarity}, \textit{sample size} and \textit{covariate shift}.
%Finally, we show a tight concentration bound for the bias and variance quantities using random matrix theory.

\textbf{Problem setup.}
Suppose we have $t$ datasets, where $t$ is a fixed value that does not grow with the feature dimension $p$.
In the high-dimensional linear regression setting (e.g. \cite{HMRT19,BLLT20}), the features of the $k$-th task, denoted by $X_k\in\real^{n_i\times p}$, consists of $n_k$ feature vectors given by $x_1, x_2, \dots, x_{n_k}$.
And each feature $x_i = \Sigma^{1/2}z_i$, where $z_i\in\real^p$ consists of i.i.d. entries with mean zero and unit variance.
The sample size $n_k $ equals $\rho_k\cdot p$ for a fixed value $\rho_k$.
The labels $Y_k = X_k \beta_k + \varepsilon_k$, where $\beta_k$ denotes the linear model parameters and $\varepsilon_k$ denotes i.i.d. noise with mean zero and variance $\sigma^2$.
%Recall that we have $t$ labeled training datasets, denoted by $(X_1, Y_1), (X_2, Y_2), \dots, (X_t, Y_t)$, where $X_i\in\real^{n_i\times p}$ and $Y_i\in\real^{n_i}$ for $1\le i\le t$.
%Following \cite{HMRT19,BLLT20}, we assume that for each task $i = 1,2,\dots,t$,  every feature vector is generated as $x = \Sigma_i^{1/2} z$, where $z\in\real^p$ is a random vector with i.i.d. entries of mean zero and unit variance and $\Sigma_i\in\real^{p\times p}$ is a positive semidefinite matrix.
%Without loss of generality, let the $t$-th task be the target task.

We focus on the commonly used hard parameter sharing model for multi-task learning \cite{R17}.
When specialized to the linear regression setting, the model consists of a linear layer $B\in\real^{p\times r}$ that is shared by all tasks and $t$ output layers $W_1, \dots, W_t$ that are in $\real^r$.
The width of $B$, denoted by $r$, plays an important regularization effect.
As observed in Proposition 1 of \cite{WZR20}, if $r \ge t$, there is no regularization effect.
Hence, we assume that $r < t$ in our study.
For example, when there are only two tasks, $r = 1$ and $B$ reduces a vector whereas $W_1, W_2$ become scalars.
We study the following procedure inspired by how hard parameter sharing models are trained in practice (e.g. \cite{MTDNN19}).
\squishlist
	\item Separate each dataset $(X_i, Y_i)$ randomly into a training set $(X_i^{tr}, Y_i^{tr})$ and a validation set $(X_i^{val}, Y_i^{val})$.
	The size of each set is described below.
	\item Learning the shared layer $B$: minimize the training loss over $B$ and $W_1, \dots, W_t$, leading to a closed form equation for $\hat{B}$ that depends on $W_1,\dots, W_k$.
		\vspace{-0.075in}
		{\small\begin{align}\label{eq_mtl}
			f(B; W_1, \dots, W_t) = \sum_{k=1}^t \norm{X_k^{tr} B W_k - Y_k^{tr}}^2.
		\end{align}}
		\vspace{-0.075in}
	\item Tuning the output layers $W_i$: set $B = \hat{B}$ and minimize the validation loss over $W_1,\dots, W_k$.
		\vspace{-0.075in}
		{\small\begin{align}\label{eq_mtl_eval}
			g(W_1, \dots, W_t) = \sum_{k=1}^t \norm{X_k^{val} \hat{B} W_k - Y_k^{val}}^2.
		\end{align}}
		\vspace{-0.075in}
\squishend
\vspace{-0.1in}
%where $B\in\real^{p\times r}$ and $W_k\in\real^r$ for every $1\le k\le t$.
%Following , we assume that $r < t$, because otherwise minimizing $f(\cdot)$ could result in $BW_i$ being the single-task optimum.
We make several remarks.
In general, the objective $f(\cdot)$ is non-convex in $B$ and the $W_k$'s.
Therefore, we first minimize $B$ in equation \eqref{eq_mtl} and then minimize $W_k$ given $B$ in equation \eqref{eq_mtl_eval}.
For our purpose, a validation set of size $\rho_i \cdot p^{0.99}$ that is much larger than the number of output layer parameters $kt$ suffices.
The size of the training set is then $\rho_i (p - p^{0.99})$.
The advantage of tuning the output layers on the validation set is to reduce the effect of noise from $\hat{B}$.
%For more details, we refer the reader to Appendix \ref{app_proof_sec3}.

%The single-task estimator $\hat{\beta}_t^{\STL}$ is given by $(X_t^{\top}X_t)^{-1}X_t^{\top}Y_t$.
%For an estimator $\hat{\beta}\in\real^p$, we define the out-of-sample prediction loss as

%which can be further decomposed as the bias plus the variance of $\hat{\beta}$.
%In order to relate the btradeoff to properties of the data, we need tight concentration bounds for the bias and variance.
%We consider the high-dimensional regime where $n_i$ is a fixed constant $\rho_i > 1$  multiple of $p$ for every $1\le i\le t$, and $p$ is sufficiently large.
%We focus on a setting where $\rho_t$ is small compared to $\set{\rho_i}_{i=1}^{t-1}$.
%This setting captures the need to add more labeled data to reduce the prediction loss of the target task.

%However, this result only applies to a single task.
%Therefore, our goal is to extend this result to multiple tasks.

%\textbf{Notations.}
%When there is no ambiguity, we drop the subscript $t$ from $\te_t(\hat{\beta}_t^{\MTL})$ and write $\te(\hat{\beta}_t^{\MTL})$ for simplicity.
%We refer to the first task as the source task when there are only two tasks.
%We call $M = \Sigma_1^{1/2}\Sigma_2^{-1/2}$ the covariate shift matrix.


%\subsection{Analyzing the Tradeoff via Random Matrix Theory}

\textbf{Problem statement.}
We focus on predicting a particular task, say the $t$-th task without loss of generality.
Let $\hat{\beta}_t^{\MTL}$ denote the multi-task estimator obtained from the procedure above.
Our goal is to compare the prediction loss of $\hat{\beta}_t^{\MTL}$, defined by
{\small\begin{align*}
		\te(\hat{\beta}_t^{\MTL}) = \exargnob{\set{\varepsilon_i}_{i=1}^t}\exarg{x = \Sigma_t^{1/2} z}{({x}^{\top}\hat{\beta} - {x}^{\top}\beta_t)^2}
		= \exargnob{\set{\varepsilon_i}_i^t}\bignorm{\Sigma_2^{1/2} (\hat{\beta}_t^{\MTL} - \beta_t)}^2,
	\end{align*}}%
%= (\hat{\beta} - \beta_t)^{\top}\Sigma_t(\hat{\beta} - \beta_t)
to the prediction loss $L(\hat{\beta}_t^{\STL})$ of the single-task estimator $\hat{\beta}_t^{\STL} = (X_t^{\top}X_t)^{-1}X_t^{\top}{Y_t}$.
We say there is negative transfer if  $L(\hat{\beta}_t^{\MTL}) > L(\hat{\beta}_t^{\STL})$, or positive transfer otherwise.

As an example, for the setting of two tasks, we can decompose $L(\hat{\beta}_t^{\MTL}) - L(\hat{\beta}_t^{\MTL})$ into a bias term and a variance term (that scales with noise variance $\sigma_2$) as follows (derived in Appendix \ref{app_proof_sec3}).
%Recall that $\hat{\beta}_t^{\MTL}$ is defined as $BW_t$ after solving equation \eqref{eq_mtl}.
%We can  the test error of $\hat{\beta}_{t}^{\MTL}$ on the target task into two parts as follows.
{\small\begin{align}
	\te(\hat{\beta}_t^{\MTL}) - L(\hat{\beta}_t^{\STL}) =& ~ \hat{v}^2 \bignorm{\Sigma_2^{1/2} (\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}X_1^{\top}X_1 (\beta_1 - \hat{v}\beta_2)}^2 \label{eq_te_model_shift} \\
	+&~ \sigma^2 \bigbrace{\bigtr{(\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_2} - \bigtr{(X_2^{\top}X_2)^{-1}\Sigma_2}}. \label{eq_te_var}
\end{align}}%
In the above, $\hat{v} = W_1 / W_2$ where $W_1, W_2$ are obtained from solving equation \eqref{eq_mtl_eval} (recalling that $W_1, W_2$ are scalars for two tasks).
The role of $\hat{v}$ is to scale the shared subspace $B$ to fit each task.

Equation \eqref{eq_te_model_shift} corresponds to the bias of $\hat{\beta}_t^{\MTL}$.
Hence, the bias term introduces a negative effect that depends on the \textit{similarity} between $\beta_1$ and $\beta_2$.
Equation \eqref{eq_te_var} corresponds to the variance of $\hat{\beta}_t^{\MTL}$, minus the variance of $\hat{\beta}_t^{\STL}$, which is always negative.
Intuitively, the more \textit{samples} we have, the smaller the variance is.
Meanwhile, \textit{covariate shift} also affects how small the variance can be.
%This part introduces a positive variance reduction effect from adding the source labels.
%Hence, whether $\te(\hat{\beta}_t^{\MTL}) < \te(\hat{\beta}_t^{\STL})$ is determined precisely by the tradeoff between the negative effect of the bias term and the positive effect of the variance term!
%(i) the negative effect from model shift bias.
%(ii) the positive effect from variance reduction;



\iffalse
\textbf{Transfer learning.}
We extend Theorem \ref{thm_main_informal} to transfer learning settings.
We study the transfer procedure used in Taskonomy by Zamir et al.'18 \cite{ZSSGM18}.
For the setting of high-dimensional linear regression, the transfer procedure is as follows.
%Specifically, the source task encoder consists of the representations learnt from one or more source tasks.
%The transfer function then tries to fit the target task data to the source task encoder.
\squishlist
	\item \textit{Learning source task representations}: we obtain $\hat{\beta}_i^{\STL}$ from each source task, for $1\le i \le t-1$.
		This forms a shared representation ${B} = [\hat{\beta}_1,\hat{\beta}_2,\dots,\hat{\beta}_{t-1}]$.
	\item \textit{Fine-tuning on the target task}: we learn the output layer $W_t\in\real^{t-1}$ similar to equation \eqref{eq_mtl}
		\begin{align}
			g(W_t) = \bignorm{X_t B W_t - Y_t}^2.
		\end{align}
\squishend
After solving $W_t$, we use $\hat{\beta}_t^{\TL} = B W_t$ as the transfer learning (TL) estimator for the target task.
Our result naturally applies to $\hat{\beta}_t^{\TL}$.
Interestingly, we show that the model shift bias is simply the projection of $\beta_t$ to the orthogonal subspace of $B^{\star} = [\beta_1,\dots,\beta_{t-1}]$, or $\norm{\Sigma_t^{1/2}(\id - B^{\star}({B^{\star}}^{\top}B^{\star})^{-1}{B^{\star}}^{\top})\beta_t}^2$ more precisely.
The formal statement is presented in Theorem \ref{prop_taskonomy} and its proof in Appendix \ref{app_proof_sec4}.
\fi


%In order to analyze the tradeoff, we develop the following technical tool, which provides a tight bound for equation \eqref{eq_te_var}.
%where $(a_1, a_2)$ is the solution to the following deterministic equations:
%	\begin{align*}
%		a_1 + a_2 = 1- \frac{1}{\rho_1 + \rho_2},\quad a_1 + \frac1{\rho_1 + \rho_2}\cdot \frac{1}{p}\sum_{i=1}^p \frac{\lambda_i^2 a_1}{\lambda_i^2 a_1 + a_2} = \frac{\rho_1}{\rho_1 + \rho_2}.
%	\end{align*}

%\textbf{Remark.} When there is only the target task, $\rho_1$ equals zero.
%Hence $a_2 = 1 - 1/ \rho_2, a_1 = 0$ and Lemma \ref{lem_cov_shift_informal} states that $\tr[(X_2^{\top}X_2)^{-1}\Sigma_2] = \sigma^2 / (\rho_2 - 1)$, which is a well-known result in random matrix theory \cite{S07}.
%Hence our result provides a necessary tool to analyze MTL in high dimensions.

