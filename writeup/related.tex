\section{Related Work}


We refer the interested readers to several excellent surveys on multi-task and transfer learning for a comprehensive survey \cite{PY09,R17,ZY17,V20}.

\textbf{Multi-task learning theory.}
Some of the earliest work on multi-task learning are Baxter \cite{B00}, Ben-David and Schuller \cite{BS03}.
Mauer \cite{M06} Generalization bounds for linear separation settings of MTL
Ben-David et al. \cite{BBCK10} provides uniform convergence bounds that combines empirical and target errors in an optimal way.
The benefit of learning multi-task representations is studied for learning certain half-spaces \cite{MPR16} and sparse regression \cite{LPTV09,LPVT11}.
Our work is closely related to Wu et al. \cite{WZR20}.
While Wu et al. provide generalization bounds to show that adding more labeled helps learn the target task more accurately, their results do not apply to explain the phenomena of negative transfer.

% Adding a regularization over $B$, e.g. .
\textbf{Multi-task learning methodology.}
Ando and Zhang \cite{AZ05} introduces an alternating minimization framework for learning multiple tasks.
Argyriou et al. \cite{AEP08} presents a convex algorithm which learns common sparse representations across a pool of related tasks.
Evgeniou et al. \cite{EMP05} develops a framework for multi-task learning in the context of kernel methods.
\cite{KD12} observed that controlling the capacity can outperform the implicit capacity control of adding regularization over $B$.
The multi-task learning model that we have focused on is based on the idea of hard parameter sharing \cite{C93,R17}.
We believe that the technical tools we have developed can also be applied to many other multi-task learning models.

\textbf{Random matrix theory.}


\section{Conclusions and Discussions}
