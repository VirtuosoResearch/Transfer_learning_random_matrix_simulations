\documentclass{article}

\usepackage{neurips_2020_author_response}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{mathabx}
\usepackage{xcolor}
\newcommand{\yell}[1]{\textcolor{red}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{{TODO: #1}}}
\newcommand{\num}[1]{\textcolor{black}{#1}}


% Squished list environment to save space
\newcommand{\squishlist}{
\begin{list}{$\bullet$}
{ \setlength{\itemsep}{0pt}
\setlength{\parsep}{1pt}
\setlength{\topsep}{1pt}
\setlength{\partopsep}{0pt}
% \setlength{\labelwidth}{1em}
}
}
\newcommand{\squishend}{\end{list}}

\begin{document}

We thank the reviewers for their excellent feedback, which significantly improved our work.
%Taking their helpful comments into account, we have sought to clarify the presentation of our work. %, which addresses the main criticism by R1 and R2.
%Due to space limit, we only respond to the most salient comments, but have taken the rest into account.
All reviewers agree that our work provides a solid foundation for understanding when MTL performs well compared to STL, a central open question in MTL.
We offer better explanations of our theoretical/algorithmic contributions in this response.
%Besides clarifying our current result, we've also extended our conceptual insight on bias-variance tradeoff to multiple tasks in response to R1 and R2's feedback.
%First, we address the main issues that are raised by the reviewers.
%Then, we respond to each individual reviewer's feedback.


\textbf{Motivation. [R2]}
We focus on a setting where we have a target task with limited labeled data and study when using MTL w/ multiple source tasks helps the target task. %,
%which differs from traditional MTL that studies the average performance of all tasks.
There are many practical applications in both industry and academia.
For example, for predicting a rare video event or classifying an Xray scan, collecting large amounts of labeled data is very expensive.
Still, auxiliary labeled data are often easier to obtain.
For these settings, while applying MTL is natural, the result can be worse than STL.
Classical MTL theory studies the average performance of all tasks and does not address whether MTL helps the target task.
Our work builds a foundation for applying MTL to these settings.
%We have included these discussion to our paper.


\textbf{What can we say for multiple tasks? [R1, R2]}
%We thank R1 and R2 for raising the important question of whether our setting applies to multiple tasks.
While we have focused on the two-task case to explain our results in the original submission, we can apply all of our conceptual insights to the multi-task case.
We have updated our draft to consolidate our results for the multi-task setting and now show the following.
\textbf{(1)} We show that \textit{as long as the output dim. of the shared layer $B$ is smaller than the  number of tasks, the variance of the MTL estimator for the target task reduces compared to STL, but the bias increases}.
The idea is similar to the two-task case, and we have included this result in the draft.
\textbf{(2)} For multi-label settings where all tasks have the same features, i.e., $X_i = X$ for any $i$, using Thm 3.6, all of our insights from the two-task case still apply except for covariate shift, which doesn't apply since tasks have the same features.
For task similarity, the more similar the models are, the more variance reduces ($\|{v_t}\|$ closer to 1), which leads to positive transfer as in Prop 3.3.
For sample ratio, the more different the models are, the more bias increases w/ more source task samples, which leads to negative transfer as in Prop 3.4.
%The reason why the issue about covariate shift does not apply to this setting is because the tasks have the same features, hence there is no covariate shift.
%Due to page limit we have not elaborated on this part in the submission, but we will elaborate the theoretical implication of this result for multiple tasks in the next version.

\textbf{Related work: [R1, R2]} We clarify how our work differs from previous work. The closest work to ours is [15], which uses standard concentration bounds to show that when two tasks are similar, MTL ensures positive transfer.
Our paper uses new tools from random matrix theory, and Thm 3.2 doesn't require tasks to be similar.
Our tools allow us to analyze the empirical phenomenon of negative transfer that is challenging otherwise using standard techniques.
In particular, Lemma 3.1 characterizes the variance of MTL w/ covariate shift, which may be of independent interest.

\textbf{Writing: [R2, R3]}
We have corrected the typos that R2 pointed out and added more explanation for the questions that R3 asked.
\textbf{L112-118}: We use $t$ to denote the number of tasks, hence for two tasks $t = 2$.
%We have improved this part to be more consistent in the use of $t$.
\textbf{L108}: We only need the validation set to be larger than the size of the hidden layer times the number of tasks $t$, which is very small compared to the training set size.
\textbf{L113}: For the prediction loss, the expectation is over a test sample $x$ whose label is $x^{\top}\beta_t$.
Taking an expectation over $\varepsilon$ gives the bias-variance decomposition, following standard linear regression literature [17,18].

\textbf{R1:}
%\textbf{($\blackdiamond$)} We thank R1 for suggesting we look at qualitative predictions of Thm 3.6 for multiple tasks, which we have added in the  draft.
%As mentioned above, we have extended our result to show that the variance of the MTL estimator decreases compared to STL and the bias increases.
%Hence the bias-variance tradeoff still decides whether MTL outperforms STL.
%Task similarity appears in the projection term $\|{v_t}\|$.
\textbf{($\blackdiamond$)} R1 asks how our method compares to loss reweighing.
Our approach is equivalent to increasing task weight until performance drops.
An advantage of our approach is that we only train over a subset of samples, whereas loss reweighting uses the full set.
\textbf{($\blackdiamond$)} We thank R1 for pointing out the vague use of ``similar performance'' in experiments, which we replaced w/ (comparable) accuracy numbers. %it means that the average accuracy of our method for all tasks is the same as the average accuracy of standard MTL training (cf. line 295).
\textbf{($\blackdiamond$)} The setting of 5 tasks w/o TREC: For any set of 5 tasks w/o one task, the result is qualitatively similar.
\textbf{($\blackdiamond$)} R1 suggests computing similarity via distance between layer parameters,
which we tried (along w/ SVCCA) as a proxy for task similarity.
It didn't perform as well as our result in Table 1.
%Thus, our method achieves the same average accuracy as MTL with less computation.
%We will clarify this part in the next version.


%Our intuition is that while the distance captures model difference we also need to carefully control task data.
%The lesson that we have learned from our work is that in order to decide whether MTL outperforms STL, all three properties including model similarity, sample ratio, and covariate shift provably matter.
%Thus, in order to define such a similarity measure, one has to carefully control the impact of the other two properties in addition to measuring the distance between parameters. We will include this discussion in the next version.
%
%R1 asks "how do the bounds differ compared to previous theory".

\vspace{-0.02in}
\textbf{R2:}
We thank R2 for bringing up the confusion about the sample size regime our theory/algorithm applies. We have clarified this in the draft.
%Regarding R2's comments on motivation and theoretical limitation, as stated above, we have \textit{extended our result on bias-variance tradeoff to multiple tasks}. While our setting is different from traditional MTL that measures average performance over all tasks, it is also a common setting in practice where MTL is used.
\textbf{(1)} R2 is correct that ``our theory applies when the sample sizes are 10-100x of feature dim''.
We think this is a practical regime to consider.
For example, in our sentiment analysis experiment, the feature dim. of a sentence is 300, while the training set size ranges from 3k to 15k.
%\textbf{(2)} As shown in Prop. 3.4, the ratio of sample sizes, instead of the sample size of a task, decides whether MTL can help STL.
%Our theory makes no assumption on sample ratio (or imbalance) and applies to reasonable regimes (e.g. Fig 1(b,c)).
\textbf{(2)} R2 refers to having an imbalanced sample size while discussing our approach. We will clarify the writing after Thm 3.2.
But our incremental training method does not assume that the tasks have imbalanced sample sizes. For example, in our sentiment analysis experiment, we have observed that our approach can help where the source/target sample ratio is $\le 1$.
One practical takeaway of this work is that the level of ``imbalance'' depends on task similarity, and it can be provably small, e.g., $\le 1$ cf. Prop 3.4.

\vspace{-0.06in}
We thank R2 for pointing out the connection between our incremental training method and curriculum learning.
We are not aware of any previous work suggesting adding training data for MTL progressively while having a strong theoretical basis.
More broadly, there is an ongoing discussion of how much data from each task the model should be trained on (cf. Google T5 and its references).
We have focused on evaluating training efficiency since our primary goal is to build/validate the foundation.
It's conceivable that combining our method w/ other ideas could improve the accuracy of predicting the target task.
As R1 also suggested, we think this is an excellent direction for future research.
%R2 asks about why "the middle part btw negative/positive transfer cannot be predicted correctly".
%\textbf{(1)} We emphasize that this experiment only uses STL performance without any MTL training.
%\textbf{(2)} The middle part where the STL performances between tasks are close
%One application of such a result is as a test diagnostic for task selection
%it should not be taken for granted that STL information can predict MTL performance at all!
%Before this work, we are not aware of any previous work that

\vspace{-0.02in}
\textbf{R3:}
We thank R3 for the comments.
\textbf{L220:} Value of our theory to our algorithm:
For two tasks, we can show that our algorithm provably finds the optimal sample ratio. As shown in Fig 1b, the performance curve, which is a quadratic function, has a single peak, and our algorithm stops at the peak point.
We provide a proof that the curve is quadratic, and highlighted the connection in our draft.
\textbf{L187:} Parameter of the metric: One can recover the entire precision-recall curve by varying it.
\textbf{L108:}
%The validation set size $p^{0.99}$ is just an example and can be set smaller.
As we stated, a validation set ... larger than $r\cdot t$ suffices and
we will clarify further. Replacing $p^{0.99}$ w/ $p^{0.5}$ fixes the issue.
%\textbf{L113:} The use of $t$: We have used $t$ to denote the number of tasks as well as the index of the target task.
\textbf{L117:} The sample covariance of task 1 is  $X_1^{\top}X_1 / n_1$, not $\Sigma_1$.
This is the prediction loss for the target task and our main results in Section 3 will relate the sample covariance of task 1 to the \textit{population} covariance $\Sigma_1$.

\end{document}
