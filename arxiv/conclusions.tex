\section{Conclusions and Discussions}\label{sec_conclude}

This work proposed a formal study of a widely used hard paramaeter sharing approach in high-dimensional linear regression, where the sample size and feature dimension increase at a fixed ratio.
We provided tight generalization bounds that scale with dataset properties such as their sample sizes and covariate shifts.
Based on these bounds, we analyzed the impact of varying sample sizes and covariate shifts on the prediction loss of hard parameter sharing.
Our work rigorously explains several empirical phenomena such as sample efficiency and negative transfer related to these dataset properties.
We validated our results and conducted further studies on a real world classification task.

We describe several open questions for future work.
First, our bound in Corollary \ref{cor_MTL_loss} involves an error term that scales down with $n_1$.
Tightening this error bound requires showing the limit of $\normFro{({Z^{(1)}}^{\top} Z^{(1)} + {Z^{(2)}}^{\top} Z^{(2)})^{-1} {Z^{(1)}}^{\top} Z^{(1)}}^2$ for two random matrices $Z^{(1)}, Z^{(2)}$ with isotropic covariances.
This requires studying the asymptotic distribution of the {singular values} of the non-symmetric matrix $({Z^{(1)}}^{\top} Z^{(1)})^{-1}{Z^{(2)}}^{\top} Z^{(2)} + 1$, which is still an open problem in random matrix theory. The eigenvalues distribution of this matrix have been obtained in \cite{Fmatrix}, which might be relevant. 
%but its singular values will follow a different distribution since the matrix is not symmetric.
 %might require new techniques beyond the current ones in random matrix theory .%\HZ{to add}.
Second, it would be interesting to extend our results to classification problems.
Several recent work have made remarkable progress in the (one-sample) high-dimensional logistic regression setting \cite{sur2019modern}.
An interesting question is to study hard parameter sharing in (multiple-sample) high-dimensional logistitic regression.
