\section{Proof of Corollary \ref{cor_MTL_loss}}\label{app_iso_cov}

We follow a similar logic to the proof of Theorem \ref{thm_many_tasks}.
We first characterize the global minimizer of $f(A, B)$ in the random-effect model.
Based on the characterization, we reduce the prediction loss of hard parameter sharing to the bias-variance asymptotic limits.
Finally, we prove Corollary \ref{cor_MTL_loss} based on these limiting estimates.
We set up several notations.
In the two-task case, the optimization objective $f(A, B)$ is equal to
	\begin{align}
		f(A, B) =   \bignorm{X^{(1)} B A_1 - Y^{(1)}}^2+ \bignorm{X^{(2)} B A_2 - Y^{(2)}}^2, \label{eq_mtl_2task_cov}
	\end{align}
	where $B \in \real^{p}$ and $A = [A_1, A_2]\in \real^{2}$ because the width of $B$ is one.
Without loss of generality, we assume that $A_1$ and $A_2$ are both nonzero.
Otherwise, the problem reduces to STL. %
Using the local optimality condition $\frac{\partial f}{\partial B} = 0$, we obtain that $\hat{B}$ satisfies the following
	\begin{align}
		\hat{B} \define  \left[A_1^2 (X^{(1)})^{\top}X^{(1)} + A_2^2 (X^{(2)})^{\top}X^{(2)}\right]^{-1} \left[A_1 (X^{(1))})^{\top}Y^{(1)} + A_2 (X^{(2)})^{\top}Y^{(2)}\right]. \label{eq_Bhat_2task} %
	\end{align}
We denote $\hat \Sigma(x)= x^2 (X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}$.
Applying $\hat B$ to equation \eqref{eq_mtl_2task_cov}, we obtain an objective that only depends on $x:=A_1/A_2$ as follows %
 \begin{align}
		 g(x) \define & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
			& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2 \nonumber\\
		  & + \left\| X^{(2)} \hat \Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (x\beta^{(1)}-x^2\beta^{(2)})+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)\epsilon^{(2)} \right. \nonumber\\
		  &\left. + x X^{(2)}\hat \Sigma(x)^{-1} (X^{(1)})^\top \epsilon^{(1)} \right\|^2. \label{eq_mtl_A12}
	\end{align}
We have that the conditional expectation of $g(x)$ over $\epsilon^{(1)}$ and $\epsilon^{(2)}$ is
\begin{align*}
		&\exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(x) \mid X_1, X_2 ,\beta^{(1)},\beta^{(2)}} \\
		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)}) +\sigma^2(n_1+n_2-p).
\end{align*}
The calculation is tedious but rather straightforward, so we leave the details to the reader.
In the random-effect model, recall that the entries of $(\beta^{(1)}- x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $p^{-1}[(x-1)^2\kappa^2 +(1+x^2) d^2]$.
Hence, by further taking expectation over $\beta^{(1)}$ and $\beta^{(2)}$, we obtain
\begin{align}
	&\ex{g(x) \mid X_1, X_2} \nonumber\\
	=& [(x-1)^2\kappa^2 + (x^2+1)d^2]\cdot {p^{-1}} \tr\left[ (X^{(1)})^\top X^{(1)} \hat \Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right]+\sigma^2(n_1+n_2-p), \label{EgA}
\end{align}



\paragraph{Part $1$: characterizing the global minimum of $f(A, B)$.}
Let $\hat{x}$ denote the global minimizer of $g(x)$.
We show that in the setting of Corollary \ref{cor_MTL_loss}, $\hat x$ is close to 1.
This gives us the global minimum of $f(A, B)$, since $\hat{B}$ is given by $\hat{x}$ using local optimality conditions.
First, we show that $g(x)$ and its expectation are close using standard concentration bounds.
\begin{claim}\label{claim_largedev1}
 In the setting of Corollary \ref{cor_MTL_loss}, for any $x$, we have that with high probability
$$\left| g(x) - \ex{g(x) \mid X^{(1)}, X^{(2)} }\right| \le p^{1/2+c} \left(\sigma^2 + \kappa^2+d^2 \right).$$
\end{claim}
\begin{proof}
 There are two terms in $g(A)$ from equation \eqref{eq_mtl_A12}.
 We will focus on dealing with the concentration error of the first term. The second term is similar to the first and we omit the details.
 For the first term, we expand into several equations under various situations involving the random noise and the random-effect model.
 \begin{align}
 & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
		& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2=h_1(x) + h_2(x) +h_3(x) + 2h_4(x) + 2h_5(x) + 2h_6(x), \label{expand_6}
 \end{align}
 where
 \begin{align*}
&h_1(x):=  (\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  ,\\
& h_2(x) := {(\epsilon^{(1)})^\top \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2\epsilon^{(1)}}  ,\\
& h_3(x):=  x^2 { (\epsilon^{(2)} )^\top X^{(2)} \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)}} , \\
& h_4(x):=  (\epsilon^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}), \\
&h_5(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}),\\
&h_6(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)}.
\end{align*}
Next, we estimate each term using Lemma \ref{largedeviation} for random variables with bounded moment up to any order.
We first state several facts that will be commonly used in the proof.
By Fact \ref{fact_minv} (ii), we have that w.h.p. the operator norm of $X^{(1)}$ and $X^{(2)}$ are both bounded by $O(\sqrt{n})$.
Furthermore, the operator norm of $\hat\Sigma(x)^{-1}$ is bounded by $(x^2 + 1)^{-1} O(n_1 + n_2) = (x^2 + 1)^{-1} O(p)$.

For $h_1(x)$, using Lemma \ref{largedeviation} and the fact that the entries of $(\beta^{(1)}-x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $b = p^{-1}((x-1)^2\kappa^2 + (x^2+1)d^2)$, we obtain the following estimate w.h.p.
\begin{align}
	&\left|h_1(x) - \exarg{\beta^{(1)},\beta^{(2)}}{h_1(x) \mid X_1, X_2}\right|\nonumber \\
\le& p^{c}\cdot p^{-1} b \cdot \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F \nonumber\\
	\le& p^{c}\cdot p^{-1} b \cdot p^{1/2} \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| \nonumber \\
	\lesssim& p^{1/2+c}\cdot \frac{b}{(x^2+1)^2}\lesssim p^{1/2+c}(\kappa^2 + d^2).\label{eq_hA1}
\end{align}
In the third step we use the operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)^{-1}$. %

For $h_2(x)$ and $h_3(x)$, since the entries of $\epsilon^{(1)},\epsilon^{(2)}\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variances $\sigma^2$, using Lemma \ref{largedeviation}, we obtain w.h.p.
\be\label{eq_hA23}\left|h_2(x) - \exarg{\epsilon^{(1)}}{h_2(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2, \quad \left|h_3(x) - \exarg{\epsilon^{(2)}}{h_3(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2.\ee
For $h_4(x)$, using Lemma \ref{largedeviation}, we obtain w.h.p.:
\begin{align}
			\left|h_4(x)\right|
	\le& p^{c}\cdot \sigma \cdot \sqrt{b / p}\left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F\nonumber \\
	\le& p^{c}\cdot \sigma \sqrt{b / p} \cdot p^{1/2} \left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
	\le&  p^{c}\cdot \sigma \sqrt{b} \cdot \left\|  x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top - \id_{n_1\times n_1} \right\| \cdot \left\|X^{(1)}\right\| \cdot \left\|\hat\Sigma(x)^{-1}\right\| \cdot \left\| (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
	\lesssim& p^{1/2+c}\frac{\sigma \sqrt{b}}{x^2+1} \lesssim p^{1/2+c}(\sigma^2 + \kappa^2 + d^2).\label{eq_hA4}
\end{align}
Above, in the fourth step we use the operator norm of $x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top - \id$ being at most one and the operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)^{-1}$.
In the last step we use AM-GM inequality. Using the same argument, we can show that
$\left|h_5(x)\right| \le p^{1/2+c}(\sigma^2 + \kappa^2 + d^2)$ and $\left|h_6(x)\right| \le p^{1/2+c}\sigma^2$.
Combining the concentration error bound for $h_1(x), h_2(x), \dots, h_6(x)$, we complete the proof.
The second term of $g(A)$ can be dealt in similar ways and we omit the details.
\end{proof}

Next, we show that the global minimizer $\hat x$ of $g(x)$ is close to 1.
\begin{claim}\label{lem_hat_v}
	Let $c$ be a sufficiently small fixed constant.
	In the setting of Corollary \ref{cor_MTL_loss}, we have that with high probability,
	\be\label{hatw_add1}|\hat x -1|\le  \frac{2d^2}{\kappa^2} + p^{-1/4+c}.
	\ee
\end{claim}

\begin{proof}
Corresponding to equation \eqref{EgA}, we define the function
\begin{align*}
	h(x)  =& [(x-1)^2\kappa^2 + (x^2+1)d^2] \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right] \\
				=& [(1-x^{-1})^2\kappa^2 + (1+x^{-2})d^2] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].
\end{align*}
Let $x^{\star}$ denote the global minimizer of $h(x)$.
Our proof involves two steps.
First, we  show that $\abs{x^{\star} - 1} \le d^2 / \kappa^2$.
Second, we use Claim \ref{claim_largedev1} to show that the global minimizer of $g(x)$ and $h(x)$ are close to each other.

For the first step, it is easy to observe that $h(x)< h(-x)$ for any positive $x$.
Hence the minimum of $h(x)$ is achieved when $x$ is positive.
Next, we consider the case where $x\ge 1$.
Notice that the following function always increases when $x$ increases in the positive orthant:
$$\tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]$$
By taking the derivative of $h(x)$, we obtain that for any $x > 1+d^2/\kappa^2$,
\begin{align}\label{h'(A)1}
h'(x) \ge \left[2(1-x^{-1})\frac{\kappa^2}{x^2} - 2\frac{d^2}{x^3}\right] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] > 0,
\end{align}
Finally, we consider the case where $x \le 1$. Notice that the following function always decreases when $x$ decreases from $1$:
$$\tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} +  [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].$$
Hence, by taking derivative of $h(x)$, we obtain that for any $x\le 1-d^2/\kappa^2$,
\begin{align}\label{h'(A)2}
	h'(x)\le [-2(1-x)\kappa^2 +2 xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] < 0.
\end{align}
In summary, the global minimizer of $h(x)$ lies within $1-d^2/\kappa^2$ and $1+d^2/\kappa^2$.





For the second step, using Claim \ref{claim_largedev1}, we have that $g(x)$ and $h(x)$ differ by at most $p^{1/2 + \e}(\sigma^2 + \kappa^2 + d^2)$.
Therefore, our goal reduces to showing that if $\hat{x}$ deviates too far from $1 \pm d^2 / \kappa^2$, it is no longer a global minimum of $g(x)$.
We prove by contradiction.
First, suppose that $\hat{x} \ge 1 + 2d^2 /\kappa^2 + p^{-1/2 + \e}$.
For any $x \ge 1+3d^2/(2\kappa^2)$, we can lower bound the derivative of $h(x)$ using equation \eqref{h'(A)1} as follows
	\[ h'(x) \ge \frac{2(x-1)\kappa^2 - 2d^2}{x^3} \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]
	\gtrsim p\kappa^2\cdot \frac{x-1}{x (1 + x^2)}. \]
Therefore, the difference between $h(x)$ and $h(1)$ is at least the following
\begin{align*}
		h(x)-h(1) \ge h(x) - h\left(1+\frac{3d^2}{2\kappa^2}\right)
	\ge \int_{1+\frac{3d^2}{2\kappa^2}}^x h'(x)\dd x \gtrsim p\kappa^2 \cdot \int_{1 + \frac{3d^2}{2\kappa^2}}^x \frac{x-1}{x(1 + x^2)} \dd x.
\end{align*}
When $\hat{x}$ is sufficiently far from $1$ (e.g. $2d^2 / \kappa^2 + p^{-1/4+\e}$), one can verify that $h(x) - h(1)$ is at least $\OO(p\frac{d^4}{\kappa^2} + p^{1/2+2c} \kappa^2) > \OO(p^{1/2 + c} (\sigma^2 + \kappa^2 + d^2))$, under the condition that $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$.
On the other hand, by triangle inequality and Claim \ref{claim_largedev1} we have that
\begin{align*}
	h(\hat{x}) - h(1) = g(\hat{x}) - g(1) + (h(\hat{x}) - g(\hat{x})) + (g(1) - h(1))
	\le \OO\left(p^{1/2+c}\left(\sigma^2 +\kappa^2+d^2 \right)\right).
\end{align*}
Hence, we have arrived at a contradition.


Second, suppose that $\hat{x} \le 1 - 2d^2/ \kappa^2 - p^{-1/2 + \e}$.
Using equation \eqref{h'(A)2}, we obtain that for any $x \le 1 - 3d^2/(2\kappa^2)$,
	\[ -h'(x)\ge  [2(1-x)\kappa^2 - 2xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]  \gtrsim p\kappa^2 \cdot \frac{(1 - x)x^2}{1 + x^2}. \]
Using a similar argument to the first case, we get that the difference between $h(x)$ and $h(1)$ is at least the integral of the abvoe derivative.
This implies that $\hat{x}$ cannot be too far from one.
Hence we have completed the proof.
\end{proof}

\paragraph{Part $2$: a reduction to the bias and variance limits.}
Recall that the hard parameter sharing estimator $\hat{\beta}_2^{\MTL}$ is equal to $\hat{B} \hat{A}_2$.
Using the local optimality condition for $\hat{B}$, we obtain the predication loss of HPS:
\begin{align}
L(\hat{\beta}_2^{\MTL}) &=\left\|(\Sigma^{(2)})^{1/2} \left( \hat B \hat A_2 - \beta^{(2)}\right)\right\| \nonumber\\
&=  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x\beta^{(1)}-\hat x^2\beta^{(2)})+ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2.\label{Lbeta_HPS}
\end{align}
Using Lemma \ref{lem_hat_v} and the concentration estimates in Lemma \ref{largedeviation}, we simplify $L(\hat{\beta}_2^{\MTL})$ as follows.

\begin{claim}\label{claim_reduction}
Recall that $\hat \Sigma(1)$ is equal to $\hat \Sigma$ (cf. equation \eqref{def hatsig}).
In the setting of Claim \ref{cor_MTL_loss}, we have the following estimate w.h.p.
\begin{align*}
&\left|L(\hat{\beta}_2^{\MTL}) - \frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \bigtr{{\hat \Sigma^{-1}  }}\right| \nonumber\\
\lesssim&  \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2).
\end{align*}
\end{claim}
\begin{proof}
Our proof is divided into two steps. First, using Lemma \ref{largedeviation}, we show that
\be\label{claim_largedev2} \left| L(\hat{\beta}_2^{\MTL})- \cal L(\hat x)\right| \le p^{-1/2+c} \left(\sigma^2 +\kappa^2 +d^2 \right),
\ee
where $\cal L(\hat x)$ is defined as
\begin{align*}
	\resizebox{0.97\hsize}{!}{%
	$\cal L(\hat x)	:=  \hat x^2\left[(\hat x-1)^2 \kappa^2 +  (\hat x^2+1)d^2 \right] \cdot p^{-1}\bigtr{ (X^{(1)})^{\top}X^{(1)} \hat\Sigma(\hat x)^{-1} \Sigma^{(2)} \hat\Sigma(\hat x)^{-1} (X^{(1)})^{\top}X^{(1)} }
	 +\sigma^2\cdot \bigtr{\Sigma^{(2)}\hat \Sigma(\hat x)^{-1}}.$
	}%
\end{align*}
Next, we further simplify $\cal L(\hat x)$ since $\hat{x}$ is close to one and $\Sigma^{(1)},\Sigma^{(2)}$ are both isotropic
\begin{align}
	&\left|\cal L(\hat x)- \frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \bigtr{\hat \Sigma^{-1}}\right| \nonumber\\
\lesssim & \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2). \label{simple12}
\end{align}
Combining equation \eqref{claim_largedev2} and \eqref{simple12}, we obtain the desired claim.
We prove these two equations one by one as follows.

First, we prove equation \eqref{simple12}.
We can bound the left hand side of equation \eqref{simple12} as
\begin{align*}
	&\left|\cal L(\hat x)-\frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left({\hat\Sigma^{-1}  }\right)\right| \\
	\lesssim &\left(|\hat x-1|^2 \kappa^2 + |\hat x-1|d^2\right)\cdot p^{-1}\bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2 } \\
	&+ \frac{d^2}{p}\left|\tr\left[\left(\hat\Sigma(\hat x)^{-2}-\hat\Sigma^{-2}\right) \left((X^{(1)})^\top X^{(1)} \right)^2\right]\right|+ \sigma^2  \left|\bigtr{\hat\Sigma(\hat x)^{-1}-\hat\Sigma^{-1}  }\right|.
\end{align*}
We deal with the trace terms in the above equation one by one.
Using Claim \ref{lem_hat_v} and operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)$, we have that w.h.p.
\begin{align}
	\|\hat\Sigma^{-1}-\hat\Sigma(\hat x)^{-1}\| &\le |\hat x^2-1| \cdot \|\hat\Sigma^{-1}\| \cdot \| (X^{(1)})^\top X^{(1)}\| \cdot \|\hat\Sigma(\hat x)^{-1}\|  \lesssim p^{-1}\left(\frac{d^2}{\kappa^2} + p^{-1/4+c}\right).\label{est111}
\end{align}
Using similar arguments, we get that w.h.p.
\begin{align}\label{est222}
&\left\|\left(\hat\Sigma^{-2}-\hat\Sigma(\hat x)^{-2}\right)\left((X^{(1)})^\top X^{(1)} \right)^2\right\| \lesssim  \frac{d^2}{\kappa^2} + p^{-1/4+c},
\end{align}
and
\begin{align}
& \bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}  \le p \bignorm{\hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}\lesssim p.\label{est333}
\end{align}
Applying the above results \eqref{est111}, \eqref{est222}, and \eqref{est333} to the bound of $\cL(\hat{x})$ above, we have shown that equation \eqref{simple12} holds.

Second, we prove equation \eqref{claim_largedev2}.
The proof is very similar to Claim \ref{claim_largedev1}.
The key difference is that $\hat x$ correlates with $\epsilon^{(1)}$, $\epsilon^{(2)}$, $\beta^{(1)}$, and $\beta^{(2)}$.
Nevertheless, Lemma \ref{largedeviation} still applies for any arbitrary $\hat x$.
We describe a proof sketch and omit the details.
Recall that $\beta_0$ is the shared component of $\beta^{(1)}$ and $\beta^{(2)}$ with i.i.d. Gaussian entries of mean zero and variance $p^{-1}\kappa^2$.
The task-specific components, denoted by $\wt\beta^{(1)}$ and $\wt\beta^{(2)}$, consist of i.i.d. Gaussian random variables with mean zero and variance $p^{-1} d^2$.
We write $L(\hat{\beta}_2^{\MTL}) $ from equation \eqref{Lbeta_HPS} as:
\begin{align}
L(\hat{\beta}_2^{\MTL})  =&  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x -\hat x^2)\beta_0+(X^{(1)})^\top X^{(1)} \hat x\wt \beta^{(1)} - (X^{(1)})^\top X^{(1)}  \hat x^2\wt \beta^{(2)} \right] \right. \nonumber\\
&\left. + (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1}\left[ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2. \label{expand_15}
\end{align}
Similar to the analysis of $g(x)$, we expand $L(\hat{\beta}_2^{\MTL})$ into the sum of 15 terms, and bound the concentration error of each term similar to $h_1(x), \dots, h_6(x)$.
For example, for the leading term 
\[\hat x^2 (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)},\] 
using Lemma \ref{largedeviation} and the operator norm bounds, we obtain the following estimate w.h.p.
\begin{align*}
&  \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)} - \frac{d^2}{p}\bigtr{(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}}\right| \\
&\le p^{-1+c}d^2 \cdot \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|_F \\
&\le p^{-1/2+c}d^2 \cdot  \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|  \lesssim p^{-1/2+c}d^2.
\end{align*} 
For the cross term $\hat x (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}$, using Lemma \ref{largedeviation} and the operator norm bounds, we obtain the following estimate w.h.p.
\begin{align*}
& \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}\right|  \\
  \le & p^c \cdot \sigma\sqrt{p^{-1}d^2} \cdot \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \|_F \\
 \lesssim & p^{c}\cdot \sigma d \cdot  \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \| \\
 \lesssim & p^{-1/2+c}\sigma d\le p^{-1/2+c}(\sigma^2+ d^2).
\end{align*}
The rest of the terms in the expansion of $L(\hat{\beta}_2^{\MTL})$ can be dealt with similarly, and we omit the details.
 \end{proof}
  
\paragraph{Part $3$: applying the bias-variance limits.}
Finally, we are ready to complete the proof of Corollary \ref{cor_MTL_loss}.
We derive the variance term $\sigma^2  \tr[\hat\Sigma^{-1}]$ and the bias term $\frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right]$ using our random matrix theory results.

\begin{proof}[Proof of Corollary \ref{cor_MTL_loss}]
For the variance term, using equation \eqref{lem_cov_shift_eq}, we obtain that
\be\label{eq_var111}\tr[\hat\Sigma^{-1}] = \tr\left[ \left((X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}\right)^{-1}\right]=\bigtr{\frac{(a_1 +a_2)^{-1}\id_{p\times p}}{n_{1}+n_2} }+\OO(p^{-c_\varphi})\ee
with high probability. Solving equation \eqref{eq_a12extra} with $\lambda_i\equiv 1$, $1\le i\le p$, we get that  
	\begin{align}
		 a_1 = \frac{n_1(n_1 + n_2 - p)}{(n_1 + n_2)^2} ,\quad
		& a_2 = \frac{n_2(n_1 + n_2 - p)}{(n_1 +n_2)^2} . \label{simplesovlea12}
			\end{align}
Applying the above to equation \eqref{eq_var111}, we obtain that with high probability
\be\label{eq_var112}\tr[\hat\Sigma^{-1}]  = \frac{p}{n_1+n_2} \cdot \frac{n_1+n_2}{n_1+n_2-p}+\OO(p^{-c_\varphi})=  \frac{p}{n_1+n_2-p}+\OO(p^{-c_\varphi}).\ee
For the bias term, since the spectrum of $(X^{(1)})^{\top} X^{(1)}$ is tightly concentrated by Fact \ref{fact_minv}, we have that
\begin{align}
 \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi})}{p} \bigtr{\hat \Sigma^{-2}}  &\le {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] \label{eq_bias111}\\
 &\le \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi})}{p} \bigtr{\hat\Sigma^{-2}}.\nonumber
\end{align}
Using the bias limit \eqref{lem_cov_derv_eq} with $\Sigma^{(1)}=\Sigma^{(2)}=\Lambda=V=\id_{p\times p}$, and $w = e_i$ (the $i$-th coordinate vector), we have w.h.p. (via a union bound)
\begin{align*}%
 e_i^{\top}\hat\Sigma^{-2} e_i = \frac{1}{(n_1+n_2)^2} \left[\frac{a_3+a_4+1}{(a_1+a_2)^2} +\OO(p^{-c_\varphi})\right], \text{ for all } i = 1, 2, \dots, p.
\end{align*}
We solve the self-consistent equations \eqref{eq_a34extra} given $a_1, a_2$, and obtain
\begin{align*}
		a_3 = \frac{p\cdot n_1}{(n_1 +n_2)(n_1 +n_2 - p)}, \quad
		&  a_4 = \frac{p\cdot n_2}{(n_1 + n_2)(n_1 + n_2 - p)}. %
\end{align*}
Applying $a_3, a_4$ to the equation above, we obtain
\begin{align*}%
 e_i^{\top}\hat\Sigma^{-2}e_i =  \frac{1}{(n_1+n_2)^2} \left[  \frac{ (n_1+n_2)^3 }{(n_1+n_2-p)^3} +\OO(p^{-c_\varphi})\right], \text{ for all } i = 1, 2, \dots, p.
\end{align*}
Applying the above result to equation \eqref{eq_bias111}, we get the desired result for the bias term.
Combining the bias and variance estimates, we get that
\begin{align*}
	 \left|L(\hat{\beta}_2^{\MTL})  -  \frac{2d^2 n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3} - \frac{\sigma^2 p}{n_1+n_2-p}   \right|&
	  \le   \left[\left( 1+\sqrt{\frac{p}{n_1}}\right)^4-1\right] \cdot \frac{2d^2n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3} \\
	&\hspace{-2cm}+\OO\left( p^{-c_\varphi} (\sigma^2 + d^2)+ \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2) \right).
\end{align*}
Since $\sigma^2 \lesssim  \kappa^2$ and $d^2 \le p^{-\e}{\kappa^2}$ by our assumption, we obtain the desired result.
The proof is complete.
\end{proof}
