\section{Proof of Corollary \ref{cor_MTL_loss}}\label{app_iso_cov}

We follow a similar logic to the proof of Theorem \ref{thm_many_tasks}.
We first characterize the global minimizer of $f(A, B)$ in the random-effect model.
Based on the characterization, we reduce the prediction loss of hard parameter sharing to the bias-variance asymptotic limits.
Finally, we prove Corollary \ref{cor_MTL_loss} based on these limiting estimates.
We set up several notations.
In the two-task case, the optimization objective $f(A, B)$ is equal to
	\begin{align}
		f(A, B) =   \bignorm{X^{(1)} B A_1 - Y^{(1)}}^2+ \bignorm{X^{(2)} B A_2 - Y^{(2)}}^2, \label{eq_mtl_2task_cov}
	\end{align}
	where $B \in \real^{p}$ and $A = [A_1, A_2]\in \real^{2}$ because the width of $B$ is one.
Without loss of generality, we assume that $A_1$ and $A_2$ are both nonzero.
Otherwise, the problem reduces to STL. % we add a tiny amount of perturbation $\delta$ to them and the result remains the same.
Using the local optimality condition $\frac{\partial f}{\partial B} = 0$, we obtain that $\hat{B}$ satisfies the following
	\begin{align}
		\hat{B} \define  \left[A_1^2 (X^{(1)})^{\top}X^{(1)} + A_2^2 (X^{(2)})^{\top}X^{(2)}\right]^{-1} \left[A_1 (X^{(1))})^{\top}Y^{(1)} + A_2 (X^{(2)})^{\top}Y^{(2)}\right]. \label{eq_Bhat_2task} %\\
		%&= (B^\star A ^{\top}) (A A^{\top})^{-1} + (X^{\top}X)^{-1}X^{\top}   \bigbrace{\sum_{j=1}^t \varepsilon_i A_i^{\top}} (A  A^{\top})^{-1}.
	\end{align}
We denote $\hat \Sigma(x)= x^2 (X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}$.
Applying $\hat B$ to equation \eqref{eq_mtl_2task_cov}, we obtain an objective that only depends on $x:=A_1/A_2$ as follows %\HZ{$A$ has been used to denote the output layers. Could you replace $A$ with another symbol (say $x$)?}
 \begin{align}
		 g(x) \define & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
			& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2 \nonumber\\
		  & + \left\| X^{(2)} \hat \Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (x\beta^{(1)}-x^2\beta^{(2)})+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)\epsilon^{(2)} \right. \nonumber\\
		  &\left. + x X^{(2)}\hat \Sigma(x)^{-1} (X^{(1)})^\top \epsilon^{(1)} \right\|^2. \label{eq_mtl_A12}
	\end{align}
We have that the conditional expectation of $g(x)$ over $\epsilon^{(1)}$ and $\epsilon^{(2)}$ is
\begin{align*}
		&\exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(x) \mid X_1, X_2 ,\beta^{(1)},\beta^{(2)}} \\
%		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  \\
%		&+ \sigma^2\tr\left[ \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2 \right]\\
%&+ 2 x^2 \sigma^2 \tr\left[ \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right]  \nonumber\\
		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)}) +\sigma^2(n_1+n_2-p).
\end{align*}
The calculation is tedious but rather straightforward, so we leave the details to the reader.
In the random-effect model, recall that the entries of $(\beta^{(1)}- x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $p^{-1}[(x-1)^2\kappa^2 +(1+x^2) d^2]$.
Hence, by further taking expectation over $\beta^{(1)}$ and $\beta^{(2)}$, we obtain
\begin{align}
	&\ex{g(x) \mid X_1, X_2} \nonumber\\
%&=[(x-1)^2\kappa^2 + (x^2+1)d^2/2] \cdot p^{-1}\tr\left[  (X^{(2)})^\top X^{(2)} \hat\Sigma(x)^{-1}(X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)}\right] \nonumber\\
%&+ A^2[(x-1)^2\kappa^2 + (x^2+1)d^2/2] \cdot p^{-1}\tr\left[  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)}\right] +\sigma^2(n_1+n_2-p) \nonumber\\
	=& ((x-1)^2\kappa^2 + (x^2+1)d^2){p^{-1}} \tr\left[ (X^{(1)})^\top X^{(1)} \hat \Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right]+\sigma^2(n_1+n_2-p), \label{EgA}
\end{align}

%\HZ{could you align the Eq so that they look better on page?}
%\begin{align}
%\exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(x) \mid X_1, X_2 ,\beta^{(1)},\beta^{(2)}}
%&= \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\right\|^2 \nonumber\\
%&+x^2 \left\| X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\right\|^2  \nonumber\\
%& + \exarg{\epsilon^{(1)}}{(\epsilon^{(1)})^\top \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2\epsilon^{(1)}} \nonumber\\
%&+\exarg{ \epsilon^{(2)}}{(\epsilon^{(2)})^\top \left(X^{(2)}\hat \Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2\epsilon^{(2)}}   \nonumber\\
%&+  x^2 \exarg{ \epsilon^{(2)}}{ (\epsilon^{(2)} )^\top X^{(2)}\hat \Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)}} \nonumber\\
%& +x^2 \exarg{ \epsilon^{(1)}} { (\epsilon^{(1)})^\top X^{(1)}\hat \Sigma(x)^{-1}  (X^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top \epsilon^{(1)}  }. \label{Eg(x)}
%\end{align}
%Using the following identity
%\begin{align*}
%(X^{(1)})^\top X^{(1)} \hat \Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} & =\left( x^2 [(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1} \right)^{-1} \\
%&= (X^{(2)})^\top X^{(2)} \hat \Sigma(x)^{-1}(X^{(1)})^\top X^{(1)},
%\end{align*}
%we can simplify that
%\begin{align*}
% & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\right\|^2  +x^2 \left\| X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\right\|^2 \\
%=&(\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1} (X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)}  (\beta^{(1)}-x\beta^{(2)})\\
%&+(\beta^{(1)}-x\beta^{(2)})^\top x^2 (X^{(1)})^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)} )^\top X^{(2)}  \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\\
%&=(\beta^{(1)}-x\beta^{(2)})^\top\left[ x^2 (X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}  \right]\hat\Sigma(x)^{-1} (X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\\
%=&(\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)}).
%\end{align*}
%Using this identity, we can simplify equation \eqref{Eg(x)}  to


\paragraph{Part $1$: characterizing the global minimum of $f(A, B)$.}
Let $\hat{x}$ denote the global minimizer of $g(x)$.
We show that in the setting of Corollary \ref{cor_MTL_loss}, $\hat x$ is close to 1.
This gives us the global minimum of $f(A, B)$, since $\hat{B}$ is given by $\hat{x}$ using local optimality conditions.
First, we show that $g(x)$ and $\ex{g(x) \mid X^{(1)}, X^{(2)}}$ are close using standard concentration bounds.
\begin{claim}\label{claim_largedev1}
 In the setting of Corollary \ref{cor_MTL_loss}, for any $x = O(1)$, we have that with high probability
$$\left| g(x) - \ex{g(x) \mid X^{(1)}, X^{(2)} }\right| \le p^{1/2+c} \left(\sigma^2 + \kappa^2+d^2 \right).$$
\end{claim}
\begin{proof}
 There are two terms in $g(A)$ from equation \eqref{eq_mtl_A12}.
 We will focus on dealing with the concentration error of the first term. The second term is similar to the first and we omit the details.
 For the first term, we expand into several equations under various situations involving the random noise and the random-effect model.
 \begin{align}
 & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
		& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2=h_1(x) + h_2(x) +h_3(x) + 2h_4(x) + 2h_5(x) + 2h_6(x), \label{expand_6}
 \end{align}
 where
 \begin{align*}
&h_1(x):=  (\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  ,\\
& h_2(x) := {(\epsilon^{(1)})^\top \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2\epsilon^{(1)}}  ,\\
& h_3(x):=  x^2 { (\epsilon^{(2)} )^\top X^{(2)} \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)}} , \\
& h_4(x):=  (\epsilon^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}), \\
&h_5(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}),\\
&h_6(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)}.
\end{align*}
Next, we estimate each term using Lemma \ref{largedeviation} for random variables with bounded moment up to any order.
We first state several facts that will be commonly used in the proof.
By Fact \ref{fact_minv} (ii), we have that w.h.p. the operator norm of $X^{(1)}$ and $X^{(2)}$ are both bounded by $O(\sqrt{n})$.
Furthermore, the operator norm of $\hat\Sigma(x)^{-1}$ is bounded by $(x^2 + 1)^{-1} O(n_1 + n_2) = (x^2 + 1) O(p)$.
 % \be\label{op_X12}
%\|X^{(1)}\|\le \sqrt{(\sqrt{n_1} + \sqrt{p})^2 + n_1 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p}, \quad \|X^{(2)}\|\le \sqrt{(\sqrt{n_2} + \sqrt{p})^2 + n_2 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p},
% \ee
% and
% \be\label{op_Sig1}
%\| \hat\Sigma(x)^{-1}\|\le \frac1{x^2[(\sqrt{n_1} - \sqrt{p})^2 - n_1 \cdot p^{-c_{\varphi}}]+[(\sqrt{n_2} - \sqrt{p})^2 - n_2 \cdot p^{-c_{\varphi}}] }\lesssim \frac{1}{(x^2+1)p},
% \ee
%where we used that $3\le n_1/p\le \tau^{-1}$ and $3\le n_2/p\le \tau^{-1}$ for a small constant $\tau>0$.

For $h_1(x)$, using Lemma \ref{largedeviation} and the fact that the entries of $(\beta^{(1)}-x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $b = p^{-1}((x-1)^2\kappa^2 + (x^2+1)d^2)$, we obtain the following estimate w.h.p.
\begin{align}
	&\left|h_1(x) - \exarg{\beta^{(1)},\beta^{(2)}}{h_1(x) \mid X_1, X_2}\right|\nonumber \\
\le& p^{c}\cdot p^{-1} b \cdot \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F \nonumber\\
	\le& p^{c}\cdot p^{-1} b \cdot p^{1/2} \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| \nonumber \\
	\lesssim& p^{1/2+c}\cdot \frac{b}{(x^2+1)^2}\lesssim p^{1/2+c}(\kappa^2 + d^2).\label{eq_hA1}
\end{align}
In the third step we use the operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)^{-1}$. % as follows
%\begin{align*}
%	\left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| & \le \left\| (X^{(2)})^\top X^{(2)}\right\|^2 \cdot \left\| (X^{(1)})^\top X^{(1)}\right\|\cdot \left\| \hat\Sigma(x)^{-1} \right\|^2 \\
%  & \lesssim \frac{p}{(x^2+1)^2}.
%\end{align*}

For $h_2(x)$ and $h_3(x)$, since the entries of $\epsilon^{(1)},\epsilon^{(2)}\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variances $\sigma^2$, using Lemma \ref{largedeviation}, we obtain w.h.p.
\be\label{eq_hA23}\left|h_2(x) - \exarg{\epsilon^{(1)}}{h_2(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2, \quad \left|h_3(x) - \exarg{\epsilon^{(2)}}{h_3(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2.\ee
For $h_4(x)$, using Lemma \ref{largedeviation}, we obtain w.h.p.:
\begin{align}
			&\left|h_4(x)\right| \nonumber\\
	\le& p^{c}\cdot \sigma \cdot \sqrt{b / p}\left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F\nonumber \\
	\le& p^{c}\cdot \sigma \sqrt{b / p} \cdot p^{1/2} \left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
	\le&  p^{c}\cdot \sigma \sqrt{b} \cdot \left\|  x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top - \id_{n_1\times n_1} \right\| \cdot \left\|X^{(1)}\right\| \cdot \left\|\hat\Sigma(x)^{-1}\right\| \cdot \left\| (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
	\lesssim& p^{1/2+c}\frac{\sigma \sqrt{b}}{x^2+1} \lesssim p^{1/2+c}(\sigma^2 + \kappa^2 + d^2).\label{eq_hA4}
\end{align}
Above, in the fourth step we use the operator norm of $x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top - \id$ being at most one and the operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)^{-1}$.
In the last step we use AM-GM inequality. Using the same argument, we can show that
$\left|h_5(x)\right| \le p^{1/2+c}(\sigma^2 + \kappa^2 + d^2)$ and $\left|h_6(x)\right| \le p^{1/2+c}\sigma^2$.
Combining the concentration error bound for $h_1(x), h_2(x), \dots, h_6(x)$, we complete the proof.
The second term of $g(A)$ can be dealt in similar ways and we omit the details.
\end{proof}

Next, we show that the global minimizer $\hat x$ of $g(x)$ is close to 1.
\begin{claim}\label{lem_hat_v}
%Suppose the assumptions of Lemma \ref{prop_model_shift_tight} hold. Assume that $ \kappa^2 \sim pd^2 \sim \sigma^2$ are of the same order.
	Let $c$ be a sufficiently small fixed constant.
	In the setting of Corollary \ref{cor_MTL_loss}, we have that with high probability,
%There exists a constant $C>0$ such that
	\be\label{hatw_add1}|\hat x -1|\le  \frac{2d^2}{\kappa^2} + p^{-1/4+c}.
	\ee
\end{claim}

\begin{proof}
Corresponding to equation \eqref{EgA}, we define the function
\begin{align*}
	h(x)  =& [(x-1)^2\kappa^2 + (x^2+1)d^2] \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right] \\
				=& [(1-x^{-1})^2\kappa^2 + (1+x^{-2})d^2] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].
\end{align*}
Let $x^{\star}$ denote the global minimizer of $h(x)$.
Our proof involves two steps.
First, we will show that $\abs{x^{\star} - 1} \le d^2 / \kappa^2$.
Second, we will use Claim \ref{claim_largedev1} to show that the global minimizer of $g(x)$ and $h(x)$ are close to each other.
%Using Lemma \ref{largedeviation} again, we can simplify equation \eqref{revise_eq_val_mtl} as $\val(v)= N_2h(v) \cdot  \left( 1+\OO(p^{-1/2+\e})\right)$, where the function $ h$ is defined as
%	%We define the function
%	\begin{align}
%		h(v) =& \frac{\rho_1}{\rho_2}\left[d^2 +\left( v-1\right)^2\kappa^2\right]\cdot \tr\left[( v^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%		& +  v^2\left[d^2 +\left( v-1\right)^2\kappa^2\right]\cdot \tr\left[( v^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%		& + \left(\frac{\rho_1}{\rho_2} v^2 + 1\right)\sigma^2 \cdot \bigtr{(v^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} }. \nonumber
%	\end{align}
%	%\cor Under the setting of Lemma \ref{prop_model_shift_tight},
%	Furthermore, the validation loss in equation \eqref{approxvalid} reduces to
%	\be\label{boundv-w}
%		g(v)=\left[N_2 h(v) + (N_1+N_2)\sigma^2\right]\cdot \left( 1+\OO(p^{-(1-\e_0)/2+\e})\right)\quad \text{w.h.p.}
%	\ee
%	for any constant $\e>0$. %\nc Thus for the following discussions, it suffices to focus on the behavior of $h (v)$.
%Let $\hat w$ be the minimizer of $h(v)$. Our proof consists of the following two steps.
%\squishlist
%	\item First, we show that $\hat{w}$ is close to $1$.
%	\item Second, with equation \eqref{boundv-w} we show that $\hat v$ is close to $\hat w$. Then we plug $\hat{v}$ into $\te(\hat{\beta}_2^{\MTL})$ to get equation \eqref{simple1}.
%\squishend
%%For the minimizer $\hat w$ of $\val(w)$, we have a similar result as in Proposition \ref{thm_cov_shift}.
%For the first step, we will prove the following result.
%To be consistent with the notation $\hat w$, we change the name of the argument to $w$ in the proof.

For the first step, it is easy to observe that $h(x)< h(-x)$ for any positive $x$.
Hence the minimum of $h(x)$ is achieved when $x$ is positive.
%$h(1)=\OO(pd^2 + \sigma^2)$ and $ h(w)\gtrsim p\kappa^2 \gg h(1)$ for $w\ge 2$ or $w\le 1/2$.  Hence it suffices to consider the case $1/2\le w\le 2$.
Next, we consider the case where $x\ge 1$.
Notice that the following function always increases when $x$ increases in the positive orthant:
$$\tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]$$
By taking the derivative of $h(x)$, we obtain that for any $x > 1+d^2/\kappa^2$,
\begin{align}\label{h'(A)1}
h'(x) \ge \left[2(1-x^{-1})\frac{\kappa^2}{x^2} - 2\frac{d^2}{x^3}\right] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] > 0,
\end{align}
Finally, we consider the case where $x \le 1$. Notice that the following function always decreases when $x$ decreases from $1$:
$$\tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} +  [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].$$
Hence, by taking derivative of $h(x)$, we obtain that for any $x\le 1-d^2/\kappa^2$,
\begin{align}\label{h'(A)2}
	h'(x)\le [-2(1-x)\kappa^2 +2 xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] < 0,
\end{align}
In summary, the global minimizer of $h(x)$ lies within $1-d^2/\kappa^2$ and $1+d^2/\kappa^2$.

%Taking derivative of $h(w)$, we obtain that
%\begin{align}
%	h'(w) \le& \frac{\rho_1}{\rho_2} \left[2\left( w-1\right) \kappa^2\right]\cdot \tr\left[( w^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%	& +  \left[2wd^2 +2w\left( w-1\right)(2w-1)\kappa^2\right]\cdot \tr\left[( w^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%	& + \frac{\rho_1}{\rho_2}(2 w\sigma^2) \cdot \bigtr{(w^2X_1^{\top}X_1 + X_2^{\top}X_2)^{-2} X_2^\top X_2  }= \frac{\rho_1}{\rho_2} \bigtr{(w^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} \cal B} , \nonumber
%\end{align}
%where the matrix $\cal B$ is
%$$\cal B= 2\left( w-1\right) \kappa^2  (X_2^{\top}X_2)^2+\frac{\rho_2}{\rho_1}\left[2wd^2 +2w\left( w-1\right)(2w-1)\kappa^2\right](X_1^{\top}X_1)^2 + 2 w\sigma^2 X_2^\top X_2 .$$
%Using Fact \ref{lem_minv}, we get that $\cal B$ is upper bounded as
%\begin{align*}
%\cal B \preceq - 2(1-w)\kappa^2 n_2^2 (\al_-(\rho_2) -\oo(1))^2 +2w d^2 n_1n_2 (\al_+(\rho_1) +\oo(1))^2 + 2w\sigma^2 n_2 (\al_+(\rho_2)+\oo(1)) \prec 0,
%\end{align*}
%as long as
%$$w< w_2:=1 -   \frac{d^2}{\kappa^2}\frac{\rho_1(\al_+(\rho_1) +\oo(1))^2}{\rho_2 \al_-^2(\rho_2) } -  \frac{\sigma^2}{n_2\kappa^2}\frac{\al_{+}(\rho_2)+\oo(1)}{\al_{-}^2(\rho_2)} .$$
%Hence $h'(w)<0$ on $[0,w_2)$, %i.e. $h(w)$ is strictly decreasing for $w<w_2$. This
%which gives $\hat w\ge w_2$.

%In sum, we have shown that $w_2\le w\le w_1$. Together with
%$$\max\{|w_1 -1|, |w_2 -1|\} =\OO\left(\frac{d^2}{\kappa^2} + \frac{\sigma^2}{p\kappa^2}\right),$$
%we conclude equation \eqref{hatw_add1}.


%For the rest of this section, we choose parameters that satisfy the following relations: \be\label{choiceofpara}
%pd^2 \sim \sigma^2 \sim 1,\quad p^{-1+c_0} \sigma^2 \le \kappa^2  \le p^{-\e_0-c_0}\sigma^2 ,
%\ee
%for some small constant $c_0>0$.

For the second step, we first argue that $\hat{x}$ is $O(1)$. Hence we can apply Claim \ref{claim_largedev1} to bound the concentration error of $g(x)$.
Using Fact \ref{fact_minv} (ii), we can obtain that with high probability,
\begin{align}
p(x^2+1)^{-1}&\lesssim \left( x^2 \left[(\sqrt{n_2}-\sqrt{p})^{2}-n_2p^{-c_\varphi}\right]^{-1} +\left[(\sqrt{n_1}-\sqrt{p})^{2}-n_1p^{-c_\varphi}\right]^{-1} \right)^{-1} \nonumber\\
&\preceq (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} =\left( x^2 [(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1} \right)^{-1} \nonumber\\
&\preceq \left( x^2 \left[(\sqrt{n_2}+\sqrt{p})^{2}+n_2p^{-c_\varphi}\right]^{-1} +\left[(\sqrt{n_1}+\sqrt{p})^{2}+n_1p^{-c_\varphi}\right]^{-1} \right)^{-1}\lesssim p(x^2+1)^{-1}, \label{op_uplow}
\end{align}
where we also used that $3\le n_1/p \le \tau^{-1}$ and $3\le  n_2/p \le \tau^{-1}$ for a small constant $\tau>0$. Then we get that
\be\label{eq_h1}h(1)= 2d^2 \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(1)^{-1}(X^{(2)})^\top X^{(2)}\right]\lesssim pd^2 .\ee
On the other hand, if $|x-1|\ge \delta$ for a small constant $\delta>0$, then we have
\begin{align}\label{eq_hA}
h(x)\gtrsim (1+x^2)\kappa^2 \cdot p\left( x^2 +1\right)^{-1} =p\kappa^2 ,
\end{align}
where in the first step we used that $|x-1|^2\gtrsim 1+x^2$ for $|x-1|\ge \delta$ and equation \eqref{op_uplow}.
and in the second step we used equation \eqref{choiceofpara0} \HZ{I'm not sure which condition we use? put it here}.
\begin{align}
g(x)&\ge  h(x) +\sigma^2(n_1+n_2-p) -p^{-1/2+c}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right) \nonumber\\
&\ge h(1) +\sigma^2(n_1+n_2-p) + p^{-1/2+c}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right) \ge g(1), \label{gA>g1}
\end{align}
under conditions $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$. %\eqref{choiceofpara0} \HZ{again, I don't see which condition we use here?}.
This gives that
Hence it remains to consider the region $|A-1|\le \delta$ for a small enough constant $\delta>0$.




First, using equations \eqref{op_uplow} and  \eqref{h'(A)1}, we obtain that for $1+3d^2/(2\kappa^2) \le x\le 1+\delta$,
$$h'(x)\ge \frac{2(x-1)\kappa^2 - 2d^2}{x^3} \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] \gtrsim p\left(d^2 + |x-1|\kappa^2\right).$$
Thus we get that if $1+2d^2/\kappa^2 +p^{-1/4+c}\le x\le 1+\delta$,
\begin{align*}
h(x)-h(1)\ge h(x)-h\left(1+\frac{3d^2}{2\kappa^2}\right) \ge \int_{1+\frac{3d^2}{2\kappa^2}}^x h'(x)\dd x \gtrsim p\kappa^2 \cdot |x-1|^2  \gtrsim p\frac{d^4}{\kappa^2} + p^{1/2+2c} \kappa^2 .
\end{align*}
On the other hand, by Claim \ref{claim_largedev1} we have that
$$ g(x)=h(x)+\sigma^2 (n_1+n_2-p)+\OO \left(p^{1/2+c}\left(\sigma^2 +\kappa^2+d^2 \right)\right)$$
with high probability. Combining the above two estimates, we obtain that under the conditions $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$,
$$g(x)-g(1) = h(x)-h(1) + \OO \left(p^{1/2+c}\left(\sigma^2 +\kappa^2+d^2 \right)\right) >0$$
with high probability, i.e. $x$ cannot be the minimizer of $g$.
%Then if we choose the constant $c$ in Claim \ref{claim_largedev1} such that $e_1<2e_2$, we have
%$$ h(x)-h(1)\gtrsim  p^{-1/2+2e_2}\cdot p\kappa^2 \gg p^{-1/2+e_1}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right),$$ where we use the assumption that $\sigma^2$ and $d^2$ are both at most $\OO(\kappa^2)$, which implies the inequality \eqref{gA>g1}.
Second, using equations \eqref{op_uplow} and  \eqref{h'(A)2}, we obtain that for $1-\delta \le x \le 1-3d^2/(2\kappa^2)$,
$$- h'(x)\ge  [2(1-x)\kappa^2 - 2xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]  \gtrsim p\left(d^2 + |x-1|\kappa^2\right).$$
Then with a similar argument as above, we can get that if $1-2d^2/\kappa^2 - p^{-1/4+c}\le x\le 1-\delta$, then
$$ h(x)-h(1) \gtrsim p\frac{d^4}{\kappa^2} +p^{1/2+2c} \kappa^2, $$
%under equation \eqref{choiceofpara0} \HZ{I don't see which condition we want to use here?},
which again implies that $g(x)>g(1)$, i.e. $x$ cannot be the minimizer of $g$. In sum, we obtain that the minimizer $\hat x$ of $g$ must satisfy equation \eqref{hatw_add1}.
%Next we prove the following estimate on the optimizer $\hat x$: with high probability,
%%\begin{lemma}
%%For the isotropic model, we have
%\be\label{hatv_add1}
%|\hat v - 1|= \OO\left(\cal E\right), \quad \cal E:=\frac{d^2}{\kappa^2} + \frac{\sigma^2}{p\kappa^2} + p^{-1/2 + \e_0 /2+ 2\e}.
%\ee
%%\end{lemma}
%%\begin{proof}
% In fact, from the proof of Claim \ref{lem_hat_v} above, one can check that if $C\cal E \le |w-1| \le 2C\cal E$ for a large enough constant $C>1$, then $|h'(w)|\gtrsim \sigma^2$. Moreover, under equation \eqref{choiceofpara} we have
%$$h(w) =\OO(\sigma^2),\quad \text{for}\quad   |w-1|\le 2C\cal E.$$
%Thus we obtain that for $|w-1|\ge 2C\cal E$,
%$$\left|h(w) - h(\hat w)\right|\ge |h(w)-\min\{h(w_1),h(w_2)\}|\gtrsim \sigma^2 \cal E \gtrsim \cal E \cdot h(\hat w),$$
%which leads to $g(w) > g(\hat w)$ with high probability by equation \eqref{boundv-w}. Thus $w$ cannot be a minimizer of $g(v)$, and we must have $|\hat v - 1|\le 2C\cal E$. %Together with equation \eqref{hatw_add1}, we conclude equation \eqref{hatv_add1}.
%%\end{proof}
%
%{\cor require $\kappa^2\gg d^2 + p^{-1/2+c}\sigma^2$}
\end{proof}

\paragraph{Part $2$: a reduction to the bias and variance limits.}
Recall that the hard parameter sharing estimator $\hat{\beta}_2^{\MTL}$ is equal to $\hat{B} \hat{A}_2$.
The predication loss of hard parameter sharing is as follows
\begin{align}
L(\hat{\beta}_2^{\MTL}) &=\left\|(\Sigma^{(2)})^{1/2} \left( \hat B \hat A_2 - \beta^{(2)}\right)\right\| \nonumber\\
&=  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x\beta^{(1)}-\hat x^2\beta^{(2)})+ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2.\label{Lbeta_HPS}
\end{align}
Using Lemma \ref{lem_hat_v} and the concentration estimates in Lemma \ref{largedeviation}, we can simplify $L(\hat{\beta}_2^{\MTL})$ as follows.

\begin{claim}\label{claim_reduction}
Recall that $\hat \Sigma(1)$ is equal to $\hat \Sigma$ (cf. equation \eqref{def hatsig}).
In the setting of Proposition \ref{cor_MTL_loss}, we have the following estimate with high probability for any small constant $c>0$:
\begin{align*}
&\left|L(\hat{\beta}_2^{\MTL}) - \frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left({\hat \Sigma^{-1}  }\right)\right| \nonumber\\
\lesssim&  \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2).
\end{align*}
\end{claim}
\begin{proof}
Our proof is divided into two steps. First, with the concentration estimates in Lemma \ref{largedeviation}, we can simplify $L(\hat{\beta}_2^{\MTL})$ as follows. With high probability, we have that for any small constant $c>0$,
\be\label{claim_largedev2} \left| L(\hat{\beta}_2^{\MTL})- \cal L(\hat x)\right| \le p^{-1/2+c} \left(\sigma^2 +\kappa^2 +d^2 \right),
\ee
where the function $\cal L(\hat x)$ is defined as
\begin{align*}
	\cal L(\hat x)	:=  &\hat x^2\left[(\hat x-1)^2 \kappa^2 +  (\hat x^2+1)d^2 \right] \cdot p^{-1}\bigtr{ (X^{(1)})^{\top}X^{(1)} \hat\Sigma(\hat x)^{-1} \Sigma^{(2)} \hat\Sigma(\hat x)^{-1} (X^{(1)})^{\top}X^{(1)} } \\
	& +\sigma^2\cdot \tr\left(\Sigma^{(2)}\hat \Sigma(\hat x)^{-1}\right).
\end{align*}
% \end{claim}
%We are now ready to finish the proof of Claim \ref{claim_reduction}.
We defer the proof of equation \eqref{claim_largedev2} until the end.
Next, we can further simplify $\cal L(\hat x)$ using Claim \ref{lem_hat_v} and $\Sigma^{(1)}=\Sigma^{(2)}=\id_{p\times p}$.
More precisely, we claim that with high probability
\begin{align}
&\left|\cal L(\hat x)- \frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left(\hat \Sigma^{-1}  \right)\right| \nonumber\\
\lesssim & \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2). \label{simple12}
\end{align}
Combining equations \eqref{claim_largedev2} and \eqref{simple12}, we conclude Claim \ref{claim_reduction}.

Now we prove equation \eqref{simple12}. Using Claim \ref{lem_hat_v} and equations \eqref{op_X12} and \eqref{op_Sig1}, we obtain that with high probability,
\begin{align}
 \|\hat\Sigma^{-1}-\hat\Sigma(\hat x)^{-1}\| &\le |\hat x^2-1|\|\hat\Sigma^{-1}\| \| (X^{(1)})^\top X^{(1)}\|\|\hat\Sigma(\hat x)^{-1}\|  \lesssim p^{-1}\left(\frac{d^2}{\kappa^2} + p^{-1/4+c}\right).\label{est111}
\end{align}
Using similar arguments, we get that with high probability,
\begin{align}\label{est222}
&\left\|\left(\hat\Sigma^{-2}-\hat\Sigma(\hat x)^{-2}\right)\left((X^{(1)})^\top X^{(1)} \right)^2\right\| \lesssim  \frac{d^2}{\kappa^2} + p^{-1/4+c} .
\end{align}
Moreover, using equations \eqref{op_X12} and \eqref{op_Sig1}, we can bound that with high probability,
%$$\bignorm{ (X^{(1)})^{\top}X^{(1)} \hat\Sigma^{-1} \Sigma^{(2)} \hat\Sigma^{-1} (X^{(1)})^{\top}X^{(1)}}\lesssim 1.$$
%which further implies that with high probability,
\begin{align}
& \bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}  \le p \bignorm{\hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}\lesssim p.\label{est333}
\end{align}
We can bound the left hand side of equation \eqref{simple12} as
\begin{align*}
&\left|\cal L(\hat x)-\frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left({\hat\Sigma^{-1}  }\right)\right| \\
\lesssim &\left(|\hat x-1|^2 \kappa^2 + |\hat x-1|d^2\right)\cdot p^{-1}\bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2 } \\
&+ \frac{d^2}{p}\left|\tr\left[\left(\hat\Sigma(\hat x)^{-2}-\hat\Sigma^{-2}\right) \left((X^{(1)})^\top X^{(1)} \right)^2\right]\right|+ \sigma^2  \left|\bigtr{\hat\Sigma(\hat x)^{-1}-\hat\Sigma^{-1}  }\right|.
\end{align*}
Applying the estimates \eqref{hatw_add1}, \eqref{est111}, \eqref{est222} and \eqref{est333} to the three terms on the right-hand side, we can conclude \eqref{simple12}.

Finally we give the proof of the estimate \eqref{claim_largedev2}.
The proof of this claim is very similar to Claim \ref{claim_largedev1}, where the only difference is that $\hat x$ now may depend on $\epsilon^{(1)}$, $\epsilon^{(2)}$, $\beta^{(1)}$ and $\beta^{(2)}$.
In this case, we can still use  Lemma \ref{largedeviation} to conclude the proof because $\hat x$ can be pulled out as a coefficient.
Here we only give a proof sketch and omit the details.
Recall that $\beta_0$ is the shared component of $\beta^{(1)}$ and $\beta^{(2)}$ with i.i.d. Gaussian entries of mean zero and variance $p^{-1}\kappa^2$. Moreover, we denote the task-specific components by $\wt\beta^{(1)}$ and $\wt\beta^{(2)}$, whose entries are i.i.d. Gaussian random variables of mean zero and variance $p^{-1} d^2 $. Then we write $L(\hat{\beta}_2^{\MTL}) $ in \eqref{Lbeta_HPS} as:
\begin{align}
L(\hat{\beta}_2^{\MTL})  =&  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x -\hat x^2)\beta_0+(X^{(1)})^\top X^{(1)} \hat x\wt \beta^{(1)} - (X^{(1)})^\top X^{(1)}  \hat x^2\wt \beta^{(2)} \right] \right. \nonumber\\
&\left. + (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1}\left[ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2. \label{expand_15}
\end{align}
As in equation \eqref{expand_6}, we can expand this expression into the sum of 15 terms, and bound each term as in equations \eqref{eq_hA1}-\eqref{eq_hA56}. For example, for the main term $\hat x^2 (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)}$, using 
Lemma \ref{largedeviation} and equations \eqref{op_X12} and \eqref{op_Sig1}, we can obtain the following estimate with high probability:
\begin{align*}
&  \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)} - \frac{d^2}{p}\bigtr{(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}}\right| \\
&\le p^{-1+c}d^2 \cdot \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|_F \\
&\le p^{-1/2+c}d^2 \cdot  \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|  \lesssim p^{-1/2+c}d^2.
\end{align*} 
For the cross term $\hat x (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}$, using Lemma \ref{largedeviation} and equations \eqref{op_X12} and \eqref{op_Sig1}, we can obtain the following estimate with high probability for any small constant $c>0$:
\begin{align*}
 \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}\right|  & \le p^c \cdot \sigma\sqrt{p^{-1}d^2} \cdot \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \|_F \\
&\lesssim p^{c}\cdot \sigma d \cdot  \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \| \\
&\lesssim p^{-1/2+c}\sigma d\le p^{-1/2+c}(\sigma^2+ d^2).
\end{align*}
The rest of the terms in the expansion of \eqref{expand_15} can be bounded in the same way, and we omit the details.
 \end{proof}
  
\paragraph{Part $3$: applying the bias and variance limits.}
Finally, we are ready to complete the proof of Corollary \ref{cor_MTL_loss}.
We derive the variance term $\sigma^2  \tr[\hat\Sigma^{-1}]$ and the bias term $\frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right]$ using Theorem \ref{thm_main_RMT}.

\begin{proof}[Proof of Corollary \ref{cor_MTL_loss}]
Using equation \eqref{lem_cov_shift_eq}, we obtain that
\be\label{eq_var111}\tr[\hat\Sigma^{-1}] = \tr\left[ \left((X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}\right)^{-1}\right]=\bigtr{\frac{(a_1 +a_2)^{-1}\id_{p\times p}}{n_{1}+n_2} }+\OO(p^{-c_\varphi})\ee
with high probability. Solving equation \eqref{eq_a12extra} with $\lambda_i\equiv 1$, $1\le i\le p$, we get that  
	\begin{align}
		 a_1 = \frac{n_1(n_1 + n_2 - p)}{(n_1 + n_2)^2} ,\quad
		& a_2 = \frac{n_2(n_1 + n_2 - p)}{(n_1 +n_2)^2} . \label{simplesovlea12}
			\end{align}
Applying the above to equation \eqref{eq_var111}, we obtain that
\be\label{eq_var112}\tr[\hat\Sigma^{-1}]  = \frac{p}{n_1+n_2} \cdot \frac{n_1+n_2}{n_1+n_2-p}+\OO(p^{-c_\varphi})=  \frac{p}{n_1+n_2-p}+\OO(p^{-c_\varphi})\ee
with high probability. 

On the other hand, we have that with high probability,
\begin{align}
 \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi})}{p}\sum_{i=1}^p \left(\hat \Sigma^{-2}\right)_{ii}  &\le {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] \label{eq_bias111}\\ 
 &\le \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi})}{p}\sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} .\nonumber
\end{align}
To obtain this inequality, we used  
$$ (\sqrt{n_1}-\sqrt{p})^4 \cdot (1-p^{-c_\varphi}) \preceq \left((X^{(1)})^\top X^{(1)} \right)^2 \preceq (\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi}) \quad \text{with high probability},$$
by Fact \ref{fact_minv} (ii), and the fact that for the product of two PSD matrices, its trace is always nonnegative.
Using equation \eqref{lem_cov_derv_eq} with $\Sigma^{(1)}=\Sigma^{(2)}=\Lambda=V=\id_{p\times p}$ and $w$ being the $i$-th coordinate vector, we can calculate that 
\begin{align}\label{eq_bias112}
 \left(\hat\Sigma^{-2}\right)_{ii} = \frac{1}{(n_1+n_2)^2} \left[\frac{a_3+a_4+1}{(a_1+a_2)^2} +\OO(p^{-c_\varphi})\right]
\end{align}
with high probability. Solving equation \eqref{eq_a34extra} with $a_1, a_2$ in \eqref{simplesovlea12} and $\lambda_i\equiv 1$, $1\le i\le p$, we can obtain that
	\begin{align*}
		% a_1 = \frac{\rho_1(\rho_1 + \rho_2 - 1)}{(\rho_1 + \rho_2)^2} ,\quad & a_2 = \frac{\rho_2(\rho_1 + \rho_2 - 1)}{(\rho_1 + \rho_2)^2} , \label{simplesovlea12}\\
		a_3 = \frac{p\cdot n_1}{(n_1 +n_2)(n_1 +n_2 - p)}, \quad
		&  a_4 = \frac{p\cdot n_2}{(n_1 + n_2)(n_1 + n_2 - p)}. %\label{simplesovlea34}
	\end{align*}
Inserting them into equation \eqref{eq_bias112}, we obtain that 
\begin{align*}%\label{eq_bias113}
 \left(\hat\Sigma^{-2}\right)_{ii} =  \frac{1}{(n_1+n_2)^2} \left[  \frac{ (n_1+n_2)^3 }{(n_1+n_2-p)^3} +\OO(p^{-c_\varphi})\right]
\end{align*}
with high probability. In fact, we need this estimate to hold simultaneously for all $1\le i \le p$ with high probability. For this purpose, we shall use equation \eqref{apply derivlocal} to get that 
\begin{align*}%\label{eq_bias113}
\left| \left(\hat\Sigma^{-2}\right)_{ii} - \frac{1}{(n_1+n_2)^2}\cdot \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\prec \frac{p^{-\frac{\varphi-4}{2\varphi}}}{(n_1+n_2)^2}
\end{align*}
on a high probability event that does not depend on $i$. Then using Fact \ref{lem_stodomin} (i), we obtain that 
\begin{align*} 
\left| \sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} - \frac{p}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\prec \frac{p^{1-\frac{\varphi-4}{2\varphi}}}{(n_1+n_2)^2}
\end{align*}
with high probability, which by Definition \ref{stoch_domination} gives that
\begin{align}\label{eq_bias113}
\left| p^{-1}\sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} - \frac{1}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\le \frac{p^{-c_\varphi}}{(n_1+n_2)^2}
\end{align}
with high probability. Inserting equation \eqref{eq_bias113} into equation \eqref{eq_bias111}, we get that
\begin{align*}
		& \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi}) - n_1^2 \cdot (1+\OO(p^{-c_\varphi}))}{(n_1+n_2)^2}\cdot   \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\nonumber \\
		\le &  {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] - \frac{n_1^2}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3} \nonumber\\
		\le &  \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi}) - n_1^2 \cdot (1+\OO(p^{-c_\varphi}))}{(n_1+n_2)^2}\cdot   \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}
\end{align*}
with high probability. 

Combining equation \eqref{eq_var112} and the above with Claim \ref{claim_reduction}, we get that
\begin{align*}
	\left|L(\hat{\beta}_2^{\MTL})  - \frac{\sigma^2 p}{n_1+n_2-p} -  \frac{2d^2 n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3}\right| \le   \left[\left( 1+\sqrt{\frac{p}{n_1}}\right)^4-1\right] \cdot \frac{2d^2n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3} \nonumber\\
+C\left( p^{-c_\varphi} (\sigma^2 + d^2)+ \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2) \right).
\end{align*}
Then using the conditions $\sigma^2 \lesssim  \kappa^2$ and $d^2 \le p^{-\e}{\kappa^2}$, we conclude the proof.
\end{proof}
