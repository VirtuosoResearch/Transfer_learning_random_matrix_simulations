\section{Proof of Corollary \ref{cor_MTL_loss}}\label{app_iso_cov}

We follow a similar logic as in the proof of Theorem \ref{thm_many_tasks}.
We first characterize the global minimizer of $f(A, B)$ in the random-effects model.
Based on the characterization, we reduce the prediction loss of hard parameter sharing to the asymptotic limits provided in Theorem \ref{thm_main_RMT}.
Then, we prove Corollary \ref{cor_MTL_loss} based on the bias and variance limits.
We set up several notations.
In the two-task case, the optimization objective $f(A, B)$ is equal to
	\begin{align}
		f(A, B) =   \bignorm{X^{(1)} B A_1 - Y^{(1)}}^2+ \bignorm{X^{(2)} B A_2 - Y^{(2)}}^2, \label{eq_mtl_2task_cov}
	\end{align}
	where $B \in \real^{p}$ and $A = [A_1, A_2]\in \real^{2}$ because the width of $B$ is one.
We assume that $A_1$ and $A_2$ are both nonzero.
Otherwise, we add a tiny amount of perturbation $\delta$ to them and the result remains the same.
Using the local optimality condition $\frac{\partial f}{\partial B} = 0$, we obtain that $\hat{B}$ satisfies the following
	\begin{align}
		\hat{B} \define  \left[A_1^2 (X^{(1)})^{\top}X^{(1)} + A_2^2 (X^{(2)})^{\top}X^{(2)}\right]^{-1} \left[A_1 (X^{(1))})^{\top}Y^{(1)} + A_2 (X^{(2)})^{\top}Y^{(2)}\right]. \label{eq_Bhat_2task} %\\
		%&= (B^\star A ^{\top}) (A A^{\top})^{-1} + (X^{\top}X)^{-1}X^{\top}   \bigbrace{\sum_{j=1}^t \varepsilon_i A_i^{\top}} (A  A^{\top})^{-1}.
	\end{align}
We denote $\hat \Sigma(x):= x^2 (X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}$.
Applying $\hat B$ to equation \eqref{eq_mtl_2task_cov}, we obtain an objective that only depends on $x:=A_1/A_2$ as follows %\HZ{$A$ has been used to denote the output layers. Could you replace $A$ with another symbol (say $x$)?}
 \begin{align}
		 g(x) \define & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
		& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2 \nonumber\\
		 + & \left\| X^{(2)} \hat \Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (x\beta^{(1)}-x^2\beta^{(2)})+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)\epsilon^{(2)} \right. \nonumber\\
		 &\left. + x X^{(2)}\hat \Sigma(x)^{-1} (X^{(1)})^\top \epsilon^{(1)} \right\|^2.
		\label{eq_mtl_A12}
	\end{align}
The conditional expectation of $g(x)$ over $\epsilon^{(1)}$ and $\epsilon^{(2)}$ is
%\HZ{could you align the Eq so that they look better on page?}
\begin{align}
\exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(x) \mid X_1, X_2 ,\beta^{(1)},\beta^{(2)}}
&= \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\right\|^2 \nonumber\\
&+x^2 \left\| X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\right\|^2  \nonumber\\
& + \exarg{\epsilon^{(1)}}{(\epsilon^{(1)})^\top \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2\epsilon^{(1)}} \nonumber\\
&+\exarg{ \epsilon^{(2)}}{(\epsilon^{(2)})^\top \left(X^{(2)}\hat \Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2\epsilon^{(2)}}   \nonumber\\
&+  x^2 \exarg{ \epsilon^{(2)}}{ (\epsilon^{(2)} )^\top X^{(2)}\hat \Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)}} \nonumber\\
& +x^2 \exarg{ \epsilon^{(1)}} { (\epsilon^{(1)})^\top X^{(1)}\hat \Sigma(x)^{-1}  (X^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top \epsilon^{(1)}  }. \label{Eg(x)}
\end{align}
Using the following identity
\begin{align*}
(X^{(1)})^\top X^{(1)} \hat \Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} & =\left( x^2 [(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1} \right)^{-1} \\
&= (X^{(2)})^\top X^{(2)} \hat \Sigma(x)^{-1}(X^{(1)})^\top X^{(1)},
\end{align*}
we can simplify that
\begin{align*}
 & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\right\|^2  +x^2 \left\| X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\right\|^2 \\
=&(\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1} (X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)}  (\beta^{(1)}-x\beta^{(2)})\\
&+(\beta^{(1)}-x\beta^{(2)})^\top x^2 (X^{(1)})^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)} )^\top X^{(2)}  \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\\
%&=(\beta^{(1)}-x\beta^{(2)})^\top\left[ x^2 (X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}  \right]\hat\Sigma(x)^{-1} (X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\\
=&(\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)}).
\end{align*}
Using this identity, we can simplify equation \eqref{Eg(x)}  to
\begin{align*}
		&\exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(x) \mid X_1, X_2 ,\beta^{(1)},\beta^{(2)}} \\
		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  \\
		&+ \sigma^2\tr\left[ \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2 \right]\\
&+ 2 x^2 \sigma^2 \tr\left[ \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right]  \nonumber\\
		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)}) +\sigma^2(n_1+n_2-p).
\end{align*}
For the random-effects model, recall that the entries of $(\beta^{(1)}- x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $p^{-1}[(x-1)^2\kappa^2 +(1+x^2) d^2]$.
Hence, 
\begin{align}
&\exarg{\epsilon^{(1)}, \epsilon^{(2)} ,\beta^{(1)},\beta^{(2)}}{g(x) \mid X_1, X_2} \nonumber\\
%&=[(x-1)^2\kappa^2 + (x^2+1)d^2/2] \cdot p^{-1}\tr\left[  (X^{(2)})^\top X^{(2)} \hat\Sigma(x)^{-1}(X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)}\right] \nonumber\\
%&+ A^2[(x-1)^2\kappa^2 + (x^2+1)d^2/2] \cdot p^{-1}\tr\left[  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)}\right] +\sigma^2(n_1+n_2-p) \nonumber\\
&=\frac{(x-1)^2\kappa^2 + (x^2+1)d^2}{p} \tr\left[ (X^{(1)})^\top X^{(1)} \hat \Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right]+\sigma^2(n_1+n_2-p), \label{EgA}
\end{align}


\paragraph{Part $1$: characterizing the global minimum of $f(A, B)$.}
Let $\hat{x}$ denote the minimizer of $g(x)$.
We show that in the setting of Corollary \ref{cor_MTL_loss}, $\hat x$ is close to 1.
This gives us the global minimum of $f(A, B)$, combined with the local optimality condition for $\hat{B}$.

First, we show that $g(x)$ and $\ex{g(x) \mid X^{(1)}, X^{(2)}}$ are close using standard concentration bounds.
\begin{claim}\label{claim_largedev1}
 In the setting of Corollary \ref{cor_MTL_loss}, we have that with high probability
$$\left| g(x) - \exarg{\epsilon^{(1)}, \epsilon^{(2)} ,\beta^{(1)},\beta^{(2)}}{g(x) \mid X_1, X_2 }\right| \le p^{1/2+c} \left(\sigma^2 + \kappa^2+d^2 \right)$$
for any small constant $c>0$.
 \end{claim}
 \begin{proof}
By Fact \ref{fact_minv} (ii), we have that with high probability,
 \be\label{op_X12}
\|X^{(1)}\|\le \sqrt{(\sqrt{n_1} + \sqrt{p})^2 + n_1 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p}, \quad \|X^{(2)}\|\le \sqrt{(\sqrt{n_2} + \sqrt{p})^2 + n_2 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p},
 \ee
 and
 \be\label{op_Sig1}
\| \hat\Sigma(x)^{-1}\|\le \frac1{x^2[(\sqrt{n_1} - \sqrt{p})^2 - n_1 \cdot p^{-c_{\varphi}}]+[(\sqrt{n_2} - \sqrt{p})^2 - n_2 \cdot p^{-c_{\varphi}}] }\lesssim \frac{1}{(x^2+1)p},
 \ee
where we used that $3\le n_1/p\le \tau^{-1}$ and $3\le n_2/p\le \tau^{-1}$ for a small constant $\tau>0$.

 Now we expand the first term on the right-hand side of \eqref{eq_mtl_A12}:
 \begin{align}
 & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
		& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2=h_1(x) + h_2(x) +h_3(x) + 2h_4(x) + 2h_5(x) + 2h_6(x) ,\label{expand_6}
 \end{align}
 where
 \begin{align*}
&h_1(x):=  (\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  ,\\
& h_2(x) := {(\epsilon^{(1)})^\top \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2\epsilon^{(1)}}  ,\\
& h_3(x):=  x^2 { (\epsilon^{(2)} )^\top X^{(2)} \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)}} , \\
& h_4(x):=  (\epsilon^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}), \\
&h_5(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}),\\
&h_6(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)}.
\end{align*}
Next we estimate each term using Lemma \ref{largedeviation} in Appendix \ref{app_tool}.

For $h_1(x)$, using Lemma \ref{largedeviation} in Appendix \ref{app_tool} and the fact that the entries of $(\beta^{(1)}-x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $p^{-1}[(x-1)^2\kappa^2 + (x^2+1)d^2]$, we obtain the following estimate with high probability for any small constant $c>0$:
\begin{align}
&\left|h_1(x) - \exarg{\beta^{(1)},\beta^{(2)}}{h_1(x) \mid X_1, X_2}\right|\nonumber \\
&\le p^{c}\cdot p^{-1}[(x-1)^2\kappa^2 + (x^2+1)d^2]\cdot \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F \nonumber\\
&\le p^{c}\cdot p^{-1}[(x-1)^2\kappa^2 + (x^2+1)d^2]\cdot p^{1/2} \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| \nonumber \\
&\lesssim p^{1/2+c}\cdot \frac{(x-1)^2\kappa^2 + (x^2+1)d^2}{(x^2+1)^2}\lesssim p^{1/2+c}(\kappa^2 + d^2).\label{eq_hA1}
\end{align}
 Here in the third step we used \eqref{op_X12} and \eqref{op_Sig1} to bound the operator norm:
\begin{align*}
 \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| & \le \left\| (X^{(2)})^\top X^{(2)}\right\|^2 \cdot \left\| (X^{(1)})^\top X^{(1)}\right\|\cdot \left\| \hat\Sigma(x)^{-1} \right\|^2 \\
 & \lesssim \frac{p}{(x^2+1)^2}.
\end{align*}
Similarly, using Lemma \ref{largedeviation}, the fact that the entries of $\epsilon^{(1)},\epsilon^{(2)}\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variances $\sigma^2$, and the bounds \eqref{op_X12}-\eqref{op_Sig1}, we can obtain that with high probability,
\be\label{eq_hA23}\left|h_2(x) - \exarg{\epsilon^{(1)}}{h_2(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2, \quad \left|h_3(x) - \exarg{\epsilon^{(2)}}{h_3(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2.\ee
For $h_4(x)$, using Lemma \ref{largedeviation} in Appendix \ref{app_tool}, we obtain the following estimate with high probability for any small constant $ c>0$:
\begin{align}
&\left|h_4(x)\right| \nonumber\\
&\le p^{c}\cdot \sigma \sqrt{p^{-1}[(x-1)^2\kappa^2 + (x^2+1)d^2]}\cdot \left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F\nonumber \\
&\le p^{c}\cdot \sigma \sqrt{p^{-1}[(x-1)^2\kappa^2 + (x^2+1)d^2]} \cdot p^{1/2} \left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
&\le  p^{c}\cdot \sigma \sqrt{ (x-1)^2\kappa^2 + (x^2+1)d^2} \cdot \left\|  \id_{n_1\times n_1}-x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top\right\|  \left\|X^{(1)}\right\| \left\|\hat\Sigma(x)^{-1}\right\|\left\| (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
&\lesssim p^{1/2+c}\frac{\sigma \sqrt{ (x-1)^2\kappa^2 + (x^2+1)d^2}}{x^2+1} \lesssim p^{1/2+c}(\sigma^2 + \kappa^2 + d^2).\label{eq_hA4}
\end{align}
Above, in the fourth step we used $\left\|  \id_{n_1}-x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top\right\|\le 1$ and equations \eqref{op_X12} and \eqref{op_Sig1}. In the last step we used AM-GM inequality. With the same argument, we can show that
\be\label{eq_hA56} \left|h_5(x)\right| \le p^{1/2+c}(\sigma^2 + \kappa^2 + d^2), \quad \left|h_6(x)\right| \le p^{1/2+c}\sigma^2,\ee
with high probability for any small constant $c>0$. %Combining \eqref{eq_hA1}, \eqref{eq_hA23}, \eqref{eq_hA4} and \eqref{eq_hA56}, we conclude that

Finally, we can obtain similar estimates for the second term on the right-hand side of \eqref{eq_mtl_A12} as in  \eqref{eq_hA1}, \eqref{eq_hA23}, \eqref{eq_hA4} and \eqref{eq_hA56}. Hence the proof is complete.
 \end{proof}

Next, we show that $\hat x$ is close to 1.
\begin{claim}\label{lem_hat_v}
%Suppose the assumptions of Lemma \ref{prop_model_shift_tight} hold. Assume that $ \kappa^2 \sim pd^2 \sim \sigma^2$ are of the same order.
 In the setting of Corollary \ref{cor_MTL_loss}, we have that with high probability,
%There exists a constant $C>0$ such that
	\be\label{hatw_add1}|\hat x -1|\le  \frac{2d^2}{\kappa^2} + p^{-1/4+c}
	\ee
	for any small constant $c>0$.
	%is $\hat{w} = 1 \pm \bigo{\frac 1 {n_1+n_2}}$ \todo{(figure out the constants)}
\end{claim}

\begin{proof}
Corresponding to equation \eqref{EgA}, we define the function
\begin{align*}
h(x):= [(x-1)^2\kappa^2 + (x^2+1)d^2] \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right].
\end{align*}
Using Fact \ref{fact_minv} (ii), we can obtain that with high probability,
\begin{align}
p(x^2+1)^{-1}&\lesssim \left( x^2 \left[(\sqrt{n_2}-\sqrt{p})^{2}-n_2p^{-c_\varphi}\right]^{-1} +\left[(\sqrt{n_1}-\sqrt{p})^{2}-n_1p^{-c_\varphi}\right]^{-1} \right)^{-1} \nonumber\\
&\preceq (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} =\left( x^2 [(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1} \right)^{-1} \nonumber\\
&\preceq \left( x^2 \left[(\sqrt{n_2}+\sqrt{p})^{2}+n_2p^{-c_\varphi}\right]^{-1} +\left[(\sqrt{n_1}+\sqrt{p})^{2}+n_1p^{-c_\varphi}\right]^{-1} \right)^{-1}\lesssim p(x^2+1)^{-1}, \label{op_uplow}
\end{align}
where we also used that $3\le n_1/p \le \tau^{-1}$ and $3\le  n_2/p \le \tau^{-1}$ for a small constant $\tau>0$. Then we get that
\be\label{eq_h1}h(1)= 2d^2 \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(1)^{-1}(X^{(2)})^\top X^{(2)}\right]\lesssim pd^2 .\ee
On the other hand, if $|x-1|\ge \delta$ for a small constant $\delta>0$, then we have
\begin{align}\label{eq_hA}
h(x)\gtrsim (1+x^2)\kappa^2 \cdot p\left( x^2 +1\right)^{-1} =p\kappa^2 ,
\end{align}
where in the first step we used that $|x-1|^2\gtrsim 1+x^2$ for $|x-1|\ge \delta$ and equation \eqref{op_uplow}.
%and in the second step we used equation \eqref{choiceofpara0} \HZ{I'm not sure which condition we use? put it here}.
Hence using Claim \ref{claim_largedev1} and equations \eqref{eq_h1} and \eqref{eq_hA}, we obtain that with high probability,
\begin{align}
g(x)&\ge  h(x) +\sigma^2(n_1+n_2-p) -p^{-1/2+c}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right) \nonumber\\
&\ge h(1) +\sigma^2(n_1+n_2-p) + p^{-1/2+c}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right) \ge g(1), \label{gA>g1}
\end{align}
under conditions $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$. %\eqref{choiceofpara0} \HZ{again, I don't see which condition we use here?}.
This gives that
%\be\label{rough A-1}
$|\hat x-1|\le \delta$ for any small constant $\delta>0$.




%Hence it remains to consider the region $|A-1|\le \delta$ for a small enough constant $\delta>0$.
To obtain the better bound \eqref{hatw_add1}, we study the function $h(x)$ more closely and find its minimizer, denoted by $x^\star$.
%Using Lemma \ref{largedeviation} again, we can simplify equation \eqref{revise_eq_val_mtl} as $\val(v)= N_2h(v) \cdot  \left( 1+\OO(p^{-1/2+\e})\right)$, where the function $ h$ is defined as
%	%We define the function
%	\begin{align}
%		h(v) =& \frac{\rho_1}{\rho_2}\left[d^2 +\left( v-1\right)^2\kappa^2\right]\cdot \tr\left[( v^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%		& +  v^2\left[d^2 +\left( v-1\right)^2\kappa^2\right]\cdot \tr\left[( v^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%		& + \left(\frac{\rho_1}{\rho_2} v^2 + 1\right)\sigma^2 \cdot \bigtr{(v^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} }. \nonumber
%	\end{align}
%	%\cor Under the setting of Lemma \ref{prop_model_shift_tight},
%	Furthermore, the validation loss in equation \eqref{approxvalid} reduces to
%	\be\label{boundv-w}
%		g(v)=\left[N_2 h(v) + (N_1+N_2)\sigma^2\right]\cdot \left( 1+\OO(p^{-(1-\e_0)/2+\e})\right)\quad \text{w.h.p.}
%	\ee
%	for any constant $\e>0$. %\nc Thus for the following discussions, it suffices to focus on the behavior of $h (v)$.
%Let $\hat w$ be the minimizer of $h(v)$. Our proof consists of the following two steps.
%\squishlist
%	\item First, we show that $\hat{w}$ is close to $1$.
%	\item Second, with equation \eqref{boundv-w} we show that $\hat v$ is close to $\hat w$. Then we plug $\hat{v}$ into $\te(\hat{\beta}_2^{\MTL})$ to get equation \eqref{simple1}.
%\squishend
%%For the minimizer $\hat w$ of $\val(w)$, we have a similar result as in Proposition \ref{thm_cov_shift}.
%For the first step, we will prove the following result.
%To be consistent with the notation $\hat w$, we change the name of the argument to $w$ in the proof.
First it is easy to observe that $h(x)< h(-x)$ for $x> 0$.
%$h(1)=\OO(pd^2 + \sigma^2)$ and $ h(w)\gtrsim p\kappa^2 \gg h(1)$ for $w\ge 2$ or $w\le 1/2$.  Hence it suffices to consider the case $1/2\le w\le 2$.
Then we consider the case $x\ge 1$. We write
\begin{align*}
h(x):= [(1-x^{-1})^2\kappa^2 + (1+x^{-2})d^2] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].
\end{align*}
Notice that
$$\tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]$$
is an increasing functions in $x$. Hence taking derivative of $h(x)$ with respect to $x$, we obtain that
\begin{align}\label{h'(A)1}
h'(x) \ge \left[2(1-x^{-1})\frac{\kappa^2}{x^2} - 2\frac{d^2}{x^3}\right] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] \ge 0,
\end{align}
as long as $x \ge 1+d^2/\kappa^2$.
%\begin{align}
%	h(w) =&  \frac{\rho_1}{\rho_2}\left[\frac{d^2}{w^4} +\frac{\left( w-1\right)^2}{w^4}\kappa^2\right]\cdot \tr\left[( X_1^{\top}X_1 +w^{-2}X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%	& + \left[\frac{d^2}{w^2} +\frac{\left( w-1\right)^2}{w^2}\kappa^2\right]\cdot \tr\left[( X_1^{\top}X_1 +w^{-2}X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%			& +  \frac{\rho_1}{\rho_2}\sigma^2  \cdot \bigtr{(X_1^{\top}X_1  + w^{-2}X_2^{\top}X_2)^{-1} }+ \sigma^2 \cdot \bigtr{(w^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} }.\nonumber%+  \sigma^2n_2 \cdot \bigtr{(X_1^{\top}X_1  + w^{-2}X_2^{\top}X_2)^{-1} }. \nonumber
%\end{align}
%Notice that
%$$\tr\left[( X_1^{\top}X_1 +w^{-2}X_2^{\top}X_2)^{-2} (X_i^{\top}X_i)^2\right],\ \ i=1,2, \quad \text{and} \quad \bigtr{(X_1^{\top}X_1  + w^{-2}X_2^{\top}X_2)^{-1} }$$
%are increasing functions in $w$. Hence taking derivative of $h(w)$ with respect to $w$, we obtain that
%\begin{align*}
%h'(w) \ge & \frac{\rho_1}{\rho_2}\left[ \frac{2(w-1)(2-w)}{w^5}\kappa^2 - \frac{4d^2}{w^5}\right] \tr\left[( X_1^{\top}X_1 +w^{-2}X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right]   \\
% &+ \left[\frac{2\left( w-1\right)}{w^3}\kappa^2 - \frac{2d^2}{w^3} \right]\cdot \tr\left[( X_1^{\top}X_1 +w^{-2}X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%		&- 2  \frac{\sigma^2}{w^3} \cdot \bigtr{(X_1^{\top}X_1 + w^{-2}X_2^{\top}X_2)^{-2} X_1^\top X_1  } =  \bigtr{(X_1^{\top}X_1 + w^{-2}X_2^{\top}X_2)^{-2} \cal A },
%\end{align*}
%where the matrix $\cal A$ is defined as
%\begin{align*}
%\cal A :&= \frac{\rho_1}{\rho_2}\left[ \frac{2(w-1)(2-w)}{w^5}\kappa^2 - \frac{4d^2}{w^5}\right](X_2^{\top}X_2)^2 + \left[\frac{2\left( w-1\right)}{w^3}\kappa^2 - \frac{2d^2}{w^3} \right](X_1^{\top}X_1)^2 - 2 \frac{\sigma^2}{w^3}X_1^\top X_1.
%\end{align*}
%Using the estimate Fact equation \eqref{lem_minv}, we get that $\cal A$ is lower bounded as
%\begin{align*}
%\cal A \succeq&~ - \frac{4d^2}{w^5}n_1n_2 (\al_+(\rho_2)+\oo(1))^2 + \left[\frac{2\left( w-1\right)}{w^3}\kappa^2 - \frac{2d^2}{w^3} \right]n_1^2 (\al_-(\rho_1)-\oo(1))^2 \\
%&~ - 2 \frac{\sigma^2}{w^3}n_1(\al_{+}(\rho_1)+\oo(1)) \succ 0,
%\end{align*}
%as long as
%$$w> w_1:=1 +\frac{d^2}{\kappa^2}+ \frac{\sigma^2}{n_1\kappa^2}\frac{\al_{+}(\rho_1)+\oo(1)}{\al_{-}^2(\rho_1)} + \frac{2d^2}{\kappa^2}\frac{\rho_2(\al_+^2(\rho_2)+\oo(1))}{\rho_1\al_-^2(\rho_1) }.$$
%Hence $h'(w)>0$ on $(w_1,\infty)$, %i.e. $h(w)$ is strictly increasing for $w>w_1$. This
%which gives $\hat w\le w_1$.
Next we consider the case $1-\delta\le x\le 1$. Notice that
$$\tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} +  [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].$$
is a decreasing functions in $x$. Hence taking derivative of $h(x)$, we obtain that
\begin{align}\label{h'(A)2}
	h'(x)\le [-2(1-x)\kappa^2 +2 xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]\le 0,
\end{align}
as long as $x\le 1-d^2/\kappa^2$. In sum, we see that the minimizer $x^\star$ must satisfy
$$1-d^2/\kappa^2\le x^\star \le 1+d^2/\kappa^2.$$

%Taking derivative of $h(w)$, we obtain that
%\begin{align}
%	h'(w) \le& \frac{\rho_1}{\rho_2} \left[2\left( w-1\right) \kappa^2\right]\cdot \tr\left[( w^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%	& +  \left[2wd^2 +2w\left( w-1\right)(2w-1)\kappa^2\right]\cdot \tr\left[( w^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%	& + \frac{\rho_1}{\rho_2}(2 w\sigma^2) \cdot \bigtr{(w^2X_1^{\top}X_1 + X_2^{\top}X_2)^{-2} X_2^\top X_2  }= \frac{\rho_1}{\rho_2} \bigtr{(w^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} \cal B} , \nonumber
%\end{align}
%where the matrix $\cal B$ is
%$$\cal B= 2\left( w-1\right) \kappa^2  (X_2^{\top}X_2)^2+\frac{\rho_2}{\rho_1}\left[2wd^2 +2w\left( w-1\right)(2w-1)\kappa^2\right](X_1^{\top}X_1)^2 + 2 w\sigma^2 X_2^\top X_2 .$$
%Using Fact \ref{lem_minv}, we get that $\cal B$ is upper bounded as
%\begin{align*}
%\cal B \preceq - 2(1-w)\kappa^2 n_2^2 (\al_-(\rho_2) -\oo(1))^2 +2w d^2 n_1n_2 (\al_+(\rho_1) +\oo(1))^2 + 2w\sigma^2 n_2 (\al_+(\rho_2)+\oo(1)) \prec 0,
%\end{align*}
%as long as
%$$w< w_2:=1 -   \frac{d^2}{\kappa^2}\frac{\rho_1(\al_+(\rho_1) +\oo(1))^2}{\rho_2 \al_-^2(\rho_2) } -  \frac{\sigma^2}{n_2\kappa^2}\frac{\al_{+}(\rho_2)+\oo(1)}{\al_{-}^2(\rho_2)} .$$
%Hence $h'(w)<0$ on $[0,w_2)$, %i.e. $h(w)$ is strictly decreasing for $w<w_2$. This
%which gives $\hat w\ge w_2$.

%In sum, we have shown that $w_2\le w\le w_1$. Together with
%$$\max\{|w_1 -1|, |w_2 -1|\} =\OO\left(\frac{d^2}{\kappa^2} + \frac{\sigma^2}{p\kappa^2}\right),$$
%we conclude equation \eqref{hatw_add1}.


%For the rest of this section, we choose parameters that satisfy the following relations: \be\label{choiceofpara}
%pd^2 \sim \sigma^2 \sim 1,\quad p^{-1+c_0} \sigma^2 \le \kappa^2  \le p^{-\e_0-c_0}\sigma^2 ,
%\ee
%for some small constant $c_0>0$.

Now we are ready to prove equation \eqref{hatw_add1}. First, using equations \eqref{op_uplow} and  \eqref{h'(A)1}, we obtain that for $1+3d^2/(2\kappa^2) \le x\le 1+\delta$,
$$h'(x)\ge \frac{2(x-1)\kappa^2 - 2d^2}{x^3} \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] \gtrsim p\left(d^2 + |x-1|\kappa^2\right).$$
Thus we get that if $1+2d^2/\kappa^2 +p^{-1/4+c}\le x\le 1+\delta$,
\begin{align*}
h(x)-h(1)\ge h(x)-h\left(1+\frac{3d^2}{2\kappa^2}\right) \ge \int_{1+\frac{3d^2}{2\kappa^2}}^x h'(x)\dd x \gtrsim p\kappa^2 \cdot |x-1|^2  \gtrsim p\frac{d^4}{\kappa^2} + p^{1/2+2c} \kappa^2 .
\end{align*}
On the other hand, by Claim \ref{claim_largedev1} we have that
$$ g(x)=h(x)+\sigma^2 (n_1+n_2-p)+\OO \left(p^{1/2+c}\left(\sigma^2 +\kappa^2+d^2 \right)\right)$$
with high probability. Combining the above two estimates, we obtain that under the conditions $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$,
$$g(x)-g(1) = h(x)-h(1) + \OO \left(p^{1/2+c}\left(\sigma^2 +\kappa^2+d^2 \right)\right) >0$$
with high probability, i.e. $x$ cannot be the minimizer of $g$.
%Then if we choose the constant $c$ in Claim \ref{claim_largedev1} such that $e_1<2e_2$, we have
%$$ h(x)-h(1)\gtrsim  p^{-1/2+2e_2}\cdot p\kappa^2 \gg p^{-1/2+e_1}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right),$$ where we use the assumption that $\sigma^2$ and $d^2$ are both at most $\OO(\kappa^2)$, which implies the inequality \eqref{gA>g1}.
Second, using equations \eqref{op_uplow} and  \eqref{h'(A)2}, we obtain that for $1-\delta \le x \le 1-3d^2/(2\kappa^2)$,
$$- h'(x)\ge  [2(1-x)\kappa^2 - 2xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]  \gtrsim p\left(d^2 + |x-1|\kappa^2\right).$$
Then with a similar argument as above, we can get that if $1-2d^2/\kappa^2 - p^{-1/4+c}\le x\le 1-\delta$, then
$$ h(x)-h(1) \gtrsim p\frac{d^4}{\kappa^2} +p^{1/2+2c} \kappa^2, $$
%under equation \eqref{choiceofpara0} \HZ{I don't see which condition we want to use here?},
which again implies that $g(x)>g(1)$, i.e. $x$ cannot be the minimizer of $g$. In sum, we obtain that the minimizer $\hat x$ of $g$ must satisfy equation \eqref{hatw_add1}.
%Next we prove the following estimate on the optimizer $\hat x$: with high probability,
%%\begin{lemma}
%%For the isotropic model, we have
%\be\label{hatv_add1}
%|\hat v - 1|= \OO\left(\cal E\right), \quad \cal E:=\frac{d^2}{\kappa^2} + \frac{\sigma^2}{p\kappa^2} + p^{-1/2 + \e_0 /2+ 2\e}.
%\ee
%%\end{lemma}
%%\begin{proof}
% In fact, from the proof of Claim \ref{lem_hat_v} above, one can check that if $C\cal E \le |w-1| \le 2C\cal E$ for a large enough constant $C>1$, then $|h'(w)|\gtrsim \sigma^2$. Moreover, under equation \eqref{choiceofpara} we have
%$$h(w) =\OO(\sigma^2),\quad \text{for}\quad   |w-1|\le 2C\cal E.$$
%Thus we obtain that for $|w-1|\ge 2C\cal E$,
%$$\left|h(w) - h(\hat w)\right|\ge |h(w)-\min\{h(w_1),h(w_2)\}|\gtrsim \sigma^2 \cal E \gtrsim \cal E \cdot h(\hat w),$$
%which leads to $g(w) > g(\hat w)$ with high probability by equation \eqref{boundv-w}. Thus $w$ cannot be a minimizer of $g(v)$, and we must have $|\hat v - 1|\le 2C\cal E$. %Together with equation \eqref{hatw_add1}, we conclude equation \eqref{hatv_add1}.
%%\end{proof}
%
%{\cor require $\kappa^2\gg d^2 + p^{-1/2+c}\sigma^2$}
\end{proof}

\paragraph{Part $2$: a reduction to the bias and variance limits.}
Recall that the hard parameter sharing estimator $\hat{\beta}_2^{\MTL}$ is equal to $\hat{B} \hat{A}_2$.
The predication loss of hard parameter sharing is as follows
\begin{align}
L(\hat{\beta}_2^{\MTL}) &=\left\|(\Sigma^{(2)})^{1/2} \left( \hat B \hat A_2 - \beta^{(2)}\right)\right\| \nonumber\\
&=  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x\beta^{(1)}-\hat x^2\beta^{(2)})+ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2.\label{Lbeta_HPS}
\end{align}
Using Lemma \ref{lem_hat_v} and the concentration estimates in Lemma \ref{largedeviation}, we can simplify $L(\hat{\beta}_2^{\MTL})$ as follows.

\begin{claim}\label{claim_reduction}
Recall that $\hat \Sigma(1)$ is equal to $\hat \Sigma$ (cf. equation \eqref{def hatsig}).
In the setting of Proposition \ref{cor_MTL_loss}, we have the following estimate with high probability for any small constant $c>0$:
\begin{align*}
&\left|L(\hat{\beta}_2^{\MTL}) - \frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left({\hat \Sigma^{-1}  }\right)\right| \nonumber\\
\lesssim&  \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2).
\end{align*}
\end{claim}
\begin{proof}
Our proof is divided into two steps. First, with the concentration estimates in Lemma \ref{largedeviation}, we can simplify $L(\hat{\beta}_2^{\MTL})$ as follows. With high probability, we have that for any small constant $c>0$,
\be\label{claim_largedev2} \left| L(\hat{\beta}_2^{\MTL})- \cal L(\hat x)\right| \le p^{-1/2+c} \left(\sigma^2 +\kappa^2 +d^2 \right),
\ee
where the function $\cal L(\hat x)$ is defined as
\begin{align*}
	\cal L(\hat x)	:=  &\hat x^2\left[(\hat x-1)^2 \kappa^2 +  (\hat x^2+1)d^2 \right] \cdot p^{-1}\bigtr{ (X^{(1)})^{\top}X^{(1)} \hat\Sigma(\hat x)^{-1} \Sigma^{(2)} \hat\Sigma(\hat x)^{-1} (X^{(1)})^{\top}X^{(1)} } \\
	& +\sigma^2\cdot \tr\left(\Sigma^{(2)}\hat \Sigma(\hat x)^{-1}\right).
\end{align*}
% \end{claim}
%We are now ready to finish the proof of Claim \ref{claim_reduction}.
We defer the proof of equation \eqref{claim_largedev2} until the end.
Next, we can further simplify $\cal L(\hat x)$ using Claim \ref{lem_hat_v} and $\Sigma^{(1)}=\Sigma^{(2)}=\id_{p\times p}$.
More precisely, we claim that with high probability
\begin{align}
&\left|\cal L(\hat x)- \frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left(\hat \Sigma^{-1}  \right)\right| \nonumber\\
\lesssim & \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2). \label{simple12}
\end{align}
Combining equations \eqref{claim_largedev2} and \eqref{simple12}, we conclude Claim \ref{claim_reduction}.

Now we prove equation \eqref{simple12}. Using Claim \ref{lem_hat_v} and equations \eqref{op_X12} and \eqref{op_Sig1}, we obtain that with high probability,
\begin{align}
 \|\hat\Sigma^{-1}-\hat\Sigma(\hat x)^{-1}\| &\le |\hat x^2-1|\|\hat\Sigma^{-1}\| \| (X^{(1)})^\top X^{(1)}\|\|\hat\Sigma(\hat x)^{-1}\|  \lesssim p^{-1}\left(\frac{d^2}{\kappa^2} + p^{-1/4+c}\right).\label{est111}
\end{align}
Using similar arguments, we get that with high probability,
\begin{align}\label{est222}
&\left\|\left(\hat\Sigma^{-2}-\hat\Sigma(\hat x)^{-2}\right)\left((X^{(1)})^\top X^{(1)} \right)^2\right\| \lesssim  \frac{d^2}{\kappa^2} + p^{-1/4+c} .
\end{align}
Moreover, using equations \eqref{op_X12} and \eqref{op_Sig1}, we can bound that with high probability,
%$$\bignorm{ (X^{(1)})^{\top}X^{(1)} \hat\Sigma^{-1} \Sigma^{(2)} \hat\Sigma^{-1} (X^{(1)})^{\top}X^{(1)}}\lesssim 1.$$
%which further implies that with high probability,
\begin{align}
& \bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}  \le p \bignorm{\hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}\lesssim p.\label{est333}
\end{align}
We can bound the left hand side of equation \eqref{simple12} as
\begin{align*}
&\left|\cal L(\hat x)-\frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left({\hat\Sigma^{-1}  }\right)\right| \\
\lesssim &\left(|\hat x-1|^2 \kappa^2 + |\hat x-1|d^2\right)\cdot p^{-1}\bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2 } \\
&+ \frac{d^2}{p}\left|\tr\left[\left(\hat\Sigma(\hat x)^{-2}-\hat\Sigma^{-2}\right) \left((X^{(1)})^\top X^{(1)} \right)^2\right]\right|+ \sigma^2  \left|\bigtr{\hat\Sigma(\hat x)^{-1}-\hat\Sigma^{-1}  }\right|.
\end{align*}
Applying the estimates \eqref{hatw_add1}, \eqref{est111}, \eqref{est222} and \eqref{est333} to the three terms on the right-hand side, we can conclude \eqref{simple12}.

Finally we give the proof of the estimate \eqref{claim_largedev2}.
The proof of this claim is very similar to Claim \ref{claim_largedev1}, where the only difference is that $\hat x$ now may depend on $\epsilon^{(1)}$, $\epsilon^{(2)}$, $\beta^{(1)}$ and $\beta^{(2)}$.
In this case, we can still use  Lemma \ref{largedeviation} to conclude the proof because $\hat x$ can be pulled out as a coefficient.
Here we only give a proof sketch and omit the details.
Recall that $\beta_0$ is the shared component of $\beta^{(1)}$ and $\beta^{(2)}$ with i.i.d. Gaussian entries of mean zero and variance $p^{-1}\kappa^2$. Moreover, we denote the task-specific components by $\wt\beta^{(1)}$ and $\wt\beta^{(2)}$, whose entries are i.i.d. Gaussian random variables of mean zero and variance $p^{-1} d^2 $. Then we write $L(\hat{\beta}_2^{\MTL}) $ in \eqref{Lbeta_HPS} as:
\begin{align}
L(\hat{\beta}_2^{\MTL})  =&  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x -\hat x^2)\beta_0+(X^{(1)})^\top X^{(1)} \hat x\wt \beta^{(1)} - (X^{(1)})^\top X^{(1)}  \hat x^2\wt \beta^{(2)} \right] \right. \nonumber\\
&\left. + (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1}\left[ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2. \label{expand_15}
\end{align}
As in equation \eqref{expand_6}, we can expand this expression into the sum of 15 terms, and bound each term as in equations \eqref{eq_hA1}-\eqref{eq_hA56}. For example, for the main term $\hat x^2 (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)}$, using 
Lemma \ref{largedeviation} and equations \eqref{op_X12} and \eqref{op_Sig1}, we can obtain the following estimate with high probability:
\begin{align*}
&  \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)} - \frac{d^2}{p}\bigtr{(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}}\right| \\
&\le p^{-1+c}d^2 \cdot \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|_F \\
&\le p^{-1/2+c}d^2 \cdot  \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|  \lesssim p^{-1/2+c}d^2.
\end{align*} 
For the cross term $\hat x (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}$, using Lemma \ref{largedeviation} and equations \eqref{op_X12} and \eqref{op_Sig1}, we can obtain the following estimate with high probability for any small constant $c>0$:
\begin{align*}
 \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}\right|  & \le p^c \cdot \sigma\sqrt{p^{-1}d^2} \cdot \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \|_F \\
&\lesssim p^{c}\cdot \sigma d \cdot  \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \| \\
&\lesssim p^{-1/2+c}\sigma d\le p^{-1/2+c}(\sigma^2+ d^2).
\end{align*}
The rest of the terms in the expansion of \eqref{expand_15} can be bounded in the same way, and we omit the details.
 \end{proof}
  
\paragraph{Part $3$: applying the bias and variance limits.}
Finally, we are ready to complete the proof of Corollary \ref{cor_MTL_loss}.
We derive the variance term $\sigma^2  \tr[\hat\Sigma^{-1}]$ and the bias term $\frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right]$ using Theorem \ref{thm_main_RMT}.

\begin{proof}[Proof of Corollary \ref{cor_MTL_loss}]
Using equation \eqref{lem_cov_shift_eq}, we obtain that
\be\label{eq_var111}\tr[\hat\Sigma^{-1}] = \tr\left[ \left((X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}\right)^{-1}\right]=\bigtr{\frac{(a_1 +a_2)^{-1}\id_{p\times p}}{n_{1}+n_2} }+\OO(p^{-c_\varphi})\ee
with high probability. Solving equation \eqref{eq_a12extra} with $\lambda_i\equiv 1$, $1\le i\le p$, we get that  
	\begin{align}
		 a_1 = \frac{n_1(n_1 + n_2 - p)}{(n_1 + n_2)^2} ,\quad
		& a_2 = \frac{n_2(n_1 + n_2 - p)}{(n_1 +n_2)^2} . \label{simplesovlea12}
			\end{align}
Applying the above to equation \eqref{eq_var111}, we obtain that
\be\label{eq_var112}\tr[\hat\Sigma^{-1}]  = \frac{p}{n_1+n_2} \cdot \frac{n_1+n_2}{n_1+n_2-p}+\OO(p^{-c_\varphi})=  \frac{p}{n_1+n_2-p}+\OO(p^{-c_\varphi})\ee
with high probability. 

On the other hand, we have that with high probability,
\begin{align}
 \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi})}{p}\sum_{i=1}^p \left(\hat \Sigma^{-2}\right)_{ii}  &\le {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] \label{eq_bias111}\\ 
 &\le \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi})}{p}\sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} .\nonumber
\end{align}
To obtain this inequality, we used  
$$ (\sqrt{n_1}-\sqrt{p})^4 \cdot (1-p^{-c_\varphi}) \preceq \left((X^{(1)})^\top X^{(1)} \right)^2 \preceq (\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi}) \quad \text{with high probability},$$
by Fact \ref{fact_minv} (ii), and the fact that for the product of two PSD matrices, its trace is always nonnegative.
Using equation \eqref{lem_cov_derv_eq} with $\Sigma^{(1)}=\Sigma^{(2)}=\Lambda=V=\id_{p\times p}$ and $w$ being the $i$-th coordinate vector, we can calculate that 
\begin{align}\label{eq_bias112}
 \left(\hat\Sigma^{-2}\right)_{ii} = \frac{1}{(n_1+n_2)^2} \left[\frac{a_3+a_4+1}{(a_1+a_2)^2} +\OO(p^{-c_\varphi})\right]
\end{align}
with high probability. Solving equation \eqref{eq_a34extra} with $a_1, a_2$ in \eqref{simplesovlea12} and $\lambda_i\equiv 1$, $1\le i\le p$, we can obtain that
	\begin{align*}
		% a_1 = \frac{\rho_1(\rho_1 + \rho_2 - 1)}{(\rho_1 + \rho_2)^2} ,\quad & a_2 = \frac{\rho_2(\rho_1 + \rho_2 - 1)}{(\rho_1 + \rho_2)^2} , \label{simplesovlea12}\\
		a_3 = \frac{p\cdot n_1}{(n_1 +n_2)(n_1 +n_2 - p)}, \quad
		&  a_4 = \frac{p\cdot n_2}{(n_1 + n_2)(n_1 + n_2 - p)}. %\label{simplesovlea34}
	\end{align*}
Inserting them into equation \eqref{eq_bias112}, we obtain that 
\begin{align*}%\label{eq_bias113}
 \left(\hat\Sigma^{-2}\right)_{ii} =  \frac{1}{(n_1+n_2)^2} \left[  \frac{ (n_1+n_2)^3 }{(n_1+n_2-p)^3} +\OO(p^{-c_\varphi})\right]
\end{align*}
with high probability. In fact, we need this estimate to hold simultaneously for all $1\le i \le p$ with high probability. For this purpose, we shall use equation \eqref{apply derivlocal} to get that 
\begin{align*}%\label{eq_bias113}
\left| \left(\hat\Sigma^{-2}\right)_{ii} - \frac{1}{(n_1+n_2)^2}\cdot \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\prec \frac{p^{-\frac{\varphi-4}{2\varphi}}}{(n_1+n_2)^2}
\end{align*}
on a high probability event that does not depend on $i$. Then using Fact \ref{lem_stodomin} (i), we obtain that 
\begin{align*} 
\left| \sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} - \frac{p}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\prec \frac{p^{1-\frac{\varphi-4}{2\varphi}}}{(n_1+n_2)^2}
\end{align*}
with high probability, which by Definition \ref{stoch_domination} gives that
\begin{align}\label{eq_bias113}
\left| p^{-1}\sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} - \frac{1}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\le \frac{p^{-c_\varphi}}{(n_1+n_2)^2}
\end{align}
with high probability. Inserting equation \eqref{eq_bias113} into equation \eqref{eq_bias111}, we get that
\begin{align*}
		& \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi}) - n_1^2 \cdot (1+\OO(p^{-c_\varphi}))}{(n_1+n_2)^2}\cdot   \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\nonumber \\
		\le &  {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] - \frac{n_1^2}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3} \nonumber\\
		\le &  \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi}) - n_1^2 \cdot (1+\OO(p^{-c_\varphi}))}{(n_1+n_2)^2}\cdot   \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}
\end{align*}
with high probability. 

Combining equation \eqref{eq_var112} and the above with Claim \ref{claim_reduction}, we get that
\begin{align*}
	\left|L(\hat{\beta}_2^{\MTL})  - \frac{\sigma^2 p}{n_1+n_2-p} -  \frac{2d^2 n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3}\right| \le   \left[\left( 1+\sqrt{\frac{p}{n_1}}\right)^4-1\right] \cdot \frac{2d^2n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3} \nonumber\\
+C\left( p^{-c_\varphi} (\sigma^2 + d^2)+ \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2) \right).
\end{align*}
Then using the conditions $\sigma^2 \lesssim  \kappa^2$ and $d^2 \le p^{-\e}{\kappa^2}$, we conclude the proof.
\end{proof}
