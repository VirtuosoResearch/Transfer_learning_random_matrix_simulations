\section{Problem Formulation and Related Work}
\label{sec_prelim}

We begin by defining our problem setup including the multi-task estimator we study.
Then, we describe the bias-variance tradeoff of the multi-task estimator and connect the bias and variance of the estimator to \textit{task similarity}, \textit{sample size}, and \textit{covariate shift}.
%Finally, we show a tight concentration bound for the bias and variance quantities using random matrix theory.

\subsection{Problem Formulation}

\todo{Consider adding some motivation later.}

\noindent\textbf{Data model.}
Suppose we have $t$ labeled regression tasks. %where $t$ is a fixed value that does not grow with the feature dimension $p$.
For each task $k$ from $1$ to $t$, we have $n_k$ samples $x_1, x_2, \dots, x_{n_k}$ that are all $p$-dimensional feature vectors with real-valued labels $y_1, y_2, \dots, y_{n_k}$.
Without loss of generality, we focus on predicting the $t$-th task and refer the $t$-th task as the target.
We refer to task $1$ to task $t-1$ as source tasks.
We assume that the labels of each task $k$ satisfy the linear model with unknown parameters $\beta_k\in\real^p$, that is,
	\[ y_i = x_i^{\top}\beta_k + e_i, \text{ for all } 1 \le i \le n_k, \]
where $e_i$ denotes random noise with mean zero and variance $\sigma^2$.
To write the above notations more succiently, let $X_k \in \real^{n_k \times p}$ denote the covariates that consists of a feature vector in every row.
Let
	\[ Y_k = X_k \beta_k + \varepsilon_k, \text{ for all } 1\le i \le t \] denote the label vector of task $k$ and $\varepsilon_k$ denote an i.i.d. random vector with mean zero and variance $\sigma^2$.

Following \citet{HMRT19} and \citet{BLLT20},
%In the high-dimensional linear regression setting (e.g. ), the features of the $k$-th task, denoted by $X_k\in\real^{n_i\times p}$, consist of $n_k$ feature vectors given by $x_1, x_2, \dots, x_{n_k}$.
we assume that for each task $k$ from $1$ to $t$, each feature vector $x_i = \Sigma^{1/2}_k z_i$ for $i = 1, \dots, n_k$, where $z_i\in\real^p$ is an i.i.d. random vector with  mean zero and unit variance.
The sample size of task $k$, given by $n_k$, is equal to $\rho_k\cdot p$ for a fixed value $\rho_k$ that does not grow with feature dimension $p$.
This setting is also known as the high-dimensional linear regression setting in the literature.
There are two motivations for studying the high-dimensional linear regression setting for multi-task learning.
First, this setting captures salient properties of modern large-scale datasets, where the sample sizes are usually on the order of tens to hundreds of the number of features \cite{sur2019modern}.
Second, as we will see soon in Section \ref{sec_main}, we can derive precise asymptotics of the generalization error that scale with properties of the task data such as sample sizes.
%The labels $Y_k = X_k \beta_k + \varepsilon_k$, where $\beta_k$ denotes the linear model parameters and $\varepsilon_k$ denotes i.i.d. noise with mean zero and variance $\sigma^2$.
%Recall that we have $t$ labeled training datasets, denoted by $(X_1, Y_1), (X_2, Y_2), \dots, (X_t, Y_t)$, where $X_i\in\real^{n_i\times p}$ and $Y_i\in\real^{n_i}$ for $1\le i\le t$.
%Following \cite{HMRT19,BLLT20}, we assume that for each task $i = 1,2,\dots,t$,  every feature vector is generated as $x = \Sigma_i^{1/2} z$, where $z\in\real^p$ is a random vector with i.i.d. entries of mean zero and unit variance and $\Sigma_i\in\real^{p\times p}$ is a positive semidefinite matrix.
%Without loss of generality, let the $t$-th task be the target task.

\medskip
\noindent\textbf{Hard parameter sharing.}
We focus on the commonly used hard parameter sharing model for multi-task learning \cite{R17}.
When specialized to the linear regression setting, the model consists of a linear layer $B\in\real^{p\times r}$ that is shared by all tasks and $t$ output layers $W_1, \dots, W_t$ that are in $\real^r$.
The width of $B$, denoted by $r$, plays an important role in regularization.
As observed in Proposition 1 of \cite{WZR20}, if $r \ge t$, there is no regularization effect.
Hence, we assume that $r < t$ in our study.
For example, when there are only two tasks, $r = 1$ and $B$ reduces to a vector whereas $W_1, W_2$ become scalars.
We study the following procedure inspired by how hard parameter sharing models are trained in practice (e.g. \cite{MTDNN19}).
\squishlist
	\item Separate each dataset $(X_i, Y_i)$ randomly into a training set $(X_i^{tr}, Y_i^{tr})$ and a validation set $(X_i^{val}, Y_i^{val})$.
	The size of each set is described below.
	\item Learn the shared layer $B$: minimize the training loss over $B$ and $W_1, \dots, W_t$, leading to a closed form equation for $\hat{B}$ that depends on $W_1,\dots, W_k$.
		\vspace{-0.075in}
		{\small\begin{align}\label{eq_mtl}
			f(B; W_1, \dots, W_t) = \sum_{k=1}^t \norm{X_k^{tr} B W_k - Y_k^{tr}}^2.
		\end{align}}
		\vspace{-0.075in}
	\item Tune the output layers $W_i$: set $B = \hat{B}$ and minimize the validation loss over $W_1,\dots, W_k$.
		\vspace{-0.075in}
		{\small\begin{align}\label{eq_mtl_eval}
			g(W_1, \dots, W_t) = \sum_{k=1}^t \norm{X_k^{val} \hat{B} W_k - Y_k^{val}}^2.
		\end{align}}
		\vspace{-0.075in}
\squishend
\vspace{-0.1in}
%where $B\in\real^{p\times r}$ and $W_k\in\real^r$ for every $1\le k\le t$.
%Following , we assume that $r < t$, because otherwise minimizing $f(\cdot)$ could result in $BW_i$ being the single-task optimum.
We make several remarks.
In general, the objective $f(\cdot)$ is non-convex in $B$ and the $W_k$'s.
Therefore, we first minimize $B$ in equation \eqref{eq_mtl} and then minimize $W_k$ given $B$ in equation \eqref{eq_mtl_eval}.
For our purpose, a validation set of size $\rho_i \cdot p^{0.99}$ that is much larger than the number of output layer parameters $r\cdot t$ suffices.
The size of the training set is then $\rho_i (p - p^{0.99})$.
The advantage of tuning the output layers on the validation set is to reduce the effect of noise from $\hat{B}$.
%For more details, we refer the reader to Appendix \ref{app_proof_sec3}.

%The single-task estimator $\hat{\beta}_t^{\STL}$ is given by $(X_t^{\top}X_t)^{-1}X_t^{\top}Y_t$.
%For an estimator $\hat{\beta}\in\real^p$, we define the out-of-sample prediction loss as

%which can be further decomposed as the bias plus the variance of $\hat{\beta}$.
%In order to relate the btradeoff to properties of the data, we need tight concentration bounds for the bias and variance.
%We consider the high-dimensional regime where $n_i$ is a fixed constant $\rho_i > 1$  multiple of $p$ for every $1\le i\le t$, and $p$ is sufficiently large.
%We focus on a setting where $\rho_t$ is small compared to $\set{\rho_i}_{i=1}^{t-1}$.
%This setting captures the need to add more labeled data to reduce the prediction loss of the target task.

%However, this result only applies to a single task.
%Therefore, our goal is to extend this result to multiple tasks.

%\textbf{Notations.}
%When there is no ambiguity, we drop the subscript $t$ from $\te_t(\hat{\beta}_t^{\MTL})$ and write $\te(\hat{\beta}_t^{\MTL})$ for simplicity.
%We refer to the first task as the source task when there are only two tasks.
%We call $M = \Sigma_1^{1/2}\Sigma_2^{-1/2}$ the covariate shift matrix.


%\subsection{Analyzing the Tradeoff via Random Matrix Theory}

\medskip
\noindent\textbf{Problem statement.}
We focus on predicting a particular task, say the $t$-th task, without loss of generality.
Let $\hat{\beta}_t^{\MTL}$ denote the multi-task estimator obtained from the procedure above.
Our goal is to compare the prediction loss of $\hat{\beta}_t^{\MTL}$, defined by
{\small\begin{align*}
		\te(\hat{\beta}_t^{\MTL}) = \exargnob{\set{\varepsilon_i}_{i=1}^t}\exarg{x = \Sigma_t^{1/2} z}{({x}^{\top}\hat{\beta} - {x}^{\top}\beta_t)^2}
		= \exargnob{\set{\varepsilon_i}_i^t}\bignorm{\Sigma_2^{1/2} (\hat{\beta}_t^{\MTL} - \beta_t)}^2,
	\end{align*}}%
%= (\hat{\beta} - \beta_t)^{\top}\Sigma_t(\hat{\beta} - \beta_t)
to the prediction loss $L(\hat{\beta}_t^{\STL})$ of the single-task estimator $\hat{\beta}_t^{\STL} = (X_t^{\top}X_t)^{-1}X_t^{\top}{Y_t}$.
We say there is negative transfer if  $L(\hat{\beta}_t^{\MTL}) > L(\hat{\beta}_t^{\STL})$ and positive transfer otherwise.



\subsection{Bias and Variance}

As an example, for the setting of two tasks, we can decompose $L(\hat{\beta}_t^{\MTL}) - L(\hat{\beta}_t^{\STL})$ into a bias term and a variance term as follows (derived in Appendix \ref{app_proof_sec3}).
%Recall that $\hat{\beta}_t^{\MTL}$ is defined as $BW_t$ after solving equation \eqref{eq_mtl}.
%We can  the test error of $\hat{\beta}_{t}^{\MTL}$ on the target task into two parts as follows.
{\small\begin{align}
	\te(\hat{\beta}_t^{\MTL}) - L(\hat{\beta}_t^{\STL}) =& ~ \hat{v}^2 \bignorm{\Sigma_2^{1/2} (\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}X_1^{\top}X_1 (\beta_1 - \hat{v}\beta_2)}^2 \label{eq_te_model_shift} \\
	+&~ \sigma^2 \bigbrace{\bigtr{(\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_2} - \bigtr{(X_2^{\top}X_2)^{-1}\Sigma_2}}. \label{eq_te_var}
\end{align}}%
In the above, $\hat{v} = W_1 / W_2$ where $W_1, W_2$ are obtained from solving equation \eqref{eq_mtl_eval} (recalling that $W_1, W_2$ are scalars for two tasks).
The role of $\hat{v}$ is to scale the shared subspace $B$ to fit each task.

Equation \eqref{eq_te_model_shift} corresponds to the bias of $\hat{\beta}_t^{\MTL}$.
Hence, the bias term introduces a negative effect that depends on the \textit{similarity} between $\beta_1$ and $\beta_2$.
Equation \eqref{eq_te_var} corresponds to the variance of $\hat{\beta}_t^{\MTL}$ minus the variance of $\hat{\beta}_t^{\STL}$, which is always negative.
Intuitively, the more \textit{samples} we have, the smaller the variance is.
Meanwhile, \textit{covariate shift} also affects how small the variance can be.
%This part introduces a positive variance reduction effect from adding the source labels.
%Hence, whether $\te(\hat{\beta}_t^{\MTL}) < \te(\hat{\beta}_t^{\STL})$ is determined precisely by the tradeoff between the negative effect of the bias term and the positive effect of the variance term!
%(i) the negative effect from model shift bias.
%(ii) the positive effect from variance reduction;


\subsection{Related Work}

We refer the interested readers to several excellent surveys on multi-task  learning for a comprehensive survey \cite{PY09,R17,ZY17,V20}.
Our setting is closely related to domain adaptation \cite{DM06,BB07,BC08,DH09,MMR09,CWB11,ZS13,NB17,ZD19}.
The important distinction is that we focus on predicting the target task using a hard parameter sharing model.
For such models, their output dimension plays an important role of regularization \cite{KD12}.
Linear models in multi-task learning have been studied in various settings, including representation learning \cite{BHKL19}, online learning \cite{CCG10,DCSP18}, and sparse regression \cite{LPVT11}.
Below, we describe several lines of work that are most related to this work.

\medskip
\noindent\textbf{Theoretical works.}
Some of the earliest works on multi-task learning are Baxter \cite{B00}, Ben-David and Schuller \cite{BS03}.
Mauer \cite{M06} studies generalization bounds for linear separation settings of MTL.
Ben-David et al. \cite{BBCK10} provides uniform convergence bounds that combines source and target errors optimally.
The benefit of learning multi-task representations has been studied for learning certain half-spaces \cite{MPR16} and sparse regression \cite{LPTV09,LPVT11}.
Our work is closely related to Wu et al. \cite{WZR20}.
While Wu et al. provide generalization bounds to show that adding more labeled helps learn the target task more accurately, their techniques cannot be used to explain when MTL outperforms STL.
\todo{spell out the challenge more explicitly}

% Adding a regularization over $B$, e.g. .
\medskip
\noindent\textbf{Methodological works.}
Ando and Zhang \cite{AZ05} introduces an alternating minimization framework for learning multiple tasks.
Argyriou et al. \cite{AEP08} present a convex algorithm which learns common sparse representations across a pool of related tasks.
Evgeniou et al. \cite{EMP05} develop a framework for multi-task learning in the context of kernel methods.
%\cite{KD12} observed that controlling the capacity can outperform the implicit capacity control of adding regularization over $B$.
The multi-task learning model that we have focused on uses the idea of hard parameter sharing \cite{C93,KD12,R17}.
We believe that our theoretical framework can apply to other approaches to multi-task learning.

\medskip
\noindent\textbf{Random matrix theory.}
The random matrix theory tool and related proof of our work fall into a paradigm of the so-called local law of random matrices \cite{erdos2017dynamical}.
For a sample covariance matrix $X^\top X$ with $\Sigma=\id$, such a local law was proved in \cite{isotropic}.
It was later extended to sample covariance matrices with non-identity $\Sigma$ \cite{Anisotropic}, and separable covariance matrices \cite{yang2019spiked}. On the other hand, one may derive the asymptotic result in Theorem \ref{lem_cov_shift_informal} with error $\oo(1)$ using the free addition of two independent random matrices in free probability theory \cite{nica2006lectures}. To the best of my knowledge, we do not find an {\it explicit result} for the sum of two sample covariance matrices with general covariates in the literature.


