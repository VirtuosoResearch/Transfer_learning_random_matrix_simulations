\section{Problem Formulation and Related Work}
\label{sec_prelim}

We begin by defining our problem setup including the multi-task estimator we study.
Then, we describe the bias-variance tradeoff of the multi-task estimator and connect the bias and variance of the estimator to \textit{task similarity}, \textit{sample size}, and \textit{covariate shift}.
%Finally, we show a tight concentration bound for the bias and variance quantities using random matrix theory.

\subsection{Problem Formulation}

Suppose we have $t$ datasets, where $t$ is a fixed value that does not grow with the feature dimension $p$.
In the high-dimensional linear regression setting (e.g. \cite{HMRT19,BLLT20}), the features of the $k$-th task, denoted by $X_k\in\real^{n_i\times p}$, consist of $n_k$ feature vectors given by $x_1, x_2, \dots, x_{n_k}$.
And each feature $x_i = \Sigma^{1/2}z_i$, where $z_i\in\real^p$ consists of i.i.d. entries with mean zero and unit variance.
The sample size $n_k $ equals $\rho_k\cdot p$ for a fixed value $\rho_k$.
The labels $Y_k = X_k \beta_k + \varepsilon_k$, where $\beta_k$ denotes the linear model parameters and $\varepsilon_k$ denotes i.i.d. noise with mean zero and variance $\sigma^2$.
%Recall that we have $t$ labeled training datasets, denoted by $(X_1, Y_1), (X_2, Y_2), \dots, (X_t, Y_t)$, where $X_i\in\real^{n_i\times p}$ and $Y_i\in\real^{n_i}$ for $1\le i\le t$.
%Following \cite{HMRT19,BLLT20}, we assume that for each task $i = 1,2,\dots,t$,  every feature vector is generated as $x = \Sigma_i^{1/2} z$, where $z\in\real^p$ is a random vector with i.i.d. entries of mean zero and unit variance and $\Sigma_i\in\real^{p\times p}$ is a positive semidefinite matrix.
%Without loss of generality, let the $t$-th task be the target task.

We focus on the commonly used hard parameter sharing model for multi-task learning \cite{R17}.
When specialized to the linear regression setting, the model consists of a linear layer $B\in\real^{p\times r}$ that is shared by all tasks and $t$ output layers $W_1, \dots, W_t$ that are in $\real^r$.
The width of $B$, denoted by $r$, plays an important role in regularization.
As observed in Proposition 1 of \cite{WZR20}, if $r \ge t$, there is no regularization effect.
Hence, we assume that $r < t$ in our study.
For example, when there are only two tasks, $r = 1$ and $B$ reduces to a vector whereas $W_1, W_2$ become scalars.
We study the following procedure inspired by how hard parameter sharing models are trained in practice (e.g. \cite{MTDNN19}).
\squishlist
	\item Separate each dataset $(X_i, Y_i)$ randomly into a training set $(X_i^{tr}, Y_i^{tr})$ and a validation set $(X_i^{val}, Y_i^{val})$.
	The size of each set is described below.
	\item Learn the shared layer $B$: minimize the training loss over $B$ and $W_1, \dots, W_t$, leading to a closed form equation for $\hat{B}$ that depends on $W_1,\dots, W_k$.
		\vspace{-0.075in}
		{\small\begin{align}\label{eq_mtl}
			f(B; W_1, \dots, W_t) = \sum_{k=1}^t \norm{X_k^{tr} B W_k - Y_k^{tr}}^2.
		\end{align}}
		\vspace{-0.075in}
	\item Tune the output layers $W_i$: set $B = \hat{B}$ and minimize the validation loss over $W_1,\dots, W_k$.
		\vspace{-0.075in}
		{\small\begin{align}\label{eq_mtl_eval}
			g(W_1, \dots, W_t) = \sum_{k=1}^t \norm{X_k^{val} \hat{B} W_k - Y_k^{val}}^2.
		\end{align}}
		\vspace{-0.075in}
\squishend
\vspace{-0.1in}
%where $B\in\real^{p\times r}$ and $W_k\in\real^r$ for every $1\le k\le t$.
%Following , we assume that $r < t$, because otherwise minimizing $f(\cdot)$ could result in $BW_i$ being the single-task optimum.
We make several remarks.
In general, the objective $f(\cdot)$ is non-convex in $B$ and the $W_k$'s.
Therefore, we first minimize $B$ in equation \eqref{eq_mtl} and then minimize $W_k$ given $B$ in equation \eqref{eq_mtl_eval}.
For our purpose, a validation set of size $\rho_i \cdot p^{0.99}$ that is much larger than the number of output layer parameters $r\cdot t$ suffices.
The size of the training set is then $\rho_i (p - p^{0.99})$.
The advantage of tuning the output layers on the validation set is to reduce the effect of noise from $\hat{B}$.
%For more details, we refer the reader to Appendix \ref{app_proof_sec3}.

%The single-task estimator $\hat{\beta}_t^{\STL}$ is given by $(X_t^{\top}X_t)^{-1}X_t^{\top}Y_t$.
%For an estimator $\hat{\beta}\in\real^p$, we define the out-of-sample prediction loss as

%which can be further decomposed as the bias plus the variance of $\hat{\beta}$.
%In order to relate the btradeoff to properties of the data, we need tight concentration bounds for the bias and variance.
%We consider the high-dimensional regime where $n_i$ is a fixed constant $\rho_i > 1$  multiple of $p$ for every $1\le i\le t$, and $p$ is sufficiently large.
%We focus on a setting where $\rho_t$ is small compared to $\set{\rho_i}_{i=1}^{t-1}$.
%This setting captures the need to add more labeled data to reduce the prediction loss of the target task.

%However, this result only applies to a single task.
%Therefore, our goal is to extend this result to multiple tasks.

%\textbf{Notations.}
%When there is no ambiguity, we drop the subscript $t$ from $\te_t(\hat{\beta}_t^{\MTL})$ and write $\te(\hat{\beta}_t^{\MTL})$ for simplicity.
%We refer to the first task as the source task when there are only two tasks.
%We call $M = \Sigma_1^{1/2}\Sigma_2^{-1/2}$ the covariate shift matrix.


%\subsection{Analyzing the Tradeoff via Random Matrix Theory}

\textbf{Problem statement.}
We focus on predicting a particular task, say the $t$-th task, without loss of generality.
Let $\hat{\beta}_t^{\MTL}$ denote the multi-task estimator obtained from the procedure above.
Our goal is to compare the prediction loss of $\hat{\beta}_t^{\MTL}$, defined by
{\small\begin{align*}
		\te(\hat{\beta}_t^{\MTL}) = \exargnob{\set{\varepsilon_i}_{i=1}^t}\exarg{x = \Sigma_t^{1/2} z}{({x}^{\top}\hat{\beta} - {x}^{\top}\beta_t)^2}
		= \exargnob{\set{\varepsilon_i}_i^t}\bignorm{\Sigma_2^{1/2} (\hat{\beta}_t^{\MTL} - \beta_t)}^2,
	\end{align*}}%
%= (\hat{\beta} - \beta_t)^{\top}\Sigma_t(\hat{\beta} - \beta_t)
to the prediction loss $L(\hat{\beta}_t^{\STL})$ of the single-task estimator $\hat{\beta}_t^{\STL} = (X_t^{\top}X_t)^{-1}X_t^{\top}{Y_t}$.
We say there is negative transfer if  $L(\hat{\beta}_t^{\MTL}) > L(\hat{\beta}_t^{\STL})$ and positive transfer otherwise.



\subsection{Bias and Variance}

As an example, for the setting of two tasks, we can decompose $L(\hat{\beta}_t^{\MTL}) - L(\hat{\beta}_t^{\STL})$ into a bias term and a variance term as follows (derived in Appendix \ref{app_proof_sec3}).
%Recall that $\hat{\beta}_t^{\MTL}$ is defined as $BW_t$ after solving equation \eqref{eq_mtl}.
%We can  the test error of $\hat{\beta}_{t}^{\MTL}$ on the target task into two parts as follows.
{\small\begin{align}
	\te(\hat{\beta}_t^{\MTL}) - L(\hat{\beta}_t^{\STL}) =& ~ \hat{v}^2 \bignorm{\Sigma_2^{1/2} (\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}X_1^{\top}X_1 (\beta_1 - \hat{v}\beta_2)}^2 \label{eq_te_model_shift} \\
	+&~ \sigma^2 \bigbrace{\bigtr{(\hat{v}^2 X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}\Sigma_2} - \bigtr{(X_2^{\top}X_2)^{-1}\Sigma_2}}. \label{eq_te_var}
\end{align}}%
In the above, $\hat{v} = W_1 / W_2$ where $W_1, W_2$ are obtained from solving equation \eqref{eq_mtl_eval} (recalling that $W_1, W_2$ are scalars for two tasks).
The role of $\hat{v}$ is to scale the shared subspace $B$ to fit each task.

Equation \eqref{eq_te_model_shift} corresponds to the bias of $\hat{\beta}_t^{\MTL}$.
Hence, the bias term introduces a negative effect that depends on the \textit{similarity} between $\beta_1$ and $\beta_2$.
Equation \eqref{eq_te_var} corresponds to the variance of $\hat{\beta}_t^{\MTL}$ minus the variance of $\hat{\beta}_t^{\STL}$, which is always negative.
Intuitively, the more \textit{samples} we have, the smaller the variance is.
Meanwhile, \textit{covariate shift} also affects how small the variance can be.
%This part introduces a positive variance reduction effect from adding the source labels.
%Hence, whether $\te(\hat{\beta}_t^{\MTL}) < \te(\hat{\beta}_t^{\STL})$ is determined precisely by the tradeoff between the negative effect of the bias term and the positive effect of the variance term!
%(i) the negative effect from model shift bias.
%(ii) the positive effect from variance reduction;


\subsection{Related Work}

We refer the interested readers to several excellent surveys on multi-task  learning for a comprehensive survey \cite{PY09,R17,ZY17,V20}.
Below, we describe several lines of work that are most related to this work.

\textit{Theoretical works.}
Some of the earliest works on multi-task learning are Baxter \cite{B00}, Ben-David and Schuller \cite{BS03}.
Mauer \cite{M06} studies generalization bounds for linear separation settings of MTL.
Ben-David et al. \cite{BBCK10} provides uniform convergence bounds that combines source and target errors optimally.
The benefit of learning multi-task representations has been studied for learning certain half-spaces \cite{MPR16} and sparse regression \cite{LPTV09,LPVT11}.
Our work is closely related to Wu et al. \cite{WZR20}.
While Wu et al. provide generalization bounds to show that adding more labeled helps learn the target task more accurately, their techniques cannot be used to explain when MTL outperforms STL.

% Adding a regularization over $B$, e.g. .
\textit{Methodological works.}
Ando and Zhang \cite{AZ05} introduces an alternating minimization framework for learning multiple tasks.
Argyriou et al. \cite{AEP08} present a convex algorithm which learns common sparse representations across a pool of related tasks.
Evgeniou et al. \cite{EMP05} develop a framework for multi-task learning in the context of kernel methods.
%\cite{KD12} observed that controlling the capacity can outperform the implicit capacity control of adding regularization over $B$.
The multi-task learning model that we have focused on uses the idea of hard parameter sharing \cite{C93,KD12,R17}.
We believe that our theoretical framework can apply to other approaches to multi-task learning.

\textit{Random matrix theory.}
The random matrix theory tool and related proof of our work fall into a paradigm of the so-called local law of random matrices \cite{erdos2017dynamical}.
For a sample covariance matrix $X^\top X$ with $\Sigma=\id$, such a local law was proved in \cite{isotropic}.
It was later extended to sample covariance matrices with non-identity $\Sigma$ \cite{Anisotropic}, and separable covariance matrices \cite{yang2019spiked}. On the other hand, one may derive the asymptotic result in Theorem \ref{lem_cov_shift_informal} with error $\oo(1)$ using the free addition of two independent random matrices in free probability theory \cite{nica2006lectures}. To the best of my knowledge, we do not find an {\it explicit result} for the sum of two sample covariance matrices with general covariates in the literature.

\iffalse
\textbf{Transfer learning.}
We extend Theorem \ref{thm_main_informal} to transfer learning settings.
We study the transfer procedure used in Taskonomy by Zamir et al.'18 \cite{ZSSGM18}.
For the setting of high-dimensional linear regression, the transfer procedure is as follows.
%Specifically, the source task encoder consists of the representations learnt from one or more source tasks.
%The transfer function then tries to fit the target task data to the source task encoder.
\squishlist
	\item \textit{Learning source task representations}: we obtain $\hat{\beta}_i^{\STL}$ from each source task, for $1\le i \le t-1$.
		This forms a shared representation ${B} = [\hat{\beta}_1,\hat{\beta}_2,\dots,\hat{\beta}_{t-1}]$.
	\item \textit{Fine-tuning on the target task}: we learn the output layer $W_t\in\real^{t-1}$ similar to equation \eqref{eq_mtl}
		\begin{align}
			g(W_t) = \bignorm{X_t B W_t - Y_t}^2.
		\end{align}
\squishend
After solving $W_t$, we use $\hat{\beta}_t^{\TL} = B W_t$ as the transfer learning (TL) estimator for the target task.
Our result naturally applies to $\hat{\beta}_t^{\TL}$.
Interestingly, we show that the model shift bias is simply the projection of $\beta_t$ to the orthogonal subspace of $B^{\star} = [\beta_1,\dots,\beta_{t-1}]$, or $\norm{\Sigma_t^{1/2}(\id - B^{\star}({B^{\star}}^{\top}B^{\star})^{-1}{B^{\star}}^{\top})\beta_t}^2$ more precisely.
The formal statement is presented in Theorem \ref{prop_taskonomy} and its proof in Appendix \ref{app_proof_sec4}.
\fi


%In order to analyze the tradeoff, we develop the following technical tool, which provides a tight bound for equation \eqref{eq_te_var}.
%where $(a_1, a_2)$ is the solution to the following deterministic equations:
%	\begin{align*}
%		a_1 + a_2 = 1- \frac{1}{\rho_1 + \rho_2},\quad a_1 + \frac1{\rho_1 + \rho_2}\cdot \frac{1}{p}\sum_{i=1}^p \frac{\lambda_i^2 a_1}{\lambda_i^2 a_1 + a_2} = \frac{\rho_1}{\rho_1 + \rho_2}.
%	\end{align*}

%\textbf{Remark.} When there is only the target task, $\rho_1$ equals zero.
%Hence $a_2 = 1 - 1/ \rho_2, a_1 = 0$ and Lemma \ref{lem_cov_shift_informal} states that $\tr[(X_2^{\top}X_2)^{-1}\Sigma_2] = \sigma^2 / (\rho_2 - 1)$, which is a well-known result in random matrix theory \cite{S07}.
%Hence our result provides a necessary tool to analyze MTL in high dimensions.

