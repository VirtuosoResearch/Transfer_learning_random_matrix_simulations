\subsection{Proof of the Bias and Variance Asymptotics}\label{appendix RMT}

\subsubsection{The Matrix Limit}\label{sec pf RMTlemma}

We now state the main random matrix result---Theorem \ref{LEM_SMALL}---which gives an almost optimal estimate on the resolvent $G(z)$ of $H$. It is conventionally called the {\it anisotropic local law} \cite{Anisotropic}. We define a domain of the spectral parameter $z$ as
\begin{equation}
\mathbf D:= \left\{z=E+ \ii \eta \in \C_+: |z|\le (\log n)^{-1} \right\}. \label{SSET1}
\end{equation}
%The following theorem gives an almost optimal estimate on the resolvent $G$, which is conventionally called the {\it anisotropic local law}. %They imply Lemma \ref{lem_cov_shift} and equation \eqref{derivative} immediately. 

\begin{theorem} \label{LEM_SMALL} %[Results on covariance matrices with small support]
Suppose Assumption \ref{assm_big1} holds. 
%and $Z^{(1)}$ and $Z^{(2)}$ satisfy the bounded support condition equation \eqref{eq_support} for a deterministic parameter $q$ satisfying $ n^{-{1}/{2}} \leq q \leq n^{- \phi} $ for some constant $\phi>0$. 
Then %there exists a sufficiently small constant $c_0>0$ such that 
the following anisotropic local law holds uniformly for all $z\in \mathbf D$:
%\begin{itemize}
%\item[(1)] {\bf Anisotropic local law}: 
for any deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb R^{p+n_1+n_2}$, %we have
\begin{equation}\label{aniso_law}
\left| \mathbf u^\top (G(z)-\Gi(z)) \mathbf v \right|  \prec  q,
\end{equation}
where recall that $\Gi$ was defined in equation \eqref{defn_piw}.
%\item[(2)] {\bf Averaged local law}: We have
%\begin{equation}
% \left\vert {p}^{-1}\sum_{i\in \cal I_1}\left[G_{ii}(z)-\Pi_{ii}(z)\right] \right\vert \prec n^{-1}. \label{aver_in} %+ q^2 
%\end{equation}
%%where $m$ is defined in equation \eqref{defn_m}. Moreover, outside of the spectrum we have the following stronger estimate
%%\begin{equation}\label{aver_out1}
%% | m(z)-M(z)|\prec q^2  + \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%%\end{equation}
%%uniformly in $z\in \wt  S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$, where $\kappa$ is defined in equation \eqref{KAPPA}. 
%\end{itemize}
\end{theorem}
As discussed in Remark \ref{rem_athmoment}, using a simple cutoff argument, it is easy to obtain from Theorem \ref{LEM_SMALL} the following corollary under certain finite moment condition. 

\begin{corollary}\label{main_cor}
Assume that the entries of $Z^{(1)}$ and $Z^{(2)}$ are i.i.d. random variables of zero mean, unit variance, and finite $\varphi$-th moment as in \eqref{assmAhigh}.
%\be\label{condition_4e} 
% \max_{i\in \cal I_0,\mu \in \cal I_1}\E \vert Z^{(1)}_{\mu i}\vert ^{a}  =\OO(1), \quad \max_{i\in \cal I_0,\nu \in \cal I_2}\E\vert Z^{(2)}_{\nu i}\vert^{a}  =\OO(1), 
%\ee 
%for a constant $a>4$. 
Assume that conditions \eqref{assm2} and \eqref{assm32} hold. Then equation \eqref{aniso_law} holds for $q= n^{-\frac{\varphi - 4}{2\varphi}}$ on an event $\Omega$ with $\P(\Omega)=1-\oo(1)$.
\end{corollary}
\begin{proof}%[Proof of Corollary \ref{main_cor}]
%Fix any sufficiently small constant $\e>0$. We choose $q= n^{-c_a +\e}$ with $c_a:=1/2-2/a $. Then 
As discussed in Remark \ref{rem_athmoment}, we introduce the truncated matrices $\wt Z^{(1)}$ and $\wt Z^{(2)}$ with entries
$$ \wt Z^{(1)}_{\mu i}:= \mathbf 1\left( n^{-1/2}|Z^{(1)}_{\mu i}|\le q \log n \right)\cdot Z^{(1)}_{\mu i}, \quad \wt Z^{(2)}_{\nu i}:= \mathbf 1\left( n^{-1/2}|Z^{(2)}_{\nu i}|\le q \log n \right)\cdot Z^{(2)}_{\nu i}, $$
%where, for convenience, we again use $j$ to denote the column index of $X$. 
for $q= n^{-\frac{\varphi - 4}{2\varphi}}$. By equation \eqref{Ptrunc}, 
%the moment conditions (\ref{condition_4e}) and a simple union bound, 
we have
\begin{equation}\label{XneX}
\mathbb P(\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}) =1-\OO ( (\log n)^{-\varphi}).
\end{equation}
By definition, we have 
\be\label{EwtZ}\E  \wt  Z^{(1)}_{\mu i} = - \mathbb E \left[ \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)Z^{(1)}_{\mu i}\right] ,\quad \E  |\wt  Z^{(1)}_{\mu i}|^2 = 1 - \mathbb E \left[ \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)|Z^{(1)}_{\mu i}|^2\right] .\ee
Using the formula for expectation in terms of the tail probabilities, we can check that
\begin{align*}
&  \mathbb E \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| = \int_0^\infty \P\left( \left| \mathbf 1\left(  |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| \ge s\right)\dd s \\
&  = \int_0^{qn^{1/2}\log n}\P\left( |Z^{(1)}_{\mu i}|> q  n^{1/2}\log n \right)\dd s +\int_{qn^{1/2}\log n}^\infty \P\left(|Z^{(1)}_{\mu i}| \ge s\right)\dd s  \\
&\lesssim \int_0^{qn^{1/2}\log n}\left(q  n^{1/2}\log n \right)^{-\varphi}\dd s +\int_{qn^{1/2}\log n}^\infty s^{-\varphi}\dd s \le n^{-2(\varphi-1)/\varphi}.
\end{align*}
where in the third step we used condition \eqref{assmAhigh} and Markov's inequality. Similarly, we can obtain that 
\begin{align*}
&  \mathbb E \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)Z^{(1)}_{\mu i}\right|^2 = 2\int_0^\infty s \P\left( \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| \ge s\right)\dd s \\
&  = 2\int_0^{qn^{1/2}\log n} s \P\left( |Z^{(1)}_{\mu i}|> q  n^{1/2}\log n \right)\dd s +2\int_{qn^{1/2}\log n}^\infty s\P\left(|Z^{(1)}_{\mu i}| \ge s\right)\dd s  \\
&\lesssim \int_0^{qn^{1/2}\log n}s\left(q  n^{1/2}\log n \right)^{-\varphi}\dd s +\int_{qn^{1/2}\log n}^\infty s^{-\varphi+1}\dd s \le n^{-2(\varphi-2)/\varphi}.
\end{align*}
Plugging the above two estimates into equation \eqref{EwtZ} and using $\varphi>4$, we get that
%Using (\ref{condition_4e}) and integration by parts, it is easy to verify that %we can get that
%\begin{align*}
%\mathbb E  |z^{(\al)}_{ij}|1_{|z^{(\al)}_{ij}|> q} =\OO(n^{-2-\e}), \quad \mathbb E |z^{(\al)}_{ij}|^2 1_{|z^{(\al)}_{ij}|> q} =\OO(n^{-2-\e}), \quad \al=1,2,
%\end{align*}
%which imply that
\be\label{meanshif}
|\mathbb E  \wt  Z^{(1)}_{\mu i}| =\OO(n^{-3/2}), \quad  \mathbb E |\wt  Z^{(1)}_{\mu i}|^2 =1+ \OO(n^{-1}).
\ee
From the first estimate in equation \eqref{meanshif}, we can also get a bound on the operator norm:
\be\label{EZ norm}\|\E \wt Z^{(1)}\|=\OO(n^{-1/2}) .
\ee
Similar estimates also hold for $\wt Z^{(2)}$. 
%and
%$$\left| \mathbb E\wt  x_{ij}^2\right| =O( n^{-2-\omega/2}), \ \ \text{if $x_{ij}$ is complex.} $$
%Moreover, we trivially have
%$$\mathbb E  |\wt  z^{(\al)}_{ij}|^4 \le \mathbb E  |z^{(\al)}_{ij}|^4 =\OO(n^{-2}), \quad \al=1,2.$$
Then we can centralize and rescale $\wt Z^{(1)}$ and $\wt Z^{(2)}$ as
$$ \wh Z^{(1)} :=\frac{\wt Z^{(1)} - \E \wt Z^{(1)} }{\left(\E|\wt Z^{(1)}_{\mu i}|^2\right)^{1/2}},\quad \wh Z^{(2)} :=\frac{\wt Z^{(2)} - \E \wt Z^{(2)} }{\left(\E|\wt Z^{(2)}_{\mu i}|^2\right)^{1/2}}.$$ 
Now $\wh Z^{(1)}$ and $\wh Z^{(2)}$ satisfy the assumptions of Theorem \ref{LEM_SMALL} with bounded support $q$, so we get that %and equation \eqref{aniso_law} gives that
\be\label{GwhZZ}\left| \mathbf u^\top (G(\wh Z^{(1)},\wh Z^{(2)},z)-\Gi(z)) \mathbf v \right|  \prec  q,\ee
where $G(\wh Z^{(1)},\wh Z^{(2)},z)$ is defined in the same way as $G(z)$, but with $(Z^{(1)}, Z^{(2)})$ replaced by $(\wh Z^{(1)},\wh Z^{(2)})$.

Note that by equations \eqref{meanshif} and \eqref{EZ norm}, we can bound that for $k=1,2,$
$$ \|\wh Z^{(k)} - \wt Z^{(k)}\|\lesssim n^{-1}\|\wt Z^{(k)}\| + \|\E \wt Z^{(k)}\|\lesssim n^{-1/2}$$
with overwhelming probability, where we also used Fact \ref{lem_minv} to bound the operator norm of $\wt Z^{(k)}$. Together with equation \eqref{priorim} below, this bound implies that % check that
$$\left| \mathbf u^\top (G(\wh Z^{(1)},\wh Z^{(2)},z)-G(Z^{(1)},Z^{(2)},z)) \mathbf v \right|  \lesssim  n^{-1/2}\|\wh Z^{(1)} - \wt Z^{(1)}\|+ n^{-1/2}\|\wh Z^{(2)} - \wt Z^{(2)}\|\lesssim n^{-1},$$
with overwhelming probability on the event $\{\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}\}$. Combining this estimate with equation \eqref{GwhZZ}, we obtain that estimate \eqref{aniso_law} also holds for $G(z)$ on the event $\{\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}\}$, which concludes the proof by equation \eqref{XneX}.
\end{proof}


 


%With this corollary, we can easily extend Lemma \ref{lem_cov_shift} and Lemma \ref{lem_cov_derivative} to the case with weaker moment assumptions. Considering the length of this paper, we will not go into further details here. 

Now we are ready to complete the proof of Theorem \ref{thm_main_RMT} using Corollary \ref{main_cor}.

\begin{proof}[Proof of Theorem \ref{thm_main_RMT}, Part i)]
Recall that in the setting of Theorem \ref{thm_main_RMT} we have equation \eqref{rewrite X as R},
%using equation \eqref{eigen2extra} and equation \eqref{mainG} we can write 
%$$\cal R:= ( (X^{(1)})^\top X^{(1)} + (X^{(2)})^\top X^{(2)})^{-1}= n^{-1}\Sigma_2^{-1/2}V \cal G(0)V^\top\Sigma_2^{-1/2}.$$
%where $$\cal G(0)=\left(   \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^{-1}.$$
%%where $\Sigma_1$, $\Sigma_2$, $Z^{(1)}$ and $Z^{(2)}$ satisfy Assumption \ref{assm_big1}. H
%(Here the extra $n^{-1}$ is due to the choice of the scaling---in the setting of Lemma \ref{lem_cov_shift} the variances of the $Z_{1}$ and $Z^{(2)}$ entries are equal to 1, while they are taken to be $n^{-1}$ in the above expression.) 
%%As in equation \eqref{eigen2}, we assume that $M:=\wt\Sig_1^{1/2} \Sig_2^{-1/2}$ has singular value decomposition
%%\be\label{tildeM}
%%M= U\Lambda V^\top, \quad \Lambda=\text{diag}( \sigma, \ldots, \sigma_p).
%%\ee
%Then as in equation \eqref{eigen2extra}, we can rewrite $\cal R$ as
%$$\cal R= n^{-1}\Sigma_2^{-1/2}V \cal G(0)V^\top\Sigma_2^{-1/2},\quad \cal G(0)=\left(   \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^{-1}.$$
and that the entries of $Z^{(1)}$ and $Z^{(2)}$ have finite $\varphi$-th moments as in equation \eqref{assmAhigh}.
%Suppose $\Sigma=\sum_i \lambda_i(\Sigma)\mathbf u_i \mathbf v_i^\top$ is a SVD of $\Sigma$ with $\{\mathbf u_i\}$ and $\{\bv_i\}$ being the left and right singular vectors. 
Then by Corollary \ref{main_cor}, for $1\le i \le p$ we have that
%$Z^{(1)}$ and $Z^{(2)}$ have bounded support $q=n^{-1/2}$. Using Theorem \ref{LEM_SMALL}, %Now by Corollary \ref{main_cor}, we obtain that for any small constant $\e>0$, %with probability $1-\oo(1)$,
\begin{align}
& \left| \left[\left((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}\right)^{-1}\Sigma - n^{-1}  \Sigma_2^{-1/2}V \Gi(0)V^\top\Sigma_2^{-1/2}\Sigma\right]_{ii}\right| \nonumber\\
&=n^{-1} \left|\mathbf e_i^\top \Sigma_2^{-1/2}V \left(\cal G(0)-\Gi(0)\right)V^\top\Sigma_2^{-1/2}\Sigma\mathbf e_i\right| \prec n^{-1}q\|\Sigma\mathbf e_i\| ,\label{G0Pi0}
\end{align}
on an event $\Omega$ with $\P(\Omega)=1-\oo(1)$, where $ q= n^{-\frac{\varphi - 4}{2\varphi}}$ and $\mathbf e_i$ denotes the standard basis vector along the $i$-th direction. Then with the calculations in equation \eqref{Gi00}, we get that
$$ n^{-1}  \Sigma_2^{-1/2}V \Gi(0)V^\top\Sigma_2^{-1/2}\Sigma= n^{-1}(a_1\Sigma_1 +  a_2\Sigma_2)^{-1}\Sigma.$$
%$$\Gi(0)= -(a_{1}(0)\Lambda^2  +  a_{2}(0))^{-1}= (a_1 V^\top M^\top M V +  a_2)^{-1},$$
%with $(a_1,a_2)$ satisfying equation \eqref{selfomega0}. 
Together with equation \eqref{G0Pi0}, this identity implies that % we get that
\begin{align*}
 \tr \left[\left((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}\right)^{-1}\Sigma\right]& = \sum_{i=1}^p\left[\left((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}\right)^{-1}\Sigma\right]_{ii} \\
 &= n^{-1}\tr \left[ \left(a_1  \Sigma_1  +  a_2\Sigma_2\right)^{-1}\Sigma\right] +\OO_\prec(q \|A\|) 
 \end{align*}
 on event $\Omega$.
%with probability $1-\oo(1)$. 
This concludes equation \eqref{lem_cov_shift_eq}. %if we rename $(r_1X^{(2)},r_2x_3)$ to $(a_1,a_2)$. 
%For equation \eqref{lem_cov_shift_eq}, it is a well-known result for inverse Whishart matrices {\color{red}(add some references)}. 
%Note that if we set $n_1=0$ and $n_2=n$, then $a_1 = 0$ and $a_2 = (n_2-p) / n_2$ is the solution to equation \eqref{eq_a12extra}. This gives equation \eqref{XXA} using equation \eqref{lem_cov_shift_eq}. 
\end{proof}
 

\begin{proof}[Proof of Theorem \ref{thm_main_RMT}, part ii)]
Recall that in the setting of Theorem \ref{thm_main_RMT}, we have equation \eqref{calculate G'}.
%\begin{align*}
% n^{2}\bignorm{\Sigma_2^{1/2} ( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)})^{-1}\Sigma_1^{1/2}w}^2 =\beta^\top \Sigma_2^{-1/2}  \left(M^\top (Z^{(1)})^{\top} Z^{(1)} M +  (Z^{(2)})^\top Z^{(2)} \right)^{-2}   \Sigma_2^{-1/2}\beta,
%\end{align*}
%%where $\wt\Sigma_1:= w^2 \Sigma_1$, $\Sigma_2$, $Z^{(1)}$ and $Z^{(2)}$ satisfy Assumption \ref{assm_big1} and $M:=\wt\Sig_1^{1/2} \Sig_2^{-1/2}$. 
%where in the second step the $n^{2}$ factor disappeared due to the choice of scaling in equation \eqref{assm1}. 
%Again we assume that $M$ has the singular value decomposition equation \eqref{tildeM}. 
%With equation \eqref{eigen2}, we can write the above expression as
%$$n^2\bignorm{\Sigma_2^{1/2} ( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)})^{-1}\beta}^2= \bv^\top \cal G^2(0)  \bv,\quad \bv:=V^\top  \Sigma_2^{-1/2} \beta .$$ 
For simplicity, we denote the vector $\bv:=V^\top  \Sigma_2^{-1/2} \Sigma_1^{1/2}w$. 
%Note that $\cal G^2(0)=\partial_z \cal G|_{z=0}$. 
Under the moment condition \eqref{assmAhigh}, using Corollary \ref{main_cor} we obtain that
%$Z^{(1)}$ and $Z^{(2)}$ have bounded support $q=n^{-1/2}$. Using Theorem \ref{LEM_SMALL}, %Now by Corollary \ref{main_cor}, we obtain that for any small constant $\e>0$, %with probability $1-\oo(1)$,
\be\nonumber 
\max_{z\in \C:|z|=(\log n)^{-1}}|\mathbf v^\top (G(z)-\Gi(z))\mathbf v| \prec q \|\mathbf v\|^2,\quad q:= n^{-\frac{\varphi - 4}{2\varphi}},
\ee
on an event $\Omega$ with $\P(\Omega)=1-\oo(1)$. Now combining this estimate with Cauchy's integral formula, we get that on $\Omega$, %with probability $1-\oo(1)$, 
\be\label{apply derivlocal}
\begin{split}
  \bv^\top \cal G'(0)\bv  = \frac{1}{2\pi \ii}\oint_{\cal C} \frac{ \bv^\top \cal G(z)\bv }{z^2}\dd z &=  \frac{1}{2\pi \ii}\oint_{\cal C} \frac{ \bv^\top\Gi(z)\bv}{z^2}\dd z +\OO_\prec(q\|\mathbf v\|^2) \\
  &=  \bv^\top \Gi'(0)\bv + \OO_\prec(q\|\mathbf v\|^2),
\end{split}
\ee
where $\cal C$ is the contour $\{z\in \C: |z| = (\log n)^{-1} \}$. We can calculate the derivative $\bv^\top \Gi'(0)\bv$ as
\be\label{dervPi}
\bv^\top \Gi'(0)\bv = \bv  \frac{a_3\Lambda^2+(1+a_4)\id_p}{(a_{1}\Lambda^2 + a_{2}\id_p)^2}\bv,
\ee
where we recall \eqref{cal G'0} and that $a_3=-a_1'(0)$ and $a_4=-a_2'(0)$. 
%Then we have
%$$  \left\|(X^{(1)})^{\top}X^{(1)} (\beta_s - \beta_t)  - \frac{n_1}{n}(\beta_s - \beta_t)\right\|_2 \le C \sqrt{\frac{p}{n}} \left\| (\beta_s - \beta_t)\right\|_2. $$
%\todo{(revise the following proof)} It remain to study the following expression
%\begin{align*}
%\frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}}  \Sigma_2 \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}}  = \Sigma_2^{-1/2}\left(\frac{1}{A^T (Z^{(1)})^{\top}Z^{(1)} A + (Z^{(2)})^{\top}Z^{(2)}} \right)^2 \Sigma_2^{-1/2} \\
%\stackrel{d}{=} \Sigma_2^{-1/2}V \left(\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)}} \right)^2 V^T \Sigma_2^{-1/2},
%\end{align*}
%where
%\be  \label{eigen2000}
%A:=\Sigma_1^{1/2}\Sigma_2^{-1/2} = U\Lambda V^T ,\quad \Lambda=\text{diag}(\lambda_1, \cdots, \lambda_p).
%\ee
%Using
%$$\left(\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)}} \right)^2 =\left. \frac{\dd }{\dd z}\right|_{z=0}\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)} - z} ,$$
%we  need to study the resolvent of
%$$G(z) = \left( \Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)} - z \right)^{-1}.$$
%Its local law can be studied as in previous subsection (be careful we need to switch the roles of $Z^{(1)}$ and $Z^{(2)}$). More precisely,  we have that
%$$ G(z) \approx \diag\left( \frac{1}{-z\left( 1+ m_3(z) + \lambda_i^2 m_4(z)\right)}\right)_{1\le i \le p}= \frac{1}{-z\left( 1+ m_3(z) + \Lambda^2 m_4(z)\right)} .$$
%Here $m_{3,4}(z)$ satisfy the following self-consistent equations
%%$$\frac{1}{G_{ii}} \approx -z \left( 1+m_3 + d_i^2m_4 \right), \quad \frac{1}{G_{\mu\mu}} = -z(1+m_1), \ \ \mu\in \cal I_1,\quad \frac{1}{G_{\nu\nu}} = -z(1+m_2), \ \ \nu\in \cal I_2,$$
%%$$m_1= \frac1n\sum_{i}G_{ii}, \quad m_2= \frac1n\sum_{i}d_i^2 G_{ii}, \quad m_3 = \frac1n\sum_{\mu\in \cal I_1} G_{\mu\mu},\quad m_4 = \frac1n\sum_{\mu\in \cal I_2} G_{\mu\mu}.$$
%\begin{align}\label{m34shift}
%\frac{n_2}{n}\frac1{m_3} = - z +\frac1n\sum_{i=1}^p \frac1{  1+m_3 + \lambda_i^2m_4  } ,\quad \frac{n_1}{n}\frac1{m_4} = - z +\frac1n\sum_{i=1}^p \frac{\lambda_i^2 }{  1+m_3 + \lambda_i^2m_4  } .
%\end{align}
It remains to derive equation \eqref{eq_a34extra} for $(a_3,a_4)$. Taking implicit differentiation of equation \eqref{selfomega_a}, we obtain that
\be \nonumber%\label{dotm34}
\begin{split}
 \rho_1\frac{-a'_{1}(0) }{a_{1}^2(0)}=   \frac1p\sum_{i=1}^p \frac{\lambda_i^2\left(1-\lambda_i^2 a'_{1}(0) - a'_{2}(0)\right) }{  ( \lambda_i^2 a_{1}(0) +a_{2}(0))^2 } ,\quad \rho_2\frac{-a'_{2}(0)}{a_2^2(0)} &=  \frac1p\sum_{i=1}^p \frac{1-\lambda_i^2 a'_{1}(0) - a'_{2}(0)}{ ( \lambda_i^2a_{1}(0) +a_{2}(0))^2  } .
\end{split}
\ee
%Using $(a_1,a_2)=(a_1(0),a_2(0))$ and $(a_3,a_4)=(-a_1'(0),-a_2'(0))$,
%Recalling that we have denoted  $(a_1, a_2)=(-r_1M_{1}(0),-r_2M_{2}(0))$, we can rewrite these two equations as
%%\be%\label{dotm34}
%%\begin{split}
%% r_1 \frac1{a_1^2}a_4 &=   \frac1n\sum_{i=1}^p \frac{\lambda_i^2\left(1+ \lambda_i^2 a_4 + a_3\right) }{  ( \lambda_i^2 a_1 +a_2)^2 } ,\\
%%r_2\frac1{a_2^2}a_3 &=  \frac1n\sum_{i=1}^p \frac{1+\lambda_i^2 a_4 + a_3}{ ( \lambda_i^2 a_1 +a_2)^2  } .
%%\end{split}
%%\ee
%%We can solve the above equations to get $u'_3$ and $u'_4$. Then we have
%%$$ G^2(z) \approx   \frac{1 +  u_3'(z) +\Lambda^2 u'_4(z)}{ \left( z+ u_3(z) + \Lambda^2  u_4(z)\right)^2}  $$
%%{\cob in certain sense}.
%%\cob $a_3\to a_2$, $a_4\to a_1$ \nc
%%We now simplify the expressions for $z\to 0$ case. When $z\to 0$, we shall have
%%$$u_3(z)= -  {a_3} + \OO(z), \quad u_4(z)= -  a_4 + \OO(z), \quad a_3,a_4 >0.$$
%%For $z\to0$, the equations in equation \eqref{m34shift} are reduced to
%%\begin{align}\label{m35shift}
%%\frac{n_2}{n}\frac{1}{\todo{a_2}} = 1 +\frac1n\sum_{i=1}^p \frac{1}{\todo{a_2} + \lambda_i^2\todo{a_1}  } ,\quad \frac{n_1}{n}\frac1{\todo{a_1}} = 1 +\frac1n\sum_{i=1}^p \frac{\lambda_i^2 }{  \todo{a_2} + \lambda_i^2 \todo{a_1} }.
%%\end{align}
%%It is easy to see that these equations are equivalent to
%%\begin{align} a_1 + a_2 = 1- \gamma_n, \quad a_1 +\frac1n\sum_{i=1}^p \frac{a_1}{a_1 + a_2/\lambda_i^2}=\frac{n_1}{n}  .\end{align}
%%The equations in equation \eqref{dotm34} reduce to
%\be \label{dotm34red}
%\begin{split}
%\left(\frac{r_2}{a_2^2}- \frac1n\sum_{i=1}^p \frac{1 }{ (   \lambda_i^2a_1+a_2   )^2  }\right) a_4 -  \left(\frac1n\sum_{i=1}^p \frac{  \lambda_i^2 }{ (   \lambda_i^2a_1+a_2   )^2  }\right)a_3 =  \frac1n\sum_{i=1}^p \frac{1 }{ (   \lambda_i^2a_1+a_2   )^2  } ,\\
% %\frac{n_1}{n}\frac1{a_4^2}b_4 =   \frac1n\sum_{i=1}^p \frac{\lambda_i^2\left(1+x_3 + \lambda_i^2 b_4\right) }{  (a_3 + \lambda_i^2a_4)^2  } , \\
% \left(  \frac{r_1}{a_1^2} -  \frac1n\sum_{i=1}^p \frac{\lambda_i^4   }{  (\lambda_i^2a_1+a_2)^2  }\right)a_3 -\left( \frac1n\sum_{i=1}^p \frac{\lambda_i^2  }{  (\lambda_i^2a_1+a_2  )^2  }\right)a_4 =   \frac1n\sum_{i=1}^p \frac{\lambda_i^2 }{  (\lambda_i^2a_1+a_2 )^2  } ,
%\end{split}
%\ee
%where we denoted  $(a_3 ,a_4):=(r_1M_{1}'(0),r_2M'_{2}(0))$. 
It is not hard to see that this equation is equivalent to equation \eqref{eq_a34extra}. 
Finally, we can calculate $\bv^\top \Gi'(0)\bv$ as in \eqref{calculatePibias}, which concludes Lemma \ref{lem_cov_derivative} by equation \eqref{apply derivlocal}. 
%Then by equation \eqref{apply derivlocal} and equation \eqref{dervPi}, we get
%\begin{align*}
%n^2\bignorm{\Sigma_2^{1/2} ( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)})^{-1}\beta}^2&=\beta^\top \Sigma_2^{-1/2} V  \frac{1 + a_4+ a_3\Lambda^2 }{(a_1\Lambda^2 + a_2)^2}  V^\top  \Sigma_2^{-1/2} \beta \\
%&=\beta^\top \Sigma_2^{-1/2} \frac{1 + a_4+ a_3M^\top M }{(a_1M^\top M + a_2)^2} \Sigma_2^{-1/2} \beta,
%\end{align*}
%where we used $M^\top M= V\Lambda^2 V^\top$ in the second step. 
%%where we denote $x_3:=u_3'(0)$ and $b_4:=u_4'(0)$. Thus we have
%%$$\left(\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)}} \right)^2  = G^2(0) \approx   \frac{ 1 + x_3 + \Lambda^2  b_4}{\left( a_3 + \Lambda^2  a_4\right)^2} .$$
%%This gives that
%%\be\label{derivative}
%%\begin{split}
%%& \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}}  \Sigma_2 \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}} =\Sigma_2^{-1/2}V \left(\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)}} \right)^2 V^T \Sigma_2^{-1/2}\\
%%& \approx  \Sigma_2^{-1/2}V \frac{ 1 + x_3 + \Lambda^2  b_4}{\left( a_3 + \Lambda^2  a_4\right)^2}V^T \Sigma_2^{-1/2}= \Sigma_2^{-1/2}  \frac{ 1 + x_3 +  b_4 A^\top A }{\left( a_3 +   a_4 A^\top A\right)^2}\Sigma_2^{-1/2} .
%%\end{split}
%%\ee
%%Hence we have
%%\begin{align*}
%%& (\beta_s - \beta_t)^{\top}(X^{(1)})^{\top}X^{(1)} \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}}  \Sigma_2 \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}} (X^{(1)})^{\top}X^{(1)} (\beta_s - \beta_t) \\
%% & \approx (\beta_s - \beta_t)^{\top}\Sigma_1 \Sigma_2^{-1/2}  \frac{ 1 + a_3 +  a_4 A^\top A }{\left( a_2 +   a_1 A^\top A\right)^2}\Sigma_2^{-1/2} \Sigma_1(\beta_s - \beta_t).
%%\end{align*}
%This concludes Lemma \ref{lem_cov_derivative}.
\end{proof}


%\todo{revise} In Section \ref{sec locallaw1}, we introduce the concept of resolvents, and give an almost optimal convergent estimate on it---Theorem \ref{LEM_SMALL}---in Section \ref{sec aymp_limit_G}. This estimate is conventionally called {\it local law} in random matrix literature. Based on Theorem \ref{LEM_SMALL}, we then complete the proof of Lemmas \ref{lem_cov_shift} and \ref{lem_cov_derivative} in Section \ref{sec pf RMTlemma}. The proof of Theorem \ref{LEM_SMALL} will be presented in Section \ref{sec_Gauss}.

\subsubsection{Self-Consistent Equations}\label{sec contract}

The rest of this section is devoted to the proof of Theorem \ref{LEM_SMALL}. In this section, we show that the limiting equation \eqref{selfomega_a} has a unique solution $(a_1(z), a_2(z))$ for any $z\in \mathbf D$ in equation \eqref{SSET1}. Otherwise, Theorem \ref{LEM_SMALL} will be a vacuous statement. 


First observe that when $z=0$, equation \eqref{selfomega_a} can be reduced to equation \eqref{eq_a12extra}, from which we can derive an equation of $a_1$ only: 
%Note that the function 
\be\label{fa1}f(a_1)=\frac{\rho_1}{\rho_1 + \rho_2},\quad \text{with}\quad f(a_1):=a_1 +\frac{1}{\rho_1+\rho_2}\cdot\frac{1}p\sum_{i=1}^p \frac{\lambda_i^2 a_1}{ \lambda_i^2 a_1+ (1- (\rho_1+\rho_2)^{-1} - a_1) } .\ee
It is not hard to see that $f$ is strictly increasing on $[0,1-(\rho_1+\rho_2)^{-1}]$. Moreover, we have $f(0)=0<1$, $f(1-(\rho_1+\rho_2)^{-1})=1>\rho_1/(\rho_1+\rho_2)$, and $f(\rho_1/(\rho_1+\rho_2))>\rho_1/(\rho_1+\rho_2)$ if $\rho_1/(\rho_1+\rho_2)\le 1-(\rho_1+\rho_2)^{-1}$. Hence by mean value theorem, there exists a unique solution $a_1$ satisfying
$$0< a_1 <\min(1-(\rho_1+\rho_2)^{-1}, \rho_1/(\rho_1+\rho_2)).$$ 
Moreover, it is easy to check that $f'(x)=\OO(1)$ for $x\in [0, 1-(\rho_1+\rho_2)^{-1}]$. Hence there exists a constant $\tau>0$, such that 
\be\label{a23}
\frac{\rho_1}{\rho_1+\rho_2} \tau \le   a_1 \le \min\left\{ (1-\gamma_n) -\frac{\rho_1}{\rho_1+\rho_2} \tau,\frac{\rho_1}{\rho_1+\rho_2}( 1-\tau)\right\},\quad \tau<a_1\le 1 -\frac{1}{\rho_1+\rho_2} - \frac{\rho_1}{\rho_1+\rho_2}\tau .
\ee 

%\begin{equation}\label{selfomega0}
%\begin{split}
%&a_1 + a_2= 1-\gamma_n,\quad    a_1 +\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2 a_1}{ \lambda_i^2 a_1+ (1-\gamma_n - a_1) }=r_1 .
%\end{split}
%\ee
%where $a_1:=-r_1M_{1}(0)$ and $a_2:=-r_2M_{2}(0)$. 

Then we prove the existence and uniqueness of the solution to the self-consistent equation \eqref{selfomega_a} for a general $z\in \mathbf D$. For later proof, it is more convenient to deal with functions
\be\label{M1M2a1a2}
M_1(z):= -\frac{\rho_1+\rho_2}{\rho_1}a_1(z),\quad M_2(z):= -\frac{\rho_1+\rho_2}{\rho_2}a_2(z),
\ee
which, as shown in equation \eqref{approx m12 add}, give the asymptotic limits of $m_1(z)$ and $m_2(z)$.
Then the equation \eqref{selfomega_a} can be written as 
% Introduce $(M_1(z),M_2(z))$. First we define the deterministic limits of $(m_1(z), m_{2}(z))$, denoted by $(M_{1}(z),M_{2}(z))$ 
%\HZ{This $M_{1}, M_{2}$ notation is awkward; consider changing it to ${M}_1(z), {M}_2(z)$ (or  better, say $a_1(z), a_2(z)$? if they correspond to $a_1, a_2$ when $z=0$)}, 
 the unique solution to the following system of equations
\begin{equation}\label{selfomega}
\begin{split}
& \frac1{M_{1}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2 r_1 M_{1} +r_2 M_{2}  } - 1 ,\  \ \frac1{M_{2}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1 M_{1} +  r_2 M_{2}  }- 1 ,
\end{split}
\ee
such that $\im M_{1}(z)>0$ and $\im M_{2}(z)>0$ for $z\in \C_+$, where, for simplicity of notations, we introduced the following ratios 
\be\label{ratios}
 \gamma_n :=\frac{p}{n}=\frac{1}{\rho_1+\rho_2},\quad r_1 :=\frac{n_1}{n}=\frac{\rho_1}{\rho_1+\rho_2},\quad r_2 :=\frac{n_2}{n}=\frac{\rho_2}{\rho_1+\rho_2}.
\ee
%For general $z$ around the origin, the existence and uniqueness of the solution $(M_{1}(z),M_{2}(z))$ is given by the following lemma. 
First we prove the following lemma, which gives the existence and uniqueness of the solution $(M_{1}(z),M_{2}(z))$ to \eqref{selfomega}.

\begin{lemma} \label{lem_mbehaviorw}
There exist constants $c_0, C_0>0$ depending only on $\tau$ in equations \eqref{assm2}, \eqref{assm32} and \eqref{a23} such that the following statements hold. 
%Fix any constants $c, C>0$. If equation \eqref{assm20} holds, then we have the following estimates.
%If $|z|\le c_0$, then 
There exists a unique solution to equation \eqref{selfomega} under the conditions
\be\label{prior1}
|z|\le c_0, \quad  |M_{1}(z) - M_{1}(0)| + |M_{2}(z) - M_{2}(0)|\le c_0.
\ee
Moreover, the solution satisfies
\be\label{Lipomega}
 |M_{1}(z) - M_{1}(0)| + |M_2(z) - M_2(0)| \le C_0|z|.
\ee
%and
%\be\label{Immcw}
%0\le \im m_{\al c}(z) \le C_0\eta ,\quad z= E+ \ii\eta \in \C_+, \ \ \al=2,3.
%\ee
%For $z\in \C_+ \cap \{z: c\le |z| \le C\}$, we have
%\begin{equation}\label{absmcw}
% \vert M_{2}(z) \vert \sim 1,  \quad \left|z^{-1} - (M_{1}(z)+M_{2}(z)) + (z-1)M_{1}(z)M_{2}(z) \right|\sim 1,
% \ee 
% and
% \be\label{Immcw} 0\le \im M_{2}(z) \sim \begin{cases}
%    {\eta}/{\sqrt{\kappa+\eta}}, & \text{ if } E \notin [\lambda_-,\lambda_+] \\
%    \sqrt{\kappa+\eta}, & \text{ if } E \in [\lambda_-,\lambda_+]\\
%  \end{cases}.
%\end{equation}
%\end{itemize}
%The above estimates also hold for $M_{1}$, $M_{2}(z)$, $m_{4c}(z)$ and $m_c(z)$. Finally, the estimates equation \eqref{absmcw}, equation \eqref{eq_mcomplexw} and equation \eqref{eq_mdiffw} hold for $h(z)$. 
\end{lemma}
%The proof is a standard application of the contraction principle. For reader's convenience, we will include its proof in Section \ref{sec contract}. 
% In this section we give the proof of Lemmas \ref{lem_mbehaviorw} and \ref{lem_stabw} using the contraction principle. 
 %We first prove the existence and continuity of the solutions to equation \eqref{falvww}. 
 \begin{proof} %[Proof of Lemma \ref{lem_mbehaviorw}]
 The proof is a standard application of the contraction principle. For reader's convenience, we give more details. First, it is easy to check that equation \eqref{selfomega} is equivalent to  
\begin{equation}\label{selfalter}
r_1M_{1}=-(1-\gamma_n) - r_2M_{2} - z\left( M_{2}^{-1}+1\right),\quad g_z(M_{2}(z))=1, 
\ee
where
$$g_z(M_{2}):= - M_{2} +\frac{\gamma_n}p\sum_{i=1}^p \frac{M_{2} }{  z -\lambda_i^2(1-\gamma_n)+ (1 - \lambda_i^2) r_2M_{2} - \lambda_i^2 z\left(  M_{2}^{-1}+1\right) }.$$
We first show that there exists a unique solution $M_{2}(z)$ to the equation $g_z(M_{2}(z))=1$ under the conditions in equation \eqref{prior1}.
%, and (ii) $M_2(z)$ satisfies equation \eqref{Lipomega}. 
We abbreviate $\delta(z):= M_{2}(z) - M_{2}(0)$. From equation \eqref{selfalter}, we obtain that 
\begin{equation} \nonumber
0=\left[g_z(M_{2}(z)) -  g_0(M_{2}(0)) -g_z'(M_{2}(0))\delta(z)\right] + g_z'(M_{2}(0))\delta(z),
\ee
which gives that
\be\nonumber%\label{selfomega1}
 \delta(z) =- \frac{ g_z(M_{2}(0)) - g_0(M_{2}(0)) }{g_z'(M_{2}(0))}- \frac{ g_z(M_{2}(0)+\delta(z)) -  g_z(M_{2}(0))-g_z'(M_{2}(0))\delta(z)}{g_z'(M_{2}(0))}.
 \ee
%By equation \eqref{selfomega}, we obtain the self-consistent equations for $(M_{2}(0),M_{2}(0))$ and $(M_{2}(z),M_{2}(z))$:
% \begin{equation}\label{selfomega1}
%\begin{split}
%& \frac{r_1}{M_{2}(0) }=- 1 + \frac1n\sum_{i=1}^p \frac{\lambda_i^2}{\lambda_i^2M_{2}(0) + M_{2} (0) },\quad \frac{r_2}{M_{2} (0)}=  - 1 +\frac1n\sum_{i=1}^p \frac{1 }{ \lambda_i^2 M_{2} (0)+  M_{2}(0)  }.
%\end{split}
%\ee
%Subtract equation \eqref{selfomega1} from equation \eqref{selfomega}, we get that
% \begin{equation}\label{selfomega3}
%\begin{split}
%& \e_{2} \frac{r_1}{(M_{2} +\e_2) M_{2}}=  \frac1n\sum_{i=1}^p \frac{\lambda_i^2 (z+\lambda_i^2 \e_2 +\e_3)}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2(M_{2}+\e_2)  + (M_{2}+\e_3) ) },\\
%&  \e_3 \frac{r_2}{(M_{2}+\e_3) M_{2}}=  \frac1n\sum_{i=1}^p \frac{z+\lambda_i^2 \e_2 +\e_3}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2(M_{2}+\e_2)  + (M_{2}+\e_3) ) }.
%\end{split}
%\ee
Inspired by this equation, we define iteratively a sequence ${\delta}^{(k)}(z) \in \C$ such that ${\delta}^{(0)}=0$, and 
\be\label{selfomega2}
 \delta^{(k+1)} =- \frac{g_z(M_{2}(0)) - g_0(M_{2}(0))}{g_z'(M_{2}(0))} -\frac{g_z(M_{2}(0)+ \delta^{(k)}) -  g_z(M_{2}(0))-g_z'(M_{2}(0))\delta^{(k)} }{g_z'(M_{2}(0))} .
 \ee
% \begin{equation}\nonumber%\label{selfomega4}
%\begin{split}
%& \left\{\frac{r_1}{M_{2}^2  } - \frac1n\sum_{i=1}^p \frac{\sigma_i^4   }{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2 M_{2}  + M_{2} ) } \right\} \e^{(k+1)}_{2}  -  \frac1n\sum_{i=1}^p \frac{\lambda_i^2   }{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2 M_{2}  + M_{2} ) } \e_3^{(k+1)} \\
%& = \frac1n\sum_{i=1}^p \frac{\lambda_i^2 z}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2M_{2}  + M_{2} ) } + \frac{r_1 [\e_2^{(k)}]^2}{M_{2}^2 (M_{2}+\e_2^{(k)})} \\
%&+   \frac1n\sum_{i=1}^p \left( \frac{\lambda_i^2 (z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)})}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2(M_{2}+\e_2^{(k)})  + (M_{2}+\e_3^{(k)}) ) } - \frac{\lambda_i^2 (z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)})}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2M_{2}  + M_{2}) }\right),\\
%& \left\{\frac{r_2}{M_{2}^2  } - \frac1n\sum_{i=1}^p \frac{1 }{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2 M_{2}  + M_{2} ) } \right\} \e^{(k+1)}_{3}  -  \frac1n\sum_{i=1}^p \frac{1 }{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2 M_{2}  + M_{2} ) } \e_2^{(k+1)} \\
%& = \frac1n\sum_{i=1}^p \frac{ z}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2M_{2}  + M_{2} ) } + \frac{r_2 [\e_3^{(k)}]^2}{M_{2}^2 (M_{2}+\e_3^{(k)})} \\
%&+   \frac1n\sum_{i=1}^p \left( \frac{ z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)}}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2(M_{2}+\e_2^{(k)})  + (M_{2}+\e_3^{(k)}) ) } - \frac{ z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)}}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2M_{2}  + M_{2}) }\right).
%\end{split}
%\ee
Then equation \eqref{selfomega2} defines a mapping $h_z:\C\to \C$, which maps $\delta^{(k)}$ to $\delta^{(k+1)}=h(\delta^{(k)})$.
 
%\be\label{iteration} 
%{\e}^{(k+1)}= \mathbf f({ \e}^{(k)}), \quad \mathbf f(\mathbf x):=S^{-1}\mathbf x_0+ S^{-1} \mathbf e(\mathbf x),
%\ee
%where $S$ is a $2\times 2$ matrix with
%\begin{align*}
%S_{11}=\frac{r_1}{M_{2}^2  } - \frac1n\sum_{i=1}^p \frac{\sigma_i^4   }{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2 M_{2}  + M_{2} ) }, \quad S_{12}=-  \frac1n\sum_{i=1}^p \frac{\lambda_i^2   }{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2 M_{2}  + M_{2} ) },\\
%S_{21}=-  \frac1n\sum_{i=1}^p \frac{1 }{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2 M_{2}  + M_{2} ) } , \quad S_{22}=\frac{r_2}{M_{2}^2  } - \frac1n\sum_{i=1}^p \frac{1 }{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2 M_{2}  + M_{2} ) } ,
%\end{align*}
%%$$
%%S:=\begin{pmatrix} \frac{c_1}{M_{1}^2  } -  \frac{ \theta_l^2 (1-\theta_l )^2 }{ (1-t_l)^2}g(M_{2})^2  & -  \frac{ (1-\theta_l )^2\theta_l }{ (1-t_l)^2} \\ -   \frac{ (1-\theta_l )^2\theta_l }{ (1-t_l)^2}  &\frac{c_2}{M_{2}^2  } -  \frac{ \theta_l^2(1-\theta_l )^2 }{ (1-t_l)^2}g(M_{1})^2  \end{pmatrix},  
%%$$
%$\mathbf x_0$ is a vector with
%$$\mathbf x_0=\begin{pmatrix} \frac1n\sum_{i=1}^p \frac{\lambda_i^2 z}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2M_{2}  + M_{2} ) } \\ \frac1n\sum_{i=1}^p \frac{ z}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2M_{2}  + M_{2} ) }  \end{pmatrix},$$
%and $\mathbf e(\mathbf x)$ is
%$$\mathbf e(\mathbf x):= \begin{pmatrix}\frac{r_1 [\e_2^{(k)}]^2}{M_{2}^2 (M_{2}+\e_2^{(k)})} +   \frac1n\sum_{i=1}^p \left( \frac{\lambda_i^2 (z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)})}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2(M_{2}+\e_2^{(k)})  + (M_{2}+\e_3^{(k)}) ) } - \frac{\lambda_i^2 (z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)})}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2M_{2}  + M_{2}) }\right) \\ 
% \frac{r_2 [\e_3^{(k)}]^2}{M_{2}^2 (M_{2}+\e_3^{(k)})} +   \frac1n\sum_{i=1}^p \left( \frac{ z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)}}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2(M_{2}+\e_2^{(k)})  + (M_{2}+\e_3^{(k)}) ) } - \frac{ z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)}}{(\lambda_i^2M_{2}  + M_{2})(z+\lambda_i^2M_{2}  + M_{2}) }\right) \end{pmatrix}.$$
%%Here we have used $\theta_lg(M_{1})g( M_{2} ) = f_c(\theta_l) = t_l$ (recall equation \eqref{fcz}) to simplify the expressions a little bit.

With direct calculation, we obtain that
$$g_z'(M_{2}(0)) = -1 - \frac{\gamma_n}p\sum_{i=1}^p \frac{ \lambda_i^2(1-\gamma_n) - z\left[1- \lambda_i^2 \left(  2M_{2}^{-1}(0)+1\right)\right]  }{  \left[z -\lambda_i^2(1-\gamma_n)+ (1 - \lambda_i^2) r_2M_{2}(0) - \lambda_i^2 z\left( M_{2}^{-1}(0)+1\right)\right]^2 }.$$
%Using equation \eqref{a23}, 
Then it is not hard to check that there exist constants $\wt c, \wt C>0$ depending only on $\tau$ in equations \eqref{assm32} and \eqref{a23} such that the following estimates hold: for all $z$, $\delta_1$ and $\delta_2$ such that $|z|\le \wt c$, $|\delta_1|  \le \wt c$ and $|\delta_2|  \le \wt c$,
\be\label{dust}
\left|\frac{1}{g_z'(M_{2}(0))} \right|\le \wt C, \quad   \left|\frac{g_z(M_{2}(0)) - g_0(M_{2}(0))}{g_z'(M_{2}(0))}\right|  \le \wt C|z| ,
\ee
and 
\be\label{dust222}
\left|\frac{g_z(M_{2}(0)+ \delta_1) -  g_z(M_{2}(0)+\delta_2)-g_z'(M_{2}(0))(\delta_1-\delta_2) }{g_z'(M_{2}(0))}\right|  \le \wt C|\delta_1-\delta_2|^2 .
\ee
 By equations \eqref{dust} and \eqref{dust222}, it is not hard to show that there exists a sufficiently small constant $c_1>0$ depending only on $\wt C$, such that 
%$$  h_z: B_{d}  \to B_{d} , \quad B_d:=\{\delta \in \C: |\delta| \le d \},$$
$h_z: B_{d}  \to B_{d}$ is a self-mapping on the ball $B_d:=\{\delta \in \C: |\delta| \le d \}$, as long as $d\le c_1$ and $|z| \le c_1$. 
%for some constant $c_\delta>0$ depending only on $\wt C$ and $\delta$. 
%\be\label{priori_cond}
%\zeta+\| \b g\|_\infty+ |z-\wt z|\le c_r
%\ee
%for some constant $ c_r>0$ depending on $r$. 
Now it suffices to prove that $h$ restricted to $B_d $ is a contraction, which then implies that ${\delta}:=\lim_{k\to\infty} { \delta}^{(k)}$ exists and $M_{2}(0)+\delta(z)$ is a unique solution to the second equation in \eqref{selfalter} subject to the condition $\|{\delta}\|_\infty \le d$. 


From the iteration relation \eqref{selfomega2}, using equation \eqref{dust} one can readily check that
\be\label{k1k}
{ \delta}^{(k+1)} - { \delta}^{(k)}= h_z({\delta}^{(k)}) - h_z({\delta}^{(k-1)}) \le \wt C | { \delta}^{(k)}-{ \delta}^{(k-1)}|^2.
\ee
Hence as long as $d$ is chosen to be sufficiently small such that $2d\wt C\le 1/2$,  
%compared to $\theta_l^{-1}-g(M_{1})g( M_{2} )= (1-t_l)\theta_l^{-1}$, we have
%%where $q(\bx)$ denotes a vector with components $q(x_i)$. 
%%Using $|q'(0)| = 0$ and equation \eqref{dust}, we get from equation \eqref{k1k} that
%$$
% \left\|\mathbf e({{} \e}^{(k)}) -\mathbf e({{} \e}^{(k-1)})\right\|_\infty \le C (\|\bx^{k }\|_\infty +\|\bx^{k-1 }\|_\infty)\|{{} \e}^{(k)} - {{} \e}^{(k-1)}\|_\infty
% %\|\bx^{k+1}-\bx^k\|_\infty\le  C_\kappa \left(\zeta+\|\bx^{k }\|_\infty +\|\bx^{k-1 }\|_\infty  \right)\cdot \|\bx^{k }-\bx^{k-1}\|_\infty
%$$
%for some constant $C >0$ depending only on $c_1, c_2$ and $\delta_l$. Thus we can choose a sufficiently small constant $0<r \le \min\{\tau, (2C)^{-1}\}$ such that $Cr \le 1/2$, i.e. 
then $h$ is indeed a contraction mapping on $ B_d$. This proves both the existence and uniqueness of the solution $M_{2}(z)=M_{2}(0)+\delta(z)$, if we choose $c_0$ in equation \eqref{prior1} as $c_0=\min\{c_1, d\}$. After obtaining $M_{2}(z)$, we can then find $M_{1}(z)$ using the first equation in \eqref{selfalter}. 

Note that with equation \eqref{dust222} and ${\delta}^{(0)}= 0$, we get from equation \eqref{selfomega2} that $ |{ \delta}^{(1)}| \le \wt C|z| .$
With the contraction mapping, we have the bound 
\be\label{endalter}|{ \delta}| \le \sum_{k=0}^\infty |{  \delta}^{(k+1)}-{ \delta}^{(k)}| \le 2\wt C|z| \ \Rightarrow \ |M_{2}(z)-M_{2}(0)|\le 2\wt C|z| .\ee
%This gives the bound equation \eqref{Lipomega} for $M_{2}(z)$. 
Using the first equation in equation \eqref{selfalter}, we immediately obtain the bound  $ r_1|M_{1}(z)-M_{1}(0)| \le C|z|$ for some constant $C>0$, which concludes equation \eqref{Lipomega} as long as if $r_1\gtrsim 1$. To deal with the $r_1=\oo(1)$ case, we go back to the first equation in \eqref{selfomega} and treat $M_{1}(z)$ as the solution to the following equation:
$$\wt g_z(M_{1}(z))=1,\quad \wt g_z(M_1):=- M_1 + \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2 x}{  z+\lambda_i^2 r_1 M_1 +r_2 M_{2} }. $$
(Note that we have found the solution $M_2(z)$, so this is an equation of $M_1$ only.) 
%We can calculate that 
%$$g_z'(M_{2}(0))= -1 +  \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2 (z + r_2M_{2}(z))}{  (z+\lambda_i^2 r_1 M_{1}(0) +r_2 M_{2}(z))^2 }.$$
%At $z=0$, we have 
%$$ |g_0'(M_{2}(0))|= \left|1+\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2  r_2 x_3}{  (\lambda_i^2 r_1 X^{(2)} + r_2 x_3)^2 }\right|\ge 1,$$
%where $X^{(2)}$ and $x_3$ satisfy equation \eqref{a23}. Thus under equation \eqref{prior1} we have $|g_z'(M_{2}(0))|\sim 1$ as long as $c_0$ is taken sufficiently small. 
Then with a similar argument as the one between equations \eqref{selfalter} and \eqref{endalter}, we can conclude $|M_{2}(z)-M_{2}(0)|=\OO(|z|)$, which further concludes equation \eqref{Lipomega} together with equation \eqref{endalter}. 
%$c_0$ is taken sufficiently small. 
We omit the details. %This concludes the proof of Lemma \ref{lem_mbehaviorw}.
\end{proof}

As a byproduct of the above contraction mapping argument, we also obtain the following stability result that will be used in the proof of Theorem \ref{LEM_SMALL}. Roughly speaking, it states that if  two complex functions $m_1(z)$ and $m_2(z)$ satisfy the self-consistent equation \eqref{selfomega} approximately up to some small error, then $m_1(z)$ and $m_2(z)$ will be close to the solutions $M_1(z)$ and $M_2(z)$. It will be applied to \eqref{approximate m1m2} to show that the averaged resolvents $m_1(z)$ and $m_2(z)$ indeed converge to $M_1(z)$ and $M_2(z)$, respectively.


\begin{lemma} \label{lem_stabw}
There exist constants $c_0, C_0>0$ depending only on $\tau$ in equations \eqref{assm2},  \eqref{assm32} and \eqref{a23} such that the self-consistent equations in \eqref{selfomega} are stable in the following sense. Suppose $|z|\le c_0$, and $m_{1}: \C \mapsto \C$ and $m_{2}: \C \mapsto \C$ are analytic functions of $z$ such that 
\be  \label{prior12}
|m_{1}(z) - M_{1}(0)| + |m_{2}(z) - M_2(0)|\le c_0.
\ee
Moreover, assume that $(m_1,m_2)$ satisfies the system of equations
\begin{equation}\label{selfomegaerror}
\begin{split}
&\frac{1}{m_{1}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2r_1m_{1} +r_2 m_{2}  } =\cal E_1,\ \ \frac{1}{m_{2}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1m_{1} +  r_2m_{2}  }=\cal E_2,
\end{split}
\ee
for some (deterministic or random) errors such that $  |\mathcal E_1| +  |\mathcal E_2| \le \theta(z),$ where $\theta(z)$ is a deterministic function of $z$ satisfying that $\theta(z) \le (\log n)^{-1}.$ Then we have 
 \begin{equation}
  \left|m_1(z)-M_{1}(z)\right| +  \left|m_2(z)-M_2(z)\right|\le C_0\delta(z).\label{Stability1}
\end{equation}
\end{lemma}



\begin{proof}%[Proof of Lemma \ref{lem_stabw}]
Under condition \eqref{prior12}, we can obtain equation \eqref{selfalter} approximately up to some small error:
\be\label{selfalter2}r_1 m_{1}=-(1-\gamma_n) - r_2m_{2} - z\left(  {m_{2}^{-1}}+1\right) + \wt{\cal E}_1(z),\quad g_z(m_{2}(z))=1+ \wt{\cal E}_2(z),\ee
where the errors satisfy that $|\wt{\cal E}_1(z)|+ |\wt{\cal E}_2(z)|=\OO(\theta(z))$. Then we subtract equation \eqref{selfalter} from equation \eqref{selfalter2}, and consider the contraction principle for the function $\delta (z):= m_{2}(z) - M_2(z)$.  The rest of the proof is exactly the same as the one for Lemma \ref{lem_mbehaviorw}, so we omit the details.
\end{proof}



%It was shown in \cite{Separable} that if $d_N \to d \in (0,\infty)$ and $\pi_A^{(n)}$, $\pi_B^{(n)}$ converge to certain probability distributions, then almost surely $\rho^{(n)}$ converges to a deterministic distributions $ \rho_{\infty}$. We now describe it through the Stieltjes transform
%$$m_{\infty}(z):=\int_{\mathbb R} \frac{\rho_{\infty}(\dd x)}{x-z}, \quad z \in \mathbb C_+.$$
%For any finite $N$ and $z\in \mathbb C_+$, we define $(m^{(n)}_{1c}(z),m^{(n)}_{2c}(z))\in \mathbb C_+^2$ as the unique solution to the system of self-consistent equations
%\begin{equation}\label{separa_m12}
%{m^{(n)}_{1c}(z)} = d_N \int\frac{x}{-z\left[1+xm^{(n)}_{2c}(z) \right]} \pi_A^{(n)}(\dd x), \quad  {m^{(n)}_{2c}(z)} =  \int\frac{x}{-z\left[1+xm^{(n)}_{1c}(z) \right]} \pi_B^{(n)}(\dd x).
%\end{equation}
%Then we define
%\begin{equation}\label{def_mc}
%m_c(z)\equiv m_c^{(n)}(z):= \int\frac{1}{-z\left[1+xm^{(n)}_{2c}(z) \right]} \pi_A^{(n)}(\dd x).
%\end{equation}
%It is easy to verify that $m_c^{(n)}(z)\in \mathbb C_+$ for $z\in \mathbb C_+$. Letting $\eta \downarrow 0$, we can obtain a probability measure $\rho_{c}^{(n)}$ with the inverse formula
%\begin{equation}\label{ST_inverse}
%\rho_{c}^{(n)}(E) = \lim_{\eta\downarrow 0} \frac{1}{\pi}\Im\, m^{(n)}_{c}(E+\ii \eta).
%\end{equation}
%If $d_N \to d \in (0,\infty)$ and $\pi_A^{(n)}$, $\pi_B^{(n)}$ converge to certain probability distributions, then $m_c^{(n)}$ also converges and we define
%$$m_{\infty}(z):=\lim_{N\to \infty} m_c^{(n)}(z), \ \ z \in \mathbb C_+.$$
%Letting $\eta \downarrow 0$, we can recover the asymptotic eigenvalue density $ \rho_{\infty}$ with
%\begin{equation}\label{ST_inverse}
%\rho_{\infty}(E) = \lim_{\eta\downarrow 0} \frac{1}{\pi}\Im\, m_{\infty}(E+\ii \eta).
%\end{equation}
%It is also easy to see that $\rho_\infty$ is the weak limit of $\rho_{c}^{(n)}$. 
%%The measure $ \rho_{\infty}$ is sometimes called the {\it{multiplicative free convolution}} of $\pi_A$, $\pi_B$ with the Marchenko-Pastur (MP) law (see e.g. \cite{AGZ,VDN}), i.e. $\pi_A \boxtimes \rho_{MP} \boxtimes \pi_B$, where $\rho_{MP}$ denotes the MP distribution. 
%
%The above definitions of $m_c^{(n)}$, $\rho_c^{(n)}$, $m_\infty$ and $\rho_\infty$ make sense due to the following theorem. Throughout the rest of this paper, we often omit the super-indices $(n)$ and $(N)$ from our notations. 
%
%%Throughout the rest of this paper, we will often omit the super-index $n$ or $N$ from our notations. 
%
%\begin{theorem} [Existence, uniqueness, and continuous density]
%For any $z\in \mathbb C_+$, there exists a unique solution $(M_{1},M_{2})\in \mathbb C_+^2$ to the systems of equations in (\ref{separa_m12}). The function $m_c$ in (\ref{def_mc}) is the Stieltjes transform of a probability measure $\mu_c$ supported on $\mathbb R_+$. Moreover, $\mu_c$ has a continuous derivative $\rho_c(x)$ on $(0,\infty)$, which is defined by equation \eqref{ST_inverse}.
%\end{theorem}
%\begin{proof}
%See {\cite[Theorem 1.2.1]{Zhang_thesis}}, {\cite[Theorem 2.4]{Hachem2007}} and {\cite[Theorem 3.1]{Separable_solution}}.
%\end{proof}
%
%
%
% 
%Now we go back to study the equations in (\ref{separa_m12}). If we define the function
%% Corresponding to the equation in (\ref{separa_m12}), we define the function 
%\begin{equation}\label{separable_MP}
%f(z,\al):=- \al + \int\frac{x}{-z+xd_N \int\frac{t}{1+t\al} \pi_A(\dd t)} \pi_B(\dd x) ,
%\end{equation}
%then $M_{2}(z)$ can be characterized as the unique solution to the equation $f(z,\al)=0$ of $\al$ with $\Im \, \al> 0$, and $M_{1}(z)$ is defined using the first equation in equation \eqref{separa_m12}.
%%as $$M_{1}(z) = d_N \int\frac{x}{-z\left[1+xM_{2}(z) \right]} \pi_A(\dd x).$$
%Moreover, $m_{1,2c}(z)$ are the Stieltjes transforms of densities $\rho_{1,2c}$:
%$$\rho_{1,2c}(E) = \lim_{\eta\downarrow 0} \frac{1}{\pi}\Im\, m_{1,2c}(E+\ii \eta).$$
%Then we have the following result.
%
%\begin{lemma}\label{lambdar}%[Support of the deformed MP law]
%The densities $\rho_{c}$ and $\rho_{1,2c}$ all have the same support on $(0,\infty)$, which is a union of intervals: %connected components:
%\begin{equation}\label{support_rho1c}
%{\rm{supp}} \, \rho_{c} \cap (0,\infty) ={\rm{supp}} \, \rho_{1,2c} \cap (0,\infty) = \bigcup_{k=1}^p [a_{2k}, a_{2k-1}] \cap (0,\infty),
%\end{equation}
%where $p\in \mathbb N$ depends only on $\pi_{A,B}$. Moreover, $(x,\al)=(a_k, M_{2}(a_k))$ are the real solutions to the equations
%\begin{equation}
%f(x,\al)=0, \ \ \text{and} \ \ \frac{\partial f}{\partial \al}(x,\al) = 0. \label{equationEm2}
%\end{equation}
%Moreover, we have $M_{1}(a_1) \in (-\wt \sigma_1^{-1}, 0)$ and $M_{2}(a_1) \in (-\sigma_1^{-1}, 0)$. %Finally, under (\ref{assm2}) and (\ref{assm3}), we have $a_1 \le C$ for some constant $C>0$. 
%\end{lemma}
%\begin{proof}
%See Section 3 of \cite{Separable_solution}.
%\end{proof}
%
% %It is easy to observe that $b_k=M_{2}(a_k)$ according to the definition of $f$. 
% We shall call $a_k$ the spectral edges. In particular, we will focus on the rightmost edge $\lambda_+ := a_1$. 
%%\begin{equation}\label{right_edge}
%%\lambda_+ := a_1 
%%\end{equation}
%%throughout the following.
%Now we make the following assumption: there exists a constant $\tau>0$ such that %{\color{red}(can we remove one of the conditions?)}
%\begin{equation}\label{assm_gap}
%1 + M_{1}(\lambda_+) \wt \sigma_1 \ge \tau, \quad 1 + M_{2}(\lambda_+) \sigma_1\ge \tau. %\quad \frac{\partial^2 f}{\partial m^2}\left(\lambda_+,M_{2}(\lambda_+)\right) \ge \tau.
%\end{equation}
%This assumption guarantees a regular square-root behavior of the spectral densities $\rho_{1,2c}$ near $\lambda_+$ as shown by the following lemma.
%%(see Lemma \ref{lem_mbehavior} below), which is used in proving the local deformed MP law at the soft edge.
%
%\begin{lemma} \label{lambdar_sqrt}
%Under the assumptions equation \eqref{assm2}, equation \eqref{assm3} and equation \eqref{assm_gap}, there exist constants $a_{1,2}>0$ such that
%\be\label{sqroot3}
%\rho_{1,2c}(\lambda_+ - x) = a_{1,2} x^{1/2} + \OO(x), \quad x\downarrow 0,
%\ee
%and
%\be\label{sqroot4}
%\quad m_{1,2c}(z) = m_{1,2c}(\lambda_+) + \pi a_{1,2}(z-\lambda_+)^{1/2} + \OO(|z-\lambda_+|), \quad z\to \lambda_+ , \ \ \im z\ge 0.
%\ee
%The estimates equation \eqref{sqroot3} and equation \eqref{sqroot4} also hold for $\rho_c$ and $m_c$ with a different constant. 
%\end{lemma}
% 




%In particular, we shall denote
%\begin{equation}
%S(c_0,C_0,-\infty):= \left\{z=E+ \ii \eta: \lambda_r - c_0 \leq E \leq C_0 \lambda_r, 0 \leq \eta \leq 1 \right\}.
%\end{equation}
%We define the distance to the rightmost edge as
%\begin{equation}
%\kappa \equiv \kappa_E := \vert E -\lambda_r\vert , \ \ \text{for } z= E+\ii \eta.\label{KAPPA}
%\end{equation}

%Then we have the following lemma, which summarizes some basic properties of $M_{2}$ and $\rho_{2c}$.
%%{\color{red}Discuss about the case equation \eqref{assm3extra}. }
%
%\begin{lemma}\label{lem_mbehavior}
%Suppose the assumptions equation \eqref{assm2}, equation \eqref{assm3} and equation \eqref{assm_gap} hold. Then
%there exists sufficiently small constant $\wt  c>0$ such that the following estimates hold:
%\begin{itemize}
%\item[(1)]
%\begin{equation}
%\rho_{1,2c}(x) \sim \sqrt{\lambda_r-x}, \quad \ \ \text{ for } x \in \left[\lambda_r - 2\wt  c,\lambda_r \right];\label{SQUAREROOT}
%\end{equation}
%\item[(2)] for $z =E+\ii \eta\in S(\wt  c,C_0,-\infty)$, 
%\begin{equation}\label{Immc}
%\vert m_{1,2c}(z) \vert \sim 1,  \quad  \im m_{1,2c}(z) \sim \begin{cases}
%    {\eta}/{\sqrt{\kappa+\eta}}, & \text{ if } E\geq \lambda_r \\
%    \sqrt{\kappa+\eta}, & \text{ if } E \le \lambda_r\\
%  \end{cases};
%\end{equation}
%%for $z = E+\ii \eta\in S(\wt  c,C_0,\omega)$;
%\item[(3)] there exists constant $\tau'>0$ such that
%\begin{equation}\label{Piii}
%\min_{\mu\in \mathcal I_2} \vert 1 + M_{1}(z)\wt  \sigma_\mu \vert \ge \tau', \quad \min_{i\in \mathcal I_1} \vert 1 + M_{2}(z)\sigma_i  \vert \ge \tau',
%\end{equation}
%for any $z \in S(\wt  c,C_0,-\infty)$.
%\end{itemize}
%The estimates equation \eqref{SQUAREROOT} and equation \eqref{Immc} also hold for $\rho_c$ and $m_c$. 
%\end{lemma}
%%and
%%\begin{equation}
%%  \operatorname{Im} M_{2}(z) \sim \begin{cases}
%%    {\eta}/{\sqrt{\kappa+\eta}}, & E\geq \lambda_r \\
%%    \sqrt{\kappa+\eta}, & E \le \lambda_r\\
%%  \end{cases},  \label{SQUAREROOTBEHAVIOR}
%%\end{equation}
%\begin{proof}
%The estimate equation \eqref{SQUAREROOT} is already given by Lemma \ref{lambdar_sqrt}. The estimate equation \eqref{Immc} can be proved easily with equation \eqref{sqroot4}. 
%%The estimate equation \eqref{SQUAREROOT} for $\rho_c$ is already given by Lemma \ref{lambdar_sqrt}. The estimate equation \eqref{Immc} for $m_c$ follows from  equation \eqref{def_mc}, equation \eqref{Piii}, and equation \eqref{Immc} for $M_{2}$.
%It remains to prove equation \eqref{Piii}. By assumption equation \eqref{assm_gap} and the fact $M_{2}(\lambda_r) \in (-\sigma_1^{-1}, 0)$, we have
%$$\left| 1+ M_{2}(\lambda_r) \sigma_i \right| \ge \tau,  \quad i\in \mathcal I_1.$$
%With equation \eqref{sqroot4}, we see that if $\kappa+\eta \le 2c_0$ for some sufficiently small constant $c_0>0$, then
%$$\left| 1+ M_{2}(z)\sigma_k \right| \ge \tau/2.$$
%Then we consider the case with $E \ge \lambda_r + c_0$ and $\eta \le c_1$ for some constant $c_1>0$. In fact, for $\eta=0$ and $E> \lambda_r$, $M_{2}(E)$ is real and it is easy to verify that $M_{2}'(E)\ge 0$ using 
%the Stieltjes transform formula 
%%(\ref{Stj_app}). Applying (\ref{SQUAREROOT}) to the Stieltjes transform
%\begin{equation}\label{Stj_app}
%M_{2}(z):=\int_{\mathbb R} \frac{\rho_{2c}(dx)}{x-z},
%\end{equation}
%Hence we have
%$$ 1+ \sigma_i M_{2}(E)  \ge 1+ \sigma_i M_{2}(\lambda_r ) \ge \tau, \ \ \text{ for }E\ge \lambda_r + c_0.$$
%Using (\ref{Stj_app}) again, we can get that 
%$$\left|\frac{\dd M_{2}(z)}{ \dd z }\right| \le c_0^{-2}, \ \ \text{for } E\ge \lambda_r + c_0.$$ 
%Thus if $c_1$ is sufficiently small, we have
%$$\left| 1+ \sigma_k M_{2}(E+\ii\eta) \right| \ge  \tau/2$$
%for $E\ge \lambda_r + c_0$ and $\eta \le c_1$. Finally, it remains to consider the case with $\eta \ge c_1$. Note that we have $|M_{2}(z)| \sim \Im \, M_{2}(z) \sim 1$ by (\ref{Immc}). If $\sigma_k \le \left|2M_{2}(z)\right|^{-1}$, then $\left| 1+ \sigma_k M_{2}(z) \right| \ge 1/2$. Otherwise, we have %Together with (\ref{Immc}), we get that
%$$\left| 1+ \sigma_k M_{2}(z) \right| \ge \sigma_k \Im\, M_{2}(z) \ge \frac{\Im\, M_{2}(z)}{2 |M_{2}(z)|}\gtrsim 1 .$$
%%for some constant $\tau'>0$. 
%In sum, we have proved the second estimate in equation \eqref{Piii}. The first estimate can be proved in a similar way. 
%\end{proof}

%Then we have the following estimates for $M_{2}$:
%%\begin{lemma}[Lemma ]\label{lem_mbehavior}
%%and $\delta_N \le (\log N)^{-1}$, 
%%we have
%\begin{equation}\label{Immc}
%\vert M_{2}(z) \vert \sim 1, \ \  \Im \, M_{2}(z) \sim \begin{cases}
%    {\eta}/{\sqrt{\kappa+\eta}}, & \text{ if } E \notin \text{supp}\, \rho_{2c}\\
%    \sqrt{\kappa+\eta}, & \text{ if } E \in \text{supp}\, \rho_{2c}\\
%  \end{cases},
%\end{equation}
%%and 
%\begin{equation}\label{Piii}
%\max_{i\in \mathcal I_1} \vert (1 + M_{2}(z)\sigma_i)^{-1} \vert = \OO(1).
%\end{equation}
%\end{lemma}

%\begin{remark}
%Recall that $a_k$ are the edges of the spectral density $\rho_{2c}$; see (\ref{support_rho1c}). Hence $\rho_{2c}(a_k)=0$, and we must have $a_k < \lambda_r - 2\wt  c$ for $2\le k \le 2p$. In particular, $S(c_0,C_0,\e)$ is away from all the other edges if we choose $c_0 \le \wt  c$. 
%\end{remark}

%\begin{definition} [Classical locations of eigenvalues]
%The classical location $\gamma_j$ of the $j$-th eigenvalue of $\mathcal Q_1$ is defined as
%\begin{equation}\label{gammaj}
%\gamma_j:=\sup_{x}\left\{\int_{x}^{+\infty} \rho_{c}(x)dx > \frac{j-1}{n}\right\}.
%\end{equation}
%In particular, we have $\gamma_1 = \lambda_r$.
%\end{definition}
%%\begin{remark}
%%If $\gamma_j$ lies in the bulk of $\rho_{2c}$, then by the positivity of $\rho_{2c}$ we can define $\gamma_j$ through the equation
%%\begin{equation*}
%%\int_{\gamma_j}^{+\infty} \rho_{2c}(x)dx = \frac{j-1}{N}.
%%\end{equation*}
%%We can also define the classical location of the $j$-th eigenvalue of $\mathcal Q_1$ by changing $\rho_{2c}$ to $\rho_{1c}$ and $(j-1)/{N}$ to $(j-1)/{M}$ in (\ref{gammaj}). By (\ref{def21}), this gives the same location as $\gamma_j$ for $j\le n\wedge N$.
%%\end{remark}
%
%In the rest of this section, we present some results that will be used in the proof of Theorem \ref{main_thm}. Their proofs will be given in subsequent sections. For any matrix $X$ satisfying Assumption \ref{assm_big1} and the tail condition (\ref{tail_cond}), we can construct a matrix $X^s$ that approximates $X$ with probability $1-\oo(1)$, and satisfies Assumption \ref{assm_big1}, the bounded support condition (\ref{eq_support}) with $q\le N^{-\phi}$ for some small constant $\phi>0$, and
%%${\bf x_3 }$:
%\begin{equation}\label{conditionA2}
%\mathbb{E}\vert  x^s_{ij} \vert^3 =\OO(N^{-{3}/{2}}), \quad   \mathbb{E} \vert  x^s_{ij} \vert^4  =\OO_\prec (N^{-2});
%\end{equation}
%see Section \ref{sec_cutoff} for the details. We will need the following local laws, eigenvalues rigidity, eigenvector delocalization, and edge universality results for separable covariance matrices with $X^s$.
%%with support $q\le N^{-\phi}$ and satisfying the condition (\ref{conditionA2}).
%
%%\cor ---------------------------------- (revise starting from here) ------------------ \nc
%
%We define the deterministic limit $\Pi$ of the resolvent $G$ in (\ref{eqn_defG}) as
%\begin{equation}\label{defn_pi}
%\Pi (z): = \left( {\begin{array}{*{20}c}
%   { -\left(1+M_{2}(z)\Sigma \right)^{-1} } & 0  \\
%   0 & { - z^{-1} (1+M_{1}(z)\wt  \Sigma )^{-1} }  \\
%\end{array}} \right) .
%\end{equation}
%Note that we have
%\be\label{mcPi}
%\frac1{nz}\sum_{i\in \mathcal I_1} \Pi_{ii} =m_c. 
%\ee
%Define the control parameters
%\begin{equation}\label{eq_defpsi}
%\Psi (z):= \sqrt {\frac{\Im \, M_{2}(z)}{{N\eta }} } + \frac{1}{N\eta}.
%\end{equation}
%Note that by (\ref{Immc}) and (\ref{Piii}), we have
%\begin{equation}\label{psi12}
%\|\Pi\|=\OO(1), \quad \Psi \gtrsim N^{-1/2} , \quad \Psi^2 \lesssim (N\eta)^{-1}, \quad \Psi(z) \sim  \sqrt {\frac{\Im \, M_{1}(z)}{{N\eta }} } + \frac{1}{N\eta},
%\end{equation}
%%and 
%%\begin{equation}\labelpsi12
%%\Psi(z) \sim  \sqrt {\frac{\Im \, M_{1}(z)}{{N\eta }} } + \frac{1}{N\eta},
%%\end{equation}
%for $z\in S(\wt  c, C_0,-\infty)$. Now we are ready to state the local laws for $G(X,z)$. For the purpose of proving Theorem \ref{main_thm}, we shall relax the condition equation \eqref{assm_3rdmoment} a little bit. 

%\begin{definition}[Deterministic limit of $G$]
%We define the deterministic limit $\Pi$ of the Green function $G$ in (\ref{green2}) as
%\begin{equation}
%\Pi (z): = \left( {\begin{array}{*{20}c}
%   { -\left(1+M_{2}(z)\Sigma \right)^{-1} } & 0  \\
%   0 & { - z^{-1} (1+M_{1}(z)\wt  \Sigma )^{-1} }  \\
%\end{array}} \right) .
%\end{equation}
%%where $\Sigma$ is defined in (\ref{def_Sigma}).
%\end{definition}



%\begin{theorem} [Local laws]\label{LEM_SMALL} %[Results on covariance matrices with small support]
%
%Suppose Assumption \ref{assm_big1} and equation \eqref{assm_gap} hold. Suppose $X$ satisfies the bounded support condition (\ref{eq_support}) with $q\le N^{-\phi}$ for some constant $\phi>0$. Furthermore, suppose $X$ satisfies equation \eqref{conditionA2} and
%\be\label{assm_3moment}
%%\mathbb E x_{ij}^3=0,  
%\left|\mathbb E x_{ij}^3\right|\le b_N N^{-2}, \quad 1\le i \le n,\ \  1\le j \le N,
%\ee
%%and
%%\begin{equation}\label{conditionA4}
%%\mathbb{E}\vert x_{ij} \vert^3 \leq C N^{-{3}/{2}}, \quad \mathbb{E} \vert x_{ij} \vert^4  \prec N^{-2},  \quad 1\le i \le n, 1\le j \le N. %\mathbb{E}\vert x_{ij} \vert^3 \prec N^{-{3}/{2}}, \quad  
%%\end{equation}
%where $b_N$ is an $N$-dependent deterministic parameter satisfying $1 \leq b_N \le N^{1/2}$. Fix $C_0>1$ and let $c_0>0$ be a sufficiently small constant. Given any $\epsilon>0$, we define the domain
%\be \label{tildeS}
%\wt  S(c_0,C_0,\e):= S(c_0,C_0,\epsilon) \cap \left\{z = E+ \ii \eta: b_N \left(\Psi^2(z) + \frac{q}{N\eta}\right)\le N^{-\e}\right\}.
%\ee
%Then for any fixed $\e>0$, the following estimates hold. 
%\begin{itemize}
%\item[(1)] {\bf Anisotropic local law}: For any $z\in \wt  S(c_0,C_0,\epsilon)$ and deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb C^{\mathcal I}$,
%\begin{equation}\label{aniso_law}
%\left| \langle \mathbf u, G(X,z) \mathbf v\rangle - \langle \mathbf u, \Pi (z)\mathbf v\rangle \right| \prec q+ \Psi(z).
%\end{equation}
%
%\item[(2)] {\bf Averaged local law}: For any $z \in \wt  S(c_0, C_0, \epsilon)$,  we have
%\begin{equation}
% \vert m(z)-M(z) \vert \prec q^2 + (N \eta)^{-1}. \label{aver_in1} %+ q^2 
%\end{equation}
%where $m$ is defined in equation \eqref{defn_m}. Moreover, outside of the spectrum we have the following stronger estimate
%\begin{equation}\label{aver_out1}
% | m(z)-M(z)|\prec q^2  + \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%uniformly in $z\in \wt  S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$, where $\kappa$ is defined in equation \eqref{KAPPA}. 
%\end{itemize}
%The above estimates are uniform in the spectral parameter $z$ and any set of deterministic vectors of cardinality $N^{\OO(1)}$. If $A$ or $B$ is diagonal, then equation \eqref{aniso_law}-equation \eqref{aver_out1} hold for $z\in S(c_0,C_0,\epsilon) $.
%\end{theorem}
%
%The following theorem gives that anisotropic local law for $\cal R(z,0)$.






\subsubsection{Beyond Multivariate Gaussian Random Matrices: an Anisotropic Local Law}\label{sec_Gauss}

%We divide the proof of Theorem \ref{LEM_SMALL} into two steps. We first prove Theorem \ref{LEM_SMALL} in the special case where $X$ is Gaussian. Then we use a self-consistent comparison arguments developed in \cite{Anisotropic} to prove Theorem \ref{LEM_SMALL} in the general case. 
 
 In this section, we prove Theorem \ref{LEM_SMALL} based on a proposition that will be proved later. The main difficulty in the proof is due to the fact that the entries of $ Z^{(1)} U\Lambda  $ and ${Z^{(2)}V}$ are not independent. However, recall that if the entries of $Z^{(1)}\equiv (Z^{(1)})^{\text{Gauss}}$ and $Z^{(2)}\equiv (Z^{(2)})^{\text{Gauss}}$ are i.i.d. Gaussian, then equation \eqref{eq in Gauss} holds. 
 %by the rotational invariance of the multivariate Gaussian distribution. 
% we have
%\be\label{eq in Gauss} (Z^{(1)})^{\text{Gauss}} U\Lambda \stackrel{d}{=} (Z^{(1)})^{\text{Gauss}} \Lambda, \quad (Z^{(2)})^{\text{Gauss}} V \stackrel{d}{=} (Z^{(2)})^{\text{Gauss}} ,\ee
%where ``$\stackrel{d}{=}$" means ``equal in distribution". 
In this case, the problem is reduced to proving the anisotropic local law for $G$ with $U=\id$ and $V=\id$, such that the entries of $ Z^{(1)} \Lambda  $ and ${Z^{(2)}}$ are independent.
%$Y_1$ and $Y_2$ are independent. 
This problem can be handled using the standard resolvent methods as in e.g. \cite{isotropic,yang2019spiked,PY}. In particular, we shall prove the following proposition for the Gaussian case.  
%To go from the Gaussian case to the general $X$ case, we will adopt a continuous self-consistent comparison argument developed in \cite{Anisotropic}. 

%As discussed above, we first prove Theorem \ref{LEM_SMALL} for the case $U=\id$ and $V=\id$, which will imply the local laws in the Gaussian $Z^{(1)}$ and $Z^{(2)}$ case. Thus 

%Now consider the case $U=\id$ and $V=\id$, where we need to deal with the following resolvent:
% \begin{equation}\label{eqn_comparison1}
% G_0 (z):= \left( {\begin{array}{*{20}c}
%   { -z \id_{p}} & \Lambda (Z^{(1)})^{\top} & (Z^{(2)})^\top  \\
%   {Z^{(1)} \Lambda  } & {-\id_{n_1}} & 0 \\
%   {Z^{(2)}} & 0 & -\id_{n_2}
%   \end{array}} \right) ^{-1} , \quad z\in \mathbb C_+ .
% \end{equation}
%separable covariance matrices of the form $\Sig^{1/2} X \wt  \Sig X^\top \Sig^{1/2}$, which will imply the local laws in the Gaussian $X$ case. Thus in this section, we deal with the following resolvent:
%\begin{equation}\label{eqn_comparison1}
%G(X,z) {=}  \left[\left( {\begin{array}{*{20}c}
%   { 0 } & \Sig^{1/2} X \wt  \Sig^{1/2}   \\
%   {\wt \Sig^{1/2}X^\top\Sig^{1/2} } & {0}  \\
%   \end{array}} \right)-\left( {\begin{array}{*{20}c}
%   { I_{n\times n}} & 0  \\
%   0 & { zI_{N\times N}}  \\
%\end{array}} \right)\right]^{-1}
%\end{equation}
%with $X$ satisfying equation \eqref{eq_support} with $q=N^{-1/2}$.
%we choose the entries of $X$ to be $i.i.d.$ Gaussian due to the following reason. If $X=X^{\text{Gauss}}$ is Gaussian, then $U^\top X^{\text{Gauss}} V \stackrel{d}{=} X^{\text{Gauss}}$. Thus for the resolvent $G$ defined in in (\ref{eqn_defG}), we have
%\begin{equation}\label{eqn_comparison1}
%G(X,z) \stackrel{d}{=}  \left[\left( {\begin{array}{*{20}c}
%   { 0 } & \Sig^{1/2} X \wt  \Sig^{1/2}   \\
%   {\wt \Sig^{1/2}X^\top\Sig^{1/2} } & {0}  \\
%   \end{array}} \right)-\left( {\begin{array}{*{20}c}
%   { I_{n\times n}} & 0  \\
%   0 & { zI_{N\times N}}  \\
%\end{array}} \right)\right]^{-1}
%\end{equation}
%provided that $X$ is Gaussian. In particular, the entries of $\Sig^{1/2} X^{\text{Gauss}}\wt  \Sig^{1/2}$ are independent and satisfies the bounded support condition equation \eqref{eq_support} with $q=N^{-1/2}$, which make the direct proof of Theorem \ref{LEM_SMALL} possible using the methods in \cite{isotropic}. 
%Then we claim the following proposition for $G_0(z)$.
\begin{proposition}\label{prop_diagonal}
 Suppose Assumption \ref{assm_big1} holds, and that the entries of $Z^{(1)}$ and $Z^{(2)}$ are i.i.d. Gaussian random variables, which satisfy the bounded support condition  \eqref{eq_support} with $q= n^{-1/2}$. Suppose $U$ and $V$ are identity. 
Then the estimate \eqref{aniso_law} holds for all $z\in \mathbf D$.
%Suppose $X$ satisfies the bounded support condition (\ref{eq_support}) with $q= N^{-1/2}$. Suppose $A$ and $B$ are diagonal, i.e. $U=I_{n\times n}$ and $V=I_{N\times N}$. Fix $C_0>1$ and let $c_0>0$ be a sufficiently small constant. Then for any fixed $\epsilon>0$, the following estimates hold. %there exist constants $C_1>0$ and $\xi_1 \ge 3$ such that the following events hold with $\xi_1$-overwhelming probability:
%\begin{itemize}
%\item[(1)] {\bf Anisotropic local law}:  For any $z\in S(c_0,C_0,\epsilon)$ and deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb C^{\mathcal I}$,
%\begin{equation}\label{aniso_diagonal}
%\left| \langle \mathbf u, G(X,z) \mathbf v\rangle - \langle \mathbf u, \Pi (z)\mathbf v\rangle \right| \prec \Psi(z).
%\end{equation}
%
%\item[(2)] {\bf Averaged local law}: We have %{\bf Local deformed MP law}:
%\begin{equation}\label{aver_diagonal}
% | m(z)-M(z)|\prec ({N\eta})^{-1}
%\end{equation}
%for any $z\in S(c_0,C_0,\epsilon)$, and 
%%Moreover, outside of the spectrum we have the following stronger averaged local law (recall equation \eqref{KAPPA})
%\begin{equation}\label{aver_out}
% | m(z)-M(z)|\prec \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%for any $z\in S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$. 
%\end{itemize}
%Both of the above estimates are uniform in the spectral parameter $z$ and the deterministic vectors $\mathbf u, \mathbf v$.
\end{proposition}

%If the entries of $Z^{(1)}$ and $Z^{(2)}$ are i.i.d. Gaussian, then by Gaussian concentration $Z^{(1)}$ and $Z^{(2)}$ have support $q=n^{-1/2}$. Hence combing equation \eqref{eq in Gauss} with Proposition \ref{prop_diagonal}, we obtain that Theorem \ref{LEM_SMALL} holds when $Z^{(1)}$ and $Z^{(2)}$ are Gaussian. 
%The proof Proposition \ref{prop_diagonal} is similar to the previous proof of the local laws, such as \cite{isotropic, DY, Anisotropic, yang2019spiked}. Thus instead of giving all the details, we only describe briefly the proof. In particular, we shall focus on the key self-consistent equation argument, which is (almost) the only part that departs significantly from the previous proof in e.g. \cite{isotropic}. 
%Next we briefly describe how to extend Theorem \ref{LEM_SMALL} from the Gaussian case to the case with generally distributed $Z^{(1)}$ and $Z^{(2)}$. We will adopt a continuous comparison argument developed in \cite{Anisotropic}. Since the proof is almost the same as the ones in Sections 7 and 8 of \cite{Anisotropic} and Section 6 of \cite{yang2019spiked}, we will not write down all the details.
 



%\subsubsection{Comparison Argument with Continuous Interpolation}\label{sec_comparison}

%Following the above discussions, we divide the proof of Theorem \ref{LEM_SMALL} into two steps. In Section \ref{sec_Gauss}, we give the proof for separable covariance matrices of the form $\Sig^{1/2} X \wt  \Sig X^\top \Sig^{1/2}$, which implies the local laws in the Gaussian $X$ case. In Section \ref{sec_comparison}, we apply the self-consistent comparison argument in \cite{Anisotropic} to extend the result to the general $X$ case. Compared with \cite{Anisotropic}, there are two differences in our setting: (1) the support of $X$ in Theorem \ref{LEM_SMALL} is $q=\OO(N^{-\phi})$ for some constant $0<\phi \le 1/2$, while \cite{Anisotropic} dealt with $X$ with smaller support $q=\OO(N^{-1/2})$; (2) one has $B=I$ in \cite{Anisotropic}, which simplifies the proof a little bit.

Next we briefly describe how to extend Theorem \ref{LEM_SMALL} from the Gaussian case to the case with general $Z^{(1)}$ and $Z^{(2)}$ satisfying the bounded support condition (\ref{eq_support}) with $q\le n^{-\phi}$ for some constant $\phi>0$. 
%As remarked at the beginning of Section \ref{sec_Gauss},
%Proposition \ref{prop_diagonal} implies that equation \eqref{aniso_law} holds for Gaussian $(Z^{(1)})^{\text{Gauss}}$ and $(Z^{(2)})^{\text{Gauss}}$ as discussed above. 
With Proposition \ref{prop_diagonal}, it suffices is to prove that for $Z^{(1)}$ and $Z^{(2)}$ satisfying the assumptions in Theorem \ref{LEM_SMALL}, we have
\begin{equation*}%\label{Gaussian_starting}
 \mathbf u^\top  \left( G(Z,z) -  G(Z^{\text{Gauss}}, z)\right) \mathbf v  \prec q 
\end{equation*}
for any deterministic unit vectors $\mathbf u,\mathbf v\in{\mathbb R}^{p+n_1+n_2}$ and $z\in \mathbf D$, where we abbreviated that 
$$Z:=\begin{pmatrix}Z^{(1)} \\ Z^{(2)}\end{pmatrix},\quad \text{and} \quad Z^{\text{Gauss}}:=\begin{pmatrix}(Z^{(1)})^{\text{Gauss}}\\ (Z^{(2)})^{\text{Gauss}}\end{pmatrix}.$$
%Now similar to Lemma \ref{lemma_Im}, we can prove the following estimates for $\mathcal G$.
%
%\begin{lemma}\label{lem_comp_gbound}
%For $i\in \mathcal I_1$ and $\mu\in \mathcal I_2$, we define $\mathbf u_i=U^\top \mathbf e_i  \in \mathbb C^{\mathcal I_1}$ and $\mathbf v_\mu=V^\top \mathbf e_\mu  \in \mathbb C^{\mathcal I_2}$, i.e. $\mathbf u_i$ is the $i$-th row vector of $U$ and $\mathbf v_\mu$ is the $\mu$-th row vector of $V$. Let $\mathbf x \in \mathbb C^{\mathcal I_1}$ and $\mathbf y \in \mathbb C^{\mathcal I_2}$. Then we have %for some constant $C>0$,
%  \begin{align}
% & \sum_{i \in \mathcal I_1 }  \left| {G_{\mathbf x \mathbf u_i} } \right|^2  =\sum_{i \in \mathcal I_1 }  \left| {G_{ \mathbf u_i \mathbf x} } \right|^2  = \frac{|z|^2}{\eta}\im\left(\frac{ G_{\mathbf x\mathbf x}}{z}\right) , \label{eq_sgsq2} \\
%& \sum_{\mu  \in \mathcal I_2 } {\left| {G_{\mathbf y \mathbf v_\mu } } \right|^2 }=\sum_{\mu  \in \mathcal I_2 } {\left| {G_{\mathbf v_\mu \mathbf y } } \right|^2 }  = \frac{{\im G_{\mathbf y\mathbf y} }}{\eta }, \label{eq_sgsq1}\\ 
%& \sum_{i \in \mathcal I_1 } {\left| {G_{\mathbf y \mathbf u_i} } \right|^2 } =\sum_{i \in \mathcal I_1 } {\left| {G_{ \mathbf u_i \mathbf y} } \right|^2 } = {G}_{\mathbf y\mathbf y}  +\frac{\bar z}{\eta} \im G_{\mathbf y\mathbf y}  , \label{eq_sgsq3} \\
%& \sum_{\mu \in \mathcal I_2 } {\left| {G_{\mathbf x \mathbf v_\mu} } \right|^2 }= \sum_{\mu \in \mathcal I_2 } {\left| {G_{\mathbf v_\mu \mathbf x } } \right|^2 }= \frac{G_{\mathbf x\mathbf x}}{z}  + \frac{\bar z}{\eta} \im \left(\frac{G_{\mathbf x\mathbf x}}{z}\right) .\label{eq_sgsq4}
% \end{align}
% All of the above estimates remain true for $G^{(\mathbb T)}$ instead of $G$ for any $\mathbb T \subseteq \mathcal I$. 
%\end{lemma}
%%\begin{proof}
%%The proof is almost the same as the proof of Lemma \ref{lemma_Im}, except that we use
%%$$ \sum_{i\in \mathcal I_1}\mathbf v_i \mathbf v_i^\dag = V_1 V_1^\dag = I_{N\times N}.$$
%%\end{proof}
%\begin{proof}
%We only prove equation \eqref{eq_sgsq1} and equation \eqref{eq_sgsq3}. The proof for equation \eqref{eq_sgsq2} and equation \eqref{eq_sgsq4} is very similar. With  equation \eqref{spectral1}, we get that
%\begin{align}\label{middle}
%\sum_{\mu  \in \mathcal I_2 } {\left| {G_{\mathbf y \mathbf v_\mu } } \right|^2 } =& \sum_{\mu  \in \mathcal I_2 } \left\langle \mathbf y,G {\mathbf v_\mu  } \right\rangle \left\langle {\mathbf v_\mu}, G^\dag \mathbf y \right\rangle  = \sum_{k = 1}^N {\frac{{\left| {\left\langle {\mathbf y,\zeta _k } \right\rangle } \right|^2  }}{{\left( {\lambda _k  - E} \right)^2  + \eta ^2 }} }   =\frac{{\im  G_{\mathbf y\mathbf y} }}{\eta }.
%\end{align}
%For simplicity, we denote $Y:=\Sig^{1/2} U^{*}X V\wt  \Sig^{1/2}$. Then with equation \eqref{green2} and equation \eqref{spectral2}, we get that
%\begin{align*}
% \sum_{i \in \mathcal I_1 } {\left| {G_{\mathbf y\mathbf u_i} } \right|^2 } =  \left( {{\mathcal G_2} Y^\dag Y \mathcal G_2^\dag  } \right)_{\mathbf y\mathbf y}=  \left( {{\mathcal G_2} \left(Y^\dag Y-\bar z\right) \mathcal G_2^\dag  } \right)_{\mathbf y\mathbf y} + \bar z \left( {{\mathcal G_2} \mathcal G_2^\dag  } \right)_{\mathbf y\mathbf y} =  {G}_{\mathbf y\mathbf y}  +\frac{\bar z}{\eta} \im G_{\mathbf y\mathbf y}  ,
% \end{align*}
% where we used $\mathcal G_2^\dag= \left(Y^\dag Y-\bar z\right)^{-1}$ and equation \eqref{middle} in the last step.
%\end{proof}
%
%
%%\subsection{Bootstrapping on the spectral scale}
%%\begin{subsection}{Self-consistent comparison}\label{subsection_selfcomp}
%Our proof basically follows the arguments in \cite[Section 7]{Anisotropic} with some modifications. Thus we will not give all the details. We first focus on proving the anisotropic local law equation \eqref{aniso_law}, and the proof of equation \eqref{aver_in1}-equation \eqref{aver_out1} will be given at the end of this section. By polarization, to prove equation \eqref{aniso_law} it suffices to prove that %the following bound:
% \begin{equation}\label{goal_ani2}
%\left\langle \mathbf v, \left(G(X,z)- \Pi(z)\right) \mathbf v \right\rangle \prec q+\Psi(z)
%\end{equation}
%uniformly in $z\in \wt  S(c_0,C_0,\e)$ and any deterministic unit vector $ \mathbf v\in{\mathbb C}^{\mathcal I}$. In fact, we can obtain the more general bound equation \eqref{aniso_law}
%%\begin{equation*}%\label{goal_ani}
%%\left\langle \mathbf u, \left(G(X,z) - \Pi(z)\right) \mathbf v \right\rangle \prec \Psi(z)
%%\end{equation*}
%by applying (\ref{goal_ani2}) to the vectors $\mathbf u + \mathbf v$ and $\mathbf u + i\mathbf v$, respectively.
%
%%\begin{proposition}\label{comparison_prop}
%%Suppose the assumptions of Theorem \ref{LEM_SMALL} hold.  Fix ${\left| z \right|^2 } \le 1 - \tau$ and suppose that the assumptions of Theorem \ref{law_wideT} hold. If (\ref{assm_3rdmoment}) holds or $\eta \ge N^{-1/2+\zeta}|M_{2}|^{-1}$, then for any regular domain $\mathbf S \subseteq \mathbf D$,
%% \begin{equation}\label{goal_ani2}
%%\left\langle \mathbf v, \left( G(w)-\Pi(w)\right) \mathbf v \right\rangle \prec \Psi(z)
%%\end{equation}
%%uniformly in $w\in \bS$ and any deterministic unit vectors $ \mathbf v\in{\mathbb C}^{\mathcal I}$.
%%\end{proposition}
%
%%We first assume that (\ref{assm_3rdmoment}) holds. Then we will show how to modify the arguments to prove the $\eta \ge N^{-1/2+\zeta}|M_{2}|^{-1}$ case.
%The proof consists of a bootstrap argument from larger scales to smaller scales in multiplicative increments of $N^{-\delta}$, where
%\begin{equation}
% \delta \in\left(0,\frac{\min\{\epsilon,\phi\}}{2C_a}\right). \label{assm_comp_delta}
%\end{equation}
%Here $\e>0$ is the constant in $\wt  S(c_0,C_0,\e)$, $\phi>0$ is a constant such that $q\le N^{-\phi}$, $C_a> 0$ is an absolute constant that will be chosen large enough in the proof. For any $\eta\ge N^{-1+\e}$, we define
%\begin{equation}\label{eq_comp_eta}
%\eta_l:=\eta N^{\delta l} \text{ for } \ l=0,...,L-1,\ \ \ \eta_L:=1.
%\end{equation}
%where
%%\begin{equation}\label{eq_comp_L}
%$L\equiv L(\eta):=\max\left\{l\in\mathbb N|\ \eta N^{\delta(l-1)}<1\right\}.$
%%\end{equation}
%%through
%% \begin{equation}\label{eq_comp_eta}
%%  \eta_l:=\eta N^{\delta l}\ \ l=0,...,L-1,\ \ \ \eta_L:=1.
%% \end{equation}
%Note that $L\le \delta^{-1}$.
%
%By (\ref{eq_gbound}), the function $z\mapsto G(z)- \Pi(z)$ is Lipschitz continuous in $\wt  S(c_0,C_0,\e)$ with Lipschitz constant bounded by $N^2$. Thus to prove (\ref{goal_ani2}) for all $z\in \wt  S(c_0,C_0,\e)$, it suffices to show that (\ref{goal_ani2}) holds for all $z$ in some discrete but sufficiently dense subset ${\mathbf S} \subset \wt  S(c_0,C_0,\e)$. We will use the following discretized domain $\bS$.
%\begin{definition}
%Let $\mathbf S$ be an $N^{-10}$-net of $\wt  S(c_0,C_0,\e)$ such that $ |\mathbf S |\le N^{20}$ and
%\[E+\ii\eta\in\mathbf S\Rightarrow E+\ii\eta_l\in\mathbf S\text{ for }l=1,...,L(\eta).\]
%\end{definition}
%
%The bootstrapping is formulated in terms of two scale-dependent properties ($\bA_m$) and ($\bC_m$) defined on the subsets
%\[\mathbf S_m:=\left\{z\in\mathbf S\mid\text{Im} \, z\ge N^{-\delta m}\right\}.\]
%${(\bA_m)}$ For all $z\in\mathbf S_m$, all deterministic unit vectors $\mathbf x \in \mathbb C^{\mathcal I_1}$ and $\mathbf y \in \mathbb C^{\mathcal I_2}$, and all $X$ satisfying the assumptions in Theorem \ref{LEM_SMALL}, we have
%\begin{equation}\label{eq_comp_Am}
% \im \left(\frac{G_{\mathbf x\mathbf x}(z)}{z}\right) + \im G_{\mathbf y\mathbf y}(z)\prec \im M_{2}(z) +N^{C_a\delta}(q+\Psi(z)).
%\end{equation}
%${(\bC_m)}$ For all $z\in\mathbf S_m$, all deterministic unit vector $\mathbf v\in \mathbb C^{\mathcal I}$, and all $X$ satisfying the assumptions in Theorem \ref{LEM_SMALL}, %(\ref{assm1})-(\ref{assm2}), 
%we have
%\begin{equation}\label{eq_comp_Cm}
% \left|G_{\mathbf v\mathbf v}(z)-\Pi_{\mathbf v\mathbf v}(z)\right|\prec N^{C_a\delta}(q+\Psi(z)).
%\end{equation}
%%The bootstrapping is started by the following result
%%\begin{lemma}\label{lemm_boot0}
%It is trivial to see that ${(\mathbf A_0)}$ holds by equation \eqref{eq_gbound} and equation \eqref{Immc}. Moreover, it is easy to observe the following result.
%%\end{lemma}
%%\begin{proof}
%% By Lemma \ref{lemma_Im} and the assumption (\ref{assm3}), we have for $w\in\widehat\bS_0$,
%% \[\text{Im} G_{\mathbf{vv}}(w)\le C |w|^{1/2}\left\|G(w)\right\| \le \frac{C}{\eta}\le C |w|^{1/2}\Im \left[M_{1}(w)+M_{2}(w)\right],\]
%%where we use (\ref{estimate1_bulk}) for $w\in{\widehat\bS}_0$.
%%\end{proof}
%
%\begin{lemma}\label{lemm_boot2}
%For any $m$, property ${(\mathbf C_m)}$ implies property $(\mathbf A_m)$.
%\end{lemma}
%\begin{proof}
%By equation \eqref{Immc}, equation \eqref{Piii} and the definition of $\Pi$ in equation \eqref{defn_pi}, it is easy to get that 
%$$\im \left(\frac{\Pi_{\mathbf x\mathbf x}(z)}{z}\right) + \im \Pi_{\mathbf y\mathbf y}(z)\lesssim \im M_{2}(z) ,$$
%%$\im \Pi_{\bv\bv}=\OO(\im M_{2})$, 
%which finishes the proof.
%%Suppose property $(\mathbf C_m)$ holds. By (\ref{def_PiPhi}), we have
%%\begin{align*}
%%\widetilde \Pi_{\mathbf v\mathbf v} = |w|^{1/2} \left\langle \mathbf v, \overline T^\dag \Pi \overline T \mathbf v \right\rangle = |w|^{1/2} \left(\Pi_d \right)_{ {\mathbf u} {\mathbf u}} ,
%%\end{align*}
%%where $ \mathbf u = \bar T  \mathbf v.$ Now (\ref{estimate_PiImw}) implies
%%\begin{equation}\label{eqn_ImPi}
%%\Im\, \widetilde \Pi_{\mathbf v\mathbf v} \le C \Im(M_{1}+M_{2}),
%%\end{equation}
%%and further
%%$$\text{Im} G_{\mathbf{vv}}(w)\le\text{Im}\, \Pi_{\mathbf {vv}}+\left| G_{\mathbf{vv}}(w)-\Pi_{\mathbf{vv}}(w)\right|\prec|w|^{1/2}\text{Im}\left[M_{1}(w)+M_{2}(w)\right]+N^{C_a\delta}\Psi(z).$$
%%Thus the property $(\mathbf A_m)$ follows.
%\end{proof}
%
%The key step is the following induction result.
%\begin{lemma}\label{lemm_boot}
%For any $1\le m\le \delta^{-1}$, property $(\mathbf A_{m-1})$ implies property $(\mathbf C_m)$.
%\end{lemma}
%
%Combining Lemmas \ref{lemm_boot2} and \ref{lemm_boot}, we conclude that (\ref{eq_comp_Cm}) holds for all $w\in\mathbf S$. Since $\delta$ can be chosen arbitrarily small under the condition (\ref{assm_comp_delta}), we conclude that (\ref{goal_ani2}) holds for all $w\in\mathbf S$, and equation \eqref{aniso_law} follows for all $z\in \wt  S(c_0,C_0,\e)$. What remains now is the proof of Lemma \ref{lemm_boot}. Denote
%\begin{equation}\label{eq_comp_F(X)}
% F_{\mathbf v}(X,z):=\left|G_{\mathbf{vv}}(X,z)-\Pi_{\mathbf {vv}}(z)\right|.
%\end{equation}
%By Markov's inequality, it suffices to prove the following lemma.
%\begin{lemma}\label{lemm_comp_0}
% Fix $p\in \mathbb N$ and $m\le \delta^{-1}$. Suppose that the assumptions of Theorem \ref{LEM_SMALL} and property $(\mathbf A_{m-1})$ hold. Then we have
% \begin{equation}
%  \mathbb EF_{\mathbf v}^p(X,z)\le\left[ N^{C_a\delta}\left(q+\Psi(z)\right)\right]^p
% \end{equation}
% for all $z\in{\mathbf S}_m$ and any deterministic unit vector $\mathbf v$.
%\end{lemma}
%In the rest of this section, we focus on proving Lemma \ref{lemm_comp_0}. 
%%\begin{subsubsection}{Rough bound}
%First, in order to make use of the assumption $(\mathbf A_{m-1})$, which has spectral parameters in $\mathbf S_{m-1}$, to get some estimates for $G$ with spectral parameters in $\mathbf S_{m}$, we shall use the following rough bounds for $ G_{\mathbf{xy}}$.
%
%\begin{lemma}\label{lemm_comp_1}
%For any $z=E+\ii\eta\in\mathbf S$ and unit vectors $\mathbf x,\mathbf y\in \mathbb C^{\mathcal I}$,  we have %{\cor need to revise}
%\begin{align*}
%\left|G_{\mathbf x\mathbf y}(z)-\Pi_{\mathbf x\mathbf y}(z)\right|\prec & N^{2\delta}\sum_{l=1}^{L(\eta)} \left[\im \left(\frac{G_{\mathbf X^{(1)}\mathbf X^{(1)}}(E+\ii\eta_l)}{E+\ii\eta_l}\right)+\im G_{\mathbf X^{(2)}\mathbf X^{(2)}}(E+\ii\eta_l) \right.\\
%& \left. +\im \left(\frac{G_{\mathbf y_1\mathbf y_1}(E+\ii\eta_l)}{E+\ii\eta_l}\right)+\im G_{\mathbf y_2\mathbf y_2}(E+\ii\eta_l)\right]+1,
%\end{align*}
%where $\mathbf x=\left( {\begin{array}{*{20}c}
%   {\mathbf x}_1   \\
%   {\mathbf x}_2 \\
%   \end{array}} \right)$ and $\mathbf y=\left( {\begin{array}{*{20}c}
%   {\mathbf y}_1   \\
%   {\mathbf y}_2 \\
%   \end{array}} \right)$ for ${\mathbf x}_1,{\mathbf y}_1\in\mathbb C^{\mathcal I_1}$ and ${\mathbf x}_2,{\mathbf y}_2\in\mathbb C^{\mathcal I_2}$, and $\eta_l$ is defined in (\ref{eq_comp_eta}).
%%recall that $L(\eta)$ and $\eta_l$ are defined in $(\ref{eq_comp_L})$ and $(\ref{eq_comp_eta})$.
%\end{lemma}
%\begin{proof} The proof is the same as the one for \cite[Lemma 7.12]{Anisotropic}.\end{proof}
%%\begin{proof}
%%By (\ref{estimate_Piw12}) and the definition of $\widetilde \Pi$ in (\ref{def_PiPhi}), we get that $\left|\Pi_{\mathbf x\mathbf y}\right| \le |\mathbf x||\mathbf y|.$ Thus it suffices to estimate $|\mathcal G_{\mathbf{xy}}|$. By the definition of $\mathcal G$ in (\ref{def_mathcalg}), we see that $\mathcal G_{\mathbf{xy}}=R_{\mathbf{\bar x \bar y}}$ for $R:=|w|^{1/2}G$ and $\bar {\mathbf u}:=\overline T\mathbf u$ for $\mathbf u\in\{\mathbf x, \mathbf y\}$.
%%Using the singular value decomposition (\ref{singular_rep}) we get that
%%\begin{equation}\label{eqn_roughbound1}
%%\left|R_{\bar{\mathbf x}_1\bar{\mathbf y}_1}\right|=\left|\inprod{\bar{\mathbf x}_1,|w|^{1/2} \sum \limits_{ k=1 }^{ N } \frac { \xi_k \xi_k ^{ \dag  } }{ \lambda _{ k }-w } \bar{\mathbf y}_1}\right|\le |w|^{1/2} \sum \limits_{ k=1 }^{ N }\frac{|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}+ |w|^{1/2} \sum \limits_{ k=1 }^{ N }\frac{|\inprod{\bar{\mathbf y}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|},
%%\end{equation}
%%and
%%\begin{equation}\label{eqn_roughbound2}
%%\left|R_{\bar{\mathbf x}_1\bar{\mathbf y}_2}\right| = \left|\inprod{\bar{\mathbf x}_1,w^{-1/2}|w|^{1/2} \sum_{k=1}^{ N } \frac { \sqrt{\lambda_k}\xi_k \zeta_{\bar k}^\dag }{\lambda_k-w } \bar{\mathbf y}_2}\right|\le \sum \limits_{ k=1 }^{ N }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}+\sum \limits_{ k=1 }^{ N }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf y}_2,\zeta_{\bar k}}|^2}{2\left|\lambda_k-w\right|}.
%%\end{equation}
%%
%%%where the second step is by
%%%\begin{align*}
%%% \frac{\sqrt{\lambda_k} }{|\lambda^k-w|}=&\sqrt{\frac{\lambda_k}{\lambda_k^2+E^2+\eta^2-2E\lambda_k}}\\
%%% \le &\sqrt{\frac{\lambda_k}{2\lambda_k\sqrt{E^2+\eta^2}-2E\lambda_k}}\\
%%% =&\sqrt{\frac{\sqrt{E^2+\eta^2}+E}{2\eta^2}}\le \frac{|w|^{1/2}}{\eta}.
%%%\end{align*}
%%%{\color{red} \[R(w)=\begin{pmatrix} |w|^{1/2} \sum \limits_{ k=1 }^{ N }\frac { \xi_k \xi_k ^{ \dag  } }{ \lambda _{ k }-w }  & w^{-1/2}|w|^{1/2}\sum \limits_{ k=1 }^{ N } \frac { \sqrt \lambda_k\xi_k \zeta_k ^{ \dag  } }{ \lambda _{ k }-w }  \\ w^{-1/2}|w|^{1/2}\sum \limits_{ k=1 }^{ N } \frac { \sqrt\lambda_k\zeta_k \xi_k ^{ \dag  } }{ \lambda _{ k }-w }  & |w|^{1/2}\sum \limits_{ k=1 }^{ N } \frac { \zeta_k \zeta_k ^{ \dag  } }{ \lambda _{ k }-w }  \end{pmatrix}.\]}
%%Recall the notation $\eta_l$ in (\ref{eq_comp_eta}), define the subsets of indices
%%\[U_l=\{k\mid\eta_{l-1}\le|\lambda_k-E|<\eta_l\},\ \ l=0,...,L+1,\]
%%where we set $\eta_{-1}=0$ and $\eta_{L+1}=\infty$. Now we split the summation in (\ref{eqn_roughbound2}) according to $U_l$. For $l=1,...,L$ we have
%%\begin{align*}
%% \sum \limits_{ k\in U_l }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}\le& \sum_{k\in U_l}\frac{\sqrt{E+\eta_l}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_l}{2(\lambda_k-E)^2} \le \sum_{k\in U_l}\frac{({2E^2+2\eta_l^2})^{1/4}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_l}{(\lambda_k-E)^2+\eta_{l-1}^2}\\
%% \le& 2^{1/4}N^{2\delta}\sum_{k\in U_l}\frac{|{E+\ii\eta_l}|^{1/2}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_l}{(\lambda_k-E)^2+\eta_{l}^2} = 2^{1/4}N^{2\delta}\text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}\left(E+\ii\eta_l\right);
%%\end{align*}
%%for $l=0$,
%%\begin{align*}
%% \sum \limits_{ k\in U_0 }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}\le& \sum_{k\in U_0}\frac{\sqrt{E+\eta}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2 \sqrt{2}\eta}{2\left[(\lambda_k-E)^2+\eta^2\right]} \le 2^{-1/4}N^{2\delta}\sum_{k\in U_0}\frac{|{E+\ii\eta_1}|^{1/2}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_1}{(\lambda_k-E)^2+\eta_1^2}\\
%% =&2^{-1/4}N^{\delta}\text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}\left(E+\ii\eta_1\right);
%%\end{align*}
%%and for $l={L+1}$,
%%\begin{align*}
%% \sum \limits_{ k\in U_{L+1} }\frac{\sqrt{\lambda_k}\inprod{\bar{\mathbf x}_1,\xi_k}^2}{2\left|\lambda_k-w\right|} & \le \sum_{k\in U_{L+1}}\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2 \sqrt{2}|\lambda_k-E|}{2\left[(\lambda_k-E)^2+\eta_L^2\right]} \prec \sum_{k\in U_{L+1}}\frac{|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_L}{(\lambda_k-E)^2+\eta_{L}^2} \\
%% & \le C \text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}\left(E+\ii\eta_L\right),
%%\end{align*}
%%where in the second step we used that $\lambda_k\prec 1$, which follows from (\ref{norm_upperbound}). Combining the above estimates we get that
%%\[\sum \limits_{ k=1 }^{ N }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}\prec N^{2\delta}\sum_{l=1}^{L}\text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}(E+\ii\eta_l).\]
%%Similarly, we can prove that
%%\[\sum \limits_{ k=1 }^{ N }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf y}_2,\zeta_k}|^2}{\left|\lambda_k-w\right|}\prec N^{2\delta}\sum_{l=1}^{L}\text{Im}\, R_{\bar{\mathbf y}_2\bar{\mathbf y}_2}(E+\ii\eta_l).\]
%%Since $|E+\ii\eta|\le|E+\ii\eta_l|$, we immediately get
%%\[|w|^{1/2}\sum \limits_{ k=1 }^{ N }\frac{\inprod{\bar{\mathbf u}_1,\xi_k}^2}{\left|\lambda_k-w\right|}\prec N^{2\delta}\sum_{l=1}^{L}\text{Im}\, R_{\bar{\mathbf u}_1\bar{\mathbf u}_1}(E+\ii\eta_l)\]
%%for $\mathbf u \in \{\mathbf x, \mathbf y\}$.
%%%and
%%%\[|w|^{1/2}\sum \limits_{ k=1 }^{ N }\frac{\inprod{\bar{\mathbf y}_1,\xi_k}^2}{\left|\lambda_k-w\right|}\prec N^{2\delta}\sum_{l=1}^{L}\text{Im}R_{\bar{\mathbf y}_1\bar{\mathbf y}_1}(E+\ii\eta_l)\]
%%Plugging into (\ref{eqn_roughbound1}) and (\ref{eqn_roughbound2}), we get that
%%\[|R_{\bar{\mathbf x}_1\bar{\mathbf y}_1}|+|R_{\bar{\mathbf x}_1\bar{\mathbf y}_2}|\prec N^{2\delta}\sum_{l=1}^{L(\eta)}\left[\text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}(E+\ii\eta_l)+\text{Im}\, R_{\bar{\mathbf y}_1\bar{\mathbf y}_1}(E+\ii\eta_l)+\text{Im}\, R_{\bar{\mathbf y}_2\bar{\mathbf y}_2}(E+\ii\eta_l)\right].\]
%%We can bound $|R_{\bar{\mathbf x}_2 \bar{\mathbf y}_1}|$ and $|R_{\bar{\mathbf x}_2\bar{\mathbf y}_2}|$ in a similar way. This concludes the proof.
%%\end{proof}
%
%Recall that for a given family of random matrices $A$, we use $A=O_\prec(\zeta)$ to mean $\left|\left\langle\mathbf v, A\mathbf w\right\rangle\right|\prec\zeta \| \mathbf v\|_2 \|\mathbf w\|_2 $ uniformly in any deterministic vectors $\mathbf v$ and $\mathbf w$ (see Definition \ref{stoch_domination} (ii)).
%
%\begin{lemma}\label{lemm_comp_2}
%Suppose $(\mathbf A_{m-1})$ holds, then
% \begin{equation}\label{eq_comp_apbound}
%  G(z)-\Pi(z)=\OO_{\prec}(N^{2\delta}),
% \end{equation}
% and
%%for all $w\in \mathbf S_m$. Moreover for any unit vector $\mathbf v$ we have
%\begin{equation}\label{eq_comp_apbound2}
%\im \left(\frac{G_{\mathbf x\mathbf x}(z)}{z}\right) + \im G_{\mathbf y\mathbf y}(z) \prec N^{2\delta}\left[ \im M_{2}(z)+N^{C_a\delta}(q+\Psi(z))\right],
%\end{equation}
% for all $z \in \mathbf S_m$ and any deterministic unit vectors $\mathbf x \in \mathbb C^{\mathcal I_1}$ and $\mathbf y \in \mathbb C^{\mathcal I_2}$.
%\end{lemma}
%\begin{proof} The proof is the same as the one for \cite[Lemma 7.13]{Anisotropic}.\end{proof}
%%\begin{proof}
%%Let $z=E+\ii\eta \in \mathbf S_m$. Then $E+\ii\eta_l \in \mathbf S_{m-1}$ for $l=1,\ldots, L(\eta)$, and (\ref{eq_comp_Am}) gives $\im G_{\mathbf v\mathbf v}(z)\prec 1.$ The estimate (\ref{eq_comp_apbound}) now follows immediately from Lemma \ref{lemm_comp_1}. To prove (\ref{eq_comp_apbound2}), we remark that if $s(w)$ is the Stieltjes transform of any positive integrable function on $\mathbb R$, the map $\eta \mapsto \eta\Im\, s(E+\ii\eta)$ is nondecreasing and the map $\eta \mapsto \eta^{-1} \im s(E+\ii\eta)$ is nonincreasing. We apply them to $\im G_{\mathbf v\mathbf v}(E+\ii\eta)$ and $\im M_{2}(E+\ii\eta)$ to get for $Z^{(1)}=E+\ii\eta_1\in \mathbf S_{m-1}$,
%%\begin{align*}
%%\im G_{\mathbf v\mathbf v}(w) & \le N^{\delta}\frac{|w|^{1/2}}{|w_1|^{1/2}}\im G_{\mathbf v\mathbf v}(w_1)\prec N^{\delta}\left[|w|^{1/2}\im\left(M_{1}(w_1)+M_{2}(w_1)\right)+N^{C_a\delta}\frac{|w|^{1/2}}{|w_1|^{1/2}}\Phi(w_1)\right] \\
%%& \le N^{2\delta}\left[|w|^{1/2}\im\left(M_{1}(w)+M_{2}(w)\right)+N^{C_a\delta}\Psi(z)\right],
%%\end{align*}
%%where we used $\Psi(z):=|w|^{1/2}\Psi(w)$ and the fact that $\eta \mapsto \Psi(E+\ii\eta)$ is nonincreasing, which is clear from the definition (\ref{eq_defpsi}).
%%\end{proof}
%%\end{subsubsection}
We will prove the above statement using a continuous comparison argument developed in \cite{Anisotropic}. %to prove Lemma \ref{lemm_comp_0}. 
%The proof is similar to the ones in Sections 7 and 8 of \cite{Anisotropic}, so we only describe briefly the basic ideas, without writing down all the details. 
%We divide the proof into three subsections. In Sections \ref{subsec_interp}-\ref{section_words}, we prove Lemma \ref{lemm_comp_0} under the condition 
Since the proof is almost the same as the ones in Sections 7 and 8 of \cite{Anisotropic} and Section 6 of \cite{yang2019spiked}, we only describe the main ideas without writing down all the details.

We define the following continuous sequence of interpolating matrices between $Z^{\text{Gauss}}$ and $Z$. 

%\begin{subsection}{Interpolation and expansion} \label{subsec_interp}
%The self-consistent comparison is performed with the following interpolation.
\begin{definition}[Interpolation]
%Introduce the notation $X^0:=X^{\text{Gauss}}$ and $X^1:=X$. We define the interpolation matrix $X^\theta$ by setting
%\begin{equation}
% X^\theta_{i\mu}:=\chi^\theta_{i\mu} X^1+(1-\chi^\theta_{i\mu})X^0, \ \ \theta\in [0,1],
%\end{equation}
%for $i\in \mathcal I_1$ and $\mu\in \mathcal I_2$ (recall the Definition \ref{def_indexsets}). Here $(\chi^\theta_{i\mu})$ is a family of i.i.d Bernoulli random variables, independent of $X^0$ and $X^1$, satisfying $\bbP(\chi_{i\mu}^\theta=1)=\theta$ and $\bbP(\chi_{i\mu}^\theta=0)=1-\theta$.
We denote $Z^0:=Z^{\text{Gauss}}$ and $Z^1:=Z$. Let $\rho_{\mu i}^0$ and $\rho_{\mu i}^1$ be the laws of $Z_{\mu i}^0$ and $Z_{\mu i}^1$, respectively, for $i\in \cal I_0$ and $\mu \in \cal I_1\cup \cal I_2$. For any $\theta\in [0,1]$, we define the interpolated law
$\rho_{\mu i}^\theta := (1-\theta)\rho_{\mu i}^0+\theta\rho_{\mu i}^1.$ We shall work on the probability space consisting of triples $(Z^0,Z^\theta, Z^1)$ of independent $n\times p$ random matrices, where the matrix $Z^\theta=(Z_{\mu i}^\theta)$ has law
\begin{equation}\label{law_interpol}
\prod_{i\in \mathcal I_0}\prod_{\mu\in \mathcal I_1\cup \cal I_2} \rho_{\mu i}^\theta(\dd Z_{\mu i}^\theta).
\end{equation}
For $\lambda \in \mathbb R$, $i\in \mathcal I_0$ and $\mu\in \mathcal I_1\cup \cal I_2$, we define the matrix $Z_{(\mu i)}^{\theta,\lambda}$ through
\[\left(Z_{(\mu i)}^{\theta,\lambda}\right)_{\nu j}:=\begin{cases}Z_{\mu i}^{\theta}, &\text{ if }(j,\nu)\ne (i,\mu)\\ \lambda, &\text{ if }(j,\nu)=(i,\mu)\end{cases},\]
that is, it replaces the $(\mu,i)$-th entry of $Z^\theta$ with $\lambda$.
%We also introduce the matrices $G^{\theta}(z):=G\left(Z^{\theta},z\right),\ \ \ G^{\theta, \lambda}_{(\mu i)}(z):=G\left(Z_{(\mu i)}^{\theta,\lambda},z\right).$
%according to (\ref{def_mathcalg}) and the Definition \ref{def_linearHG}.
\end{definition}

\begin{proof}[Proof of Theorem \ref{LEM_SMALL}]
We shall prove %Lemma \ref{lemm_comp_0} 
equation \eqref{aniso_law} through interpolation matrices $Z^\theta$ between $Z^0$ and $Z^1$. We have seen that equation \eqref{aniso_law} holds for $Z^0$ by Proposition \ref{prop_diagonal}. %as remarked at the beginning of Section \ref{sec_Gauss}.
%\begin{lemma}\label{Gaussian_case}
%Lemma \ref{lemm_comp_0} holds if $X=X^0$.
%\end{lemma}
%\begin{proof}
%As remarked above (\ref{Gaussian_starting}), the anisotropic law (\ref{goal_ani}) holds for $X^0$, i.e. $F_{\mathbf v}^p(X^0,w)\prec \Phi^p$. Now to apply (iii) of Lemma \ref{lem_stodomin}, we need an upper bound $\mathbb E\left(F_{\mathbf v}^p(X^0,w)\right)^2 \le N^{C_p}$ for some constant $C_p$. This follows easily from (\ref{eq_gbound}) and
%(\ref{estimate_Piw12}).
%\end{proof}
Using the definition in (\ref{law_interpol}) and fundamental calculus, we get the following basic interpolation formula:
%\begin{lemma}\label{lemm_comp_3}
for differentiable $F:\mathbb R^{n \times p}\rightarrow \mathbb C$,
\begin{equation}\label{basic_interp}
\frac{\dd}{\dd\theta}\mathbb E F(Z^\theta)=\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left[\mathbb E F\left(Z^{\theta,Z_{\mu i}^1}_{(\mu i)}\right)-\mathbb E F\left(Z^{\theta,Z_{\mu i}^0}_{(\mu i)}\right)\right],
\end{equation}
 provided all the expectations exist.
%\end{lemma}
We shall apply equation \eqref{basic_interp} to $F(Z):=F_{\bu\mathbf v}^s(Z,z)$ for any fixed $s\in 2\N$, where %$F_{\mathbf u\mathbf v}(Z,z)$ defined as
\begin{equation*}%\label{eq_comp_F(X)}
 F_{\bu\mathbf v}(Z,z):=\left|\mathbf u^\top \left(G (Z,z)-\Gi(z)\right)\mathbf v\right|.
\end{equation*}
%Here for simplicity of notations, we introduce the following notation of generalized entries: for $\mathbf u,\mathbf v \in \mathbb R^{\mathcal I}$, we shall denote $
%G_{\mathbf{uv}}:= \mathbf u^\top G\mathbf v   . %\quad G_{a\mathbf{w}}:=\langle \mathbf e_a,G\mathbf w\rangle,
%$
%Moreover, we shall abbreviate $G_{\mathbf{u}{a}}:= G_{\bu\mathbf e_{{a}}}$ for ${a}\in \mathcal I$, where $\mathbf e_{{a}}$ is the standard unit vector along ${a}$-th axis. Given any vector $\mathbf u\in \mathbb \R^{\mathcal I_{1,2,3}}$, we always identify it with its natural embedding in $\mathbb R^{\mathcal I}$. 
%%$\left( {\begin{array}{*{20}c}
%%   {\mathbf x}  \\
%%   0 \\
%%\end{array}} \right)$ and $\left( {\begin{array}{*{20}c}
%%   0  \\
%%   \mathbf y \\
%%\end{array}} \right)$ in $\mathbb C^{\mathcal I}$.
%The exact meanings will be clear from the context. 
The main part of the proof is to show the following self-consistent estimate for the right-hand side of equation (\ref{basic_interp}): for any fixed $s\in 2\N$, any constant $\e>0$ and all $\theta\in[0,1]$,
%\begin{lemma}\label{lemm_comp_4}
 %Fix $p\in 2\mathbb N$ and $m\le \delta^{-1}$. Suppose (\ref{3moment}) and $\mathbf{(A_{m-1})}$ hold, then we have
 \begin{equation}\label{lemm_comp_4}
  \sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left[\mathbb EF_{\bu\mathbf v}^s\left(Z^{\theta,Z_{\mu i}^1}_{(\mu i)},z\right)-\mathbb EF_{\bu\mathbf v}^s\left(Z^{\theta,Z_{\mu i}^0}_{(\mu i)},z\right)\right]\le (n^\e q)^{s}+C\E F_{\bu\mathbf v}^s\left(Z^{\theta},z\right) 
 \end{equation}
 for some constant $C>0$. If equation \eqref{lemm_comp_4} holds, then combining equation \eqref{basic_interp} with  Gr\"onwall's inequality we obtain that for any fixed $s\in 2\N$ and constant $\e>0$, 
 $$\E\left|\bu^\top \left(G(Z^1,z)-\Pi(z)\right)\bv\right|^s  \lesssim (n^\e q)^{s}.$$
Finally applying Markov's inequality and noticing that $\e$ can be chosen arbitrarily small, we conclude equation \eqref{aniso_law}. 
%we can conclude Lemma \ref{lemm_comp_0} and hence equation \eqref{goal_ani2}. %Theorem \ref{LEM_SMALL}.%Proposition \ref{comparison_prop}.
%In order to prove equation \eqref{lemm_comp_4}, we compare $Z^{\theta,Z_{\mu i}^0}_{(\mu i)}$ and $Z^{\theta,Z_{\mu i}^1}_{(\mu i)}$ via a common $Z^{\theta,0}_{(\mu i)}$, i.e. we will prove that
%\begin{equation}\label{lemm_comp_5}
%\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left[\mathbb EF_{\bu\mathbf v}^p\left(Z^{\theta,Z_{\mu i}^a}_{(\mu i)},z\right)-\mathbb EF_{\mathbf v}^p\left(Z^{\theta,0}_{(\mu i)},z\right)\right]= \OO\left((n^\e q)^{p}+\E F_{\bu\mathbf v}^p\left(Z^{\theta},z\right) \right)
% \end{equation}
%for all $a\in \{0,1\}$ and $\theta\in[0,1]$. 
Underlying the proof of the estimate (\ref{lemm_comp_4}) is an expansion approach, which is very similar to the ones for Lemma 7.10 of \cite{Anisotropic} and Lemma 6.11 of \cite{yang2019spiked}. So we omit the details.
\end{proof}
%This follows from an improved self-consistent comparison argument for sample covariance matrices in \cite[Section 8]{Anisotropic}. The argument for our case is almost the same except for some notational differences, so we omit the details. 

%-------------remove-------------
%%Throughout the rest of the proof, 
%%During the proof, we always assume that $(\mathbf A_{m-1})$ holds. Also the rest of the proof is performed at a fixed $z\in \mathbf S_m$. 
%We define the $\mathcal I \times \mathcal I$ matrix $\Delta_{(\mu i)}^\lambda$ as
%\begin{equation}\label{deltaimu}
%\Delta_{(\mu i)}^{\lambda} :=\lambda \left( {\begin{array}{*{20}c}
%   { 0 } &   \mathbf u_i^{(\mu)} \mathbf e_\mu^\top     \\
%   {\mathbf e_\mu (\mathbf u_i^{(\mu)})^\top } & {0}  \\
%   \end{array}} \right),%   \lambda \mathbf u_i \delta_{is}\delta_{\mu t}+\lambda\delta_{it}\delta_{\mu s}, \ \  i\in \mathcal I_1, \mu\in \mathcal I_2,
%\end{equation}
%where we denote $\bu_i^{(\mu)}:=\Lambda U\mathbf e_i$ if $\mu \in \cal I_2$ and $\bu_i^{(\mu)}:=V\mathbf e_i$ if $\mu \in \cal I_3$. Then by the definition of $H$ in equation \eqref{linearize_block}), we have for any $\lambda,\lambda'\in \mathbb R$ and $K\in \mathbb N$,
%\begin{equation}\label{eq_comp_expansion}
%G_{(i \mu)}^{\theta,\lambda'} = G_{(\mu i)}^{\theta,\lambda}+\sum_{k=1}^{K}  G_{(\mu i)}^{\theta,\lambda}\left( \Delta_{(\mu i)}^{\lambda-\lambda'} G_{(\mu i)}^{\theta,\lambda}\right)^k+ G_{(\mu i)}^{\theta,\lambda'}\left(\Delta_{(\mu i)}^{\lambda-\lambda'} G_{(\mu i)}^{\theta,\lambda}\right)^{K+1}.
%\end{equation}
%%In the remainder of this section, we prove (\ref{lemm_comp_5}) for $u=1$.
%%where
%%$\overline V:=\begin{pmatrix}V_1 & 0\\ 0 & I\end{pmatrix}$ and $\alpha:=\frac{w^{1/2}}{|w|^{1/2}}.$
%%The following result provides a priori bounds for the entries of $G_{(\mu i)}^{\theta,\lambda}$.
%Using this expansion and the a priori bound equation \eqref{priorim}, it is easy to prove the following estimate: if $y$ is a random variable satisfying $|y|\prec q$, then
% \begin{equation}\label{comp_eq_apriori}
%   G_{(\mu i)}^{\theta,y}=\OO (1),\quad i\in\sI_1, \ \mu\in\sI_2 \cup \cal I_3,
% \end{equation}
% with overwhelming probability.
%% for all $i\in\sI^M_1$ and $\mu\in\sI_2$.
%%\end{lemma}
%%\begin{proof} The proof is the same as the one for \cite[Lemma 7.14]{Anisotropic}. \end{proof}
%%\begin{proof}
%% It suffices to show that $G_{(\mu i)}^{\theta,y}=O_{\prec}(N^{2\delta})$ since $\|\Pi\|=\OO(1)$ by equation \eqref{Piii}. By assumption ($\mathbf A_{m-1}$), the Lemma \ref{lemm_comp_2} holds for the matrix ensemble $X^\theta$ (since it satisfies (\ref{assm1})-(\ref{assm2})). In particular $G^{\theta,X_{i\mu}^u}_{(\mu i)}=O_\prec(N^{2\delta})$. Now we apply the expansion (\ref{eq_comp_expansion}) with $\lambda:=X_{i\mu}^\theta$, $\lambda':=y$ and large enough $K$ such that $K(2\delta-\phi)\le -2$. Using $|\lambda-\lambda'|\prec q$, it is easy to estimate all the terms in (\ref{eq_comp_expansion}) using Lemma \ref{lemm_comp_2} except the rest term.
%%To handle the rest term, we use the rough bound $G_{(\mu i)}^{\theta,\lambda'}\prec N$ coming from a simple modification of (\ref{eq_gbound}).
%%\end{proof}
%
%In the following proof, for simplicity of notations, we introduce $f_{(\mu i)}(\lambda):=F_{\mathbf v}^p(Z_{(\mu i)}^{\theta, \lambda})$. We use $f_{(\mu i)}^{(r)}$ to denote the $r$-th derivative of $f_{(\mu i)}$. By equation \eqref{comp_eq_apriori}, it is easy to see that for any fixed $r\in\bbN$, $ f_{(\mu i)}^{(r)}(y) =\OO(1)$ with overwhelming probability for any random variable $y$ satisfying $|y|\prec q$. 
%% With Lemma \ref{lemm_comp_6} and (\ref{eq_comp_expansion}), it is easy to prove the following result.
%%\begin{lemma}
%%Suppose that $y$ is a random variable satisfying $|y|\prec q$. Then for fixed $r\in\bbN$,
%%  \begin{equation}
%%  \left|f_{(\mu i)}^{(r)}(y)\right|\prec N^{2\delta(r+p)}.
%% \end{equation}
%%\end{lemma}
%%Then in Section \ref{subsec_3moment}, we show how to relax equation \eqref{3moment} to equation \eqref{assm_3moment} for $z\in \wt  S(c_0,C_0,\e)$.
%Then the Taylor expansion of $f_{(\mu i)}$ gives
%\begin{equation}\label{eq_comp_taylor}
%f_{(\mu i)}(y)=\sum_{r=0}^{p+4}\frac{y^r}{r!}f^{(r)}_{(\mu i)}(0)+\OO_\prec\left( q^{p+4}\right),
%\end{equation}
%%provided $C_a$ is chosen large enough in (\ref{assm_comp_delta}). 
%Therefore we have for $a\in\{0,1\}$,
%\begin{align}
%&\mathbb EF_{\mathbf v}^p\left(Z^{\theta,Z_{\mu i}^a}_{(\mu i)}\right)-\mathbb EF_{\mathbf v}^p\left(Z^{\theta,0}_{(\mu i)}\right)=\bbE\left[f_{(\mu i)}\left(Z_{i\mu}^a\right)-f_{(\mu i)}(0)\right]\nonumber\\
%& =\bbE f_{(\mu i)}(0)+\frac{1}{2n}\bbE f_{(\mu i)}^{(2)}(0)+\sum_{r=4}^{p+4}\frac{1}{r!}\bbE f^{(r)}_{(\mu i)}(0)\bbE\left(Z_{i\mu}^a\right)^r+\OO_\prec(q^{p+4}). \label{taylor1}
%\end{align}
%Here to illustrate the idea in a more concise way, we assume the extra condition 
%\be\label{3moment}
%%\mathbb E x_{ij}^3=0,  
%\mathbb E (Z^1_{\mu i})^3=0, \quad 1\le \mu \le n,\ \  1\le i \le p.
%\ee
%Hence the $r=3$ term in the Taylor expansion vanishes. However, this is not necessary as we will explain at the end of the proof.
%
%%where we used that $Z_{\mu i}^u$ has vanishing first and third moments and its variance is $1/N$. (Note that this is the only place where we need the condition equation \eqref{3moment}.) 
%
%
%By equation \eqref{conditionA2} and the bounded support condition, we have
%\be\label{moment-4}
%\left|\bbE\left(Z_{i\mu}^a\right)^r\right| \prec n^{-2}q^{r-4} , \quad r \ge 4.
%\ee
%Thus to show (\ref{lemm_comp_5}), we only need to prove for $r=4,5,...,p+4$,
%\begin{equation}\label{eq_comp_est}
%n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left|\bbE f^{(r)}_{(\mu i)}(0)\right|=\OO\left(\left(n^\e q\right)^p+\mathbb EF_{\bu\mathbf v}^p(Z^\theta,z)\right).\end{equation}
%%where we used (\ref{assm2}). 
%In order to get a self-consistent estimate in terms of the matrix $Z^\theta$ on the right-hand side of (\ref{eq_comp_est}), we want to replace $Z^{\theta,0}_{(\mu i)}$ in $f_{(\mu i)}(0)=F_{\bu\mathbf v}^p(Z_{(\mu i)}^{\theta, 0})$ with $Z^\theta = Z_{(\mu i)}^{\theta, Z_{\mu i}^\theta}$. %We have the following lemma.
%\begin{lemma}
%Suppose that
%\begin{equation}\label{eq_comp_selfest}
%n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2 \cup \cal I_3}\left|\bbE f^{(r)}_{(\mu i)}(Z_{i\mu}^\theta)\right|=\OO\left(\left(n^\e q\right)^p+\mathbb EF_{\mathbf v}^p(X^\theta,z)\right)
%\end{equation}
%holds for $r=4,...,4p+4$. Then (\ref{eq_comp_est}) holds for $r=4,...,4p+4$.
%\end{lemma}
%\begin{proof}
%The proof is the same as the one for \cite[Lemma 7.16]{Anisotropic}.
%%We abbreviate $ f_{(\mu i)}\equiv f$ and $X_{i\mu}^\theta \equiv \xi$. Then with (\ref{eq_comp_taylor}) we can get
%%\begin{equation}\label{eq_comp_taylor2}
%%\E f^{(l)}(0)=\E f^{(l)}(\xi)-\sum_{k=1}^{4p+4-l}\E f^{(l+k)}(0)\frac{\E \xi^k}{k!}+\OO_\prec(q^{p+4-l}).
%%\end{equation}
%%The estimate equation \eqref{eq_comp_est} then follows from a repeated application of (\ref{eq_comp_taylor2}).  Fix $r=4,...,4p+4$. Using (\ref{eq_comp_taylor2}), we get
%%\begin{align*}
%%\mathbb E f^{(r)}(0)&=\mathbb E f^{(r)}(\xi) - \sum_{k_1\ge 1} \mathbf 1(r+k_1 \le 4p +4)\mathbb E f^{(r+k_1)}(0) \frac{\mathbb E\xi^{k_1}}{k_1!}+\OO_\prec(q^{p+4-r}) \\
%%&=\mathbb E f^{(r)}(\xi) - \sum_{k_1\ge 1} \mathbf 1(r+k_1 \le 4p +4)\mathbb E f^{(r+k_1)}(\xi) \frac{\mathbb E\xi^{k_1}}{k_1!} \\
%%&+\sum_{k_1,k_2\ge 1} \mathbf 1(r+k_1+k_2 \le 4p +4)\mathbb E f^{(r+k_1+k_2)}(0) \frac{\mathbb E\xi^{k_1}}{k_1!} \frac{\mathbb E\xi^{k_2}}{k_2!} + \OO_\prec(q^{p+4-r}) \\
%%&=\cdots=\sum_{t=0}^{4p+4-r}(-1)^t \sum_{k_1,\cdots, k_t \ge 1}\mathbf 1\left(r+\sum_{j=1}^t k_j \le 4p +4\right)\mathbb E f^{(r+\sum_{j=1}^t k_j)}(\xi)\prod_{j=1}^t \frac{\mathbb E\xi^{k_j}}{k_j!} + \OO_\prec(q^{p+4-r}).
%%\end{align*}
%%The lemma now follows easily by using equation \eqref{moment-4}.
%\end{proof}
%%\end{subsection}
%
%%\begin{subsection}{Conclusion of the proof with words}\label{section_words}
%
%
%
%What remains now is to prove (\ref{eq_comp_selfest}). For simplicity of notations, we shall abbreviate $Z^\theta \equiv Z$. %for the remainder of the proof. 
%For any $k\in \N$, we denote
%\[A_{\mu i}(k):= \left(\frac{\partial}{\partial Z_{\mu i}}\right)^k \left( G_{\mathbf u\mathbf v}-\Pi_{\mathbf u\mathbf v}\right).\]
%The derivative on the right-hand side can be calculated using the expansion equation \eqref{eq_comp_expansion}. In particular, it is easy to verify that it satisfies the following bound 
% \begin{equation}\label{eq_comp_A2}
% |A_{\mu i}(k)|\prec \begin{cases}(\mathcal R_i^{(\mu)})^2+\mathcal R_\mu^2 , \ & \text{if } k \ge 2 \\ 
% \mathcal R_i^{(\mu)}\mathcal R_\mu , \ & \text{if } k = 1 \end{cases},
%  \end{equation}
%where for $i\in \cal I_1$ and $\mu \in \cal I_1\cup \cal I_2$, we denote
%\begin{equation}\label{eq_comp_Rs}
%\mathcal R_i^{(\mu)}:=|G_{\mathbf u \bu_i^{(\mu)}}|+|G_{ \mathbf v \bu_i^{(\mu)}}|,\quad \mathcal R_\mu:=|G_{\mathbf u \mu}|+|G_{ \mathbf v \mu}|.
%\end{equation}
%%for $a\in \sI$, where $\mathbf w_i:= \Sigma^{1/2} \bu_i$ for $i\in\sI_1$ and $\mathbf w_\mu:=\wt  \Sigma^{1/2} \bv_\mu$ for $\mu\in\sI_2$.
%%\begin{definition}[Words]\label{def_comp_words}
%%Given $i\in \mathcal I_1$ and $\mu\in \mathcal I_1\cup \cal I_2$. Let $\sW$ be the set of words of even length in two letters $\{\mathbf i, {\mu}\}$. We denote the length of a word $w\in\sW$ by $2m(w)$ with $m(w)\in \mathbb N$. We use bold symbols to denote the letters of words. For instance, $w=\mathbf t_1\mathbf s_2\mathbf t_2\mathbf s_3\cdots\mathbf t_r\mathbf s_{r+1}$ denotes a word of length $2r$.
%%Define $\sW_r:=\{w\in \mathcal W: m(w)=r\}$ to be the set of words of length $2r$, and such that
%%%We require that 
%%each word $w\in \sW_r$ satisfies that $\mathbf t_l\mathbf s_{l+1}\in\{\mathbf i{}{\mu},{}{\mu}\mathbf i\}$ for all $1\le l\le r$.
%%
%%Next we assign to each letter $*$ a value $[*]$ through $[\mathbf i]:=\Sigma \bu_i$, $[{} {\mu}]:=\wt  \Sigma \mathbf v_\mu,$ where $\mathbf u_i$ and $\bv_\mu$ are defined in Lemma \ref{lem_comp_gbound} and are regarded as summation indices. Note that it is important to distinguish the abstract letter from its value, which is a summation index. Finally, to each word $w$ we assign a random variable $A_{\mathbf v, i, \mu}(w)$ as follows. If $m(w)=0$ we define
%% $$A_{\mathbf v, i, \mu}(w):=G_{\mathbf v\mathbf v}-\Pi_{\mathbf v\mathbf v}.$$
%% If $m(w)\ge 1$, say $w=\mathbf t_1\mathbf s_2\mathbf t_2\mathbf s_3\cdots\mathbf t_r\mathbf s_{r+1}$, we define
%% \begin{equation}\label{eq_comp_A(W)}
%% A_{\mathbf v, i, \mu}(w):=G_{\bv[\mathbf t_1]} G_{[\mathbf s_2][\mathbf t_2]}\cdots G_{[\mathbf s_r][\mathbf t_r]} G_{[\mathbf s_{r+1}]\bv}.
%% \end{equation}
%%\end{definition}
%%Notice the words are constructed such that, by equation \eqref{deltaimu} and (\ref{eq_comp_expansion}) ,
%%\[\left(\frac{\partial}{\partial X_{i\mu}}\right)^r \left( G_{\mathbf v\mathbf v}-\Pi_{\mathbf v\mathbf v}\right)=(-1)^r r!\sum_{w\in \mathcal W_r} A_{\mathbf v, i, \mu}(w),\quad r\in \mathbb N,\]
%%with which we get that
%Then we can calculate the derivative
%\begin{align*}
%\left(\frac{\partial}{\partial Z_{\mu i}}\right)^r F_{\bu\bv}^p(Z)= \sum_{k_1+\cdots+k_p=r}\prod_{t=1}^{p/2} \left(A_{\mu i}(k_t)\overline{A_{\mu i}(k_{t+p/2})}\right).
%\end{align*}
%Then to prove (\ref{eq_comp_selfest}), it suffices to show that
%\begin{equation}
%n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left|\bbE\prod_{t=1}^{p/2}A_{\mu i}(k_t)\overline{A_{\mu i}(k_{t+p/2})}\right|=\OO\left(\left(n^\e q\right)^p+\mathbb EF_{\bu\mathbf v}^p(Z,z)\right)\label{eq_comp_goal1}
%\end{equation}
%for  $4\le r\le p+4$ and $(k_1,\cdots,k_p)\in \N^p$ satisfying $k_1 +\cdots+k_p=r$. 
%%To avoid the unimportant notational complications associated with the complex conjugates, we will actually prove that
%%\begin{equation}\label{eq_comp_goal2}
%%N^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE\prod_{t=1}^{p}A_{\mathbf v, i, \mu}(w_t)\right|=\OO\left(\left(n^\e q\right)^p+\mathbb EF_{\bu\mathbf v}^p(Z,z)\right).
%%\end{equation}
%%The proof of $(\ref{eq_comp_goal1})$ is essentially the same but with slightly heavier notations. 
%Treating zero $k$'s separately (note $A_{\mu i}(0)=(G_{\mathbf u\mathbf v}-\Pi_{\mathbf u\mathbf v})$ by definition), we find that it suffices to prove
%\begin{equation}
%\label{eq_comp_goal3}n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\bbE|A_{\mu i}(0)|^{p-l}\prod_{t=1}^{l}\left|A_{\mu i}(k_t)\right|=\OO\left(\left(n^\e q\right)^p+\mathbb EF_{\bu\mathbf v}^p(Z,z)\right)
%\end{equation}
%for  $4\le r\le p+4$ and $1\le l \le p$. Here without loss of generality, we assume that $k_t=0$ for $l+1\le t \le p$, and $\sum_{t=1}^l k_t=r$ with $k_t \ge 1$ for $t\le l$.
%
%%To estimate (\ref{eq_comp_goal3}) we introduce the quantity
%%\begin{equation}\label{eq_comp_Rs}
%%\mathcal R_a:=|G_{\mathbf v \mathbf w_a}|+|G_{\mathbf w_a \mathbf v}|
%%\end{equation}
%%for $a\in \sI$, where $\mathbf w_i:= \Sigma^{1/2} \bu_i$ for $i\in\sI_1$ and $\mathbf w_\mu:=\wt  \Sigma^{1/2} \bv_\mu$ for $\mu\in\sI_2$.
%%
%%\begin{lemma}\label{lem_comp_A}
%%  For $w\in\sW$, we have the rough bound
%%  \begin{equation}
%%  |A_{\mathbf v, i, \mu}(w)|\prec N^{2\delta(m(w)+1)}.\label{eq_comp_A1}
%%  \end{equation}
%%  Furthermore, for $m(w)\ge 1$ we have
%%  \begin{equation}
%%  |A_{\mathbf v, i, \mu}(w)|\prec(\mathcal R_i^2+\mathcal R_\mu^2)N^{2\delta(m(w)-1)}.\label{eq_comp_A2}
%%  \end{equation}
%%  For $m(w)=1$, we have the better bound
%%  \begin{equation}
%%  |A_{\mathbf v, i, \mu}(w)|\prec \mathcal R_i\mathcal R_\mu.\label{eq_comp_A3}
%%  \end{equation}
%%\end{lemma}
%%\begin{proof}
%%The estimates (\ref{eq_comp_A1}) and (\ref{eq_comp_A2}) follow immediately from the rough bound (\ref{eq_comp_apbound}) and definition (\ref{eq_comp_A(W)}).  
%%%For (\ref{eq_comp_A2}), we break $A_{\mathbf v, i, \mu}(w)$ into $G_{\bv[\mathbf t_1]}(G_{[\mathbf s_2][\mathbf t_2]}\cdots G_{[\mathbf s_n][\mathbf t_n]})^{1/2}$ times $(G_{[\mathbf s_2][\mathbf t_2]}\cdots G_{[\mathbf s_n][\mathbf t_n]})^{1/2}G_{[\mathbf s_{n+1}]\bv}$ and use Cauchy-Schwarz inequality. 
%%The estimate (\ref{eq_comp_A3}) follows from the constraint $\mathbf t_1\ne\mathbf s_2$ in the definition (\ref{eq_comp_A(W)}).
%%\end{proof}
%Now we first consider the case $r\le 2l-2$. Then by pigeonhole principle, there exist at least two $k_t$'s with $k_t=1$. Therefore by equation \eqref{eq_comp_A2} we have
%\begin{equation}\label{eq_comp_r1}
%\prod_{t=1}^{l}\left|A_{\mu i}(k_t)\right| \prec  \one(r\ge 2l-1)\left[(\mathcal R_i^{(\mu)})^2+\mathcal R_\mu^2\right]+\one(r\le 2l-2)(\mathcal R_i^{(\mu)})^2\mathcal R_\mu^2 .
%\end{equation}
%%Let $\mathbf v=\left( {\begin{array}{*{20}c}
%%   {\mathbf v}_1   \\
%%   {\mathbf v}_2 \\
%%   \end{array}} \right)$ for ${\mathbf v}_1 \in\mathbb C^{\mathcal I_1}$ and ${\mathbf v}_2\in\mathbb C^{\mathcal I_2}$. 
%Using equation \eqref{priorim} and a similar argument as in equation \eqref{GG*}, we get that
%\begin{align}
%\sum_{i\in\sI_1}(\mathcal R_i^{(\mu)})^2 =\OO(1),\quad  \sum_{\mu\in\sI_2 \cup \cal I_3}\mathcal R_{\mu}^2 =\OO(1), \quad \text{with overwhelming probability}.\label{eq_comp_r2}
%\end{align}
%%where in the second step we used the two bounds in Lemma \ref{lemm_comp_2} and $\eta =\OO(\im M_{2})$ by equation \eqref{Immc}, and in the last step the definition of $\Psi$ in equation \eqref{eq_defpsi}. Using the same method we can get
%%\begin{equation}\label{eq_comp_r3}
%%\frac{1}{N^2}\sum_{i\in\sI_1}\sum_{\mu\in\sI_2}\mathcal R_i^2\mathcal R_\mu^2\prec \left[N^{(C_a+2)\delta}\left(\Psi^2(z) + \frac{q}{N\eta}\right)\right]^2.
%%\end{equation}
%Using (\ref{eq_comp_r2}) and $n^{-1/2}\le q$, we get that %the left-hand side of (\ref{eq_comp_goal3}) is bounded by
%\begin{align*}
%n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}|A_{\mu i}(0)|^{p-l}\prod_{t=1}^{l}\left|A_{\mu i}(k_t)\right| &\prec  q^{r-4} F_{\bu\bv}^{p-l}(Z)\left[\one(r\ge 2l-1) n^{-1} +\one(r\le 2l-2)n^{-2}\right] \\
%&\le  F_{\bu\bv}^{p-l}(Z)\left[\one(r\ge 2l-1)q^{r-2}+\one(r\le 2l-2)q^r\right].
%%\\ &\le \bbE F_{\bv}^{p-l}(X)\left[\one(r\ge 2l-1)\left(N^{C_a\delta/2+12\delta}(q+\Psi)\right)^{r-2}+\one(r\le 2l-2)\left(N^{C_a\delta/2+12\delta}(q+\Psi)\right)^r\right],
%\end{align*}
%%\[q^{m-4}N^{2\delta(n+l+2)}\bbE F_{\bv}^{p-l}(X)\left[\one(m\ge 2l-1)\left(N^{C_a\delta/2}(q+\Psi(z))\right)^2+\one(m\le 2l-2)\left(N^{C_a\delta/2}(q+\Psi(z))\right)^4\right].\]
%%Using $\Phi \gtrsim N^{-1/2}$, we find that the left hand side of (\ref{eq_comp_goal3}) is bounded by
%%\begin{align*}
%% & N^{2\delta(n+q+2)} \bbE F_{\bv}^{p-q}(X)\left(\one(m\ge 2l-1)\left(N^{C_0\delta/2}\Phi\right)^{n-2}+\one(m\le 2l-2)\left(N^{C_0\delta/2}\Phi\right)^n\right)\\
%% &\le \bbE F_{\bv}^{p-q}(X)\left(\one(m\ge 2l-1)\left(N^{C_0\delta/2+12\delta}\Phi\right)^{n-2}+\one(m\le 2l-2)\left(N^{C_0\delta/2+12\delta}\Phi\right)^n\right)
%%\end{align*}
%If $r\le 2l-2$, then we get $q^r\le q^l$ using the trivial inequality $r\ge l$. On the other hand, if $r\ge 4$ and $r\ge 2l-1$, then $r\ge l+2$ and we get $q^r\le q^{l+2}$. Therefore we conclude that %the left-hand side of $(\ref{eq_comp_goal3})$ is bounded by
%\begin{equation}\nonumber
%n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2} |A_{\mu i}(0)|^{p-l}\prod_{t=1}^{l}\left|A_{\mu i}(k_t)\right| \prec F_{\bu\bv}^{p-l}(Z) q^l.
%\end{equation}
%Now (\ref{eq_comp_goal3}) follows from H\"older's inequality. This concludes the proof of (\ref{eq_comp_selfest}), and hence of (\ref{lemm_comp_5}), and hence of equation \eqref{aniso_law}. 
%%This proves equation \eqref{goal_ani2}, and hence equation \eqref{aniso_law} under the condition equation \eqref{3moment}.
%
%
%%Proposition \ref{comparison_prop} under the assumption (\ref{assm_3rdmoment}).
%%the anisotropic local law in Theorem \ref{law_wideT}.
%%\end{subsection}
%%\end{subsection}
%
%
%
%%\subsection{Non-vanishing third moment}\label{subsec_3moment}
%%
%%In this subsection, we prove Lemma \ref{lemm_comp_0} under equation \eqref{assm_3moment} for $z\in \wt  S(c_0, C_0,\e)$. 
%%%In this case, we can verify that
%%%\begin{equation}\label{eq_comp_boundPhi}
%%%\Phi \le N^{-1/4-\zeta/2}.
%%%\end{equation}
%%Following the arguments in Sections \ref{subsec_interp}-\ref{section_words}, we see that it suffices to prove the estimate ($\ref{eq_comp_selfest}$) in the $r=3$ case. In other words, we need to prove the following lemma. 
%%\begin{lemma}\label{lemm_comparison_big}
%%Fix $p\in 2\mathbb N$ and $m \le \delta^{-1}$. Let $z\in {\mathbf S}_m $ and suppose $(\mathbf A_{m-1})$ holds. Then %we have
%%\begin{equation}\label{eq_comp_selfest_generalX}
%%b_N N^{-2}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE f^{(3)}_{(\mu i)}(X_{i\mu}^\theta)\right|=\OO\left(\left[N^{C_a\delta} (q+\Psi)\right]^p+\mathbb EF_{\mathbf v}^p(X^\theta,z)\right).
%%\end{equation}
%%\end{lemma}
%%\begin{proof}
%%The main new ingredient of the proof is a further iteration step at a fixed $z$. Suppose
%%\begin{equation}\label{comp_geX_iteration}
%%G-\wt \Pi=\OO_\prec(\Phi)
%%\end{equation}
%%for some deterministic parameter $\Phi\equiv \Phi_N$. By the a priori bound (\ref{eq_comp_apbound}), we can take $\Phi\le N^{2\delta}$. Assuming (\ref{comp_geX_iteration}), we shall prove a self-improving bound of the form
%%\begin{equation}\label{comp_geX_self-improving-bound}
%%b_N N^{-2}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE f^{(3)}_{(\mu i)}(X_{i\mu}^\theta)\right|=\OO\left(\left[N^{C_a\delta} (q+\Psi)\right]^p+(N^{-\epsilon/2}\Phi)^p+\mathbb EF_{\mathbf v}^p(X^\theta,w)\right).
%%\end{equation}
%%Once (\ref{comp_geX_self-improving-bound}) is proved, we can use it iteratively to get an increasingly accurate bound 
%%for $\left|G_{\mathbf{vv}}(X,z)-\Pi_{\mathbf {vv}}(z)\right|$. After each step, we obtain a better bound (\ref{comp_geX_iteration}) with $\Phi$ reduced by $N^{-\e/2}$. Hence after $\OO(\e^{-1})$ many iterations we can get (\ref{eq_comp_selfest_generalX}).
%%
%%As in Section \ref{section_words}, to prove (\ref{comp_geX_self-improving-bound}) it suffices to show 
%%\begin{equation}\label{comp_geX_words}
%%b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}A^{p-l}_{\mathbf v, i, \mu}(w_0)\prod_{t=1}^{l}A_{\mathbf v, i, \mu}(w_t)\right|\prec F_{\bv}^{p-l}(X)\left[N^{(C_0-1)\delta}(q+\Psi) + N^{-\e/2}\Phi\right]^l,
%%\end{equation}
%%which follows from the bound
%%\begin{equation}\label{comp_geX_words2}
%%b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\prod_{t=1}^{l}A_{\mathbf v, i, \mu}(w_t)\right|\prec \left[N^{(C_0-1)\delta}(q+\Psi) + N^{-\e/2}\Phi\right]^l.
%%\end{equation}
%%%Each of the three cases $l=1,\, 2,\, 3$ can be proved as in \cite[Lemma 12.7]{Anisotropic}, and we leave the details to the reader. This concludes Lemma \ref{lemm_comparison_big}.
%%%The rest of the proof is straightforward. 
%%We now list all the three cases with $l=1,\, 2,\, 3$, and discuss each case separately.  
%%
%%When $l = 1$, the single factor $A_{\mathbf v, i, \mu}(w_1)$ is of the form
%%\[ G_{\mathbf v[\mathbf t_1]} G_{[\mathbf s_2][\mathbf t_2]} G_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v}.\]
%%%\[\wt G_{[s][t]}= G_{[s][t]}-\wt\Pi_{[s][t]},\] 
%%Then we split it as
%%\begin{align}
%%G_{\mathbf v[\mathbf t_1]} G_{[\mathbf s_2][\mathbf t_2]} G_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v}
%%=& G_{\mathbf v[\mathbf t_1]} \Pi_{[\mathbf s_2][\mathbf t_2]} \Pi_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v} + G_{\mathbf v[\mathbf t_1]}\wt G_{[\mathbf s_2][\mathbf t_2]} \Pi_{[\mathbf s_3][\mathbf t_3]}G_{[\mathbf s_4]\mathbf v}\nonumber\\
%% + & G_{\mathbf v[\mathbf t_1]} \Pi_{[\mathbf s_2][\mathbf t_2]} \wt G_{[\mathbf s_3][\mathbf t_3]}  G_{[\mathbf s_4]\mathbf v}+ G_{\mathbf v[\mathbf t_1]}\wt G_{[\mathbf s_2][\mathbf t_2]}
%%\wt G_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v},\label{comp_geX_expG}
%%\end{align}
%%where we abbreviate $\wt G: = G - \Pi$. For the second term, we have
%%\begin{align}\label{term11}
%%b_N N^{-2}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|G_{\mathbf v[\mathbf t_1]}\wt G_{[\mathbf s_2][\mathbf t_2]} \Pi_{[\mathbf s_3][\mathbf t_3]}G_{[\mathbf s_4]\mathbf v}\right|\prec b_N \Phi \cdot N^{(C_a+2)\delta}\left(\Psi^2 + \frac{q}{N\eta}\right)\prec N^{-\e/2}\Phi
%%\end{align}
%%provided $\delta$ is small enough, where we used (\ref{eq_comp_r2}), (\ref{comp_geX_iteration}) and the definition equation \eqref{tildeS}. The third and fourth term of (\ref{comp_geX_expG}) can be dealt with in a similar way. For the first term, when $[\mathbf t_1]=\mathbf w_i$ and $[\mathbf s_4]=\bw_\mu$, we have
%%\begin{align*}
%%& \Big|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2} G_{\mathbf v \mathbf w_i} \Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]} G_{\mathbf w_\mu \mathbf v}\Big| \prec N^{1+2\delta}\left(\sum_{\mu\in\mathcal I_2}| G_{\mathbf w_\mu\mathbf v}|^2\right)^{1/2}\prec N^{3/2+(C_a/2+3)\delta}(q+\Psi),
%%\end{align*}
%%where we used (\ref{eq_comp_r2}) and the fact that $\Pi$ is deterministic, such that the a priori bound (\ref{comp_eq_apriori}) gives
%%$$\Big|\sum_{i\in\mathcal I_0} G_{\mathbf v \mathbf w_i} \Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]} \Big| \prec N^{1/2+2\delta} .$$
%%%Cauchy-Schwarz inequality, a priori bounds (\ref{comp_eq_apriori}) and (\ref{eq_comp_r2}), and $\|\sum_i\mathbf v_i\|\le \sqrt N$. 
%%If $[\mathbf t_1]=\mathbf w_\mu$ and $[\mathbf s_4]=\mathbf v_i$, the proof is similar. If $[\mathbf t_1]=[\mathbf s_4]$, then at least one of the terms $\Pi_{[\mathbf s_2][\mathbf t_2]}$ and $\Pi_{[\mathbf s_3][\mathbf t_3]}$ must be of the form $\Pi_{\mathbf w_i\mathbf w_\mu}$ or $\Pi_{\mathbf w_\mu\mathbf w_i}$, and hence we have
%%$$\sum_i|\Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]}|=\OO(N^{1/2}) \quad\text{ or }\quad \sum_\mu | \Pi_{[\mathbf s_2][\mathbf t_2]} \Pi_{[\mathbf s_3][\mathbf t_3]}|=\OO(N^{1/2}).$$
%%Therefore using $(\ref{eq_comp_r2})$ and (\ref{tildeS}), we get
%%\begin{align*}
%%\Big|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}G_{\mathbf v[\mathbf t_1]} \Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]}G_{[\mathbf s_4]\mathbf v}\Big|
%%& \prec N^{3/2+(C_a+2)\delta}\left(q^2 + \Psi^2 \right) \le  N^{3/2}(q+\Psi) .
%%\end{align*}
%%provided $\delta$ is small enough. 
%%%where we used $(\ref{eq_comp_r2})$ and (\ref{eq_comp_boundPhi}).
%%In sum, we obtain that
%%$$b_NN^{-2}\Big|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2} G_{\mathbf v[\mathbf t_1]} \Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v}\Big|\prec N^{(C_a-1)\delta}(q+\Psi)$$
%%provided that $C_a\ge 8$. Together with equation \eqref{term11}, this proves  (\ref{comp_geX_words2}) for $l=1$.
%%
%%When $l=2$, $\prod_{t=1}^2 A_{\mathbf v, i, \mu}(w_t)$ is of the form
%%\begin{align}
%% & G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i\mathbf v}, \quad   G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_\mu} G_{\mathbf w_i \mathbf w_i} G_{\mathbf w_\mu\mathbf v}, \label{eqn_q21}  \\
%% &  G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_i} G_{\mathbf w_\mu \mathbf w_i} G_{\mathbf w_\mu\mathbf v}, \quad   G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_\mu} G_{\mathbf w_i \mathbf w_\mu} G_{\mathbf w_i\mathbf v},\label{eqn_q22}
%%\end{align}
%%or an expression obtained from one of these four by exchanging $\mathbf w_i$ and $\mathbf w_\mu$. The first expression in (\ref{eqn_q21}) can be estimated using (\ref{comp_eq_apriori}), (\ref{eq_comp_r2}) and (\ref{comp_geX_iteration}):
%%\begin{equation}
%%\label{q=2_1}\sum_\mu G_{\mathbf w_\mu \mathbf v} G_{\mathbf w_\mu\mathbf w_\mu}=\sum_\mu G_{\mathbf w_\mu \mathbf v}\wt G_{\mathbf w_\mu\mathbf w_\mu}+\sum_\mu  G_{\mathbf w_\mu \mathbf v}\Pi_{\mathbf w_\mu\mathbf w_\mu}=\OO_\prec\left[N^{1+(C_a/2+1)\delta}\Phi\left(\Psi^2 + \frac{q}{N\eta}\right)^{1/2}+ N^{1/2+2\delta}\right],
%%\end{equation}
%%and
%%\begin{equation}\label{q=2_2}
%%\Big|\sum_i G_{\mathbf v\mathbf w_i} G_{\mathbf v \mathbf w_i} G_{\mathbf w_i\mathbf v}\Big|\prec N^{1+(C_a+4)\delta} \left(\Psi^2 + \frac{q}{N\eta}\right).
%%\end{equation}
%%Combining equation \eqref{tildeS}, (\ref{q=2_1}) and (\ref{q=2_2}), we get that 
%%\[b_N N^{-2}\Big|\sum_i\sum_\mu G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i\mathbf v}\Big| \prec \left(N^{(C_a-1)\delta}(q+\Psi) + N^{-\e/2}\Phi\right)^2,\]
%%provided $\delta$ is small enough. The second expression in (\ref{eqn_q21}) can be estimated similarly. The first expression of (\ref{eqn_q22}) can be estimated using equation \eqref{tildeS}, (\ref{comp_eq_apriori}) and (\ref{eq_comp_r2}) by
%%\begin{equation*}
%%\begin{split}
%%b_N N^{-2}\left|\sum_i\sum_\mu  G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_i} G_{\mathbf w_\mu \mathbf w_i} G_{\mathbf w_\mu\mathbf v}\right|& \prec b_N N^{-2+2\delta}\sum_i\sum_\mu\left| G_{\mathbf v\mathbf w_i}\right|^2\left| G_{\mathbf w_\mu\mathbf v}\right|^2 \\
%%& \prec b_N N^{(2C_0+6)\delta}\left( \Psi^2 +\frac{q}{N\eta}\right)^2 \le (q +\Psi)^2
%%\end{split}
%%\end{equation*}
%%for small enough $\delta$. The second expression in (\ref{eqn_q22}) is estimated similarly.  This proves (\ref{comp_geX_words2}) for $l=2$.
%%
%%When $l = 3$, $\prod_{t=1}^3 A_{\mathbf v, i, \mu}(w_t)$ is of the form 
%%$( G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v})^3$ or an expression obtained by exchanging $\mathbf w_i$ and $\mathbf w_\mu$ in some of the three factors. We use (\ref{eq_comp_r2}) and $\sum_i|\Pi_{\mathbf v\mathbf w_i}|^2 = \OO(1)$ to get that
%%\[\left|\sum_i( G_{\mathbf v\mathbf w_i})^3\right|\prec \sum_i|\wt G_{\mathbf v\mathbf w_i}|^3+\sum_i|\Pi_{\mathbf v\mathbf w_i}|^3\prec \Phi\sum_i \left(| G_{\mathbf v\mathbf w_i}|^2+|\Pi_{\mathbf v\mathbf w_i}|^2 \right)+1\prec N^{1+(C_0+2)\delta}\left(\Psi^2 +\frac{q}{N\eta}\right)\Phi+\Phi+1.\]
%%Now we conclude (\ref{comp_geX_words2}) for $l=3$ using equation \eqref{tildeS} and $N^{-1/2}=\OO( q+\Psi)$.
%%\end{proof}
%----------remove end----------------

%Finally, if the condition equation \eqref{3moment} does not hold, then there is also an $r=3$ term in the Taylor expansion equation \eqref{taylor1}:
%$$\frac{1}{6}\bbE f^{(3)}_{(\mu i)}(0)\bbE\left(Z_{i\mu}^a\right)^3.$$
%Note that $\bbE\left(Z_{i\mu}^a\right)^3$ is of order $n^{-3/2}$, while the sum over $i$ and $\mu$ in equation \eqref{lemm_comp_5} provides a factor $n^2$. In fact, $\bbE f^{(3)}_{(\mu i)}(0)$ will provide an extra $n^{-1/2}$ to compensate the remaining $n^{1/2}$ factor. This follows from an improved self-consistent comparison argument for sample covariance matrices in \cite[Section 8]{Anisotropic}. The argument for our case is almost the same except for some notational differences, so we omit the details. 



%\subsection{Weak averaged local law}\label{section_averageTX}

%We first prove the following weak averaged local law.
%
%\begin{lemma} \label{thm_largerigidity2}
%Suppose the assumptions in Theorem \ref{thm_largerigidity} hold. %Fix the constants $c_0$ and $C_0$ as given in Theorem \ref{LEM_SMALL}. 
%Then for any fixed $\epsilon>0$, we have
%%there exists constant $C_1>0$, depending only on $c_0$, $C_0$, $B$ and $\phi$, such that %with overwhelming probability we have
%\begin{equation}
% \vert m(z)-M(z) \vert \prec  q^2 + (N \eta)^{-1}, \label{NEWMPBOUNDS2}
%\end{equation}
%uniformly for all $z \in  S(c_0, C_0, \epsilon)$. Moreover, outside of the spectrum we have the following stronger averaged local law (recall equation \eqref{KAPPA})
%\begin{equation}\label{aver_out2}
% | m(z)-M(z)|\prec q^2   + \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%uniformly in $z\in S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$ for any constant $\epsilon>0$. If $A$ or $B$ is diagonal, then equation \eqref{NEWMPBOUNDS2} and equation \eqref{aver_out2} hold without the condition equation \eqref{assm_3moment}.
%\end{lemma}

%In this section, we prove the weak averaged local laws in equation \eqref{aver_in1} and equation \eqref{aver_out1}. %\begin{proof}
%The proof is similar to that for equation \eqref{aniso_law} in previous subsections, and we only explain the differences. Note that the bootstrapping argument is not necessary, since we already have a good a priori bound by equation \eqref{aniso_law}.
%%In this section we prove the averaged local law in Theorem \ref{law_wideT}. The anisotropic local law proved in the previous section gives a good a priori bound. 
%In analogy to (\ref{eq_comp_F(X)}), we define
%%\begin{align*}
%%\wt F(X,w) : &=|w|^{1/2} |m_2(w)-M_{2}(w)|=|w|^{1/2}\left|\frac{1}{N}\sum_{\nu\in\sI_2}G_{\nu\nu}(w)-M_{2}(w)\right|\\
%%&=\left|\frac{1}{N}\sum_{\nu\in\sI_2} G_{\nu\nu}(w)-|w|^{1/2}M_{2}(w)\right|.
%%\end{align*}
%\begin{align*}
%\wt F(X,z) : &= |m(z)-M(z)| =\left|\frac{1}{nz}\sum_{i\in\sI_1} \left(G_{ii}(X,z)- \Pi_{ii}(z)\right)\right|,
%\end{align*}
%where we used equation \eqref{mcPi}. 
%%$\Phi^2 =\OO(|w|^{1/2}/{(N\eta)})$, 
%%it suffices to prove that $\wt F\prec (q+\Psi(z))^2$. 
%Moreover, by Proposition \ref{prop_diagonal}, equation \eqref{aver_in1} and equation \eqref{aver_out1} hold for Gaussian $X$ (without the $q^2$ term). For now, we assume equation \eqref{3moment} and prove the following stronger estimates:
%\begin{equation}
% \vert m(z)-M(z) \vert \prec (N \eta)^{-1} \label{aver_ins} %+ q^2 
%\end{equation}
%for $z\in S(c_0,C_0,\epsilon)$, and 
%\begin{equation}\label{aver_outs}
% | m(z)-M(z)|\prec \frac{q}{N\eta}  + \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%for $z\in S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$. At the end of this section, we will show how to relax equation \eqref{3moment} to equation \eqref{assm_3moment} for $z\in \wt  S(c_0,C_0,\e)$.
%
%Note that
%\be\label{psi2}
%\Psi^2(z) \lesssim \frac{1}{N\eta}, \quad \text{and} \quad \Psi^2(z) \lesssim \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}} \ \text{ outside of the spectrum}.
%\ee
%Then following the argument in Section \ref{subsec_interp}, analogous to (\ref{eq_comp_selfest}), we only need to prove that
%\begin{equation}\label{eq_comp_selfestAvg}
%N^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE \left(\frac{\partial}{\partial X_{i\mu}}\right)^r\wt F^p(X)\right|=\OO\left(\left[N^{\delta}\left(\Psi^2+\frac{q}{N\eta}\right)\right]^p+\mathbb E\wt F^p(X)\right)
%\end{equation}
%for all $r=4,...,4p+4$, where $\delta>0$ is any positive constant. Analogous to (\ref{eq_comp_goal2}), it suffices to prove that for $r=4,...,4p+4$,
%\begin{equation}\label{eq_comp_goalAvg}
%N^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE\prod_{t=1}^{p}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)\right|=\OO\left(\left[N^{\delta}\left(\Psi^2+\frac{q}{N\eta}\right)\right]^p+\mathbb E\wt F^p(X)\right)
%\end{equation}
%for $\sum_t m(w_t)=r$. 
%%The only difference in the definition of $A_{\mathbf v, i, \mu}(w)$ is that when $m(w)=0$, we define
%%\[A_{\mathbf v, i, \mu}(w):= G_{\mathbf v\mathbf v}-|w|^{1/2}M_{2}.\]
%Similar to (\ref{eq_comp_Rs}) we define
%\begin{equation}\nonumber%\label{eq_comp_RsAvg}
%\mathcal R_{j, a}:=| G_{j \mathbf w_a}|+| G_{\mathbf w_a j}|.
%\end{equation}
%Using equation \eqref{aniso_law} and Lemma \ref{lem_comp_gbound}, similarly to equation \eqref{eq_comp_r2}, we get that
%\begin{equation}\label{eq_comp_r22}
%\begin{split}
% \frac{1}{n}\sum_{j\in\sI_1}\mathcal R_{j,a}^2 & \prec \frac{ \im \left(z^{-1}G_{\mathbf w_i\mathbf w_i}\right) + \im G_{\mathbf w_\mu\mathbf w_\mu} + \eta\left(\left| G_{\mathbf w_i\mathbf w_i} \right|+ \left| G_{\mathbf w_\mu \mathbf w_\mu} \right|\right)}{N\eta} \prec \Psi^2+\frac{q}{N\eta}.
% \end{split}
%\end{equation}
%Since $G=\OO_\prec(1)$ by equation \eqref{aniso_law}, we have 
%\begin{equation}\label{average_bound}
%\left|\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w)\right|\prec \frac{1}{n}\sum_{j\in\sI_1}\left(\mathcal R_{j,i}^2+\mathcal R_{j,\mu}^2\right)\prec \Psi^2 +\frac{q}{N\eta} \quad \text{ for any $w$ such that }m(w)\ge 1.
%\end{equation}
%With (\ref{average_bound}), for any $r\ge 4$, the left-hand side of (\ref{eq_comp_goalAvg}) is bounded by
%\[\bbE\wt F^{p-l}(X)\left(\Psi^2+\frac{q}{N\eta}\right)^{l}.\]
%Applying Holder's inequality, we get equation \eqref{eq_comp_selfestAvg}, which completes the proof of equation \eqref{aver_ins} and equation \eqref{aver_outs} under equation \eqref{3moment}. %\cor about removing 3rd moment assumption \nc
%

%\end{proof}


%Then we prove the averaged local law for $z\in \wt  S(c_0,C_0,\e)$ under equation \eqref{assm_3moment}. By equation \eqref{psi2}, it suffices to prove 
%\begin{equation}\label{comp_avg_geX_self-improving-bound}
%b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\bbE \left(\frac{\partial}{\partial X_{i\mu}}\right)^3\wt F^p(X)\right|=\OO\left(\left[N^\delta (q^2 +\Psi^2)\right]^p + \left( \frac{N^{-\e/2}}{N\eta}\right)^p+\mathbb E\wt F^p(X)\right),
%\end{equation}
%for any constant $\delta>0$. Analogous to the arguments in Section \ref{subsec_3moment}, it reduces to showing that
%\begin{equation}\label{eq_comp_goalAvg_genX}
%b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2} \prod_{t=1}^{l}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)\right|=\OO_\prec\left(\left(q^2+\Psi^2\right)^{l} + \left( \frac{N^{-\e/2}}{N\eta}\right)^l\right),
%\end{equation}
%where $l\in \{1,2,3\}$ is the number of words with nonzero length. Then we can discuss these three cases using a similar argument as in Section \ref{subsec_3moment}, with the only difference being that we now can use the anisotropic local law equation \eqref{aniso_law} instead of the a priori bounds equation \eqref{comp_eq_apriori}  and (\ref{comp_geX_iteration}). %As an example, we only give the proof for the case with $l=1$.
%
%%Again we can prove the three cases $q=1,\, 2,\, 3$ as in \cite[Lemma 12.8]{Anisotropic}, and we leave the details to the reader. This concludes the averaged local law. 
%%
%%Again we discuss the three cases $q=1,\, 2,\, 3$ separately. During the proof we tacitly use the anisotropic local law proved above.
%
%In the $l=1$ case, we first consider the expression $A_{ \mathbf e_j, i, \mu}(w_1) =  G_{j\mathbf w_i} G_{\mathbf w_\mu \mathbf w_\mu} G_{\mathbf w_i\mathbf w_i} G_{\mathbf w_\mu j}$. We have 
%\begin{equation}\nonumber
%\left|\sum_i G_{j\mathbf w_i} G_{\mathbf w_i\mathbf w_i}\right| \le \left|\sum_i G_{j\mathbf w_i} \Pi_{\mathbf w_i\mathbf w_i}\right| + \sum_i (q+\Psi)\left| G_{j\mathbf w_i} \right|\prec \sqrt N + N (q+\Psi) \left(\Psi^2 +\frac{q}{N\eta}\right)^{1/2},
%\end{equation}
%where we used equation \eqref{aniso_law} and equation \eqref{eq_comp_r2}.
%%since the leading term is $\sum_i\wt\Pi_{\nu\mathbf v_i}\wt\Pi_{\mathbf v_i\mathbf v_i}$. 
%Similarly, we also have
%\begin{equation}\nonumber
% \left|\sum_\mu G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_\mu j}\right|\prec \left|\sum_\mu {\Pi}_{\mathbf w_\mu \mathbf w_\mu}  G_{\mathbf w_\mu j}\right|  + \left|\sum_\mu \wt { G}_{\mathbf w_\mu \mathbf w_\mu}  G_{\mathbf w_\mu j}\right|  \prec \sqrt N(q+\Psi)+N (q+\Psi) \left(\Psi^2 +\frac{q}{N\eta}\right)^{1/2},
%\end{equation} 
%where we also used $\Pi_{\mathbf w_\mu j}=0$ for any $\mu$ in the second step. Then with equation \eqref{tildeS}, we can see that the LHS of (\ref{eq_comp_goalAvg_genX}) is bounded by $\OO_\prec(q^2 + \Psi^2)$ in this case.
%%$$b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_1)\right)\right| \prec q^2+\Psi^2 .$$
%For the case $A_{ \mathbf e_j, i, \mu}(w_1) =  G_{j\mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i \mathbf w_\mu} G_{\mathbf w_i j}$, we can estimate that
%$$\left|\sum_\mu  G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i \mathbf w_\mu} \right| \le \left|\sum_\mu  \Pi_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i \mathbf w_\mu} \right| + \sum_\mu (q+\Psi)\left|G_{\mathbf w_i \mathbf w_\mu} \right| \prec \sqrt N + N (q+\Psi) \left(\Psi^2 +\frac{q}{N\eta}\right)^{1/2},$$
%and
%$$ \sum_i \left|G_{j\mathbf w_i} G_{\mathbf w_i j}\right|\prec N\left(\Psi^2 +\frac{q}{N\eta}\right).$$
%Thus in this case the LHS of (\ref{eq_comp_goalAvg_genX}) is also bounded by $\OO_\prec(q^2 + \Psi^2)$. The case $A_{ \mathbf e_j, i, \mu}(w_1) =  G_{j\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu \mathbf w_\mu} G_{\mathbf w_i j}$ can be handled similarly. Finally in the case $A_{ \mathbf e_j, i, \mu}(w_1) =  G_{j\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu \mathbf w_i} G_{\mathbf w_\mu j}$, we can estimate that
%$$ \left|\sum_{i,\mu}  G_{j\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu \mathbf w_i} G_{\mathbf w_\mu j} \right| \prec  \sum_{i,\mu} \left(\left| G_{j\mathbf w_i}\right|^2 +\left| G_{\mathbf w_\mu j} \right|^2 \right) | G_{\mathbf w_\mu \mathbf w_i}|^2 \prec N^2 \left(\Psi^2 +\frac{q}{N\eta}\right)^2.$$
%Again in this case the LHS of (\ref{eq_comp_goalAvg_genX}) is bounded by $\OO_\prec(q^2 + \Psi^2)$. All the other expressions are obtained from these four by exchanging $\mathbf w_i$ and $\mathbf w_\mu$.
%
%In the $l=2$ case, $\prod_{t=1}^{2}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)$ is of the forms
%\[\frac{1}{N^2}\sum_{j_1,j_2} G_{j_1\mathbf w_i} G_{\mathbf w_\mu j_1} G_{j_2 \mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i j_2}\quad \text{ or }\quad \frac{1}{N^2}\sum_{j_1,j_2} G_{j_1\mathbf w_i} G_{\mathbf w_\mu j_1} G_{j_2\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu j_2},\]
%or an expression obtained from one of these terms by exchanging $\mathbf w_i$ and $\mathbf w_\mu$. These two expressions can be written as 
%\be\label{2terms}
%N^{-2}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}(G^{\times 2})_{\mathbf w_i\mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu}, \quad N^{-2}( G^{\times 2})^2_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i}, \quad G^{\times 2}:= G \begin{pmatrix}I_{\mathcal I_1 \times \mathcal I_1} & 0\\ 0 & 0\end{pmatrix} G.
%\ee
%For the second term, using equation \eqref{green2}, equation \eqref{spectral1} and recalling that $Y=\Sig^{1/2} U^{*}X V\wt  \Sig^{1/2}$, we can get that
%\begin{align}
%& \left|\frac{1}{N^2}\sum_{i,\mu} ( G^{\times 2})^2_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i}\right| \le \frac{1}{N^2}\sum_{i,\mu} \left|( G^{\times 2})_{\mathbf w_\mu\mathbf w_i} \right|^2 = \frac{|z|^2}{N^2}\text{Tr}\left[(\mathcal G_1^{*})^2 YY^\top (\mathcal G_1)^2\right] \nonumber\\
%& =  \frac{|z|^2}{N^2}\text{Tr}\left[\mathcal G_1^{*} (\mathcal G_1)^2\right]  +  \frac{\bar z |z|^2}{N^2}\text{Tr}\left[(\mathcal G_1^{*})^2 (\mathcal G_1)^2\right]  \lesssim \frac{1}{N^2}\sum_k \frac{1}{\left[(\lambda_k-E)^2 +\eta^2\right]^{3/2}}+ \frac{1}{N^2}\sum_k \frac{1}{\left[(\lambda_k-E)^2 +\eta^2\right]^{2}} \nonumber\\
%& \lesssim \frac{1}{N\eta^3}\left(\frac1n \sum_k \frac{\eta}{(\lambda_k-E)^2 +\eta^2} \right) =\frac{\im m}{N\eta^3}\prec  \frac{\im m_c + q+\Psi}{N\eta^3} \lesssim \eta^{-2}\left(\Psi^2 +\frac{q}{N\eta}\right).\label{3term}
%\end{align}
%Using equation \eqref{aniso_law} and equation \eqref{eq_comp_r2}, it is easy to show that
%\be \label{3.5term}
%\left|\sum_{\mu}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i} \Pi_{\mathbf w_\mu\mathbf w_\mu}\right| \prec N^{3/2}\left( \Psi^2 + \frac{q}{N\eta}\right),\quad \text{ and } \quad \left|(G^{\times 2})_{\mathbf x\mathbf y} \right| \prec N\left( \Psi^2 + \frac{q}{N\eta}\right), \ee
%for any deterministic unit vectors $\mathbf x$, $\mathbf y$. Thus for the first term in equation \eqref{2terms}, we have
%\begin{align}
%\left|\frac{1}{N^2}\sum_{i,\mu}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}(G^{\times 2})_{\mathbf w_i\mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu}\right| & \le \left|\frac{1}{N^2}\sum_{i,\mu}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}(G^{\times 2})_{\mathbf w_i\mathbf w_i} \wt  G_{\mathbf w_\mu\mathbf w_\mu}\right| + \left|\frac{1}{N^2}\sum_{i,\mu}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}(G^{\times 2})_{\mathbf w_i\mathbf w_i} \Pi_{\mathbf w_\mu\mathbf w_\mu}\right| \nonumber\\
%& \prec N(q+\Psi)\left( \Psi^2 + \frac{q}{N\eta}\right)\left(\frac{1}{N^2}\sum_{i,\mu}\left|( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}\right|^2\right)^{1/2} +N^{3/2}\left( \Psi^2 + \frac{q}{N\eta}\right)^2 \nonumber\\
%& \prec N\eta^{-1}(q+\Psi)\left( \Psi^2 + \frac{q}{N\eta}\right)^{3/2} +N^{3/2}\left( \Psi^2 + \frac{q}{N\eta}\right)^2,\label{4term}
%\end{align}
%where in the last step we used the bound in equation \eqref{3term}. Now using equation \eqref{3term}, equation \eqref{4term} and equation \eqref{tildeS}, we get
%$$b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2} \prod_{t=1}^{2}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)\right| \prec \left(q^2+\Psi^2\right)^{2} + \left( \frac{N^{-\e/2}}{N\eta}\right)^2 .$$%\left( q^2 + \Psi^2 + \frac{N^{-\e/2}}{N\eta}\right)^2.$$
%
%Finally, in the $l=3$ case, $\prod_{t=1}^{3}\left(\frac{1}{N}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)$ is of the form 
%${N^{-3}}( G^{\times 2})^3_{\mathbf w_i\mathbf w_\mu}$, or an expression obtained by exchanging $\mathbf w_i$ and $\mathbf w_\mu$ in some of the three factors. Using equation \eqref{3.5term} and the bound in equation \eqref{3term}, we can estimate that %Using $\|\mathcal G_2\|\prec 1$, we can estimate $\left|\sum_i( G_2)^3_{\mathbf v_i\mu}\right|\prec 1$. Therefore 
%$$\frac{1}{N^3}\left|\sum_{i,\mu}( G^{\times 2})^3_{\mathbf w_i\mathbf w_\mu}\right| \prec \left( \Psi^2 + \frac{q}{N\eta}\right)\frac{1}{N^2}\sum_{i,\mu}\left|( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}\right|^2 \prec \eta^{-2}\left(\Psi^2 +\frac{q}{N\eta}\right)^2,$$
%Then the LHS of (\ref{eq_comp_goalAvg_genX}) is bounded by 
%$$O_\prec\left(\left(q^2 + \Psi^2\right) \left(\frac{N^{-\e/2}}{N\eta}\right)^2\right).$$
%
%Combining the above three cases, we conclude equation \eqref{comp_avg_geX_self-improving-bound}, which finishes the proof of equation \eqref{aver_in1} and equation \eqref{aver_out1}. %under equation \eqref{assm_3moment}.

%If $A$ or $B$ is diagonal, then by the remark at the end of Section \ref{subsec_3moment}, the anisotropic local law equation \eqref{aniso_law} holds for all $z\in S(c_0,C_0,\e)$ even in the case with $b_N=N^{1/2}$ in equation \eqref{assm_3moment}. Then with equation \eqref{aniso_law} and the self-consistent comparison argument in \cite[Section 9]{Anisotropic}, we can prove equation \eqref{aver_in1} and equation \eqref{aver_out1} for $z\in S(c_0,C_0,\e)$. Again most of the arguments are the same as the ones in \cite[Section 9]{Anisotropic}, hence we omit the details. 


\iffalse
This section is organized as follows. In Section \ref{sec tools}, we collect some basic estimates and resolvent identities that will be used in the proof of Theorem \ref{LEM_SMALL} and Proposition \ref{prop_diagonal}. Then in Section \ref{sec entry} we give the proof of Proposition \ref{prop_diagonal}, which concludes Theorem \ref{LEM_SMALL} when $Z^{(1)}$ and $Z^{(2)}$ have i.i.d. Gaussian entries. In Section \ref{sec_comparison}, we describe how to extend the result in Theorem \ref{LEM_SMALL} from the Gaussian case to the case where the entries of $Z^{(1)}$ and $Z^{(2)}$ are generally distributed. Finally, in Section \ref{sec contract}, we give the proof of Lemma \ref{lem_mbehaviorw} and Lemma \ref{lem_stabw}. In the proof, we always denote the spectral parameter by $z=E+\ii\eta$. 
\fi

Now it remains to prove Proposition \ref{prop_diagonal}, 
%. The core of the proof is  
%The main goal of this subsection is to prove 
whose proof is based on the following entrywise local law, Lemma \ref{prop_entry}. 
\begin{lemma}\label{prop_entry}
Under the assumptions of Proposition \ref{prop_diagonal}, %Fix $C_0>0$ and let $c_0>0$ be a sufficiently small constant. 
the following estimate holds uniformly in $z\in \mathbf D$: %there exist constants 
\begin{equation}\label{entry_diagonal}
\max_{a,b\in \cal I}\left| G_{ab}(z)  - \Gi_{ab} (z) \right| \prec n^{-1/2}.
\end{equation} 
\end{lemma}


With Lemma \ref{prop_entry}, we can complete the proof of the anisotropic local law  \eqref{aniso_law} in Proposition \ref{prop_diagonal}. %using a standard polynomialization method. %as we will explain later. 

\begin{proof}[Proof of Proposition \ref{prop_diagonal}]
With estimate (\ref{entry_diagonal}), one can use the polynomialization method in \cite[Section 5]{isotropic} to get the anisotropic local law (\ref{aniso_law}) with $q=n^{-1/2}$. The proof is exactly the same, except for some minor differences in notations. Hence we omit the details.
\end{proof}


%\subsubsection{Some Basic Estimates}\label{sec tools}
\subsubsection{An Entrywise Local Law}\label{sec entry}
 
 Finally, this subsection is devoted to the proof of Lemma \ref{prop_entry}. We first collect some preliminary results, Lemmas \ref{lemm apri}-\ref{lemm_resolvent}, that will be used in the proof. We remark that theses results work under the general setting in Theorem \ref{LEM_SMALL}, that is, we do not require $Z^{(1)}$ and $Z^{(2)}$ to be Gaussian as in Lemma \ref{prop_entry}.
 
% {\cor(start here)} In this subsection, we collect some basic estimates for the proof, while the main argument will be given in Section \ref{sec entry}. We remark that the estimates in this subsection work for general $G$, that is, we do not require $U$ and $V$ to be identity.

First with Fact \ref{lem_minv}, we can obtain the following a priori estimate on the resolvent $G(z)$ for $z\in \mathbf D$.% (recall equation \eqref{SSET1}).

\begin{lemma}\label{lemm apri}
Suppose Assumption \ref{assm_big1} holds. Then there exists a constant $C>0$ such that the following estimates hold uniformly in $z,z'\in \mathbf D$ with overwhelming probability: 
\be\label{priorim}
\|G(z)\| \le C,%|\im \langle \bu,\cal R  (z,0)\bv\rangle| \le C\eta, 
\ee
and %for any deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb R^{p+n}$,
\be\label{priordiff} 
\left\|G  (z) - G(z')\right\| \le C|z-z'|.
\ee
\end{lemma}
\begin{proof}
Our proof is a simple application of the spectral decomposition of $G$. Recall the matrix $A$ defined in Section \ref{sec_diff}. Let
\be\label{SVDW}
A= \sum_{k = 1}^{p} {\sqrt {\mu_k} \xi_k } \zeta _{k}^\top ,\quad \mu_1\ge \mu_2 \ge \ldots \ge \mu_{p} \ge 0 =\mu_{p+1} = \ldots = \mu_{n},\ee
be the singular value decomposition of $A$, where
%$\lambda_1\ge \lambda_2 \ge \ldots \ge \lambda_{p} \ge 0 = \lambda_{p+1} = \ldots = \lambda_{n}$ are the eigenvalues, 
$\{\xi_{k}\}_{k=1}^{p}$ are the left-singular vectors and $\{\zeta_{k}\}_{k=1}^{n}$ are the right-singular vectors.
%orthonormal bases of $\mathbb R^{\mathcal I_1}$ and $\mathbb R^{\mathcal I_2}$, respectively. 
Then using the definition (\ref{green2}), we get that for $i,j\in \mathcal I_1$ and $\mu,\nu\in \mathcal I_1\cup \cal I_2$,
\be\label{spectral}
\begin{split}
& G_{ij} = \sum_{k = 1}^{p} \frac{\xi_k(i) \xi_k^\top(j)}{\mu_k-z}, \ \ G_{\mu\nu} = 
%z\sum_{k = 1}^{n} \frac{\zeta_k(\mu) \zeta_k^\top(\nu)}{\lambda_k-z}=
z\sum_{k = 1}^{n} \frac{\zeta_k(\mu) \zeta_k^\top(\nu)}{\mu_k-z} , \ \ G_{i\mu} = G_{\mu i}= \sum_{k = 1}^{p} \frac{\sqrt{\mu_k}\xi_k(i) \zeta_k^\top(\mu)}{\mu_k-z}.
%\\&   \quad G_{\mu i} = \sum_{k = 1}^{p} \frac{\sqrt{\lambda_k}\zeta_k(\mu) \xi_k^\top(i)}{\lambda_k-z}. 
\end{split}
\ee
%As in equation \eqref{SVDW}, $\{\mu_k\}_{1\le k \le p}$ are the eigenvalues of $WW^\top$. 
By Fact \ref{lem_minv} and equation \eqref{assm2}, we have that with overwhelming probability
%\be\label{lambdap} 
$\mu_p \ge \lambda_p((Z^{(2)})^\top Z^{(2)}) \ge c_\tau $ for some constant $c_\tau>0$ depending only on $\tau$. This further implies that
$$ \inf_{z\in \mathbf D}\min_{1\le k \le p}|\mu_k-z| \ge c_\tau - (\log n)^{-1}.$$
Combining this bound with equation \eqref{spectral}, we can easily conclude estimates \eqref{priorim} and \eqref{priordiff}.
\end{proof}

%Note that by equation \eqref{priordiff} and local law equation \eqref{aniso_outstrong}, we have the rough bound
%\be\label{roughinitial}
%\max_{z\in \mathbf D} \max_{{a} , {b}\in \cal I}|\cal R_{{a} {b}} (z)- \Pi_{{a}{b}}(\theta_l)|\le C(\log n)^{-1} 
%\ee
%with overwhelming probability. 

%For simplicity, we denote $Y:=\Sig^{1/2} X \wt  \Sig^{1/2}$. 

The following lemma collects basic properties of stochastic domination $\prec$, which will be used tacitly in the proof.

\begin{lemma}[Lemma 3.2 in \cite{isotropic}]\label{lem_stodomin}
Let $\xi$ and $\zeta$ be two families of nonnegative random variables depending on some parameters $u\in \cal U$ or $v\in \cal V$.
\begin{itemize}
\item[(i)] Suppose that $\xi (u,v)\prec \zeta(u,v)$ uniformly in $u\in \cal U$ and $v\in \cal V$. If $|\cal V|\le n^C$ for some constant $C>0$, then $\sum_{v\in \cal V} \xi(u,v) \prec \sum_{v\in \cal V} \zeta(u,v)$ uniformly in $u$.

\item[(ii)] If $\xi_1 (u)\prec \zeta_1(u)$ and $\xi_2 (u)\prec  \zeta_2(u)$ uniformly in $u\in \cal U$, then $\xi_1(u)\xi_2(u) \prec \zeta_1(u) \zeta_2(u)$ uniformly in $u\in \cal U$.

\item[(iii)] Suppose that $\Psi(u)\ge n^{-C}$ is a family of deterministic parameters, and $\xi(u)$ satisfies $\mathbb E\xi(u)^2 \le n^C$. If $\xi(u)\prec \Psi(u)$ uniformly in $u$, then we also have $\mathbb E\xi(u) \prec \Psi(u)$ uniformly in $u$.
\end{itemize}
\end{lemma}

%Now we introduce the concept of minors, which are defined by removing certain rows and columns of the matrix $H$.
%
%\begin{definition}[Minors]\label{defnMinor}
%For any $ (p+n)\times (p+n)$ matrix $\cal A$ and index subset $\mathbb T \subseteq \mathcal I$, we define the minor $\cal A^{(\mathbb T)}:=(\cal A_{{a}{b}}:{a},{b} \in \mathcal I\setminus \mathbb T)$ as the $ (p+n-|\mathbb T|)\times (p+n-|\mathbb T|)$ matrix obtained by removing all rows and columns indexed by $\mathbb T$. Note that we keep the names of indices when defining $\cal A^{(\mathbb T)}$, i.e. $(\cal A^{(\mathbb{T})})_{{a}{b}}= \cal A_{{a}{b}}$ for ${a},{b} \notin \mathbb{{T}}$. Correspondingly, we define the resolvent minor as (recall equation \eqref{green2})
%\begin{align*}
%G^{(\mathbb T)}:&=\left[\left(H - \left( {\begin{array}{*{20}c}
%   { zI_{p}} & 0  \\
%   0 & { I_{n}}  \\
%\end{array}} \right)\right)^{(\mathbb T)}\right]^{-1} = \left( {\begin{array}{*{20}c}
%   { \mathcal G^{(\mathbb T)}} & \mathcal G^{(\mathbb T)} W^{(\mathbb T)}  \\
%   {\left(W^{(\mathbb T)}\right)^\top\mathcal G^{(\mathbb T)}} & { \mathcal G_R^{(\mathbb T)} }  \\
%\end{array}} \right)  ,
%%= \left( {\begin{array}{*{20}c}
%%   { z\mathcal G_1^{(\mathbb T)}} & Y^{(\mathbb T)}\mathcal G_2^{(\mathbb T)}   \\
%%   {\mathcal G_2^{(\mathbb T)}}\left(Y^{(\mathbb T)}\right)^\top & { \mathcal G_2^{(\mathbb T)} }  \\
%%\end{array}} \right),
%\end{align*}
%and define the partial traces $m^{(\mathbb T)}$ and $m^{(\mathbb T)}_k$, $k=0,1,2,$ by replacing $G$ with $G^{(\mathbb T)}$ in equation \eqref{defm}.
%%$$m_1^{(\mathbb T)}:=\frac{1}{Nz}\sum_{i\notin \mathbb T}\sigma_i G_{ii}^{(\mathbb T)},\ \ m_2^{(\mathbb T)}:= \frac{1}{N}\sum_{\mu \notin \mathbb T}\wt  \sigma_\mu G_{\mu\mu}^{(\mathbb T)}.$$ 
%%\be\label{m123}
%%\begin{split} 
%%m^{(\mathbb T)} := \frac1p \sum_{i \in \cal I_1}G^{(\mathbb T)}_{ii}(z),\quad & m^{(\mathbb T)}_1:=\frac1p\sum^{(\mathbb T)}_{i \in \cal I_1}\lambda_i^2 G^{(\mathbb T)}_{ii}(z),\\
%% m_2^{(\mathbb T)}(z):=  \frac1{n_1} \sum_{\mu\in \cal I_2} G_{\mu\mu}^{(\mathbb T)}(z),\quad  &m_3^{(\mathbb T)}(z):=  \frac1{n_2} \sum_{\mu\in \cal I_3} G_{\mu\mu}^{(\mathbb T)}(z),
%%\end{split}
%%\ee
%%where we abbreviated that $\sum_{a}^{(\mathbb T)} := \sum_{a\notin \mathbb T} $. 
%For convenience, we will adopt the convention that for any minor $\cal A^{(\mathbb T)}$ defined as above, $\cal A^{(\mathbb T)}_{{a}{b}} = 0$ if ${a} \in \mathbb T$ or ${b} \in \mathbb T$. Moreover, we will abbreviate $(\{{a}\})\equiv ({a})$ and $(\{{a}, {b}\})\equiv ({a}{b})$ for ${a},{b}\in \cal I$.
%\end{definition}

%\begin{definition} [Minor of matrix] For a $M \times N$ matrix $X$, $\ \mathbb{T}$ is a subset of  $\ \{1,2,\cdots,N\}$, we define $X^{\{\mathbb{T}\}}$ as the $M \times (N- \vert \mathbb{T} \vert)$ minor of matrix $X$ by deleting the $i$-th($i \in \mathbb{T}$) columns of $X$. We will keep the name of index of $X$ for $X^{\{\mathbb{T} \}}$, namely,
%$(X^{\{\mathbb{T}\}})_{ij}=\mathbf{1}_{ \{j \notin \mathbb{{T}}\}} X_{ij}$. 
%\end{definition}


%\begin{lemma}\label{Ward_id}
%%Fix constants $c_0,C_0>0$. The following estimates hold uniformly for all $z\in S(c_0,C_0,a)$ for any $a\in \mathbb R$:
%%\begin{equation}
%%\left\| G \right\| \le C\eta ^{ - 1} ,\ \ \left\| {\partial _z G} \right\| \le C\eta ^{ - 2}. \label{eq_gbound}
%%\end{equation}
%%Furthermore, 
%We have the following identities:
%\begin{align}
%& \sum_{i \in \mathcal I_1 }  \left| {G_{j i} } \right|^2 = \sum_{i \in \mathcal I_1 }  \left| {G_{ij} } \right|^2  = \frac{|z|^2}{\eta}\Im\left(\frac{G_{jj}}{z}\right) ,  \label{eq_gsq2} \\
%& \sum_{\mu  \in \mathcal I_2 } {\left| {G_{\nu \mu } } \right|^2 } = \sum_{\mu  \in \mathcal I_2 } {\left| {G_{\mu \nu} } \right|^2 }  = \frac{{\Im \, G_{\nu\nu} }}{\eta}, \label{eq_gsq1}\\ 
%& \sum_{i \in \mathcal I_1 } {\left| {G_{\mu i} } \right|^2 } = \sum_{i \in \mathcal I_1 } {\left| {G_{i\mu} } \right|^2 } = {G}_{\mu \mu}  + \frac{\bar z}{\eta} \Im \, G_{\mu\mu} , \label{eq_gsq3} \\ 
%&\sum_{\mu \in \mathcal I_2 } {\left| {G_{i \mu} } \right|^2 } = \sum_{\mu \in \mathcal I_2 } {\left| {G_{\mu i} } \right|^2 } =  \frac{{G}_{ii}}{z}  + \frac{\bar z}{\eta} \Im\left(\frac{{G_{ii} }}{z}\right) . \label{eq_gsq4} 
% \end{align}
%All of the above estimates remain true for $G^{(\mathbb T)}$ instead of $G$ for any $\mathbb T \subseteq \mathcal I$, and in the case where $A$ and $B$ are not diagonal.
%%Finally, suppose $\{\mathbf v_{i}\}_{i=1}^{N}$ and $\{\mathbf w_{\mu}\}_{\mu=1}^{N}$ are orthonormal bases of $\mathbb C^{\mathcal I_1}$ and $\mathbb C^{\mathcal I_2}$, respectively, then the above estimates remain true if we replace $\mathbf e_i$ with $\mathbf v_i$ and $\mathbf e_\mu$ with $\mathbf v_{\mu}$.
%\label{lemma_Im}
%\end{lemma}
%\begin{proof}
%These estimates and identities can be proved through simple calculations with (\ref{green2}), (\ref{spectral1}) and (\ref{spectral2}). We refer the reader to \cite[Lemma 4.6]{Anisotropic} and \cite[Lemma 3.5]{XYY_circular}.
%\end{proof}
%






%Finally, we have the following lemma, which is a consequence of the Assumption \ref{assm_big2}.
%\begin{lemma}\label{lem_assm3}
%There exists constants $c_0, \tau' >0$ such that 
%\begin{equation}\label{Piii}
%|1+M_{2}(z)\sigma_k |\ge \tau',
%\end{equation}
%for all $z \in S(c_0,C_0,C_1)$ and $1\le k \le M$.
%\end{lemma}
%\begin{proof}
%By Assumption \ref{assm_big2} and the fact $M_{2}(\lambda_r) \in (-\sigma_1^{-1}, 0)$, we have
%$$\left| 1+ M_{2}(\lambda_r) \sigma_k \right| \ge \tau,  \ \ 1\le k \le M.$$
%Applying (\ref{SQUAREROOT}) to the Stieltjes transform
%\begin{equation}\label{Stj_app}
%M_{2}(z):=\int_{\mathbb R} \frac{\rho_{2c}(dx)}{x-z},
%\end{equation}
%one can verify that $M_{2}(z) \sim \sqrt{z-\lambda_r}$ for $z$ close to $\lambda_r$. Hence if $\kappa+\eta \le 2c_0$ for some sufficiently small constant $c_0>0$, we have
%$$\left| 1+ M_{2}(z)\sigma_k \right| \ge \tau/2.$$
%Then we consider the case with $E-\lambda_r \ge c_0$ and $\eta \le c_1$ for some constant $c_1>0$. In fact, for $\eta=0$ and $E\ge \lambda_r + c_0$, $M_{2}(E)$ is real and it is easy to verify that $M_{2}'(E)\ge 0$ using the formula (\ref{Stj_app}). Hence we have
%$$\left| 1+ \sigma_k M_{2}(E) \right| \ge \left| 1+ \sigma_k M_{2}(\lambda_r + c_0) \right| \ge \tau/2, \ \ \text{ for }E\ge \lambda_r + c_0.$$
%Using (\ref{Stj_app}) again, we can get that 
%$$\left|\frac{dM_{2}(z)}{ d z }\right| \le c_0^{-2}, \ \ \text{for } E\ge \lambda_r + c_0.$$ 
%So if $c_1$ is sufficiently small, we have
%$$\left| 1+ \sigma_k M_{2}(E+\ii\eta) \right| \ge \frac{1}{2}\left| 1+ \sigma_k M_{2}(E) \right| \ge \tau/4$$
%for $E\ge \lambda_r + c_0$ and $\eta \le c_1$. Finally, it remains to consider the case with $\eta \ge c_1$. If $\sigma_k \le \left|2M_{2}(z)\right|^{-1}$, then we have $\left| 1+ \sigma_k M_{2}(z) \right| \ge 1/2$. Otherwise, we have $\Im \, M_{2}(z) \sim 1$ by (\ref{SQUAREROOTBEHAVIOR}). Together with (\ref{Immc}), we get that
%$$\left| 1+ \sigma_k M_{2}(z) \right| \ge \sigma_k \Im\, M_{2}(z) \ge \frac{\Im\, M_{2}(z)}{2 M_{2}(z)} \ge \tau' $$
%for some constant $\tau'>0$.
%\end{proof}



%For the proof of Proposition \ref{prop_diagonal}, it is convenient to introduce the following random control parameters.
%
%\begin{definition}[Control parameters]
%We define the random errors
%\begin{equation}\label{eqn_randomerror}
%\Lambda : = \mathop {\max }\limits_{a,b \in \mathcal I} \left| {\left( {G - \Pi } \right)_{ab} } \right|,\ \ \Lambda _o : = \mathop {\max }\limits_{a \ne b \in \mathcal I} \left| {G_{ab} } \right|, \ \ \theta:= |m_1-M_{1}| +  |m_2-M_{2}| ,
%\end{equation}
%%Moreover, we define 
%and the random control parameter (recall $\Psi$ defined in equation \eqref{eq_defpsi})
%\begin{equation}\label{eq_defpsitheta}
%\Psi _\theta  : = \sqrt {\frac{{\Im \, M_{2}  + \theta }}{{N\eta }}} + \frac{1}{N\eta}.
%\end{equation}
%%and the deterministic control parameter
%%\begin{equation}\label{eq_defpsi}
%%\Psi := \sqrt {\frac{\Im\, M_{2} }{{N\eta }} } + \frac{1}{N\eta}.
%%\end{equation}
%\end{definition}



%We fix a $\xi_1\ge 3$ throughout this section. 
%Our goal is to prove that $G$ is close to $\Pi$ in the sense of entrywise and averaged local laws. Hence it is convenient to introduce the following random control parameters.

%\begin{definition}[Control parameters]
%We define the entrywise and averaged errors
%\begin{equation}\label{eqn_randomerror}
%\Lambda : = \mathop {\max }\limits_{a,b \in \mathcal I} \left| {\left( {G - \Pi } \right)_{ab} } \right|,\ \ \Lambda _o : = \mathop {\max }\limits_{a \ne b \in \mathcal I} \left| {G_{ab} } \right|, \ \ \theta:= |m_2-M_{2}| .
%\end{equation}
%Moreover, we define the random control parameter
%\begin{equation}\label{eq_defpsitheta}
%\Psi _\theta  : = \sqrt {\frac{{\Im \, M_{2}  + \theta }}{{N\eta }}} + \frac{1}{N\eta},
%\end{equation}
%and the deterministic control parameter
%\begin{equation}\label{eq_defpsi}
%\Psi := \sqrt {\frac{\Im\, M_{2} }{{N\eta }} } + \frac{1}{N\eta}.
%\end{equation}
%\end{definition}

For the rest of this subsection, we present the proof of Lemma \ref{prop_entry}, which is the most technical part of the whole proof. 

\begin{proof}[Proof of Lemma \ref{prop_entry}]
Recall that under the assumptions of Lemma \ref{prop_entry},  we have %have %$q=n^{-1/2}$ and
\be\label{diagW}A \stackrel{d}{=} n^{-1/2}(\Lambda (Z^{(1)})^{\top}, (Z^{(2)})^\top),\ee
%Hence in the following proof, 
and it suffices to consider the resolvent in equation \eqref{resolv Gauss1} throughout the whole proof. 
%For simplicity, we will denote $G\equiv G_0$ in the following proof, while keeping in mind that $W$ takes the form in equation \eqref{diagW}. 
The proof is divided into three steps. For simplicity, we introduce the following notations: for two (deterministic or random) nonnegative quantities $\xi$ and $\zeta$, we write $\xi\lesssim \zeta$ if there exists a constant $C>0$ such that $\xi\le C\zeta$, and we write $\xi\sim \zeta$ if $\xi\lesssim \zeta$ and $\zeta\lesssim \xi$.

\vspace{5pt}

\noindent{\bf Step 1: Large deviation estimates.} In this step, we prove some (almost) optimal large deviation estimates on the off-diagonal entries of $G$, and on the following $\cal Z$ variables. In analogy to \cite[Section 3]{EKYY1} and \cite[Section 5]{Anisotropic}, we introduce the $\cal Z$ variables  
\begin{equation*}
 \cal  Z_{{a}} :=(1-\mathbb E_{{a}})\big(G_{{a}{a}}\big)^{-1}, %\quad {a}\notin \mathbb T,
\end{equation*}
where $\mathbb E_{{a}}[\cdot]:=\mathbb E[\cdot\mid H^{({a})}]$ denotes the partial expectation over the entries in the ${a}$-th row and column of $H$. Now using equation (\ref{resolvent2}), we get that for $i \in \cal I_0$, 
\begin{align}
\cal Z_i =  \frac{\lambda_i^2}{n} \sum_{\mu ,\nu\in \mathcal I_1}  G^{(i)}_{\mu\nu} \left(\delta_{\mu\nu} - Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}\right)+\frac1n \sum_{\mu ,\nu\in \mathcal I_2}  G^{(i)}_{\mu\nu} \left( \delta_{\mu\nu} - Z^{(2)}_{\mu i}Z^{(2)}_{\nu i}\right) - 2 \frac{\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu\nu},  \label{Zi}
\end{align}
and for $\mu\in \cal I_1$ and $\nu\in \cal I_2$, 
\begin{align}
&\cal  Z_\mu= \frac{1}{n} \sum_{i,j \in \mathcal I_0}  {\lambda_i \lambda_j}G^{(\mu)}_{ij} \left(\delta_{ij} - Z^{(1)}_{\mu i}Z^{(1)}_{\mu j}\right), \quad \cal Z_\nu = \frac{1}{n} \sum_{i,j \in \mathcal I_0} G^{(\nu)}_{ij} \left( \delta_{ij} - Z^{(2)}_{\nu i}Z^{(2)}_{\nu j}\right).\label{Zmu} 
\end{align}
Moreover, we introduce the random error
\begin{equation}  \label{eqn_randomerror}
%\begin{split}
 \Lambda _o : = % \max_{{a} \ne {b} } \left|  G_{{a}{b}}   \right| +  
 \max_{{a} \ne {b} } \left|  G_{{a}{a}}^{-1}G_{{a}{b}}   \right| ,
%\end{split}
\end{equation}
which controls the size of the off-diagonal entries. The following lemma gives the desired large deviation estimate on $\Lambda_o$ and $\cal Z$ variables.

\begin{lemma}\label{Z_lemma}
%Suppose the assumptions of Lemma \ref{prop_entry} hold. 
%Let $c_0>0$ be a sufficiently small constant and fix $C_0, \epsilon >0$. Define the $z$-dependent event $\Xi(z):=\{\Lambda(z) \le (\log N)^{-1}\}$. Then there exists constant $C>0$ such that 
Under the assumptions of Proposition \ref{prop_diagonal}, the estimate 
\begin{align}
\Lambda_o + \max_{{a}\in \cal I} |\cal Z_{{a}}|  \prec n^{-1/2} \label{Zestimate1}
\end{align}
holds uniformly in all $z\in \mathbf D$.
%and 
%\begin{align}
%{\mathbf 1}\left(\eta \ge 1 \right)\left(\Lambda_o + |Z_{a}|\right)\prec \Psi_\theta. \label{Zestimate2}
%\end{align}
%Moreover, then for $z\in S(C_0)$,
%\begin{align}
%1(\Xi)\Lambda_0 \le C(\log N)^{2\xi}\left(q+\Psi_\theta\right) \label{offestimate}
%\end{align}
%holds with $\xi$-overwhelming probability.
\end{lemma}
\begin{proof}
%Suppose $\Xi$ holds, then we have $|G_{{a}{a}} -\Pi|\lesssim (\log N)^{-1}$ on $\Xi$, and  
Note that for any ${a}\in \cal I$, $H^{({a})}$ and $G^{({a})}$ also satisfies the assumptions in Lemma \ref{lemm apri}. Hence equations \eqref{priorim} and \eqref{priordiff} also hold for $G^{({a})}$. Now applying bounds \eqref{eq largedev1} and \eqref{eq largedev2} to equations (\ref{Zi}) and \eqref{Zmu}, %and using the a priori bound equation \eqref{priorim}, 
we get that for any $i\in \cal I_0$, %on $\Xi$,
\begin{equation}\nonumber%\label{estimate_Zi}
\begin{split}
\left| \cal Z_{i}\right|&\lesssim \frac{1}{n} \left|\sum_{\mu ,\nu\in \mathcal I_1}  G^{(i)}_{\mu\nu} \left(\delta_{\mu\nu} - Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}\right)\right|+\frac1n \left|\sum_{\mu ,\nu\in \mathcal I_2}  G^{(i)}_{\mu\nu} \left( \delta_{\mu\nu} - Z^{(2)}_{\mu i}Z^{(2)}_{\nu i}\right)\right| + \frac{1}{n} \left|\sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu\nu}\right| \\
&\prec n^{-1/2}+ \frac{1}{n} \Big( \sum_{\mu,\nu \in \cal I_1\cup \cal I_2 }  {| G_{\mu\nu}^{(i)}|^2 }  \Big)^{1/2} \prec n^{-1/2} , 
%+ \frac{1}{\sqrt{n}} \left( \frac{1}{n}\sum_{\mu \in \cal I_{\al+2}}  {\left(\cal R^{(i)} J_{\al+2} (\cal R^{(i)})^*\right)_{\mu\mu} }  \right)^{1/2} \prec n^{-1/2},
%\\& \prec  \phi_n + \frac{1}{n} \left[ \sum_{\mu\in \cal I_3} \left( 1+ \frac{ \im \left( U(z) G_{[\mu\mu]}^{(i)} \right)_{11} }{\eta}   \right]  \right)^{1/2} ,%= q+ \frac{1}{N}\left( {\sum_\mu \frac{\wt \sigma_\mu  \im G_{\mu\mu}^{(i)} }{\eta} } \right)^{1/2}= q + \sqrt { \frac{ \Im\, m_2^{(i)}  } {N\eta} },
\end{split}
\end{equation}
where in the last step we used equation \eqref{priorim} to get that for any $\mu\in \cal I_1\cup \cal I_2$,
\be\label{GG*}\sum_{\nu \in \cal I_1\cup \cal I_2 }  | G_{\mu\nu}^{(i)} |^2\le \sum_{{a} \in \cal I } | G_{\mu{a}}^{(i)} |^2 =\left[G^{(i)}(G^{(i)})^* \right]_{\mu\mu} =\OO(1),\quad \text{with overwhelming probability,}\ee
where $(G^{(i)})^*$ denotes the complex conjugate transpose of $G^{(a)}$. Similarly, applying bounds \eqref{eq largedev1} and \eqref{eq largedev2} to $\cal Z_{\mu}$ and $\cal Z_\nu$ in equation (\ref{Zmu}) and using the estimate \eqref{priorim}, we can obtain the same bound.
%\begin{equation}\label{estimate_Zmu} \|Z_{[\mu]}\|\prec \frac{1}{n} \left( \sum_{i,j \in \cal I_{1}\cup \cal I_2}  {\left| \cal R_{ij}^{[\mu]}  \right|^2 }  \right)^{1/2} =  \frac{1}{\sqrt{n}} \left(  \frac{1}{n}\sum_{i \in \cal I_{1}\cup \cal I_2}  {\left(\cal R^{[\mu]} (J_{1}+J_2) (\cal R^{[\mu]})^*\right)_{ii} } \right)^{1/2} \prec n^{-1/2}.\ee
%This completes the proof of equation \eqref{Zestimate1}.
%we have
%\begin{equation}
%G_{i{a}}   = -G_{ii}  \left( WG^{\left( {i} \right)} \right)_{i{a}},\quad  G_{\mu {b} }  = - G_{\mu \mu }  \left( W^\top  G^{\left( {\mu } \right)}  \right)_{\mu {b} }.  
%\end{equation}
Then we prove the off-diagonal estimate on $\Lambda_o$. For $i\in \mathcal I_1$ and ${a}\in \cal I\setminus \{i\}$, using equation \eqref{resolvent3}, Lemma \ref{largedeviation} and equation \eqref{priorim}, we can obtain that 
\begin{align*}
& \left|G_{ii}^{-1}G_{i{a}}\right| \prec n^{-1/2}+ n^{-1/2}\Big( \sum_{\mu \in \cal I_1\cup \cal I_2}  {| G_{\mu {a}}^{(i)} |^2 }  \Big)^{1/2} \prec n^{-1/2}. 
%\\& \left|G_{\mu\mu}^{-1} G_{\mu{b}} \right| \prec n^{-1/2}+  \frac{1}{\sqrt n} \left( \sum_{i \in \cal I_{1}}  {\left| G_{i{b}}^{(\mu)}  \right|^2 }  \right)^{1/2} \prec n^{-1/2}.
 \end{align*}
 We can get the same estimate for $\left|G_{\mu\mu}^{-1} G_{\mu{b}} \right|$, $\mu \in \mathcal I_1\cup \cal I_2$ and ${b}\in \cal I\setminus \{ \mu\}$, using a similar argument.  
%For $i\in \cal I_1\cup \cal I_2$ and $\mu \in \mathcal I_3$, using equation \eqref{resolvent3}, Lemma \ref{largedeviation} and equation \eqref{priorim}, we obtain that  
%$$ \left| G_{ii}^{-1}G_{i\mu} \right|+ \left| G_{\mu\mu}^{-1}G_{\mu i} \right| \prec n^{-1/2} + \frac{1}{\sqrt n} \left( \sum_{\nu \in \cal I_{2}\cup \cal I_3 }  {\left|G^{(i)}_{\nu\mu}  \right|^2 }  \right)^{1/2} + \frac{1}{\sqrt n} \left( \sum_{j \in \cal I_{1} }  {\left|G^{(\mu)}_{ji}  \right|^2 }  \right)^{1/2} \prec   n^{-1/2}.$$
Thus we obtain that $\Lambda_o\prec n^{-1/2}$, which concludes equation \eqref{Zestimate1}.
\end{proof}

Note that combining $\max_a|G_{aa}|=\OO(1)$ by \eqref{priorim} with equation \eqref{Zestimate1}, we immediately conclude equation \eqref{entry_diagonal} for ${a}\ne {b}$.


%{\cor
%\begin{lemma}
%Fix constants $c_0,C_0>0$. For any $\mathbb T \subseteq \mathcal I$ and $a\in \mathbb R$, the following bounds hold uniformly in $z\in S(c_0,C_0,a)$:
%\begin{equation}\label{m_T}
%\big| {m_1  - m_1^{\left( \mathbb T \right)} } \big| + \big| {m_2  - m_2^{\left( \mathbb T \right)} } \big| \le \frac{{C\left| \mathbb T \right|}}{{N\eta }}, %\ \ i= 1,2, 
%\end{equation}
%%and 
%%\begin{equation}\label{m11_T}
%%\left| {\frac{1}{N}\sum_{i=1}^M \sigma_i \left(G_{ii}^{(\mathbb T)} - G_{ii}\right)} \right| \le \frac{{C\left| \mathbb T \right|}}{{N\eta }}, %\ \ i= 1,2, 
%%\end{equation}
%where $C>0$ is a constant depending only on $\tau$.
%%where $C>0$ depends only on $C_0 \lambda_r$. %is an absolute constant. %depending only on the aspect ratio $d$.
%\end{lemma}
%\begin{proof}
%For $\mu\in\mathcal I_2$, we have
%\begin{align*}
%\left|m_2-m_2^{(\mu)}\right|& =\frac{1}{N}\left|\sum_{\nu\in\mathcal I_2}  \wt  \sigma_\nu\frac{G_{\nu\mu}G_{\mu\nu}}{G_{\mu\mu}}\right| \le \frac{C}{N|G_{\mu\mu}|} \sum_{\nu\in\mathcal I_2} |G_{\nu\mu}|^2 = \frac{C\Im\, G_{\mu\mu}}{N\eta |G_{\mu\mu}|} \le \frac{C}{N\eta}, % \label{rough_boundmi}
%\end{align*}
%where in the first step we used (\ref{resolvent8}), and in the second and third steps we used (\ref{eq_gsq1}). Similarly, using (\ref{resolvent8}) and (\ref{eq_gsq3}) we get
%\begin{align*}
%\left|m_2 -m_2^{(i)}\right| & = \frac{1}{N}\left|\sum_{\nu \in\mathcal I_2}\wt  \sigma_\nu\frac{G_{\nu i}G_{i\nu}}{G_{ii}}\right| \le \frac{C}{N|G_{ii}|} \left( \frac{{G}_{ii}}{z}  + \frac{\bar z}{\eta} \Im\left(\frac{{G_{ii} }}{z}\right)\right)   \le \frac{C}{N\eta}.
%\end{align*}
%Similarly, we can prove the same bounds for the $m_1$ case. Then (\ref{m_T}) can be proved by induction on the indices in $\mathbb T$. %The proof for (\ref{m11_T}) is similar except that one needs to use the assumption (\ref{assm3}).
%\end{proof}
%}

\vspace{5pt}

\noindent{\bf Step 2: Self-consistent equations.} 
%This is the key step of the proof for Lemma \ref{prop_entry}, where 
In this step, we derive the approximate self-consistent equations satisfied by $m_1(z)$ and $m_2(z)$ defined in equation \eqref{defm}. More precisely, we will show that $(m_1(z),m_2(z))$ satisfies equation \eqref{selfomegaerror} for some small errors $|\cal E_{1}|+ |\cal E_{2}|\prec n^{-1/2}$. Then in Step 3, we will apply Lemma \ref{lem_stabw} to show that $(m_1(z),m_2(z))$ is close to $(M_{1}(z),M_{2}(z))$. %---this will discussed .

We define the following $z$-dependent event 
%\be\label{Xiz}\Xi(z):=\left\{\max_{{a}\in \cal I}|G_{{a}{a}}(z)-\Pi_{{a}{a}}(z)| \le (\log n)^{-1/2}\right\}.\ee
\be\label{Xiz}\Xi(z):=\left\{ |m_{1}(z)-M_{1}(z)| + |m_{2}(z)-M_{2}(z)| \le (\log n)^{-1/2}\right\}.\ee
Note that by equation \eqref{Lipomega}, we have 
$$|M_{1}(z)-M_1(0)|=|M_{1}(z)+r_1^{-1}a_1|\lesssim (\log n)^{-1},\quad |M_{2}(z)+M_2(0)|=|M_{2}+r_2^{-1}a_2|\lesssim (\log n)^{-1},$$
for $z\in \mathbf D$. 
%(recall that $a_1=-r_1M_{1}(0)$ and $a_2=-r_2M_{2}(0)$). 
Together with equations \eqref{a23} and \eqref{assm32}, we obtain the following estimates 
\be\label{Gsim1}
 |M_{1}| \sim |M_{2}| \sim 1, \quad |z+\lambda_i^2 r_1M_{1} + r_2 M_{2}|\sim 1,\quad \text{uniformly in $z\in \mathbf D$}  . \ee
 Moreover, using equation \eqref{selfomega} we get
 \be\label{Gsim0}
 \quad \left|1 + \gamma_n M(z)\right| \sim |M_2^{-1}(z)| \sim 1, \quad  |1+\gamma_n M_{0}(z)|\sim |M_2^{-1}(z)| \sim 1  ,
\ee
 uniformly in $z\in \mathbf D$, where we abbreviated
 \be\label{defn mc1c}M(z):=-\frac1p\sum_{i=1}^p\frac{1}{z+\lambda_i^2 r_1M_{1}(z) +r_2M_{2}(z)},\quad M_{0}(z):=-\frac1p\sum_{i=1}^p\frac{\lambda_i^2}{z+\lambda_i^2 r_1M_{1}(z) +r_2M_{2}(z)},\ee
 which are the asymptotic limit of $m(z)$ and $m_0(z)$, respectively. Plugging equation \eqref{Gsim1} into equation \eqref{defn_piw}, we get that
\be
|\Gi_{{a}{a}}(z)| \sim 1 \ \ \text{uniformly in } z\in \mathbf D, \ {a}\in \cal I.
\ee 
%In particular, on $\Xi$ we have
%\be\label{Gsim1}
%\mathbf 1(\Xi)|G_{{a}{a}}(z)| \sim 1 .
%\ee 
Then we prove the following key lemma, which shows that $(m_1(z),m_2(z))$ satisfies equation \eqref{selfomegaerror} with some small errors $\cal E_{1}$ and $\cal E_{2}$.
%that $M_{2}(z)$ is the solution to the equation $z=f(m)$ for $f$ defined in (\ref{deformed_MP2}).

\begin{lemma}\label{lemm_selfcons_weak}
Under the assumptions of Proposition \ref{prop_diagonal}, the following estimates hold uniformly in $z \in \mathbf D$: 
\begin{equation} \label{selfcons_lemm}
{\mathbf 1}(\Xi) \left|\frac{1}{m_{1}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2r_1m_{1} + r_2m_{2}  } \right|\prec n^{-1/2},\ee
and
\begin{equation} \label{selfcons_lemm2}
  {\mathbf 1}(\Xi)\left|\frac{1}{m_{2}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1m_{1} +  r_2m_{2}  }\right|\prec n^{-1/2}.
\ee
\end{lemma}

\begin{proof}
By equations (\ref{resolvent2}), (\ref{Zi}) and (\ref{Zmu}), we obtain that %for  $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
\begin{align}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu\in \mathcal I_1} G^{\left( i \right)}_{\mu\mu}- \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} + \cal Z_i =  - z - \lambda_i^2 r_1m_1 - r_2m_2 + \cal E_i, \quad \text{for }i \in \cal I_0, \label{self_Gii}\\
\frac{1}{{G_{\mu\mu} }}&=  - 1 - \frac{1}{n} \sum_{i\in \mathcal I_0}\lambda_i^2 G^{\left(\mu\right)}_{ii}+ \cal Z_{\mu} =  - 1  - \gamma_n m_0 + \cal E_\mu, \quad \text{for }\mu \in \cal I_1,  \label{self_Gmu1}\\
\frac{1}{{G_{\nu\nu} }}&=  - 1 - \frac{1}{n} \sum_{i\in \mathcal I_0} G^{\left(\nu\right)}_{ii}+\cal Z_{\nu} =   - 1 - \gamma_n m + \cal E_\nu, \quad \text{for }\nu \in \cal I_2, \label{self_Gmu2}
\end{align}
where we denoted (recall equation \eqref{defm} and Definition \ref{defn_Minor}) %, and define %equation \eqref{m123}, and 
$$\cal E_i :=\cal Z_i + \lambda_i^2 r_1\left(m_1 - m_1^{(i)}\right) + r_2\left(m_2-m_2^{(i)}\right) ,$$
and
$$\cal E_\mu :=\cal Z_{\mu} + \gamma_n(m_0-m_0^{(\mu)}),\quad \cal E_\nu:=\cal Z_{\nu} +\gamma_n(m-m^{(\nu)}) .$$
Now using equations (\ref{resolvent8}), \eqref{eqn_randomerror} and (\ref{Zestimate1}), we can bound that  
\begin{equation}\nonumber
  |m_1 - m_1^{(i)}| \le   \frac1{n_1}\sum_{\mu\in \mathcal I_1}  \left|\frac{G_{\mu i} G_{i\mu}}{G_{ii}}\right| \le |\Lambda_o|^2|G_{ii}| \prec n^{-1}.
\end{equation}
where we also used bound \eqref{priorim} in the last step. Similarly, we can get that 
\be \nonumber%\label{higherr}  
 |m_2 - m_2^{(i)}| \prec n^{-1} , \quad |m_0 - m_0^{(\mu)}| \prec n^{-1},\quad  |m-m^{(\nu)}| \prec n^{-1},  \ee
for any $i\in \cal I_0$, $\mu \in \cal I_{1}$ and $\nu\in \cal I_2$. Together with equation (\ref{Zestimate1}), we obtain the bound %for all $i$ and $\mu$,
\begin{equation}\label{erri}
\max_{i\in \cal I_0} |\cal E_i | +\max_{\mu \in \cal I_1\cup \cal I_2} |\cal E_\mu|  \prec n^{-1/2}.
\end{equation}
%with $\xi $-overwhelming probability.
%Similarly, we can also get that
%\begin{equation}\label{self_Gmu}
%\frac{1}{{G_{\mu\mu} }}=  - z - \frac{1}{N} \sum_{i\in \mathcal I_1}\sigma_i G^{\left( \mu\right)}_{ii}+ Z_{\mu} =  - z - \frac{1}{N} \sum_{i\in \mathcal I_1}\sigma_i G_{ii} + \epsilon_\mu,
%\end{equation}
%where 
%$$\epsilon_\mu := Z_{\mu} + \frac{1}{N} \sum_{i\in \mathcal I_1}\sigma_i \left(G^{\left( \mu\right)}_{ii}-G_{ii}\right).$$
%Moreover, we have

 With equation \eqref{Gsim1} and the definition of the event $\Xi$, we get that 
 $$\mathbf 1(\Xi)|z + \lambda_i^2 r_1m_1+r_2m_2|\sim1.$$ 
 Combining it with equations (\ref{self_Gii}) and \eqref{erri}, we obtain that
\be\label{Gmumu0}
\mathbf 1(\Xi)G_{ii}=\mathbf 1(\Xi)\left[-\frac{1}{z + \lambda_i^2 r_1 m_1+r_2 m_2} +\OO_\prec\left(n^{-1/2}\right)\right].
\ee
Plugging \eqref{Gmumu0} into the definitions of $m$ and $m_0$ in equation \eqref{defm}, we get
\begin{align}
\mathbf 1(\Xi)m&=\mathbf 1(\Xi)\left[-\frac1p\sum_{i\in \cal I_0}\frac{1}{z + \lambda_i^2 r_1 m_1+r_2 m_2}  +\OO_\prec\left(n^{-1/2}\right)\right],\label{Gmumu} \\
\mathbf 1(\Xi)m_0&=\mathbf 1(\Xi)\left[-\frac1p\sum_{i\in \cal I_0}\frac{\lambda_i^2}{z + \lambda_i^2 r_1 m_1+r_2 m_2}   +\OO_\prec\left(n^{-1/2}\right)\right]. \label{Gmumu2}
\end{align}
As a byproduct, we obtain from these two estimates and equation \eqref{defn mc1c} that  
\be\label{Gsim11}
\mathbf 1(\Xi)\left(|m(z)-M(z)| +|m_0(z)-M_{0}(z)| \right)\lesssim (\log n)^{-1/2}, \quad \text{with overwhelming probability on } \Xi. 
\ee
Together with equation \eqref{Gsim0}, we get that %from equation \eqref{self_Gmu1} and equation \eqref{self_Gmu2} that
\be\label{Gsim2}
|1+\gamma_nm_0(z)|\sim 1, \quad |1+\gamma_nm(z)|\sim 1, \quad \text{with overwhelming probability on } \Xi.
\ee
Now combining equations \eqref{self_Gmu1}, \eqref{self_Gmu2}, \eqref{erri} and  \eqref{Gsim2}, we obtain that for $\mu \in \cal I_1$ and $\nu \in \cal I_2,$ %can obtain that with overwhelming probability,
\begin{align}\label{Gii0} 
&\mathbf 1(\Xi)\left(G_{\mu\mu}+\frac{1}{1 + \gamma_nm_0}\right) =\OO_\prec\left(n^{-1/2}\right) , \quad \mathbf 1(\Xi)\left(G_{\nu\nu}+\frac{1}{1 + \gamma_nm}\right)= \OO_\prec\left(n^{-1/2}\right) .
%\\ \quad \nu \in \cal I_3.\label{Gii1}
\end{align}
Taking average over $\mu\in \cal I_1$ and $\nu\in \cal I_2$, we get that %with overwhelming probability,
\begin{align}\label{Gii000}
& \mathbf 1(\Xi)\left(m_1+\frac{1}{1 + \gamma_n m_0}\right)  = \OO_\prec\left(n^{-1/2}\right) ,\quad  \mathbf 1(\Xi)\left(m_2+\frac{1}{1 +\gamma_n  m}\right)= \OO_\prec\left(n^{-1/2}\right) ,
\end{align}
which further implies
\be\label{Gii}
 \mathbf 1(\Xi)\left(\frac{1}{m_1} + 1 + \gamma_nm_0\right) \prec  n^{-1/2} ,\quad \mathbf 1(\Xi)\left(\frac{1}{m_2} + 1 + \gamma_nm\right) \prec  n^{-1/2} .
\ee
Finally, plugging equations \eqref{Gmumu} and \eqref{Gmumu2} into equation \eqref{Gii}, we conclude equations (\ref{selfcons_lemm}) and (\ref{selfcons_lemm2}). 
\end{proof}

%The following lemma gives the stability of the equation $ f(z,m)=0$. Roughly speaking, it states that if $f(z, m_{2}(z))$ is small and $m_2(\wt  z)-M_{2}(\wt  z)$ is small for $\Im\, \wt  z \ge \Im\, z$, then $m_{2}(z)-M_{2}(z)$ is small. For an arbitrary $z\in S(c_0,C_0, \e)$, we define the discrete set
%\begin{align*}%\label{eqn_def_L}
%L(w):=\{z\}\cup \{z'\in S(c_0,C_0, \e): \text{Re}\, z' = \text{Re}\, z, \text{Im}\, z'\in [\text{Im}\, z, 1]\cap (N^{-10}\mathbb N)\} .
%\end{align*}
%Thus, if $\text{Im}\, z \ge 1$, then $L(z)=\{z\}$; if $\text{Im}\, z<1$, then $L(z)$ is a 1-dimensional lattice with spacing $N^{-10}$ plus the point $z$. Obviously, we have $|L(z)|\le N^{10}$. %The following lemma is stated as Definition 5.4 of \cite{KY2} %and Lemma 4.5 of \cite{BEKYY}.
%
%\begin{lemma}\label{stability}
%Let $c_0>0$ be a sufficiently small constant and fix $C_0,\epsilon>0$. The self-consistent equation $f(z,m)=0$ is stable on $S(c_0,C_0, \epsilon)$ in the following sense. Suppose the $z$-dependent function $\delta$ satisfies $N^{-2} \le \delta(z) \le (\log N)^{-1}$ for $z\in S(c_0,C_0, \epsilon)$ and that $\delta$ is Lipschitz continuous with Lipschitz constant $\le N^2$. Suppose moreover that for each fixed $E$, the function $\eta \mapsto \delta(E+\ii\eta)$ is non-increasing for $\eta>0$. Suppose that $u_2: S(c_0,C_0,\epsilon)\to \mathbb C$ is the Stieltjes transform of a probability measure. Let $z\in S(c_0,C_0,\epsilon)$ and suppose that for all $z'\in L(z)$ we have 
%\begin{equation}\label{Stability0}
%\left| f(z, u_2)\right| \le \delta(z).
%\end{equation}
%Then we have
%\begin{equation}
%\left|u_2(z)-M_{2}(z)\right|\le \frac{C\delta}{\sqrt{\kappa+\eta+\delta}},\label{Stability1}
%\end{equation}
%for some constant $C>0$ independent of $z$ and $N$, where $\kappa$ is defined in (\ref{KAPPA}). 
%%Similarly, the self-consistent equation $\mathcal D_2$ in (\ref{def_D12}) is also stable on $S(C_1)$.
%\end{lemma}
%\begin{proof}
%This lemma can proved with the same method as in e.g. \cite[Lemma 4.5]{isotropic} and \cite[Appendix A.2]{Anisotropic}. The only input is Lemma \ref{lambdar_sqrt}. 
%\end{proof}

%\vspace{5pt}

\noindent{\bf Step 3: $\Xi$ holds with overwhelming probability.} In this step, we show that the event $\Xi(z)$ in \eqref{Xiz} actually holds with overwhelming probability for all $z\in \mathbf D$. Once we have proved this fact, applying Lemma \ref{lem_stabw} to equations \eqref{selfcons_lemm} and  \eqref{selfcons_lemm2} immediately shows that $(m_1(z),m_2(z))$ is close to $(M_{1}(z),M_{2}(z))$ up to an error of order $\OO_\prec(n^{-1/2})$. 

We claim that it suffices to show that
\be\label{Xiz0}
|m_{1}(0)-M_{1}(0)| + |m_{2}(0)-M_{2}(0)| \prec n^{-1/2}.
\ee
 In fact, notice that by equations \eqref{Lipomega} and \eqref{priordiff} we have
$$ |M_{1}(z)-M_{1}(0)|+|M_2(z)-M_2(0)|=\OO((\log n)^{-1}),\quad  |m_{1}(z)-m_{1}(0)|+ |m_{2}(z)-m_{2}(0)|=\OO((\log n)^{-1})$$ 
with overwhelming probability for all $z\in \mathbf D$. Thus if equation \eqref{Xiz0} holds, we can obtain that 
\be\label{roughh1} 
\sup_{z\in \mathbf D} \left(|m_{1}(z)-M_{1}(z)| + |m_{2}(z)-M_{2}(z)|\right)  \lesssim (\log n)^{-1} \quad \text{with overwhelming probability},\ee
and %Moreover, with this estimate and equation \eqref{Lipomega}, we conclude that 
\be\label{roughh2} \sup_{z\in \mathbf D} \left( |m_{1}(z)-M_{1}(0)|+ |m_{2}(z)-M_{2}(0)|\right) \lesssim (\log n)^{-1} \quad \text{with overwhelming probability}.\ee
The equation \eqref{roughh1} shows that $\Xi$ holds with overwhelming probability, while the equation \eqref{roughh2} verifies the condition \eqref{prior12} of Lemma \ref{lem_stabw}. Now applying Lemma \ref{lem_stabw} to equations \eqref{selfcons_lemm} and \eqref{selfcons_lemm2}, we obtain that
\be\label{Xizz}
|m_{1}(z)-M_{1}(z)|+ |m_{2}(z)-M_2(z)| \prec n^{-1/2} 
\ee
uniformly for all $z\in \mathbf D$. Together with equations \eqref{Gii0} and \eqref{Gii000}, equation \eqref{Xizz} implies that
\be\label{Xizz2} \max_{\mu\in \cal I_1} |G_{\mu\mu}(z)-M_{1}(z)|+ \max_{\nu\in \cal I_2} |G_{\nu\nu}(z)-M_{2}(z)|\prec n^{-1/2}.
\ee
Then plugging estimate \eqref{Xizz} into equation \eqref{Gmumu0} and recalling \eqref{M1M2a1a2}, we obtain that 
$$\max_{{i}\in \cal I_1}|G_{ii}(z)-\Gi_{ii}(z)| \prec n^{-1/2}. 
$$
Together with equation \eqref{Xizz2}, it gives the diagonal estimate
\be\label{diagest}
\max_{{a}\in \cal I}|G_{{a}{a}}(z)-\Pi_{{a}{a}}(z)| \prec n^{-1/2}. 
\ee
Combining equation \eqref{diagest} with the off-diagonal estimate on $\Lambda_o$ in equation \eqref{Zestimate1}, we conclude the proof of Lemma \ref{prop_entry}. 
\end{proof}
Finally, we give the proof of equation \eqref{Xiz0}.
%It remains to show that equation \eqref{Xiz0} holds.% In the following lemma, we pick $0=0$.
%\begin{lemma}
%Under the assumptions of Proposition \ref{prop_diagonal}, the estimate equation \eqref{Xiz0} holds.
%\end{lemma}
\begin{proof}[Proof of equation \eqref{Xiz0}]
By equation \eqref{spectral}, we get
$$ m(0)=\frac1p\sum_{i\in \cal I_0}G_{ii}(0) = \frac1p\sum_{i\in \cal I_0}\sum_{k = 1}^{p} \frac{|\xi_k(i)|^2 }{\lambda_k} \ge \lambda_1^{-1} \gtrsim 1,$$
where we used Fact \ref{lem_minv} in the last step.
Similarly, we can also get that $m_0(0)$ is positive and has size $m_0(0)\sim 1$. Hence we have the estimates
$$1+\gamma_n m(0)\sim 1,\quad 1+\gamma_n m_0(0)\sim 1.$$ 
Combining these estimates with equations \eqref{self_Gmu1}, \eqref{self_Gmu2} and  \eqref{erri}, we obtain that equation \eqref{Gii000} holds at $z=0$ even without the indicator function $\mathbf 1(\Xi)$. Furthermore, we have that with overwhelming probability,
$$  \left|\lambda_i^2 r_1m_1(0)+r_2m_2(0)\right|=\left|\frac{\lambda_i^2 r_1}{ 1+\gamma_n m_0(0)} +\frac{r_2}{1+\gamma_n m(0)}+ \OO_\prec (n^{-1/2})\right| \sim 1 .$$
 Then combining this estimate with equations (\ref{self_Gii}) and \eqref{erri}, we obtain that equations \eqref{Gmumu} and \eqref{Gmumu2} also hold at $z=0$ even without the indicator function $\mathbf 1(\Xi)$. Finally, plugging equations \eqref{Gmumu} and \eqref{Gmumu2} into equation \eqref{Gii}, we conclude that equations \eqref{selfcons_lemm} and  \eqref{selfcons_lemm2}  hold at $z=0$, that is,
\begin{equation} \label{selfcons_lemma222}
\begin{split}
& \left|\frac{1}{m_{1}(0)} + 1 -\frac1n\sum_{i=1}^p \frac{\lambda_i^2}{ \lambda_i^2r_1m_{1}(0) + r_2m_{2}(0)  } \right|\prec n^{-1/2},\\ 
&\left|\frac{1}{m_{2}(0)} + 1 -\frac1n\sum_{i=1}^p \frac{1 }{ \lambda_i^2 r_1m_{1}(0) + r_2 m_{2}(0)  }\right|\prec n^{-1/2}.
\end{split}
\ee

Denoting $y_{1}=-m_{1}(0)$ and $y_{2}=-m_{2}(0)$, by equation \eqref{Gii} we have
$$y_1= \frac{1}{1+\gamma_n m_{0}(0)} +\OO_\prec(n^{-1/2}),\quad y_2= \frac{1}{1+\gamma_n m(0)}+\OO_\prec(n^{-1/2}).$$ 
Hence there exists a constant $c>0$ such that 
\be\label{omega12} c \le y_1 \le 1, \quad  c\le y_2\le 1, \quad \text{with overwhelming probability}.\ee
%This shows that we can choose a sufficiently small constant $c_1$ such that 
%\be \nonumber %\label{omega12}
%c \le  \omega_2 \le 1 - \gamma_n - c,\quad c \le  \omega_3 \le 1 - \gamma_n - c , \quad \text{with overwhelming probability},
%\ee
%where we also used $1-\gamma_n - r_{1,2}\gtrsim 1$ by equation \eqref{assm2}. 
Also one can verify from equation \eqref{selfcons_lemma222} that $(r_1y_1,r_2y_2)$ satisfies approximately the same system of equations as equation \eqref{eq_a12extra}:
\be\label{selfcons_lemm000}
r_1y_1+r_2 y_2 = 1-\gamma_n + \OO_\prec (n^{-1/2}),\quad r_1^{-1}f(r_1y_1)=1 + \OO_\prec (n^{-1/2}),
\ee
where recall that the function $f$ was defined in equation \eqref{fa1}. The first equation of \eqref{selfcons_lemm000} and equation \eqref{omega12} together imply that $y_1 \in [0,r_1^{-1}(1-\gamma_n)]$ with overwhelming probability. For the second equation of \eqref{selfcons_lemm000}, we know that $y_1=r_1^{-1}a_1$ is a solution. Moreover, it is easy to check that the function $g(y_1):= r_1^{-1}f(r_1y_1)$ is strictly increasing and has bounded derivative on $[0,r_1^{-1}(1-\gamma_n)]$. So by basic calculus, %the second equation in equation \eqref{selfcons_lemm000} gives 
we obtain that 
$$|m_1(0)-M_1(0)|=|y_1-r_1^{-1}a_1|\prec n^{-1/2}.$$ 
Plugging it into the first equation of equation \eqref{selfcons_lemm000}, we get 
$$|m_2(0)-M_2(0)|=|y_2-r_2^{-1}a_2|\prec n^{-1/2}.$$ The above two estimates conclude equation \eqref{Xiz0}.
\end{proof}
%\begin{lemma}[Weak entrywise local law]\label{alem_weak} 
%Let $c_0>0$ be a sufficiently small constant and fix $C_0,\epsilon>0$. Then we have %there exists $C>0$ such that with $\xi$-overwhelming probability,
%\begin{equation} \label{localweakm}
%\Lambda(z) \prec (N\eta)^{-1/4},
%\end{equation}
%uniformly in $z \in S(c_0,C_0,\epsilon)$.
%\end{lemma}
%\begin{proof}
%One can prove this lemma using a continuity argument as in e.g. \cite[Section 4.1]{isotropic}, \cite[Section 5.3]{Semicircle} or \cite[Section 3.6]{EKYY1}. The key inputs are Lemmas \ref{Z_lemma}-\ref{stability}, and the estimates (\ref{average_L})-(\ref{diag_L}) in the $\eta \ge 1$ case. All the other parts of the proof are essentially the same. 
%\end{proof}

%This lemma concludes equation \eqref{Xiz0}, and as explained above, concludes the proof of Lemma \ref{prop_entry}. 
%It remains to show that equation \eqref{aver_in} holds.
%
%\vspace{10pt}
%
%\noindent{\bf Step 4: Fluctuation averaging.} By equation \eqref{Gmumu} and using $\Pi_{ii}=-(z + \lambda_i^2 M_{2}+M_{2})^{-1}$, to show equation \eqref{aver_in} it suffices to prove that
%\be \label{avergoal} 
%|m_{2}-M_{2}|+|m_{3}-M_{2}|+|[Z]| \prec n^{-1}.
%\ee
%For this purpose, we need stronger bounds on $[Z]_1$ and $[Z]_2$ in (\ref{selfcons_improved}). They follow from the following {\it{fluctuation averaging lemma}}. 
%
%\begin{lemma}[Fluctuation averaging] \label{abstractdecoupling}
%%Suppose $\Phi$ and $\Phi_o$ are positive, $N$-dependent deterministic functions on $S(c_0,C_0,\epsilon)$ satisfying $N^{-1/2} \le \Phi, \Phi_o \le N^{-c}$ for some constant $c>0$. Suppose moreover that $\Lambda \prec \Phi$ and $\Lambda_o \prec \Phi_o$. Then for all $z \in S(c_0,C_0,\epsilon)$ we have
%Under the assumptions in Proposition \ref{prop_diagonal}, we have for all $z\in \mathbf D$.
%\begin{equation}\label{flucaver_ZZ}
%|[Z]|+|[Z^{(1)}]|+|[Z^{(2)}]|+|[Z_3]| \prec n^{-1}.
%\end{equation}
%%Fix a constant $\xi>0$. Suppose $q\le \varphi^{-5\xi}$ and that there exists $\wt  S\subseteq S(c_0,C_0,L)$ with $L\ge 18\xi$ such that with $\xi$-overwhelming probability,
%%\begin{equation} 
%%\Lambda(z) \le \gamma(z) \text{ for } z\in \wt  S,
%%\end{equation}
%%where $\gamma$ is a deterministic function satisfying $\gamma(z)\le \varphi^{-\xi}$. Then we have that with $(\xi-\tau_N)$-overwhelming probability,
%%\begin{equation}
%%\left|[Z]_1(z)\right|+ \left|[Z]_2(z)\right| \le \varphi^{18\xi} \left(q^2 + \frac{1}{(N\eta)^2} + \frac{\Im \, M_{2}(z) + \gamma(z)}{N\eta} \right),
%%\end{equation}
%%for $z\in \wt  S$, where $\tau_N:=2/\log \log N$. 
%\end{lemma}
%\begin{proof}
%The proof is the same as the one for Lemma 5.6 of \cite{Anisotropic}.
%\end{proof}
%Now we give the proof of Proposition \ref{prop_entry}.
%
%%Fix $c_0,C_0>0$, $\xi> 3$ and set 
%%$$L:=120\xi, \ \ \wt  \xi:= 2/\log 2 + \xi.$$
%%Hence we have $\wt  \xi \le 2\xi$ and $L\ge 60\wt  \xi$. Then to prove (\ref{DIAGONAL}), it suffices to prove 
%%\begin{equation}\label{goal_law1}
%%\bigcap_{z \in S(c_0,C_0,L)} \left\{ \Lambda(z) \leq C\varphi^{20\wt  \xi}\left(q+ \sqrt{\frac{\operatorname{Im} M_{2}(z) }{N \eta}}+ \frac{1}{N\eta}\right) \right\},
%%\end{equation}
%%with $\xi$-overwhelming probability. %For notational convenience, we shall denote $m_c:=M_{1}+M_{2}$. 
%
%By Lemma \ref{alem_weak}, the event $\Xi$ holds with overwhelming probability. Then by Lemma \ref{alem_weak} and Lemma \ref{Z_lemma}, we can take
%\be\label{initial_phio}
%\Phi_o = \sqrt{\frac{\im M_{2} + (N\eta)^{-1/4}}{N\eta}} + \frac{1}{N\eta},\quad \Phi= \frac{1}{(N\eta)^{1/4}},
%\ee
% in Lemma \ref{abstractdecoupling}. 
%%have that
%%$\Lambda\prec |w|^{-3/8}(N\eta)^{-1/4}$. Therefore  $\theta\prec |w|^{-3/8}(N\eta)^{-1/4}$ and
%%$$\Lambda_o\prec\Psi_\theta\prec\sqrt{\frac{\Im(M_{1}+M_{2})+|w|^{-3/8}(N\eta)^{-1/4}}{N\eta}}, $$
%%where we use $|w|^{-1/2}(N\eta)^{-3/8}\ge (N\eta)^{-1}$ by the definition (\ref{eq_domainD}) of $\bD$.
%%Lemma \ref{fluc_aver} then gives 
%%\[\Phi_o = |w|^{1/2}\sqrt{\frac{\Im(M_{1}+M_{2})+|w|^{-3/8}(N\eta)^{-1/4}}{N\eta}},\ \ \ \Phi=\left(\frac{|w|^{1/2}}{N\eta}\right)^{1/4}\]
%%$\|[Z]\| + \|\langle Z \rangle\|\prec |w|^{-1/2}\Phi_o^2.$ 
%Then (\ref{selfcons_improved}) gives
%$$|f(z,m_2)| \prec\frac{ \im M_{2} + (N\eta)^{-1/4}}{N\eta}.$$
%Using Lemma \ref{stability}, we get
%\be\label{m2}
%|m_2-M_{2}|\prec\frac{\im M_{2}}{N\eta\sqrt{\kappa+\eta}}+\frac{1}{(N\eta)^{5/8}} \prec \frac{1}{(N\eta)^{5/8}} ,
%\ee
%where we used $\im M_{2}=\OO(\sqrt{\kappa+\eta})$ by equation \eqref{Immc} in the second step. With (\ref{selfcons_improved2}) and equation \eqref{m2}, we get the same bound for $m_1$, which gives
%\be\label{m1}
%\theta \prec {(N\eta)^{-5/8}} ,
%\ee
%Then using Lemma \ref{Z_lemma} and (\ref{m1}), we obtain that
%\begin{align}\label{1iteration}
%\Lambda_o \prec  \sqrt{\frac{\im M_{2} + (N\eta)^{-5/8}}{N\eta}} + \frac{1}{N\eta}
%\end{align}
%uniformly in $z\in S(c_0,C_0,\epsilon)$, which is a better bound than the one in (\ref{initial_phio}). Taking the RHS of equation \eqref{1iteration} as the new $\Phi_o$, we can obtain an even better bound for $\Lambda_o$. Iterating the above arguments, we get the bound
%$$\theta \prec \left({N\eta}\right)^{-\sum_{k=1}^l 2^{-k} - 2^{-l-2} }$$
%after $l$ iterations. This implies %the averaged local law
%\be\label{aver_proof}
%\theta\prec(N\eta)^{-1}
%\ee
%since $l$ can be arbitrarily large. Now with equation \eqref{aver_proof}, Lemma \ref{Z_lemma}, equation \eqref{Gii0} and equation \eqref{Gmumu0}, we can obtain equation \eqref{entry_diagonal}. 
%%that
%%$$\Lambda(z)\prec \Psi(z),$$
%%which proves Proposition \ref{prop_entry}.
%
%\end{proof}
%
%
%\subsection{Proof of Proposition \ref{prop_diagonal}}
%
%Plugging equation \eqref{flucaver_ZZ} into equation \eqref{selfcons_improved}, we get 
%\begin{equation} \nonumber
%\begin{split}
%&  \left|\frac{r_1}{m_{2}} + 1 -\frac1n\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2m_{2} + m_{3}  } \right|\prec   n^{-1},\quad \left|\frac{r_2}{m_{3}} + 1 -\frac1n\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 m_{2} +  m_{3}  }\right|\prec  n^{-1} .
%  %\\ {\mathbf 1}(\Xi)\left|f(z, m_2)\right| \prec  {\mathbf 1}(\Xi)\left(\left|[Z]_1\right| + \left|[Z]_2\right|\right) + \Psi^2_\theta, 
%\end{split}
%\end{equation}
%Then using Lemma \ref{lem_stabw}, we get $|m_{2}-M_{2}|+|m_{3}-M_{2}|\prec n^{-1}$, which concludes equation \eqref{avergoal}. This concludes the proof of equation \eqref{aver_in}, and hence Lemma \ref{prop_entry}.







\iffalse
%In this section, we will work under the assumptions of Lemma \ref{lem_cov_shift}. However,
In random matrix theory, it is much more convenient to consider matrices $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ with an extra $n^{-1/2}$ factor, where we denote $n:=n_1+n_2$.
%rescale the matrices $Z^{(1)}$ and $Z^{(2)}$ such that their entries have variance $n^{-1}$, where $n:=n_1+n_2$. \HZ{I don't think is the point; I think you want to say that the sample covariance of both tasks $((Z^{(1)})^{\top}Z^{(1)} + (Z^{(2)})^{\top}Z^{(2)})/n$ is what matters}
The advantage of this scaling is that the singular eigenvalues of $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ all lie in a bounded support that does not grow with $n$. For reader's convenience, we first state the exact properties (some of which have already been stated in words in Section \ref{sec_prelim}) that we shall need for the results and proofs in this section. %Moreover, we will also introduce a couple of other notations.
%\HZ{Remove A.1 and A.2 below, which are already in Sec 2.}


%\textbf{Basic setting.}
%We denote the two sample covariance matrices by $\mathcal Q_1:=(X^{(1)})^\top X^{(1)}$ and $\cal Q_2:= (X^{(2)})^\top X^{(2)}$.
We have assumed that $Z^{(1)}$ and $Z^{(2)}$ are $n_1\times p$ and $n_2\times p$ random matrices with i.i.d. entries satisfying
\begin{equation}\label{assm1}
\mathbb{E} (Z^{(1)})_{11} =\mathbb{E} (Z^{(2)})_{11} =0, \ \quad \ \mathbb{E} \vert (Z^{(1)})_{11} \vert^2=\mathbb{E} \vert (Z^{(2)})_{11} \vert^2  =1.
\end{equation}
%\HZ{What is the subscript $11$ above?}
%where we denote $n:=n_1+n_2$. Here we have chosen the scaling that is more standard in the random matrix theory literature---under this $n^{-1/2}$ scaling, the eigenvalues of $\cal Q_1$ and $\cal Q_2$ are all of order 1.
%Moreover, we assume that the fourth moments exist:
%\be \label{conditionA2}
%\mathbb{E} \vert (Z^{(1)})_{11} \vert^4 +\mathbb{E} \vert (Z^{(2)})_{11} \vert^4  \le C,
%\ee
%for some constant $C>0$.
%In this paper, we regard $N$ as the fundamental (large) parameter and $n \equiv n(N)$ as depending on $N$.
Recall that we have defined $\rho_1= n_1/p$ and $\rho_2=n_2/p$ in introduction. We assume that they satisfy
%\HZ{We have defined $\rho_1,\rho_2$ already in sec 2. Consider using ``Recall that $\rho_1 =, \rho_2 =$.''}
\be\label{assm2}
0\le \rho_1 \le \tau^{-1}, \quad 1+\tau \le \rho_{2} \le \tau^{-1},
\ee
%\HZ{Remove $k$ above.}
for a small constant $0<\tau<1$.
\fi


 \iffalse
We assume that $\Sig_1$ and $\Sig_2$ have eigendecompositions
%\HZ{Using $O_1, O_2$ in eigendecomposition seems non-standard; suggestion: change to $Q_1, Q_2$.}
\be\label{eigen}\Sig_1= Q_1\Lambda_1 Q_1^\top, \ \ \Sig_2= Q_2\Lambda_2 Q_2^\top,\ \ \Lambda_1=\text{diag}(\lambda_1^{(1)}, \ldots, \lambda^{(1)}_p), \ \ \Lambda_2=\text{diag}( \lambda^{(2)}_1, \ldots, \lambda^{(2)}_p),
\ee
where the eigenvalues satisfy that
\begin{equation}\label{assm3}
\tau \le \lambda^{(1)}_p \le \ldots \le \lambda^{(1)}_2  \le \lambda^{(1)}_1 \le \tau^{-1}, \quad \tau \le   \lambda^{(2)}_p \le \ldots \le \lambda^{(2)}_2 \le \lambda^{(2)}_1 \le \tau^{-1}.
\ee
%such that $\Sigma_1$ and $\Sigma_2$ are both well-conditioned.


We assume that $M=\Sig_1^{1/2} \Sig_2^{-1/2}$ has a singular value decomposition \eqref{eigen2} .....
\be%\label{eigen2}
M= U\Lambda V^\top, \quad \Lambda=\text{diag}( \lambda_1, \ldots, \lambda_p),
\ee
where by equation \eqref{assm3} we have % the singular values satisfy that
\begin{equation}\label{assm32}
\tau \le \lambda_p \le \lambda_1 \le \tau^{-1} .%, \quad \max\left\{\pi_A^{(n)}([0,\tau]), \pi_B^{(n)}([0,\tau])\right\} \le 1 - \tau .
\end{equation}
\fi


%\begin{remark}
%Note that the Gaussian distribution satisfies the condition (\ref{eq_support}) with $q< N^{-\phi}$ for any $\phi<1/2$. We also remark that if (\ref{eq_support}) holds, then the event $\left\{\vert x_{ij}\vert \le q, \forall 1\le i \le M,1\le j \le N\right\}$ holds with $\xi$-overwhelming probability for any fixed $\xi>0$ according to Definition \ref{high_prob}. For this reason, the bad event $\left\{\vert x_{ij}\vert > q \text{ for some }i,j\right\}$ is negligible, and we will not consider the case it happens throughout the proof.
%\end{remark}

%Since $(Z^{(1)})^{\top} Z^{(1)}$ (resp.\;$(Z^{(2)})^\top Z^{(2)}$) is a standard sample covariance matrix, and it is well-known that its nonzero eigenvalues are all inside the support of the Marchenko-Pastur law $[(1-\sqrt{d_1})^2 , (1+\sqrt{d_1})^2]$ (resp. $[(1-\sqrt{d_2})^2 , (1+\sqrt{d_2})^2]$) with probability $1-\oo(1)$ \cite{No_outside}. In our proof, we shall need a slightly stronger probability bound, which is given by the following lemma.

\iffalse
Before entering into the formal proof, we state two preparation lemmas, both of which have been used crucially in some previous proofs. First, for $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ with bounded support $q$, we have the following estimates on their singular values.


%Then we state the following lemma on the eigenvalues of $(Z^{(1)})^{\top} Z^{(1)}$ and $(Z^{(2)})^\top Z^{(2)}$, which are denoted as $\lambda_1 ((Z^{(1)})^{\top} Z^{(1)}) \ge \cdots \ge \lambda_{p} ((Z^{(1)})^{\top} Z^{(1)})$ and $\lambda_1 ((Z^{(2)})^\top Z^{(2)}) \ge \cdots \ge \lambda_p ((Z^{(2)})^\top Z^{(2)})$. We have used it in our previous proofs; see equation \eqref{eq_isometric}.


\begin{lemma}\label{SxxSyy}
Suppose Assumption \ref{assm_big1} holds. % and $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ satisfy the bounded support condition \eqref{eq_support} for some deterministic parameter $q$ satisfying $ n^{-{1}/{2}} \leq q \leq n^{- \phi} $ for a small constant $\phi>0$.
Then for any constant $\e>0$, we have that with overwhelming probability,
\be\label{op rough1} %(1-\sqrt{d_1})^2 - \e \le  \lambda_p((Z^{(1)})^{\top} Z^{(1)})   \le
\lambda_1\left((Z^{(1)})^{\top} Z^{(1)}\right) \le {(\sqrt{n_1}+\sqrt{p})^2} + n_1^{1+\e} q,
\ee
and
\be\label{op rough2}
 (\sqrt{n_2}-\sqrt{p})^2  -  n_2^{1+\e} q \le  \lambda_p \left((Z^{(2)})^\top Z^{(2)}\right)  \le  \lambda_1\left((Z^{(2)})^\top Z^{(2)}\right) \le  (\sqrt{n_2}+\sqrt{p})^2 +  n_2^{1+\e} q .
\ee
where $\lambda_i(Z_k^\top Z_k)$, $k=1,2$ and $i=1,\cdots,p$, is the $i$-th largest eigenvalue of $Z_k^\top Z_k$.
\end{lemma}
\begin{proof}
%\HZ{This proof does not parse.}
%This lemma essentially follows from \cite[Theorem 2.10]{isotropic}, although the authors considered the case with $q = n^{-1/2}$ only. The results for the general case with $ n^{-{1}/{2}} \leq q \leq n^{- \phi} $ follows from \cite[Lemma 3.12]{DY}.
%, but only the bounds for largest eigenvalues are given there in order to avoid the issue with the smallest eigenvalue when $d_{2}$ is close to 1. However, under the assumption equation \eqref{assm2}, the lower bound for the smallest eigenvalue follows from the same arguments as in \cite{DY}. Hence we omit the details.
This lemma is a corollary of  \cite[Theorem 2.10]{isotropic} and \cite[Lemma 3.12]{DY}.
\end{proof}
\begin{remark}
it is well-known that the eigenvalues of $n_2^{-1}(Z^{(2)})^\top Z^{(2)}$ are all inside the support of the Marchenko-Pastur law $[(1-\sqrt{p/n_2})^2-\oo(1) ,(1+\sqrt{p/n_2})^2+\oo(1)]$ with probability $1-\oo(1)$ \cite{No_outside}. Here the estimate \eqref{op rough2} has improved the error to $n_2^\e q$ and the probability to $1-\OO(n^{-D})$.
\end{remark}
\fi




\iffalse
\subsection{Resolvents and Local Laws}\label{sec locallaw1}


Our main goal is to study the matrix inverse $((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}$ for $X^{(1)}= Z^{(1)}\Sigma_1^{1/2}$ and $X^{(2)}= Z^{(2)}\Sigma_2^{1/2}$.
%\begin{align*}
%(\cal Q_1+\cal Q_2)^{-1}=\left( \Sigma_1^{1/2}(Z^{(1)})^{\top} Z^{(1)}\Sigma_1^{1/2}+\Sigma_2^{1/2}(Z^{(2)})^\top Z^{(2)}\Sigma_2^{1/2}\right)^{-1} .
%\end{align*}
Using equation \eqref{eigen2}, we can rewrite it as
\be\label{eigen2extra}((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}= \Sigma_2^{-1/2}V\left(   \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^{-1}V^\top\Sigma_2^{-1/2}.\ee
For our purpose, we introduce a convenient self-adjoint linearization trick,
%This idea dates back at least to Girko, see e.g., the works \cite{girko1975random,girko1985spectral} and references therein.
which has been proved to be useful in studying the local laws of random matrices of the Gram type \cite{Anisotropic, AEK_Gram, XYY_circular}. We define the following $(p+n)\times (p+n)$ self-adjoint block matrix, which is a linear function of $Z^{(1)}$ and $Z^{(2)}$:
%\begin{definition}[Linearizing block matrix]\label{def_linearHG}%definiton of the Green function
%We define the $(n+N)\times (n+N)$ block matrix
 \begin{equation}\label{linearize_block}
   H \equiv H(Z^{(1)},Z^{(2)}): = n^{-1/2}\left( {\begin{array}{*{20}c}
   { 0 } & \Lambda U^{\top}(Z^{(1)})^{\top} & V^\top (Z^{(2)})^\top  \\
   {Z^{(1)} U\Lambda  } & {0} & 0 \\
   {Z^{(2)}V} & 0 & 0
   \end{array}} \right).
 \end{equation}
For simplicity of notations, we define the index sets
$$\cal I_0:=\llbracket 1,p\rrbracket, \quad  \cal I_1:=\llbracket p+1,p+n_1\rrbracket, \quad \cal I_2:=\llbracket p+n_1+1,p+n_1+n_2\rrbracket ,\quad \cal I:=\cal I_0\cup \cal I_1\cup \cal I_2  .$$
 We will consistently use the latin letters $i,j\in\sI_{0}$, greek letters $\mu,\nu\in\sI_{1}\cup \sI_{2}$, and ${a},{b},\mathfrak c\in \cal I$. Correspondingly, the indices of the matrices $Z^{(1)}$ and $Z^{(2)}$ are labelled as
 \be\label{labelZ}
 Z^{(1)}= (z_{\mu i}:i\in \mathcal I_0, \mu \in \mathcal I_1), \quad Z^{(2)}= (z_{\nu i}:i\in \mathcal I_0, \nu \in \mathcal I_2).\ee
Now we define the resolvents as follows.
\begin{definition}[Resolvents]
We define the resolvent (or Green's function) of $H$ as
 \begin{equation}\label{eqn_defG}
 G \equiv G (Z^{(1)},Z^{(2)},z):= \left[H(Z^{(1)},Z^{(2)})-\left( {\begin{array}{*{20}c}
   { z\id_{p}} & 0 & 0 \\
   0 & { \id_{n_1}}  & 0\\
      0 & 0  & { \id_{n_2}}\\
\end{array}} \right)\right]^{-1} , \quad z\in \mathbb C .
 \end{equation}
%It is easy to verify that the eigenvalues $\lambda_1(H)\ge \ldots \ge \lambda_{n+N}(H)$ of $H$ are related to the ones of $\mathcal Q_1$ through
%\begin{equation}\label{Heigen}
%\lambda_i(H)=-\lambda_{n+N-i+1}(H)=\sqrt{\lambda_i\left(\mathcal Q_2\right)}, \ \ 1\le i \le n\wedge N, \quad \text{and}\quad \lambda_i(H)=0, \ \ n\wedge N + 1 \le i \le n\vee N.
%\end{equation}
%and
%$$\lambda_i(H)=0, \ \ n\wedge N + 1 \le i \le n\vee N.$$
%where we used the notations $n\wedge N:=\min\{N,M\}$ and $n\vee N:=\max\{N,M\}$.
%\begin{definition}[Index sets]\label{def_index}
and the resolvent of $  n^{-1}\Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + n^{-1}V^\top (Z^{(2)})^\top Z^{(2)}V$ as
\be\label{mainG}
\cal G(z):=\left( n^{-1}  \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + n^{-1}V^\top (Z^{(2)})^\top Z^{(2)}V -z\right)^{-1},\quad z\in \C.
\ee
Moreover, we also define the following averaged resolvents, which are the (weighted) partial traces of $G$:
\be\label{defm}
\begin{split}
m(z) :=\frac1p\sum_{i=1}^p G_{ii}(z) ,\quad & m_0(z):=\frac1p\sum_{i=1}^p \lambda_i^2 G_{ii}(z),\\
 m_1(z):= \frac{1}{n_1}\sum_{\mu =p+1}^{p+n_1}G_{\mu\mu}(z) ,\quad & m_2(z):= \frac{1}{n_2}\sum_{\nu=p+n_1+1}^{p+n_1+n_2}G_{\nu\nu}(z) .
\end{split}
\ee
\end{definition}

Then we define the resolvent minors, which are defined by removing certain rows and columns of the matrix $H$.

\begin{definition}[Resolvent minors]\label{defn_Minor}
 For any $ (p+n)\times (p+n)$ matrix $\cal A$ and ${a} \in \mathcal I$, we define the minor $\cal A^{(\mathfrak c)}:=(\cal A_{{a}{b}}:{a},{b} \in \mathcal I\setminus \{\mathfrak a\})$ as the $ (p+n-1)\times (p+n-1)$ matrix obtained by removing the $\mathfrak c$-th row and column in $\cal A$. Note that we keep the names of indices when defining $\cal A^{(\mathfrak c)}$, i.e. $(\cal A^{(\mathfrak c)})_{{a}{b}}= \cal A_{{a}{b}}$ for ${a},{b} \ne \mathfrak c$. Correspondingly, we define the resolvent minor as %(recall equation \eqref{green2})
\begin{align*}
G^{(\mathfrak c)}:&=\left[\left(H - \left( {\begin{array}{*{20}c}
   { zI_{p}} & 0  \\
   0 & { I_{n}}  \\
\end{array}} \right)\right)^{(\mathfrak c)}\right]^{-1} ,
%= \left( {\begin{array}{*{20}c}
%   { \mathcal G^{(\mathbb T)}} & \mathcal G^{(\mathbb T)} W^{(\mathbb T)}  \\
%   {\left(W^{(\mathbb T)}\right)^\top\mathcal G^{(\mathbb T)}} & { \mathcal G_R^{(\mathbb T)} }  \\
%\end{array}} \right)  ,
\end{align*}
and define the partial traces $m^{(\mathfrak c)}$ and $m^{(\mathfrak c)}_k$, $k=0,1,2,$ by replacing $G$ with $G^{(\mathfrak c)}$ in equation \eqref{defm}. For convenience, we will adopt the convention that for the resolvent minor $G^{(\mathfrak c)}$ defined as above, $G^{(\mathfrak c)}_{{a}{b}} = 0$ if ${a} =\mathfrak c$ or ${b} =\mathfrak c$.
\end{definition}

Note that the resolvent minor $G^{(\mathfrak c)}$ is defined such that it is independent of the entries in the $\mathfrak c$-th row and column of $H$. One will see a crucial use of this fact in the heuristic proof below.

Notice that using equation \eqref{mainG}, we can write  equation \eqref{eigen2extra} as
\be\label{rewrite X as R} ((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}=n^{-1}\Sigma_2^{-1/2}V\cal G(0)V^\top\Sigma_2^{-1/2}.\ee
In the above definition, we have taken the argument of $\cal G$ to be a general complex number, because we will need to use $\cal G'(0)$ in the proof of Lemma \ref{lem_cov_derivative}, which requires a good estimate of $\cal G(z)$ for $z$ around the origin. Moreover, by Schur complement formula, we can obtain that
 \begin{equation} \label{green2}
 G (z):=  \left( {\begin{array}{*{20}c}
   { \cal G (z)} & \cal G(z)W  \\
   W^\top \cal G(z) & z \cal G_R
\end{array}} \right)^{-1},\quad \cal G_R:=(W^\top W - z)^{-1} ,
 \end{equation}
 where we abbreviated $W:=n^{-1/2}(\Lambda U^\top (Z^{(1)})^{\top}, V^\top (Z^{(2)})^\top)$, and
%one can find that %(recall equation \eqref{mainG})
%\be \label{green2}
%\begin{split}
%& \cal G_{L}=\left(WW^\top -z \right)^{-1} =\cal G ,\quad \cal G_{LR}=\cal G_{RL}^\top  = \cal G W , \quad \cal G_R= z\left(W^\top W - z\right)^{-1}.
%\end{split}
%\ee
the subindex of $\cal G_R$ means the lower-right block. This shows that a control of $G(z)$ yields directly a control of $\mathcal G(z)$. On the other hand, $G(z)$ is a little more easier to use than $\cal G(z)$ in the proof.
%Hence we choose to work with the


%Moreover, we denote $\overline i:= i+p$ for $i\in \cal I_1$, $\overline j:= j-p$ for $j\in \cal I_2$, $\overline \mu : = \mu +n $ for $\mu \in \cal I_3$, and $\overline \nu : = \nu - n $ for $\nu \in \cal I_4$.
%\end{definition}
%\begin{definition}[Resolvents]\label{resol_not}
%For $z = E+ \ii \eta \in \mathbb C_+,$ we define the resolvents $G(z)$.
%Then we denote the $\cal I_\al \times \cal I_\beta$ block of $ G(z)$ by $ \cal G_{\al\beta}(z)$ for $\al,\beta=1,2,3$. Moreover,
%Then we denote the $\cal I_1 \times \cal I_1$ block of $ G(z)$ by $ \cal G_{L}(z)$, the $\cal I_1 \times (\cal I_2 \cup \cal I_3)$ block by $\cal G_{LR}$, the $ (\cal I_2 \cup \cal I_3)\times \cal I_1$ block by $\cal G_{RL}$, and the $ (\cal I_2 \cup \cal I_3)\times (\cal I_2 \cup \cal I_3)$ block by $\cal G_R$.
%We denote the $(\cal I_1\cup \cal I_2)\times (\cal I_1\cup \cal I_2)$ block of $ G(z)$ by $ \cal G_L(z)$, the $(\cal I_1\cup \cal I_2)\times (\cal I_3\cup \cal I_4)$ block by $ \cal G_{LR}(z)$, the $(\cal I_3\cup \cal I_4)\times (\cal I_1\cup \cal I_2)$ block  by $ \cal G_{RL}(z)$, and the $(\cal I_3\cup \cal I_4)\times (\cal I_3\cup \cal I_4)$ block by $ \cal G_R(z)$.
%Recalling the notations in equation \eqref{def Sxy}, we define $\cal H:=S_{xx}^{-1/2}S_{xy}S_{yy}^{-1/2}$ and
%\be\label{Rxy}
%\begin{split}
% R_1(z):=(\cal C_{XY}-z)^{-1}&=(\cal H\cal H^T-z)^{-1}, \\
% R_2(z):=(\cal C_{YX}-z)^{-1}&=(\cal H^T\cal H-z)^{-1},  \quad m(z):= q^{-1}\tr  R_2(z).
% \end{split}
%\ee
%Note that we have $R_1\cal H = \cal HR_2$, $\cal H^T R_1 = R_2 \cal H^T $, and
%\be\label{R12} \tr  R_1 = \tr  R_2 - \frac{p-q}{z}= q  m(z) - \frac{p-q}{z},\ee
%since $\cal C_{XY}$ has $(p-q)$ more zeros eigenvalues than $\cal C_{YX}$.
%Finally, we can define ${\cal G}^b_L(z)$, ${\cal G}^b_R(z)$, $ m^b_\al(z)$, $\cal H^b$, $R_{1,2}^b$, etc.\;in the obvious way by replacing $Y$ with $\cal Y$.
%for $\wt {\mathcal Q}_{1,2}$ as
%\begin{equation}\label{def_green}
%\mathcal G_1(X,z):=\left(\wt{\mathcal Q}_1(X) -z\right)^{-1} , \ \ \ \mathcal G_2 (X,z):=\left(\wt{\mathcal Q}_2(X)-z\right)^{-1} .
%\end{equation}
% We denote the ESD $\rho^{(n)}$ of $\wt {\mathcal Q}_{1}$ and its Stieltjes transform as
%\be\label{defn_m}
%\rho\equiv \rho^{(n)} := \frac{1}{n} \sum_{i=1}^n \delta_{\lambda_i(\wt{\mathcal Q}_1)},\quad m(z)\equiv m^{(n)}(z):=\int \frac{1}{x-z}\rho_{1}^{(n)}(dx)=\frac{1}{n} \mathrm{Tr} \, \mathcal G_1(z).
%\ee
%We also introduce the following quantities:
%$$m_1(z)\equiv m_1^{(n)}(z):= \frac{1}{N}\sum_{i=1}^n\sigma_i (\mathcal G_1)_{ii}(z) ,\quad m_2(z)\equiv m_2^{(n)}(x):=\frac{1}{N}\sum_{\mu=1}^N \wt\sigma_\mu (\mathcal G_2)_{\mu\mu}(z). $$
%\end{definition}

%Now using Schur complement formula, we can verify that the (recall equation \eqref{def_green})
%\begin{align}
%G = \left( {\begin{array}{*{20}c}
%   { z\mathcal G_1} & \mathcal G_1 \Sig^{1/2} U^{*}X V\wt  \Sig^{1/2}  \\
%   {\wt \Sig^{1/2}V^\topX^\top U\Sig^{1/2} \mathcal G_1} & { \mathcal G_2 }  \\
%\end{array}} \right) = \left( {\begin{array}{*{20}c}
%   { z\mathcal G_1} & \Sig^{1/2} U^{*}X V\wt  \Sig^{1/2} \mathcal G_2   \\
%   {\mathcal G_2}\wt \Sig^{1/2}V^\topX^\top U\Sig^{1/2} & { \mathcal G_2 }  \\
%\end{array}} \right). \label{green2}
%\end{align}
%where $\mathcal G_{1,2}$ are defined in (\ref{def_green}).


\subsubsection{Asymptotic Limit of the Resolvent}\label{sec aymp_limit_G}



%We denote the eigenvalues of $\mathcal Q_1$ and $\mathcal Q_2$ in descending order by $\lambda_1(\mathcal Q_1)\geq \ldots \geq \lambda_{p}(\mathcal Q_1)$ and $\lambda_1(\mathcal Q_2) \geq \ldots \geq \lambda_p(\mathcal Q_2)$. Since $\mathcal Q_1$ and $\mathcal Q_2$ share the same nonzero eigenvalues, we will for simplicity write $\lambda_j$, $1\le j \le N\wedge n$, to denote the $j$-th eigenvalue of both $\mathcal Q_1$ and $\mathcal Q_2$ without causing any confusion.


%\subsection{Resolvents and limiting law}
%
%In this paper, we will study the eigenvalue statistics of $\mathcal Q_{1}$ and $\mathcal Q_2$ through their {\it{resolvents}} (or  {\it{Green's functions}}). It is equivalent to study the matrices
%\be\label{Qtilde}
%\wt{\mathcal Q}_1(X):=\Sig^{1/2} U^{*}XBX^\topU\Sig^{1/2}, \quad \wt{\mathcal Q}_2(X):=\wt\Sig^{1/2}V^\topX^\top A X V\wt \Sig^{1/2}.
%\ee
%In this paper, we shall denote the upper half complex plane and the right half real line by
%$$\mathbb C_+:=\{z\in \mathbb C: \im z>0\}, \quad \mathbb R_+:=[0,\infty).$$ %\quad  \mathbb R_*:=(0,\infty).$$
%
%\begin{definition}[Resolvents]\label{resol_not}
%For $z = E+ \ii \eta \in \mathbb C_+,$ we define the resolvents for $\wt {\mathcal Q}_{1,2}$ as
%\begin{equation}\label{def_green}
%\mathcal G_1(X,z):=\left(\wt{\mathcal Q}_1(X) -z\right)^{-1} , \ \ \ \mathcal G_2 (X,z):=\left(\wt{\mathcal Q}_2(X)-z\right)^{-1} .
%\end{equation}
% We denote the ESD $\rho^{(n)}$ of $\wt {\mathcal Q}_{1}$ and its Stieltjes transform as
%\be\label{defn_m}
%\rho\equiv \rho^{(n)} := \frac{1}{n} \sum_{i=1}^n \delta_{\lambda_i(\wt{\mathcal Q}_1)},\quad m(z)\equiv m^{(n)}(z):=\int \frac{1}{x-z}\rho_{1}^{(n)}(\dd x)=\frac{1}{n} \mathrm{Tr} \, \mathcal G_1(z).
%\ee
%We also introduce the following quantities:
%$$m_1(z)\equiv m_1^{(n)}(z):= \frac{1}{N}\sum_{i=1}^n\sigma_i (\mathcal G_1(z) )_{ii},\quad m_2(z)\equiv m_2^{(n)}(x):=\frac{1}{N}\sum_{\mu=1}^N \wt\sigma_\mu (\mathcal G_2(z) )_{\mu\mu}. $$
%
%%, \ \ \rho_{2}^{(n)} := \frac{1}{N} \sum_{i=1}^N \delta_{\lambda_i(\mathcal Q_2)}.$$
%%Then the Stieltjes transforms of $\rho_{1}$ is given by
%%\begin{align*}
%%& m_1^{(n)}(z):=\int \frac{1}{x-z}\rho_{1}^{(n)}(\dd x)=\frac{1}{n} \mathrm{Tr} \, \mathcal G_1(z).
%%%& m_2^{(n)}(z):=\int \frac{1}{x-z}\rho_{2}^{(n)}(\dd x)=\frac{1}{N} \mathrm{Tr} \, \mathcal G_2(z). %\label{ST_m12}
%%\end{align*}
%%and
%%\begin{equation}
%%m_2^{(n)}(z):=\int \frac{1}{x-z}d\rho_{2}^{M}(x)=\frac{1}{N}\sum_{i=1}^N (\mathcal G_2)_{ii}(z)=\frac{1}{N} \mathrm{Tr} \, \mathcal G_2(z). \label{ST_m2}
%%\end{equation}
%%Similarly, we can also define $m_1(z)\equiv m_1^{(M)}(z):= M^{-1}\mathrm{Tr} \, \mathcal G_1(z)$.
%\end{definition}


We now describe the asymptotic limit of $G(z)$ and $\cal G(z)$ as $n\to \infty$. First we define the deterministic limits of $(m_1(z), m_{2}(z))$, denoted by $(M_{1}(z),M_{2}(z))$
%\HZ{This $M_{1}, M_{2}$ notation is awkward; consider changing it to ${M}_1(z), {M}_2(z)$ (or  better, say $a_1(z), a_2(z)$? if they correspond to $a_1, a_2$ when $z=0$)},
as the unique solution to the following system of equations
\begin{equation}\label{selfomega}
\begin{split}
& \frac1{M_{1}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2 r_1 M_{1} +r_2 M_{2}  } - 1 ,\  \ \frac1{M_{2}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1 M_{1} +  r_2 M_{2}  }- 1 ,
\end{split}
\ee
such that $(M_{1}(z), M_{2}(z))\in \C_+^2$ for $z\in \C_+$, where, for simplicity, we  introduced the following ratios
\be\label{ratios}
 \gamma_n :=\frac{p}{n}=\frac{1}{\rho_1+\rho_2},\quad r_1 :=\frac{n_1}{n}=\frac{\rho_1}{\rho_1+\rho_2},\quad r_2 :=\frac{n_2}{n}=\frac{\rho_2}{\rho_1+\rho_2}.
\ee
We then define the matrix limit of $G(z)$ as
\be \label{defn_piw}
\Pi(z) := \begin{pmatrix} -(z+r_1 M_{1}(z)\Lambda^2  +  r_2 M_{2}(z))^{-1} & 0 & 0 \\ 0 &  M_{1}(z)\id_{n_1} & 0 \\ 0 & 0 & M_{2}(z)\id_{n_2}  \end{pmatrix}.\ee
In particular, the matrix limit of $\cal G(z)$ is given by $-(z+r_1 M_{1}\Lambda^2 + r_2 M_{2})^{-1}$.

\vspace{5pt}
\noindent{\bf Proof overview:}
\HZ{Divide into several parts like sec 7 to improve readability.}
Now we give a heuristic derivation of the matrix limit when the entries of $Z^{(1)}$ and $Z^{(2)}$ are i.i.d. Gaussian. Note that in this case,
%However, notice that if the entries of $Z^{(1)}\equiv (Z^{(1)})^{\text{Gauss}}$ and $Z^{(2)}\equiv (Z^{(2)})^{\text{Gauss}}$ are i.i.d. Gaussian, then
by the rotational invariance of the multivariate Gaussian distribution we have
\be\label{eq in Gauss} Z^{(1)} U\Lambda \stackrel{d}{=} Z^{(1)} \Lambda, \quad Z^{(2)} V \stackrel{d}{=} Z^{(2)},\ee
where ``$\stackrel{d}{=}$" means ``equal in distribution". Hence it suffices to consider the following resolvent
 \begin{equation} \nonumber
   G(z)= \left( {\begin{array}{*{20}c}
   { -z\id_{p} } & n^{-1/2}\Lambda (Z^{(1)})^{\top} & n^{-1/2} (Z^{(2)})^\top  \\
   {n^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1}} & 0 \\
   {n^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2}}
   \end{array}} \right)^{-1}.
 \end{equation}
Using Schur complement formula (see \alert{equation} (\ref{resolvent2}) \HZ{check other usage of equation ref}), we have that %for $i \in \cal I_0$, $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
\begin{align}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu,\nu\in \mathcal I_1} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu} - \frac{1}{n} \sum_{\mu,\nu\in \mathcal I_2} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu} -2 \frac{\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu},  \ \ \text{for }\  i \in \cal I_0 , \label{0self_Gii}\\
\frac{1}{{G_{\mu\mu} }}&=  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}\lambda_i \lambda_j z_{\mu i}z_{\mu j} G^{\left(\mu\right)}_{ij}, \quad \frac{1}{{G_{\nu\nu} }}=  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}  z_{\nu i}z_{\nu j}  G^{\left(\nu\right)}_{ij},  \ \ \text{for }\  \mu \in \cal I_1, \ \nu\in \cal I_2, \label{0self_Gmu1}
\end{align}
where we recall the notations in equation \eqref{labelZ}. For the right-hand side of equation \eqref{0self_Gii}, $G^{(i)}$ is independent of the entries $z_{\mu i}$ and $z_{\nu i}$. Hence by the concentration inequalities in Lemma \ref{largedeviation}, we have that the  right-hand side of equation \eqref{0self_Gii} concentrates around the partial expectation over the entries $\{z_{\mu i}: \mu \in \cal I_1\cup \cal I_2\}$, i.e., with overwhelming probability,
\begin{align*}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu \in \mathcal I_1}  G^{\left( i \right)}_{\mu\mu} - \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} +\oo(1)= - z - \lambda_i^2 r_1 m_1^{(i)}(z)-  r_2m_2^{(i)}(z)+\oo(1),
\end{align*}
where we used the definition of $m_1^{(i)}$ and $m_2^{(i)}$ in equation \eqref{defm} with $G$ replaced by $G^{(i)}$. Intuitively, since we have only removed only one column and one row out of the $(p+n)$ columns and rows in $H$, $m_1^{(i)}$ and $m_2^{(i)}$ should be close to the original $m_1$ and $m_2$. Hence we obtain from the above equation that
\begin{align}\label{1self_Gii}
 G_{ii}  = -\frac{1}{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}.
\end{align}
Similarly, we can obtain from equation \eqref{0self_Gmu1} that for $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
\be\label{1self_Gmu} G_{\mu \mu }=-\frac{1}{1+\gamma_n m_0 + \oo(1)},\quad G_{\nu\nu}=-\frac1{1+\gamma_n m+\oo(1)},\ee
with overwhelming probability. Taking average we obtain that
\be\label{2self_Gmu} m_1= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu\mu}=-\frac{1}{1+\gamma_n m_0 + \oo(1)},\quad m_2=\frac{1}{n_2}\sum_{\nu \in \cal I_2}G_{\nu\nu}=-\frac{1}{1+\gamma_n m + \oo(1)},
\ee
with overwhelming probability. Together with the definition of $m$ and $m_0$ in equation \eqref{defm}, the two equations in equation \eqref{2self_Gmu} give that
\be\label{3self_Gmu}  \frac1{m_1}= -1- \frac{\gamma_n}{p} \sum_{i=1}^p \lambda_i^2 G_{ii}+ \oo(1),\quad \frac1{m_2}=-1-\frac{\gamma_n}{p} \sum_{i=1}^p G_{ii}  + \oo(1),\ee
with overwhelming probability. Plugging equation \eqref{1self_Gii} into equation \eqref{3self_Gmu}, we obtain that
\begin{align*}
& \frac1{m_1}= -1+ \frac{\gamma_n}{p} \sum_{i=1}^p \frac{\lambda_i^2 }{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}+ \oo(1),\\
& \frac1{m_2}=-1+\frac{\gamma_n}{p} \sum_{i=1}^p \frac{1}{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}  + \oo(1),
\end{align*}
with overwhelming probability, which give the approximate self-consistent equations for $(m_1,m_2)$. Compare them to the deterministic self-consistent equations in equation \eqref{selfomega}, one can observe that we should have $(m_1,m_2) =(M_{1}, M_{2})+\oo(1)$ with overwhelming probability. Inserting this approximate identity into equation \eqref{1self_Gii}-equation \eqref{2self_Gmu}, we see that for  $i \in \cal I_0$, $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
$$G_{ii}=-(z +\lambda_i^2 r_1  M_{1} + r_2 M_{2}+\oo(1))^{1/2},\quad G_{\mu\mu}=M_{1}+\oo(1),\quad G_{\nu\nu}=M_{2}+\oo(1),$$
with overwhelming probability. These explain the diagonal entries of $\Pi$ in equation \eqref{defn_piw}. For the off-diagonal entries, they are close to zero due to concentration. For example, for $i\ne j\in \cal I_1$, by Schur complement formula (see (\ref{resolvent3})), we have
$$G_{ij}=-G_{ii}\left(\frac{\lambda_i}{n^{1/2}}\sum_{\mu \in \cal I_1} z_{\mu i} G^{(i)}_{\mu j} + \frac{1}{n^{1/2}}\sum_{\mu \in \cal I_2} z_{\mu i} G^{(i)}_{\mu j} \right).$$
Using Lemma \ref{largedeviation}, we can show that $n^{-1/2}\sum_{\mu \in \cal I_1} z_{\mu i} G^{(i)}_{\mu j}$ and $n^{-1/2}\sum_{\mu \in \cal I_2} z_{\mu i} G^{(i)}_{\mu j}$ are both close to zero. The other off-diagonal entries can be bounded in the same way.

The above arguments are the core of the main proof. To have a rigorous proof, we need to estimate each error carefully, and extend the Gaussian case to the more general case where the entries of $Z^{(1)}$ and $Z^{(2)}$ only satisfy certain moment assumptions. These will make the real argument rather tedious, but the methods we used are standard in the random matrix literature \cite{erdos2017dynamical,Anisotropic}.

\fi


%\subsubsection{The Local Laws}\label{sec locallaw1}
%In the following proof, we choose a sufficiently small constants $c_0>0$ such that Lemma \ref{lem_mbehaviorw} and Lemma \ref{lem_stabw} hold. Then

