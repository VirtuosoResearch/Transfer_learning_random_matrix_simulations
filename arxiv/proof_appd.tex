\subsection{Limit of the Resolvent}\label{sec pf RMTlemma}

We now state the main random matrix result---Theorem \ref{main_cor}---which gives an almost optimal estimate on the resolvent $G(z)$ of $H$. It is conventionally called the {\it anisotropic local law} \cite{Anisotropic}. We define a domain of the spectral parameter $z$ as
\begin{equation}
\mathbf D:= \left\{z=E+ \ii \eta \in \C_+: |z|\le (\log n)^{-1} \right\}. \label{SSET1}
\end{equation}

\begin{theorem}\label{main_cor}
In the setting of Theorem \ref{thm_main_RMT}, let $q$ be equal to $n^{-\frac{\varphi - 4}{2\varphi}}$.
We have that the resolvent $G(z)$ converges to the matrix limit $\Gi(z)$:
for any deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb R^{p+n_1+n_2}$, the following estimate \begin{equation}\label{aniso_law}
	\max_{z\in \mathbf D}\left| \mathbf u^\top (G(z)-\Gi(z)) \mathbf v \right|  \prec  q
\end{equation}
holds on the high probability event
\be\label{one_event}\left\{ \max_{1\le i\le n_1, 1\le j \le p}|Z^{(1)}_{i j}| \le (\log n) n^{\frac{2}{\varphi}}, \ \max_{1\le i\le n_2, 1\le j \le p}|Z^{(2)}_{i j}|\le (\log n) n^{\frac{2}{\varphi}} \right\}.\ee
\end{theorem}

The above result can be derived using the following lemma, which holds under a more general bounded support assumption on the random matrices.

\begin{lemma} \label{LEM_SMALL} %
In the setting of Theorem \ref{main_cor}, assume that $Z^{(1)}$ and $Z^{(2)}$ satisfy the bounded support condition \eqref{eq_support} with $Q=\sqrt{n}q = n^{ \frac{2}{\varphi}}$.
Then we have that the anisotropic local law in equation \eqref{aniso_law} holds 
for any deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb R^{p+n_1+n_2}$.
\end{lemma}

\begin{remark}
The reason why we say the bounded support assumption is more general is because it provides greater flexibility in dealing with bounded moments. For example, we can also replace equation \eqref{Ptrunc} with
\begin{align*}
	\P\left(\max_{1\le i\le n, 1\le j \le p}|Z_{i j}|\ge n^{\frac{2}{\varphi}+\delta}\right) = \OO(  n^{-\varphi \delta}) 
	\end{align*}
	for a small constant $\delta>0$. Hence we can replace event \eqref{one_event} with 
	\be\nonumber \left\{ \max_{1\le i\le n, 1\le j \le p}|Z^{(1)}_{i j}| \le n^{\frac{2}{\varphi}+\delta}, \ \max_{1\le i\le n, 1\le j \le p}|Z^{(2)}_{i j}|\le   n^{\frac{2}{\varphi}+\delta} \right\},\ee
	which holds with higher probability. But on this event we need to take a larger $q=n^{-\frac{\varphi - 4}{2\varphi}+\delta}$, which means a worse convergence rate. In general, with Lemma \ref{LEM_SMALL} one can determine the most suitable trade-off between probability and convergence rate depending on one's need.
\end{remark}

Using the above result, we prove Theorem \ref{main_cor} using a simple cutoff argument.
\begin{proof}[Proof of Theorem \ref{main_cor}]%
We introduce the truncated matrices $\wt Z^{(1)}$ and $\wt Z^{(2)}$ with entries
$$ \wt Z^{(1)}_{\mu i}:= \mathbf 1\left( n^{-1/2}|Z^{(1)}_{\mu i}|\le q \log n \right)\cdot Z^{(1)}_{\mu i}, \quad \wt Z^{(2)}_{\nu i}:= \mathbf 1\left( n^{-1/2}|Z^{(2)}_{\nu i}|\le q \log n \right)\cdot Z^{(2)}_{\nu i}, $$
for $q= n^{-\frac{\varphi - 4}{2\varphi}}$. By equation \eqref{Ptrunc}, 
we have
\begin{equation}\label{XneX}
\mathbb P(\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}) =1-\OO ( (\log n)^{-\varphi}).
\end{equation}
By definition, we have 
\be\label{EwtZ}
\begin{split}
\E  \wt  Z^{(1)}_{\mu i} &= - \mathbb E \left[ \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)Z^{(1)}_{\mu i}\right] ,\\ 
\E  |\wt  Z^{(1)}_{\mu i}|^2 &= 1 - \mathbb E \left[ \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)|Z^{(1)}_{\mu i}|^2\right] .
\end{split}
\ee
Using the formula for expectation in terms of the tail probabilities, we can check that
\begin{align*}
&  \mathbb E \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| \\
= &\int_0^\infty \P\left( \left| \mathbf 1\left(  |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| > s\right)\dd s \\
 = &\int_0^{qn^{1/2}\log n}\P\left( |Z^{(1)}_{\mu i}|> q  n^{1/2}\log n \right)\dd s +\int_{qn^{1/2}\log n}^\infty \P\left(|Z^{(1)}_{\mu i}| > s\right)\dd s  \\
\lesssim &\int_0^{qn^{1/2}\log n}\left(q  n^{1/2}\log n \right)^{-\varphi}\dd s +\int_{qn^{1/2}\log n}^\infty s^{-\varphi}\dd s \le n^{-2(\varphi-1)/\varphi},
\end{align*}
where in the third step we used the finite $\varphi$-th moment condition of $Z_{\mu i}^{(1)}$ %
and Markov's inequality. Similarly, we can obtain that
\begin{align*}
&  \mathbb E \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)Z^{(1)}_{\mu i}\right|^2 \\
= & 2\int_0^\infty s \P\left( \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| > s\right)\dd s \\
= & 2\int_0^{qn^{1/2}\log n} s \P\left( |Z^{(1)}_{\mu i}|> q  n^{1/2}\log n \right)\dd s +2\int_{qn^{1/2}\log n}^\infty s\P\left(|Z^{(1)}_{\mu i}| > s\right)\dd s  \\
\lesssim & \int_0^{qn^{1/2}\log n}s\left(q  n^{1/2}\log n \right)^{-\varphi}\dd s +\int_{qn^{1/2}\log n}^\infty s^{-\varphi+1}\dd s \le n^{-2(\varphi-2)/\varphi}.
\end{align*}
Plugging the above two estimates into equation \eqref{EwtZ} and using $\varphi>4$, we get that
\be\label{meanshif}
|\mathbb E  \wt  Z^{(1)}_{\mu i}| =\OO(n^{-3/2}), \quad  \mathbb E |\wt  Z^{(1)}_{\mu i}|^2 =1+ \OO(n^{-1}).
\ee
From the first estimate in equation \eqref{meanshif}, we can also get a bound on the operator norm:
\be\label{EZ norm}\|\E \wt Z^{(1)}\|=\OO(n^{-1/2}) .
\ee
Similar estimates also hold for $\wt Z^{(2)}$. 
Then we can centralize and rescale $\wt Z^{(1)}$ and $\wt Z^{(2)}$ as
$$ \wh Z^{(1)} :=\frac{\wt Z^{(1)} - \E \wt Z^{(1)} }{\left(\E|\wt Z^{(1)}_{\mu i}|^2\right)^{1/2}},\quad \wh Z^{(2)} :=\frac{\wt Z^{(2)} - \E \wt Z^{(2)} }{\left(\E|\wt Z^{(2)}_{\mu i}|^2\right)^{1/2}}.$$ 
Now $\wh Z^{(1)}$ and $\wh Z^{(2)}$ satisfy the assumptions of Lemma \ref{LEM_SMALL} with bounded support $\sqrt{n}q = n^{ \frac{2}{\varphi}}$, so we get that %
\be\label{GwhZZ}\left| \mathbf u^\top (G(\wh Z^{(1)},\wh Z^{(2)},z)-\Gi(z)) \mathbf v \right|  \prec  q,\ee
where $G(\wh Z^{(1)},\wh Z^{(2)},z)$ is defined in the same way as $G(z)$, but with $(Z^{(1)}, Z^{(2)})$ replaced by $(\wh Z^{(1)},\wh Z^{(2)})$.

Note that by equations \eqref{meanshif} and \eqref{EZ norm}, we can bound that for $k=1,2,$
$$ \|\wh Z^{(k)} - \wt Z^{(k)}\|\lesssim n^{-1}\|\wt Z^{(k)}\| + \|\E \wt Z^{(k)}\|\lesssim n^{-1/2}$$
with overwhelming probability, where we also used Fact \ref{fact_minv}(ii) to bound the operator norm of $\wt Z^{(k)}$. Together with estimate \eqref{priorim} below, this bound implies that %
$$\left| \mathbf u^\top (G(\wh Z^{(1)},\wh Z^{(2)},z)-G(Z^{(1)},Z^{(2)},z)) \mathbf v \right|  \lesssim  n^{-1/2}\|\wh Z^{(1)} - \wt Z^{(1)}\|+ n^{-1/2}\|\wh Z^{(2)} - \wt Z^{(2)}\|\lesssim n^{-1},$$
with overwhelming probability on the event $\{\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}\}$. Combining this estimate with equation \eqref{GwhZZ}, we obtain that estimate \eqref{aniso_law} holds for $G(z)$ on the event $\{\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}\}$, which concludes the proof by equation \eqref{XneX}.
\end{proof}


Now we are ready to complete the proof of Theorem \ref{thm_main_RMT} using Theorem \ref{main_cor}.

\begin{proof}[Proof of Theorem \ref{thm_main_RMT}, Part i)]
With the definition of matrix $W$ in equation \eqref{eigen2extra}, we can express $\Sigma^{(2)}\hat \Sigma^{-1}$ as
$$\Sigma^{(2)}\hat \Sigma^{-1} = n^{-1} ({\Sigma^{(2)}})^{1/2} V \cal G(0) V^{\top} ({\Sigma^{(2)}})^{-1/2},$$
where we recall that $\cal G(z)=(W-z\id )^{-1}$ is the resolvent of $W$.
Then by Theorem \ref{main_cor}, for any $1\le i \le p$ we have that
\begin{align}
& \left| \left[\Sigma^{(2)}\hat \Sigma^{-1} - n^{-1} (\Sigma^{(2)})^{1/2}V \Gi(0)V^\top(\Sigma^{(2)})^{-1/2} \right]_{ii}\right|  \nonumber\\
 = & n^{-1} \left|\mathbf e_i^\top (\Sigma^{(2)})^{1/2}V \left(\cal G(0)-\Gi(0)\right)V^\top (\Sigma^{(2)})^{-1/2} \mathbf e_i\right| \nonumber\\
\prec & n^{-1}q\|V^\top (\Sigma^{(2)})^{-1/2} \mathbf e_i\|\cdot \|V^\top (\Sigma^{(2)})^{1/2} \mathbf e_i\|   \lesssim n^{-1}q ,\label{G0Pi0}
\end{align}
on the event \eqref{one_event}, %
where $ q= n^{-\frac{\varphi - 4}{2\varphi}}$ and $\mathbf e_i$ denotes the standard basis vector along the $i$-th direction.
Next, we can verify that %
$$ n^{-1}  (\Sigma^{(2)})^{1/2}V \Gi(0)V^\top (\Sigma^{(2)})^{-1/2} = n^{-1}  \Sigma^{(2)}(a_1 \Sigma^{(1)}+  a_2\Sigma^{(2)})^{-1} .$$
Together with equation \eqref{G0Pi0}, this identity implies that %
\begin{align*}
 \tr \left[\Sigma^{(2)}\hat \Sigma^{-1} \right]& = \sum_{i=1}^p\left(\Sigma^{(2)}\hat \Sigma^{-1} \right)_{ii} = n^{-1}\tr \left[ \Sigma^{(2)}(a_1 \Sigma^{(1)}+  a_2\Sigma^{(2)})^{-1} \right] +\OO_\prec(q  ) 
 \end{align*}
on the event \eqref{one_event}, where we used Fact \ref{lem_stodomin} (i) in the second step. 
This concludes equation \eqref{lem_cov_shift_eq} using Definition \ref{stoch_domination} and the fact that $c_{\varphi}$ is any fixed value within $(0, \frac{\varphi - 4}{2\varphi})$.  %
\end{proof}
 

\begin{proof}[Proof of Theorem \ref{thm_main_RMT}, Part ii)]
Recall that in the setting of Theorem \ref{thm_main_RMT}, we have equation \eqref{calculate G'}.
For simplicity, we denote the vector $\bv:=V^\top  (\Sigma^{(2)})^{-1/2} \Sigma^{(1)}w$. 
By Corollary \ref{main_cor}, we have that
\be\nonumber 
\max_{z\in \C:|z|=(\log n)^{-1}}|\mathbf v^\top (G(z)-\Gi(z))\mathbf v| \prec q \|\mathbf v\|^2,%
\ee
on the event \eqref{one_event} with $ q:= n^{-\frac{\varphi - 4}{2\varphi}}$. Now combining this estimate with Cauchy's integral formula, we get that %
\be\label{apply derivlocal}
\begin{split}
  \bv^\top \cal G'(0)\bv  = \frac{1}{2\pi \ii}\oint_{\cal C} \frac{ \bv^\top \cal G(z)\bv }{z^2}\dd z &=  \frac{1}{2\pi \ii}\oint_{\cal C} \frac{ \bv^\top\Gi(z)\bv}{z^2}\dd z +\OO_\prec(q\|\mathbf v\|^2) \\
  &=  \bv^\top \Gi'(0)\bv + \OO_\prec(q\|\mathbf v\|^2),
\end{split}
\ee
where $\cal C$ is the contour $\{z\in \C: |z| = (\log n)^{-1} \}$. We can calculate the derivative $\bv^\top \Gi'(0)\bv$ as
\be\label{dervPi}
\bv^\top \Gi'(0)\bv = \bv^\top  \frac{a_3\Lambda^2+(1+a_4)\id_p}{(a_{1}\Lambda^2 + a_{2}\id_p)^2}\bv,
\ee
where we recall equation \eqref{cal G'0} and that $a_3 = - \frac{\dd a_1(0)}{\dd z}$ and $a_4 = - \frac{\dd a_2(0)}{\dd z}$. 
Taking the derivatives of the system of equations \eqref{selfomega_a}, we can derive equation \eqref{eq_a34extra} for $(a_3,a_4)$. This concludes the proof together with equation \eqref{apply derivlocal}.
\end{proof}



\subsection{Self-Consistent Equations}\label{sec contract}

The rest of this section is devoted to the proof of Lemma \ref{LEM_SMALL}. In this section, we show that the limiting equation \eqref{selfomega_a} has a unique solution $(a_1(z), a_2(z))$ for any $z\in \mathbf D$ in equation \eqref{SSET1}. Otherwise, Lemma \ref{LEM_SMALL} will be a vacuous statement. 


When $z=0$, the system of equations \eqref{selfomega_a} reduces to equation \eqref{eq_a12extra}, from which we can derive an equation of $a_1$ only:
\be\label{fa1}f(a_1)=\frac{n_1}{n_1 + n_2},\quad \text{with}\quad f(a_1):=a_1 +\frac{1}{n_1+n_2} \sum_{i=1}^p \frac{\lambda_i^2 a_1}{ \lambda_i^2 a_1+ (1- \frac{p}{n_1+n_2} - a_1) } .\ee
It is not hard to see that $f$ is strictly increasing on $[0,1-\frac{p}{n_1+n_2} ]$. Moreover, we have $f(0)=0<1$, $f(1-\frac{p}{n_1+n_2} )=1>\frac{n_1}{n_1+n_2}$, and $f(\frac{n_1}{n_1+n_2})>\frac{n_1}{n_1+n_2}$ if $\frac{n_1}{n_1+n_2}\le 1-\frac{p}{n_1+n_2}$. Hence by mean value theorem, there exists a unique solution $a_1$ satisfying
$$0< a_1 <\min\left(1-\frac{p}{n_1+n_2}, \frac{n_1}{n_1+n_2}\right).$$ 
Moreover, it is easy to check that $f'(x)=\OO(1)$ for $x\in [0, 1-\frac{p}{n_1+n_2}]$. Hence there exists a constant $\tau>0$, such that 
\be\label{a23}
\begin{split}
\frac{n_1}{n_1+n_2} \tau &\le   a_1 \le \min\left\{ 1-\frac{p}{n_1+n_2}  -\frac{n_1}{n_1+n_2}  \tau,\frac{n_1}{n_1+n_2} ( 1-\tau)\right\},\\ 
\tau & \le a_2\le 1 -\frac{p}{n_1+n_2}  - \frac{n_1}{n_1+n_2} \tau .
\end{split}
\ee 


Next, we prove the existence and uniqueness of the solution to the self-consistent equation \eqref{selfomega_a} for a general $z\in \mathbf D$.
We denote
\be\label{M1M2a1a2}
M_1(z):= -\frac{n_1+n_2}{n_1}a_1(z),\quad M_2(z):= -\frac{n_1+n_2}{n_2}a_2(z),
\ee
which are the asymptotic limits of $m_1(z)$ and $m_2(z)$ in equation \eqref{approximate m1m2}.
Then, it is not hard to verify that the system of equations \eqref{selfomega_a} can be rewritten as 
 the following system of equations:
\begin{equation}\label{selfomega}
\begin{split}
& \frac1{M_{1}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2 r_1 M_{1} +r_2 M_{2}  } - 1 ,\  \ \frac1{M_{2}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1 M_{1} +  r_2 M_{2}  }- 1 ,
\end{split}
\ee
where, for simplicity of notations, we introduced the following ratios 
\be\label{ratios}
 \gamma_n :=\frac{p}{n_1+n_2} ,\quad r_1 :=\frac{n_1}{n_1+n_2} ,\quad r_2 :=\frac{n_2}{n_1+n_2} .
\ee
One can compare equation \eqref{selfomega} for $(M_1(z),M_2(z))$ to equation \eqref{approximate m1m2} for $(m_1(z),m_2(z))$.
In the following proof, we shall focus on the system of equations \eqref{selfomega} because it is more suitable than equation \eqref{selfomega_a}  for our purpose of showing that $(m_1(z),m_2(z))$ converges to the asymptotic limit $(M_1(z),M_2(z))$.

Now we claim the following lemma, which gives the existence and uniqueness of the solution $(M_{1}(z),M_{2}(z))$ to the system of equations \eqref{selfomega}.

\begin{lemma} \label{lem_mbehaviorw}
There exist constants $c_0, C_0>0$ depending only on $\tau$ in Assumption \ref{assume_rm} and equation \eqref{a23} such that the following statements hold.
There exists a unique solution to equation \eqref{selfomega} under the conditions
\be\label{prior1}
|z|\le c_0, \quad  |M_{1}(z) - M_{1}(0)| + |M_{2}(z) - M_{2}(0)|\le c_0.
\ee
Moreover, the solution satisfies
\be\label{Lipomega}
 |M_{1}(z) - M_{1}(0)| + |M_2(z) - M_2(0)| \le C_0|z|.
\ee
\end{lemma}
 \begin{proof} %
 The proof is a standard application of the contraction principle. For reader's convenience, we give more details. First, it is easy to check that equation \eqref{selfomega} is equivalent to  
\begin{equation}\label{selfalter}
r_1M_{1}=-(1-\gamma_n) - r_2M_{2} - z\left( M_{2}^{-1}+1\right),\quad g_z(M_{2}(z))=1, 
\ee
where
$$g_z(M_{2}):= - M_{2} +\frac{\gamma_n}p\sum_{i=1}^p \frac{M_{2} }{  z -\lambda_i^2(1-\gamma_n)+ (1 - \lambda_i^2) r_2M_{2} - \lambda_i^2 z\left(  M_{2}^{-1}+1\right) }.$$
 We first show that there exists a unique solution $M_{2}(z)$ to the equation $g_z(M_{2}(z))=1$ under the conditions in equation \eqref{prior1}.
We abbreviate $\delta(z):= M_{2}(z) - M_{2}(0)$. From equation \eqref{selfalter}, we obtain that 
\begin{equation} \nonumber
0=\left[g_z(M_{2}(z)) -  g_0(M_{2}(0)) -g_z'(M_{2}(0))\delta(z)\right] + g_z'(M_{2}(0))\delta(z),
\ee
which gives that
\be\nonumber%
 \delta(z) =- \frac{ g_z(M_{2}(0)) - g_0(M_{2}(0)) }{g_z'(M_{2}(0))}- \frac{ g_z(M_{2}(0)+\delta(z)) -  g_z(M_{2}(0))-g_z'(M_{2}(0))\delta(z)}{g_z'(M_{2}(0))}.
 \ee
Inspired by this equation, we define iteratively a sequence ${\delta}^{(k)}(z) \in \C$ such that ${\delta}^{(0)}=0$, and 
\be\label{selfomega2}
 \delta^{(k+1)} =- \frac{g_z(M_{2}(0)) - g_0(M_{2}(0))}{g_z'(M_{2}(0))} -\frac{g_z(M_{2}(0)+ \delta^{(k)}) -  g_z(M_{2}(0))-g_z'(M_{2}(0))\delta^{(k)} }{g_z'(M_{2}(0))} .
 \ee
Then equation \eqref{selfomega2} defines a mapping $h_z:\C\to \C$, which maps $\delta^{(k)}$ to $\delta^{(k+1)}=h_z(\delta^{(k)})$.
 

With direct calculation, we obtain that
$$g_z'(M_{2}(0)) = -1 - \frac{\gamma_n}p\sum_{i=1}^p \frac{ \lambda_i^2(1-\gamma_n) - z\left[1- \lambda_i^2 \left(  2M_{2}^{-1}(0)+1\right)\right]  }{  \left[z -\lambda_i^2(1-\gamma_n)+ (1 - \lambda_i^2) r_2M_{2}(0) - \lambda_i^2 z\left( M_{2}^{-1}(0)+1\right)\right]^2 }.$$
Then it is not hard to check that there exist constants $\wt c, \wt C>0$ depending only on $\tau$ such that the following estimates hold: for all $z$, $\delta_1$ and $\delta_2$ such that $|z|\le \wt c$, $|\delta_1|  \le \wt c$ and $|\delta_2|  \le \wt c$,
\be\label{dust}
\left|\frac{1}{g_z'(M_{2}(0))} \right|\le \wt C, \quad   \left|\frac{g_z(M_{2}(0)) - g_0(M_{2}(0))}{g_z'(M_{2}(0))}\right|  \le \wt C|z| ,
\ee
and 
\be\label{dust222}
\left|\frac{g_z(M_{2}(0)+ \delta_1) -  g_z(M_{2}(0)+\delta_2)-g_z'(M_{2}(0))(\delta_1-\delta_2) }{g_z'(M_{2}(0))}\right|  \le \wt C|\delta_1-\delta_2|^2 .
\ee
Using equations \eqref{dust} and \eqref{dust222}, it is not hard to show that there exists a sufficiently small constant $c_1>0$ depending only on $\wt C$, such that 
$h_z: B_{d}  \to B_{d}$ is a self-mapping on the ball $B_d:=\{\delta \in \C: |\delta| \le d \}$, as long as $d\le c_1$ and $|z| \le c_1$. 
Now it suffices to prove that $h$ restricted to $B_d $ is a contraction, which then implies that ${\delta}:=\lim_{k\to\infty} { \delta}^{(k)}$ exists and $M_{2}(0)+\delta(z)$ is a unique solution to equation $g_z(M_{2}(z))=1$ subject to the condition $\|{\delta}\|_\infty \le d$. 


From the iteration relation \eqref{selfomega2}, using equation \eqref{dust222} one can readily check that
\be\label{k1k}
{ \delta}^{(k+1)} - { \delta}^{(k)}= h_z({\delta}^{(k)}) - h_z({\delta}^{(k-1)}) \le \wt C | { \delta}^{(k)}-{ \delta}^{(k-1)}|^2.
\ee
Hence as long as $d$ is chosen to be sufficiently small such that $2d\wt C\le 1/2$,  
then $h$ is indeed a contraction mapping on $ B_d$. This proves both the existence and uniqueness of the solution $M_{2}(z)=M_{2}(0)+\delta(z)$, if we choose $c_0$ in equation \eqref{prior1} as $c_0=\min\{c_1, d\}$. After obtaining $M_{2}(z)$, we can then find $M_{1}(z)$ using the first equation in \eqref{selfalter}. 

Note that with equation \eqref{dust} and ${\delta}^{(0)}= 0$, we can obtain from equation \eqref{selfomega2} that $ |{ \delta}^{(1)}(z)| \le \wt C|z| .$
With the contraction mapping, we have the bound 
\be\label{endalter}|{ \delta}| \le \sum_{k=0}^\infty |{  \delta}^{(k+1)}-{ \delta}^{(k)}| \le 2\wt C|z| \ \Rightarrow \ |M_{2}(z)-M_{2}(0)|\le 2\wt C|z| .\ee
Then using the first equation in equation \eqref{selfalter}, we immediately obtain the bound  $ r_1|M_{1}(z)-M_{1}(0)| \le C|z|$ for some constant $C>0$, which concludes equation \eqref{Lipomega} as long as if $r_1\gtrsim 1$. To deal with the $r_1=\oo(1)$ case, we go back to the first equation in \eqref{selfomega} and treat $M_{1}(z)$ as the solution to the following equation:
$$\wt g_z(M_{1}(z))=1,\quad \wt g_z(M_1):=- M_1 + \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2 M_1}{  z+\lambda_i^2 r_1 M_1 +r_2 M_{2}(z) }. $$
(Note that we have found the solution $M_2(z)$, so this is an equation of $M_1$ only.) 
Then with a similar argument as above (i.e. the proof between equation \eqref{selfalter} and equation \eqref{endalter}), we can conclude $|M_{2}(z)-M_{2}(0)|=\OO(|z|)$, which further concludes equation \eqref{Lipomega} together with equation \eqref{endalter}. 
We omit the details. %
\end{proof}

As a byproduct of the above contraction mapping argument, we also obtain the following stability result that will be used in the proof of Lemma \ref{LEM_SMALL}. Roughly speaking, it states that if  two complex functions $m_1(z)$ and $m_2(z)$ satisfy the self-consistent equation \eqref{selfomega} approximately up to some small errors, then $m_1(z)$ and $m_2(z)$ will be close to the solutions $M_1(z)$ and $M_2(z)$. Later this result will be applied to equation \eqref{approximate m1m2} to show that the averaged resolvents $m_1(z)$ and $m_2(z)$ indeed converge to $M_1(z)$ and $M_2(z)$, respectively.


\begin{lemma} \label{lem_stabw}
There exist constants $c_0, C_0>0$ depending only on $\tau$ in Assumption \ref{assume_rm} and equation \eqref{a23} such that the self-consistent equations in equation \eqref{selfomega} are stable in the following sense. Suppose $|z|\le c_0$, and $m_{1} $ and $m_{2} $ are analytic functions of $z$ such that
\be  \label{prior12}
|m_{1}(z) - M_{1}(0)| + |m_{2}(z) - M_2(0)|\le c_0.
\ee
Moreover, assume that $(m_1,m_2)$ satisfies the system of equations
\begin{equation}\label{selfomegaerror}
\begin{split}
&\frac{1}{m_{1}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2r_1m_{1} +r_2 m_{2}  } =\cal E_1,\ \ \frac{1}{m_{2}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1m_{1} +  r_2m_{2}  }=\cal E_2,
\end{split}
\ee
for some (deterministic or random) errors such that $  |\mathcal E_1| +  |\mathcal E_2| \le \theta(z),$ where $\theta(z)$ is a deterministic function of $z$ satisfying that $\theta(z) \le (\log n)^{-1}.$ Then we have 
 \begin{equation}
  \left|m_1(z)-M_{1}(z)\right| +  \left|m_2(z)-M_2(z)\right|\le C_0\delta(z).\label{Stability1}
\end{equation}
\end{lemma}



\begin{proof}%
Under condition \eqref{prior12}, we can obtain equation \eqref{selfalter} approximately up to some small error:
\be\label{selfalter2}r_1 m_{1}=-(1-\gamma_n) - r_2m_{2} - z\left(  {m_{2}^{-1}}+1\right) + \wt{\cal E}_1(z),\quad g_z(m_{2}(z))=1+ \wt{\cal E}_2(z),\ee
where the errors satisfy that $|\wt{\cal E}_1(z)|+ |\wt{\cal E}_2(z)|=\OO(\theta(z))$. Then we subtract equation \eqref{selfalter} from equation \eqref{selfalter2}, and consider the contraction principle for the function $\delta (z):= m_{2}(z) - M_2(z)$.  The rest of the proof is exactly the same as the one for Lemma \ref{lem_mbehaviorw}, so we omit the details.
\end{proof}





















\subsection{Beyond Multivariate Gaussian Matrices: an Anisotropic Local Law}\label{sec_Gauss}

 
In this section, we prove Lemma \ref{LEM_SMALL} by extending from the Gaussian random matrices to general random matrices.
The main difficulty in the proof is due to the fact that the entries of $Z^{(1)} U\Lambda$ and ${Z^{(2)}V}$ are not independent.
When the entries of $Z^{(1)}$ and $Z^{(2)}$ are sampled i.i.d. from an isotropic Gaussian distribution, $Z^{(1)} U$ and $Z^{(2)}V$ still obey the Gaussian distribution.
In this case, the problem is reduced to proving the anisotropic local law for $G(z)$ with $U=\id$ and $V=\id$, such that the entries of $ Z^{(1)} \Lambda  $ and ${Z^{(2)}}$ are independent.
For this case, we use the standard resolvent methods in \citet{isotropic,yang2019spiked,PY} and prove the following result.


\begin{proposition}\label{prop_diagonal}
 In the setting of Lemma \ref{LEM_SMALL}, assume further that the entries of $ Z^{(1)}$ and $ Z^{(2)}$ are i.i.d. Gaussian random variables. %
 Suppose $U$ and $V$ are identity.
	Then, the estimate \eqref{aniso_law} holds for all $z\in \mathbf D$ with $q= n^{-1/2}$.
\end{proposition}

 

Note that if the entries of $ Z^{(1)}$ and $ Z^{(2)}$ are Gaussian, then we have $\varphi=\infty$, which gives $q= n^{-\frac{\varphi - 4}{2\varphi}}=n^{-1/2}$.



Next we briefly describe how to extend Lemma \ref{LEM_SMALL} from the Gaussian case to the case with general $Z^{(1)}$ and $Z^{(2)}$ satisfying the bounded support condition (\ref{eq_support}) with $Q=\sqrt{n}q=n^{\frac{2}{\varphi}}$. 
With Proposition \ref{prop_diagonal}, it suffices is to prove that for $Z^{(1)}$ and $Z^{(2)}$ satisfying the assumptions in Lemma \ref{LEM_SMALL}, we have
\begin{equation*}%
 \left|\mathbf u^\top  \left( G(Z,z) -  G(Z^{\text{Gauss}}, z)\right) \mathbf v \right| \prec q 
\end{equation*}
for any deterministic unit vectors $\mathbf u,\mathbf v\in{\mathbb R}^{p+n_1+n_2}$ and $z\in \mathbf D$, where we abbreviated that 
$$Z:=\begin{pmatrix}Z^{(1)} \\ Z^{(2)}\end{pmatrix},\quad \text{and} \quad Z^{\text{Gauss}}:=\begin{pmatrix}(Z^{(1)})^{\text{Gauss}}\\ (Z^{(2)})^{\text{Gauss}}\end{pmatrix}.$$
We will prove the above statement using a continuous comparison argument developed in \citet{Anisotropic}. %
Since the proof is almost the same as the ones in Sections 7 and 8 of \citet{Anisotropic} and Section 6 of \citet{yang2019spiked}, we only describe the main ideas without writing down all the details.

We define the following continuous sequence of interpolating matrices between $Z^{\text{Gauss}}$ and $Z$. 

\begin{definition}[Interpolation]
We denote $Z^0:=Z^{\text{Gauss}}$ and $Z^1:=Z$. Let $\rho_{\mu i}^0$ and $\rho_{\mu i}^1$ be the laws of $Z_{\mu i}^0$ and $Z_{\mu i}^1$, respectively, for $i\in \cal I_0$ and $\mu \in \cal I_1\cup \cal I_2$. For any $\theta\in [0,1]$, we define the interpolated law
$\rho_{\mu i}^\theta := (1-\theta)\rho_{\mu i}^0+\theta\rho_{\mu i}^1.$ We shall work on the probability space consisting of triples $(Z^0,Z^\theta, Z^1)$ of independent $n\times p$ random matrices, where the matrix $Z^\theta=(Z_{\mu i}^\theta)$ has law
\begin{equation}\label{law_interpol}
\prod_{i\in \mathcal I_0}\prod_{\mu\in \mathcal I_1\cup \cal I_2} \rho_{\mu i}^\theta(\dd Z_{\mu i}^\theta).
\end{equation}
For $\lambda \in \mathbb R$, $i\in \mathcal I_0$ and $\mu\in \mathcal I_1\cup \cal I_2$, we define the matrix $Z_{(\mu i)}^{\theta,\lambda}$ through
\[\left(Z_{(\mu i)}^{\theta,\lambda}\right)_{\nu j}:=\begin{cases}Z_{\mu i}^{\theta}, &\text{ if }(j,\nu)\ne (i,\mu)\\ \lambda, &\text{ if }(j,\nu)=(i,\mu)\end{cases},\]
that is, it replaces the $(\mu,i)$-th entry of $Z^\theta$ with $\lambda$.
\end{definition}

\begin{proof}[Proof of Lemma \ref{LEM_SMALL}]
We shall prove %
equation \eqref{aniso_law} through interpolation matrices $Z^\theta$ between $Z^0$ and $Z^1$. We have seen that equation \eqref{aniso_law} holds for $G(Z^0,z)$ by Proposition \ref{prop_diagonal}. %
Using equation (\ref{law_interpol}) and fundamental calculus, we get the following basic interpolation formula:
for differentiable $F:\mathbb R^{n \times p}\rightarrow \mathbb C$,
\begin{equation}\label{basic_interp}
\frac{\dd}{\dd\theta}\mathbb E F(Z^\theta)=\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left[\mathbb E F\left(Z^{\theta,Z_{\mu i}^1}_{(\mu i)}\right)-\mathbb E F\left(Z^{\theta,Z_{\mu i}^0}_{(\mu i)}\right)\right],
\end{equation}
 provided all the expectations exist.
We shall apply equation \eqref{basic_interp} to the function $F(Z):=F_{\bu\mathbf v}^s(Z,z)$ for any fixed $s\in 2\N$, where %
\begin{equation*}%
 F_{\bu\mathbf v}(Z,z):=\left|\mathbf u^\top \left(G (Z,z)-\Gi(z)\right)\mathbf v\right|.
\end{equation*}
The main part of the proof is to show the following self-consistent estimate for the right-hand side of equation (\ref{basic_interp}): for any fixed $s\in 2\N$, any constant $\e>0$ and all $\theta\in[0,1]$,
 \begin{equation}\label{lemm_comp_4}
  \sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left[\mathbb EF_{\bu\mathbf v}^s\left(Z^{\theta,Z_{\mu i}^1}_{(\mu i)},z\right)-\mathbb EF_{\bu\mathbf v}^s\left(Z^{\theta,Z_{\mu i}^0}_{(\mu i)},z\right)\right]\le (n^\e q)^{s}+C\E F_{\bu\mathbf v}^s\left(Z^{\theta},z\right) ,
 \end{equation}
 for some constant $C>0$. If equation \eqref{lemm_comp_4} holds, then combining equation \eqref{basic_interp} with  Gr\"onwall's inequality we obtain that for any fixed $s\in 2\N$ and constant $\e>0$, 
 $$\E\left|\bu^\top \left(G(Z^1,z)-\Pi(z)\right)\bv\right|^s  \lesssim (n^\e q)^{s}.$$
Finally applying Markov's inequality and noticing that $\e$ can be chosen arbitrarily small, we conclude equation \eqref{aniso_law}. 
Underlying the proof of the estimate (\ref{lemm_comp_4}) is an expansion approach, which is the same as the ones for Lemma 7.10 of \citet{Anisotropic} and Lemma 6.11 of \citet{yang2019spiked}. So we omit the details.
\end{proof}













\iffalse
This section is organized as follows. In Section \ref{sec tools}, we collect some basic estimates and resolvent identities that will be used in the proof of Lemma \ref{LEM_SMALL} and Proposition \ref{prop_diagonal}. Then in Section \ref{sec entry} we give the proof of Proposition \ref{prop_diagonal}, which concludes Lemma \ref{LEM_SMALL} when $Z^{(1)}$ and $Z^{(2)}$ have i.i.d. Gaussian entries. In Section \ref{sec_comparison}, we describe how to extend the result in Lemma \ref{LEM_SMALL} from the Gaussian case to the case where the entries of $Z^{(1)}$ and $Z^{(2)}$ are generally distributed. Finally, in Section \ref{sec contract}, we give the proof of Lemma \ref{lem_mbehaviorw} and Lemma \ref{lem_stabw}. In the proof, we always denote the spectral parameter by $z=E+\ii\eta$. 
\fi

Now it remains to prove Proposition \ref{prop_diagonal}, 
whose proof is based on the following entrywise local law. %
\begin{lemma}\label{prop_entry}
Under the assumptions of Proposition \ref{prop_diagonal}, %
the following estimate holds uniformly in $z\in \mathbf D$: %
\begin{equation}\label{entry_diagonal}
\max_{a,b\in \cal I}\left| G_{ab}(z)  - \Gi_{ab} (z) \right| \prec n^{-1/2}.
\end{equation} 
\end{lemma}


With Lemma \ref{prop_entry}, we can complete the proof of %
Proposition \ref{prop_diagonal}. %

\begin{proof}[Proof of Proposition \ref{prop_diagonal}]
With estimate (\ref{entry_diagonal}), one can use the polynomialization method in Section 5 of \citet{isotropic} to get the anisotropic local law (\ref{aniso_law}) with $q=n^{-1/2}$. The proof is exactly the same, except for some minor differences in notations. Hence we omit the details.
\end{proof}


\subsection{An Entrywise Local Law}\label{sec entry}
 
Finally, this subsection is devoted to the proof of Lemma \ref{prop_entry}. 
First, we claim the following a priori estimate on the resolvent $G(z)$ for $z\in \mathbf D$.

\begin{lemma}\label{lemm apri}
	In the setting of Lemma \ref{LEM_SMALL}, there exists a constant $C>0$ such that the following estimates hold uniformly in $z,z'\in \mathbf D$ with overwhelming probability:
\be\label{priorim}
\|G(z)\| \le C,%
\ee
and %
\be\label{priordiff} 
\left\|G  (z) - G(z')\right\| \le C|z-z'|.
\ee
\end{lemma}
\begin{proof}
 Our proof is a simple application of the spectral decomposition of $G$. Recall the matrix $\AF$ defined in equation \eqref{defn AF}. Let
\be\label{SVDW}
\AF= \sum_{k = 1}^{p} {\sqrt {\mu_k} \xi_k } \zeta _{k}^\top ,\quad \mu_1\ge \mu_2 \ge \cdots \ge \mu_{p} \ge 0 =\mu_{p+1} = \ldots = \mu_{n},\ee
be a singular value decomposition of $A$, where
$\{\xi_{k}\}_{k=1}^{p}$ are the left-singular vectors and $\{\zeta_{k}\}_{k=1}^{n}$ are the right-singular vectors.
Then using equation \eqref{green2}, we get that for $i,j\in \mathcal I_1$ and $\mu,\nu\in \mathcal I_1\cup \cal I_2$,
\be\label{spectral}
\begin{split}
& G_{ij} = \sum_{k = 1}^{p} \frac{\xi_k(i) \xi_k^\top(j)}{\mu_k-z}, \ \ G_{\mu\nu} =
z\sum_{k = 1}^{n} \frac{\zeta_k(\mu) \zeta_k^\top(\nu)}{\mu_k-z} , \ \ G_{i\mu} = G_{\mu i}= \sum_{k = 1}^{p} \frac{\sqrt{\mu_k}\xi_k(i) \zeta_k^\top(\mu)}{\mu_k-z}.
\end{split}
\ee
By Fact \ref{fact_minv} (ii), we have that with overwhelming probability
$\mu_p \ge \lambda_p(n^{-1}(Z^{(2)})^\top Z^{(2)}) \ge c_\tau $ for some constant $c_\tau>0$ depending only on $\tau$.  \HZ{to check} This further implies that
$$ \inf_{z\in \mathbf D}\min_{1\le k \le p}|\mu_k-z| \ge c_\tau - (\log n)^{-1}.$$
Combining this bound with equation \eqref{spectral}, we can easily conclude the estimates \eqref{priorim} and \eqref{priordiff}.
\end{proof}





















For the rest of this subsection, we present the proof of Lemma \ref{prop_entry}, which is the most technical part of the whole proof of Lemma \ref{LEM_SMALL}. 

\begin{proof}[Proof of Lemma \ref{prop_entry}]
Recall that under the assumptions of Lemma \ref{prop_entry},  we have %
\be\label{diagW}\AF \stackrel{d}{=} n^{-1/2}[\Lambda (Z^{(1)})^{\top}, (Z^{(2)})^\top],\ee
and it suffices to consider the resolvent in equation \eqref{resolv Gauss1} throughout the whole proof. 
The proof is divided into three steps. For simplicity, we introduce the following notation: for two (deterministic or random) nonnegative quantities $\xi$ and $\zeta$, 
we write $\xi\sim \zeta$ if $\xi\lesssim \zeta$ and $\zeta\lesssim \xi$.

\vspace{5pt}

\noindent{\bf Step 1: Large deviation estimates.} In this step, we prove some (almost) optimal large deviation estimates on the off-diagonal entries of $G$, and on the following $\cal Z$ variables. In analogy to Section 3 of \citet{EKYY1} and Section 5 of \citet{Anisotropic}, we introduce the $\cal Z$ variables  
\begin{equation*}
 \cal  Z_{{a}} :=(1-\mathbb E_{{a}})\left[\big(G_{{a}{a}}\big)^{-1}\right], %
\end{equation*}
where $\mathbb E_{{a}}[\cdot]:=\mathbb E[\cdot\mid H^{({a})}]$ denotes the partial expectation over the entries in the ${a}$-th row and column of $H$. Now using equation (\ref{resolvent2}), we get that for $i \in \cal I_0$, 
\begin{align}
\cal Z_i = & \frac{\lambda_i^2}{n} \sum_{\mu ,\nu\in \mathcal I_1}  G^{(i)}_{\mu\nu} \left(\delta_{\mu\nu} - Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}\right)+\frac1n \sum_{\mu ,\nu\in \mathcal I_2}  G^{(i)}_{\mu\nu} \left( \delta_{\mu\nu} - Z^{(2)}_{\mu i}Z^{(2)}_{\nu i}\right) \nonumber\\
& - 2 \frac{\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu\nu},  \label{Zi}
\end{align}
and for $\mu\in \cal I_1$ and $\nu\in \cal I_2$, 
\begin{align}
&\cal  Z_\mu= \frac{1}{n} \sum_{i,j \in \mathcal I_0}  {\lambda_i \lambda_j}G^{(\mu)}_{ij} \left(\delta_{ij} - Z^{(1)}_{\mu i}Z^{(1)}_{\mu j}\right), \quad \cal Z_\nu = \frac{1}{n} \sum_{i,j \in \mathcal I_0} G^{(\nu)}_{ij} \left( \delta_{ij} - Z^{(2)}_{\nu i}Z^{(2)}_{\nu j}\right).\label{Zmu} 
\end{align}
Moreover, we introduce the random error
\begin{equation}  \label{eqn_randomerror}
 \Lambda _o : = %
 \max_{{a} \ne {b} } \left|  G_{{a}{a}}^{-1}G_{{a}{b}}   \right| ,
\end{equation}
which controls the size of the off-diagonal entries. The following lemma gives the desired large deviation estimate on $\Lambda_o$ and $\cal Z$ variables.

\begin{lemma}\label{Z_lemma}
Under the assumptions of Proposition \ref{prop_diagonal}, the following estimate holds uniformly in all $z\in \mathbf D$:
\begin{align}
\Lambda_o + \max_{{a}\in \cal I} |\cal Z_{{a}}|  \prec n^{-1/2}. \label{Zestimate1}
\end{align}
\end{lemma}
\begin{proof}
 Note that for any ${a}\in \cal I$, $H^{({a})}$ and $G^{({a})}$ also satisfies the assumptions in Lemma \ref{lemm apri}. Hence equations \eqref{priorim} and \eqref{priordiff} also hold for $G^{({a})}$ with overwhelming probability. Now applying equations \eqref{eq largedev1} and \eqref{eq largedev2} to equation (\ref{Zi}), %
we get that for any $i\in \cal I_0$, %
\begin{equation}\nonumber%
\begin{split}
\left| \cal Z_{i}\right|&\lesssim \frac{1}{n} \left|\sum_{\mu ,\nu\in \mathcal I_1}  G^{(i)}_{\mu\nu} \left(\delta_{\mu\nu} - Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}\right)\right|+\frac1n \left|\sum_{\mu ,\nu\in \mathcal I_2}  G^{(i)}_{\mu\nu} \left( \delta_{\mu\nu} - Z^{(2)}_{\mu i}Z^{(2)}_{\nu i}\right)\right| + \frac{1}{n} \left|\sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu\nu}\right| \\
&\prec  \frac{1}{n} \left( \sum_{\mu,\nu \in \cal I_1\cup \cal I_2 }  {| G_{\mu\nu}^{(i)}|^2 }  \right)^{1/2} \prec n^{-1/2} .
\end{split}
\end{equation}
Here in the last step we used equation \eqref{priorim} to get that for any $\mu\in \cal I_1\cup \cal I_2$,
\be\label{GG*}\sum_{\nu \in \cal I_1\cup \cal I_2 }  | G_{\mu\nu}^{(i)} |^2\le \sum_{{a} \in \cal I } | G_{\mu{a}}^{(i)} |^2 =\left[G^{(i)}(G^{(i)})^* \right]_{\mu\mu} =\OO(1),\quad \text{with overwhelming probability,}\ee
where $(G^{(i)})^*$ denotes the complex conjugate transpose of $G^{(i)}$. Similarly, applying equations \eqref{eq largedev1} and \eqref{eq largedev2} to $\cal Z_{\mu}$ and $\cal Z_\nu$ in equation (\ref{Zmu}) and using equation \eqref{priorim}, we can obtain the same bound.

Next we prove the off-diagonal estimate on $\Lambda_o$. For $i\in \mathcal I_1$ and ${a}\in \cal I\setminus \{i\}$, using equations \eqref{resolvent3}, \eqref{eq largedev1} and \eqref{priorim}, we can obtain that 
\begin{align*}
  \left|G_{ii}^{-1}G_{i{a}}\right| &\le {n^{-1/2}}\left|  {\lambda_i}\sum_{\mu \in \cal I_1} Z^{(1)}_{\mu i} G^{(i)}_{\mu a}(z)\right| + n^{-1/2}\left|\sum_{\mu \in \cal I_2} Z^{(2)}_{\mu i} G^{(i)}_{\mu a}(z)  \right| \\
& \prec  n^{-1/2}\left( \sum_{\mu \in \cal I_1\cup \cal I_2}  {| G_{\mu {a}}^{(i)} |^2 }  \right)^{1/2} \prec n^{-1/2}. 
 \end{align*}
 We can get the same estimate for $\left|G_{\mu\mu}^{-1} G_{\mu{b}} \right|$, $\mu \in \mathcal I_1\cup \cal I_2$ and ${b}\in \cal I\setminus \{ \mu\}$, using a similar argument.  
Thus we obtain that $\Lambda_o\prec n^{-1/2}$. %
\end{proof}

Note that combining $\max_a|G_{aa}|=\OO(1)$ by equation \eqref{priorim} with equation \eqref{Zestimate1}, we immediately conclude equation \eqref{entry_diagonal} for the off-diagonal entries with ${a}\ne {b}$.



\vspace{5pt}

\noindent{\bf Step 2: Self-consistent equations.} 
In this step, we derive the approximate self-consistent equations in \eqref{approximate m1m2} satisfied by $m_1(z)$ and $m_2(z)$ with more precise error rates. %
More precisely, we will show that $(m_1(z),m_2(z))$ satisfies equation \eqref{selfomegaerror} for some small errors satisfying $|\cal E_{1}|+ |\cal E_{2}|\prec n^{-1/2}$. Later in Step 3, we will apply Lemma \ref{lem_stabw} to show that $(m_1(z),m_2(z))$ is close to $(M_{1}(z),M_{2}(z))$. %

We define the following $z$-dependent event 
\be\label{Xiz}\Xi(z):=\left\{ |m_{1}(z)-M_{1}(z)| + |m_{2}(z)-M_{2}(z)| \le (\log n)^{-1/2}\right\}.\ee
Note that by equation \eqref{Lipomega}, we have that for $z\in \mathbf D$ the following estimates hold:
$$|M_{1}(z)-M_1(0)|=|M_{1}(z)+r_1^{-1}a_1|\lesssim (\log n)^{-1},\quad |M_{2}(z)+M_2(0)|=|M_{2}+r_2^{-1}a_2|\lesssim (\log n)^{-1}.$$
Together with the estimates in equation \eqref{a23} and the assumption that the singular values $\lambda_i$ are bounded, we obtain the following estimates
\be\label{Gsim1}
 |M_{1}| \sim |M_{2}| \sim 1, \quad |z+\lambda_i^2 r_1M_{1} + r_2 M_{2}|\sim 1,\quad \text{uniformly in $z\in \mathbf D$}  . \ee
 Moreover, using equation \eqref{selfomega} we get
 \be\label{Gsim0}
 \quad \left|1 + \gamma_n M(z)\right| = |M_2^{-1}(z)| \sim 1, \quad  |1+\gamma_n M_{0}(z)| = |M_1^{-1}(z)| \sim 1  ,
\ee
 uniformly in $z\in \mathbf D$, where we abbreviated
 \be\label{defn mc1c}M(z):=-\frac1p\sum_{i=1}^p\frac{1}{z+\lambda_i^2 r_1M_{1}(z) +r_2M_{2}(z)},\quad M_{0}(z):=-\frac1p\sum_{i=1}^p\frac{\lambda_i^2}{z+\lambda_i^2 r_1M_{1}(z) +r_2M_{2}(z)}. \ee
In fact, $M(z)$ and $M_0(z)$ are the asymptotic limits of $m(z)$ and $m_0(z)$, respectively. Plugging equation \eqref{Gsim1} into equation \eqref{defn_piw}, we get that
\be
|\Gi_{{a}{a}}(z)| \sim 1 \ \ \text{uniformly in } z\in \mathbf D \ \text{ and } \ {a}\in \cal I.
\ee 
Then we prove the following key lemma, which shows that $(m_1(z),m_2(z))$ satisfies equation \eqref{selfomegaerror} with some small errors $\cal E_{1}$ and $\cal E_{2}$.

\begin{lemma}\label{lemm_selfcons_weak}
Under the assumptions of Proposition \ref{prop_diagonal}, the following estimates hold uniformly in $z \in \mathbf D$: 
\begin{equation} \label{selfcons_lemm}
{\mathbf 1}(\Xi) \left|\frac{1}{m_{1}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2r_1m_{1} + r_2m_{2}  } \right|\prec n^{-1/2},\ee
and
\begin{equation} \label{selfcons_lemm2}
  {\mathbf 1}(\Xi)\left|\frac{1}{m_{2}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1m_{1} +  r_2m_{2}  }\right|\prec n^{-1/2}.
\ee
\end{lemma}

\begin{proof}
By equations (\ref{resolvent2}), (\ref{Zi}) and (\ref{Zmu}), we obtain that %
\begin{align}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu\in \mathcal I_1} G^{\left( i \right)}_{\mu\mu}- \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} + \cal Z_i =  - z - \lambda_i^2 r_1m_1 - r_2m_2 + \cal E_i, \quad \text{for }i \in \cal I_0, \label{self_Gii}\\
\frac{1}{{G_{\mu\mu} }}&=  - 1 - \frac{1}{n} \sum_{i\in \mathcal I_0}\lambda_i^2 G^{\left(\mu\right)}_{ii}+ \cal Z_{\mu} =  - 1  - \gamma_n m_0 + \cal E_\mu, \quad \text{for }\mu \in \cal I_1,  \label{self_Gmu1}\\
\frac{1}{{G_{\nu\nu} }}&=  - 1 - \frac{1}{n} \sum_{i\in \mathcal I_0} G^{\left(\nu\right)}_{ii}+\cal Z_{\nu} =   - 1 - \gamma_n m + \cal E_\nu, \quad \text{for }\nu \in \cal I_2, \label{self_Gmu2}
\end{align}
where we denoted (recall equation \eqref{defm} and Definition \ref{defn_Minor}) %
$$\cal E_i :=\cal Z_i + \lambda_i^2 r_1\left(m_1 - m_1^{(i)}\right) + r_2\left(m_2-m_2^{(i)}\right) ,$$
and
$$\cal E_\mu :=\cal Z_{\mu} + \gamma_n(m_0-m_0^{(\mu)}),\quad \cal E_\nu:=\cal Z_{\nu} +\gamma_n(m-m^{(\nu)}) .$$
Using equations (\ref{resolvent8}), \eqref{eqn_randomerror} and (\ref{Zestimate1}), we can bound that  
\begin{equation}\nonumber
  |m_1 - m_1^{(i)}| \le   \frac1{n_1}\sum_{\mu\in \mathcal I_1}  \left|\frac{G_{\mu i} G_{i\mu}}{G_{ii}}\right| \le |\Lambda_o|^2|G_{ii}| \prec n^{-1}.
\end{equation}
where we also used bound \eqref{priorim} in the last step. Similarly, we also have that 
\be \nonumber%
 |m_2 - m_2^{(i)}| \prec n^{-1} , \quad |m_0 - m_0^{(\mu)}| \prec n^{-1},\quad  |m-m^{(\nu)}| \prec n^{-1},  \ee
for any $i\in \cal I_0$, $\mu \in \cal I_{1}$ and $\nu\in \cal I_2$. Together with equation (\ref{Zestimate1}), we obtain the bound %
\begin{equation}\label{erri}
\max_{i\in \cal I_0} |\cal E_i | +\max_{\mu \in \cal I_1\cup \cal I_2} |\cal E_\mu|  \prec n^{-1/2}.
\end{equation}

 With equation \eqref{Gsim1} and the definition of the event $\Xi$ in \eqref{Xiz}, we get that 
 $$\mathbf 1(\Xi)|z + \lambda_i^2 r_1m_1+r_2m_2|\sim1.$$ 
 Combining it with equations (\ref{self_Gii}) and \eqref{erri}, we obtain that
\be\label{Gmumu0}
\mathbf 1(\Xi)G_{ii}=\mathbf 1(\Xi)\left[-\frac{1}{z + \lambda_i^2 r_1 m_1+r_2 m_2} +\OO_\prec\left(n^{-1/2}\right)\right].
\ee
Plugging \eqref{Gmumu0} into the definitions of $m$ and $m_0$ in equation \eqref{defm}, we get
\begin{align}
\mathbf 1(\Xi)m&=\mathbf 1(\Xi)\left[-\frac1p\sum_{i\in \cal I_0}\frac{1}{z + \lambda_i^2 r_1 m_1+r_2 m_2}  +\OO_\prec\left(n^{-1/2}\right)\right],\label{Gmumu} \\
\mathbf 1(\Xi)m_0&=\mathbf 1(\Xi)\left[-\frac1p\sum_{i\in \cal I_0}\frac{\lambda_i^2}{z + \lambda_i^2 r_1 m_1+r_2 m_2}   +\OO_\prec\left(n^{-1/2}\right)\right]. \label{Gmumu2}
\end{align}
As a byproduct, we obtain from these two equations and equation \eqref{defn mc1c} that  
\be\label{Gsim11}
 |m(z)-M(z)| +|m_0(z)-M_{0}(z)|  \lesssim (\log n)^{-1/2}, \quad \text{with overwhelming probability on } \Xi. 
\ee
Together with equation \eqref{Gsim0}, we get that %
\be\label{Gsim2}
|1+\gamma_nm (z)|\sim 1, \quad |1+\gamma_nm_0(z)|\sim 1, \quad \text{with overwhelming probability on } \Xi.
\ee
Now combining equations \eqref{self_Gmu1}, \eqref{self_Gmu2}, \eqref{erri} and  \eqref{Gsim2}, we obtain that for $\mu \in \cal I_1$ and $\nu \in \cal I_2,$ %
\begin{align}\label{Gii0} 
&\mathbf 1(\Xi)\left(G_{\mu\mu}+\frac{1}{1 + \gamma_nm_0}\right)  \prec n^{-1/2}  , \quad \mathbf 1(\Xi)\left(G_{\nu\nu}+\frac{1}{1 + \gamma_nm}\right) \prec n^{-1/2}  .
\end{align}
Taking average over $\mu\in \cal I_1$ and $\nu\in \cal I_2$, we get that %
\begin{align}\label{Gii000}
& \mathbf 1(\Xi)\left(m_1+\frac{1}{1 + \gamma_n m_0}\right) \prec n^{-1/2}  ,\quad  \mathbf 1(\Xi)\left(m_2+\frac{1}{1 +\gamma_n  m}\right) \prec n^{-1/2}  ,
\end{align}
which further implies
\be\label{Gii}
 \mathbf 1(\Xi)\left(\frac{1}{m_1} + 1 + \gamma_nm_0\right) \prec  n^{-1/2} ,\quad \mathbf 1(\Xi)\left(\frac{1}{m_2} + 1 + \gamma_nm\right) \prec  n^{-1/2} .
\ee
Finally, plugging equations \eqref{Gmumu} and \eqref{Gmumu2} into equation \eqref{Gii}, we conclude equations (\ref{selfcons_lemm}) and (\ref{selfcons_lemm2}). 
\end{proof}



\noindent{\bf Step 3: $\Xi$ holds with overwhelming probability.} In this step, we show that the event $\Xi(z)$ in \eqref{Xiz} actually holds with overwhelming probability for all $z\in \mathbf D$. Once we have proved this fact, applying Lemma \ref{lem_stabw} to equations \eqref{selfcons_lemm} and  \eqref{selfcons_lemm2} immediately shows that $(m_1(z),m_2(z))$ is close to $(M_{1}(z),M_{2}(z))$ up to an error of order $\OO_\prec(n^{-1/2})$. 

We claim that it suffices to show that
\be\label{Xiz0}
|m_{1}(0)-M_{1}(0)| + |m_{2}(0)-M_{2}(0)| \prec n^{-1/2}.
\ee
 In fact, notice that by equations \eqref{Lipomega} and \eqref{priordiff} we have
$$ |M_{1}(z)-M_{1}(0)|+|M_2(z)-M_2(0)|=\OO((\log n)^{-1}),\quad  |m_{1}(z)-m_{1}(0)|+ |m_{2}(z)-m_{2}(0)|=\OO((\log n)^{-1}),$$ 
with overwhelming probability for all $z\in \mathbf D$. Thus if equation \eqref{Xiz0} holds, we can obtain that 
\be\label{roughh1} 
\sup_{z\in \mathbf D} \left(|m_{1}(z)-M_{1}(z)| + |m_{2}(z)-M_{2}(z)|\right)  \lesssim (\log n)^{-1} \quad \text{with overwhelming probability},\ee
and %
\be\label{roughh2} \sup_{z\in \mathbf D} \left( |m_{1}(z)-M_{1}(0)|+ |m_{2}(z)-M_{2}(0)|\right) \lesssim (\log n)^{-1} \quad \text{with overwhelming probability}.\ee
The equation \eqref{roughh1} shows that $\Xi$ holds with overwhelming probability, while the equation \eqref{roughh2} verifies the condition \eqref{prior12} of Lemma \ref{lem_stabw}. Now applying Lemma \ref{lem_stabw} to equations \eqref{selfcons_lemm} and \eqref{selfcons_lemm2}, we obtain that
\be\label{Xizz}
|m_{1}(z)-M_{1}(z)|+ |m_{2}(z)-M_2(z)| \prec n^{-1/2} 
\ee
uniformly for all $z\in \mathbf D$. Together with equations \eqref{Gii0} and \eqref{Gii000}, equation \eqref{Xizz} implies that
\be\label{Xizz2} \max_{\mu\in \cal I_1} |G_{\mu\mu}(z)-M_{1}(z)|+ \max_{\nu\in \cal I_2} |G_{\nu\nu}(z)-M_{2}(z)|\prec n^{-1/2}.
\ee
Then plugging estimate \eqref{Xizz} into equation \eqref{Gmumu0} and recalling \eqref{M1M2a1a2}, we obtain that 
$$\max_{{i}\in \cal I_1}|G_{ii}(z)-\Gi_{ii}(z)| \prec n^{-1/2}. 
$$
Together with equation \eqref{Xizz2}, it gives the diagonal estimate
\be\label{diagest}
\max_{{a}\in \cal I}|G_{{a}{a}}(z)-\Pi_{{a}{a}}(z)| \prec n^{-1/2}. 
\ee
Combining equation \eqref{diagest} with the off-diagonal estimate on $\Lambda_o$ in equation \eqref{Zestimate1}, we conclude the proof of Lemma \ref{prop_entry}. 

Finally, we give the proof of equation \eqref{Xiz0}.
By equation \eqref{spectral}, we have that with overwhelming probability,
$$ m(0)=\frac1p\sum_{i\in \cal I_0}G_{ii}(0) = \frac1p\sum_{i\in \cal I_0}\sum_{k = 1}^{p} \frac{|\xi_k(i)|^2 }{\mu_k} \ge \mu_1^{-1} \gtrsim 1,$$
where we used Fact \ref{fact_minv} in the last step to bound  $\mu_1 \ge \lambda_1(n^{-1}(Z^{(2)})^\top Z^{(2)}) \gtrsim 1$ with overwhelming probability. \HZ{to check}
Similarly, we can also get that $m_0(0)$ is positive and has size $m_0(0)\sim 1$. Hence we have the estimates
\be\label{add_1+m}1+\gamma_n m(0)\sim 1,\quad 1+\gamma_n m_0(0)\sim 1.\ee
Combining these estimates with equations \eqref{self_Gmu1}, \eqref{self_Gmu2} and  \eqref{erri}, we obtain that equation \eqref{Gii000} holds at $z=0$ even without the indicator function $\mathbf 1(\Xi)$. Furthermore, we have that with overwhelming probability,
$$  \left|\lambda_i^2 r_1m_1(0)+r_2m_2(0)\right|=\left|\frac{\lambda_i^2 r_1}{ 1+\gamma_n m_0(0)} +\frac{r_2}{1+\gamma_n m(0)}+ \OO_\prec (n^{-1/2})\right| \sim 1 .$$
 Then combining this estimate with equations (\ref{self_Gii}) and \eqref{erri}, we obtain that equations \eqref{Gmumu} and \eqref{Gmumu2} also hold at $z=0$ even without the indicator function $\mathbf 1(\Xi)$. Finally, plugging equations \eqref{Gmumu} and \eqref{Gmumu2} into equation \eqref{Gii}, we conclude that equations \eqref{selfcons_lemm} and  \eqref{selfcons_lemm2}  hold at $z=0$, that is,
\begin{equation} \label{selfcons_lemma222}
\begin{split}
& \left|\frac{1}{m_{1}(0)} + 1 -\frac1n\sum_{i=1}^p \frac{\lambda_i^2}{ \lambda_i^2r_1m_{1}(0) + r_2m_{2}(0)  } \right|\prec n^{-1/2},\\ 
&\left|\frac{1}{m_{2}(0)} + 1 -\frac1n\sum_{i=1}^p \frac{1 }{ \lambda_i^2 r_1m_{1}(0) + r_2 m_{2}(0)  }\right|\prec n^{-1/2}.
\end{split}
\ee

Denoting $y_{1}=-m_{1}(0)$ and $y_{2}=-m_{2}(0)$, by equation \eqref{Gii000} we have
$$y_1= \frac{1}{1+\gamma_n m_{0}(0)} +\OO_\prec(n^{-1/2}),\quad y_2= \frac{1}{1+\gamma_n m(0)}+\OO_\prec(n^{-1/2}).$$ 
Hence by \eqref{add_1+m}, there exists a constant $c>0$ such that 
\be\label{omega12} c \le y_1 \le 1, \quad  c\le y_2\le 1, \quad \text{with overwhelming probability}.\ee
Also one can verify from equation \eqref{selfcons_lemma222} that $(r_1y_1,r_2y_2)$ satisfies approximately the same system of equations as equation \eqref{eq_a12extra}:
\be\label{selfcons_lemm000}
r_1y_1+r_2 y_2 = 1-\gamma_n + \OO_\prec (n^{-1/2}),\quad r_1^{-1}f(r_1y_1)=1 + \OO_\prec (n^{-1/2}),
\ee
where recall that the function $f$ was defined in equation \eqref{fa1}. The first equation of \eqref{selfcons_lemm000} and equation \eqref{omega12} together imply that $y_1 \in [0,r_1^{-1}(1-\gamma_n)]$ with overwhelming probability. For the second equation of \eqref{selfcons_lemm000}, we know that $y_1=r_1^{-1}a_1$ is a solution. Moreover, it is easy to check that the function $g(y_1):= r_1^{-1}f(r_1y_1)$ is strictly increasing and has bounded derivative on $[0,r_1^{-1}(1-\gamma_n)]$. So by basic calculus, %
we obtain that 
$$|m_1(0)-M_1(0)|=|y_1-r_1^{-1}a_1|\prec n^{-1/2}.$$ 
Plugging it into the first equation of equation \eqref{selfcons_lemm000}, we get 
$$|m_2(0)-M_2(0)|=|y_2-r_2^{-1}a_2|\prec n^{-1/2}.$$ The above two estimates conclude equation \eqref{Xiz0}.
\end{proof}









\iffalse
In random matrix theory, it is much more convenient to consider matrices $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ with an extra $n^{-1/2}$ factor, where we denote $n:=n_1+n_2$.
The advantage of this scaling is that the singular eigenvalues of $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ all lie in a bounded support that does not grow with $n$. For reader's convenience, we first state the exact properties (some of which have already been stated in words in Section \ref{sec_prelim}) that we shall need for the results and proofs in this section. %


We have assumed that $Z^{(1)}$ and $Z^{(2)}$ are $n_1\times p$ and $n_2\times p$ random matrices with i.i.d. entries satisfying
\begin{equation}\label{assm1}
\mathbb{E} (Z^{(1)})_{11} =\mathbb{E} (Z^{(2)})_{11} =0, \ \quad \ \mathbb{E} \vert (Z^{(1)})_{11} \vert^2=\mathbb{E} \vert (Z^{(2)})_{11} \vert^2  =1.
\end{equation}
Recall that we have defined $\rho_1= n_1/p$ and $\rho_2=n_2/p$ in introduction. We assume that they satisfy
\be\label{assm2}
0\le \rho_1 \le \tau^{-1}, \quad 1+\tau \le \rho_{2} \le \tau^{-1},
\ee
for a small constant $0<\tau<1$.
\fi


 \iffalse
We assume that $\Sig_1$ and $\Sig_2$ have eigendecompositions
\be\label{eigen}\Sig_1= Q_1\Lambda_1 Q_1^\top, \ \ \Sig_2= Q_2\Lambda_2 Q_2^\top,\ \ \Lambda_1=\text{diag}(\lambda_1^{(1)}, \ldots, \lambda^{(1)}_p), \ \ \Lambda_2=\text{diag}( \lambda^{(2)}_1, \ldots, \lambda^{(2)}_p),
\ee
where the eigenvalues satisfy that
\begin{equation}\label{assm3}
\tau \le \lambda^{(1)}_p \le \ldots \le \lambda^{(1)}_2  \le \lambda^{(1)}_1 \le \tau^{-1}, \quad \tau \le   \lambda^{(2)}_p \le \ldots \le \lambda^{(2)}_2 \le \lambda^{(2)}_1 \le \tau^{-1}.
\ee


We assume that $M=\Sig_1^{1/2} \Sig_2^{-1/2}$ has a singular value decomposition \eqref{eigen2} .....
\be%
M= U\Lambda V^\top, \quad \Lambda=\text{diag}( \lambda_1, \ldots, \lambda_p),
\ee
where by equation \eqref{assm3} we have %
\begin{equation}\label{assm32}
\tau \le \lambda_p \le \lambda_1 \le \tau^{-1} .%
\end{equation}
\fi




\iffalse
Before entering into the formal proof, we state two preparation lemmas, both of which have been used crucially in some previous proofs. First, for $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ with bounded support $q$, we have the following estimates on their singular values.




\begin{lemma}\label{SxxSyy}
Suppose Assumption \ref{assm_big1} holds. %
Then for any constant $\e>0$, we have that with overwhelming probability,
\be\label{op rough1} %
\lambda_1\left((Z^{(1)})^{\top} Z^{(1)}\right) \le {(\sqrt{n_1}+\sqrt{p})^2} + n_1^{1+\e} q,
\ee
and
\be\label{op rough2}
 (\sqrt{n_2}-\sqrt{p})^2  -  n_2^{1+\e} q \le  \lambda_p \left((Z^{(2)})^\top Z^{(2)}\right)  \le  \lambda_1\left((Z^{(2)})^\top Z^{(2)}\right) \le  (\sqrt{n_2}+\sqrt{p})^2 +  n_2^{1+\e} q .
\ee
where $\lambda_i(Z_k^\top Z_k)$, $k=1,2$ and $i=1,\cdots,p$, is the $i$-th largest eigenvalue of $Z_k^\top Z_k$.
\end{lemma}
\begin{proof}
This lemma is a corollary of  \cite[Theorem 2.10]{isotropic} and \cite[Lemma 3.12]{DY}.
\end{proof}
\begin{remark}
it is well-known that the eigenvalues of $n_2^{-1}(Z^{(2)})^\top Z^{(2)}$ are all inside the support of the Marchenko-Pastur law $[(1-\sqrt{p/n_2})^2-\oo(1) ,(1+\sqrt{p/n_2})^2+\oo(1)]$ with probability $1-\oo(1)$ \cite{No_outside}. Here the estimate \eqref{op rough2} has improved the error to $n_2^\e q$ and the probability to $1-\OO(n^{-D})$.
\end{remark}
\fi




\iffalse
\subsection{Resolvents and Local Laws}\label{sec locallaw1}


Our main goal is to study the matrix inverse $((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}$ for $X^{(1)}= Z^{(1)}\Sigma_1^{1/2}$ and $X^{(2)}= Z^{(2)}\Sigma_2^{1/2}$.
Using equation \eqref{eigen2}, we can rewrite it as
\be\label{eigen2extra}((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}= \Sigma_2^{-1/2}V\left(   \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^{-1}V^\top\Sigma_2^{-1/2}.\ee
For our purpose, we introduce a convenient self-adjoint linearization trick,
which has been proved to be useful in studying the local laws of random matrices of the Gram type \cite{Anisotropic, AEK_Gram, XYY_circular}. We define the following $(p+n)\times (p+n)$ self-adjoint block matrix, which is a linear function of $Z^{(1)}$ and $Z^{(2)}$:
 \begin{equation}\label{linearize_block}
   H \equiv H(Z^{(1)},Z^{(2)}): = n^{-1/2}\left( {\begin{array}{*{20}c}
   { 0 } & \Lambda U^{\top}(Z^{(1)})^{\top} & V^\top (Z^{(2)})^\top  \\
   {Z^{(1)} U\Lambda  } & {0} & 0 \\
   {Z^{(2)}V} & 0 & 0
   \end{array}} \right).
 \end{equation}
For simplicity of notations, we define the index sets
$$\cal I_0:=\llbracket 1,p\rrbracket, \quad  \cal I_1:=\llbracket p+1,p+n_1\rrbracket, \quad \cal I_2:=\llbracket p+n_1+1,p+n_1+n_2\rrbracket ,\quad \cal I:=\cal I_0\cup \cal I_1\cup \cal I_2  .$$
 We will consistently use the latin letters $i,j\in\sI_{0}$, greek letters $\mu,\nu\in\sI_{1}\cup \sI_{2}$, and ${a},{b},\mathfrak c\in \cal I$. Correspondingly, the indices of the matrices $Z^{(1)}$ and $Z^{(2)}$ are labelled as
 \be\label{labelZ}
 Z^{(1)}= (z_{\mu i}:i\in \mathcal I_0, \mu \in \mathcal I_1), \quad Z^{(2)}= (z_{\nu i}:i\in \mathcal I_0, \nu \in \mathcal I_2).\ee
Now we define the resolvents as follows.
\begin{definition}[Resolvents]
We define the resolvent (or Green's function) of $H$ as
 \begin{equation}\label{eqn_defG}
 G \equiv G (Z^{(1)},Z^{(2)},z):= \left[H(Z^{(1)},Z^{(2)})-\left( {\begin{array}{*{20}c}
   { z\id_{p}} & 0 & 0 \\
   0 & { \id_{n_1}}  & 0\\
      0 & 0  & { \id_{n_2}}\\
\end{array}} \right)\right]^{-1} , \quad z\in \mathbb C .
 \end{equation}
and the resolvent of $  n^{-1}\Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + n^{-1}V^\top (Z^{(2)})^\top Z^{(2)}V$ as
\be\label{mainG}
\cal G(z):=\left( n^{-1}  \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + n^{-1}V^\top (Z^{(2)})^\top Z^{(2)}V -z\right)^{-1},\quad z\in \C.
\ee
Moreover, we also define the following averaged resolvents, which are the (weighted) partial traces of $G$:
\be\label{defm}
\begin{split}
m(z) :=\frac1p\sum_{i=1}^p G_{ii}(z) ,\quad & m_0(z):=\frac1p\sum_{i=1}^p \lambda_i^2 G_{ii}(z),\\
 m_1(z):= \frac{1}{n_1}\sum_{\mu =p+1}^{p+n_1}G_{\mu\mu}(z) ,\quad & m_2(z):= \frac{1}{n_2}\sum_{\nu=p+n_1+1}^{p+n_1+n_2}G_{\nu\nu}(z) .
\end{split}
\ee
\end{definition}

Then we define the resolvent minors, which are defined by removing certain rows and columns of the matrix $H$.

\begin{definition}[Resolvent minors]\label{defn_Minor}
 For any $ (p+n)\times (p+n)$ matrix $\cal A$ and ${a} \in \mathcal I$, we define the minor $\cal A^{(\mathfrak c)}:=(\cal A_{{a}{b}}:{a},{b} \in \mathcal I\setminus \{\mathfrak a\})$ as the $ (p+n-1)\times (p+n-1)$ matrix obtained by removing the $\mathfrak c$-th row and column in $\cal A$. Note that we keep the names of indices when defining $\cal A^{(\mathfrak c)}$, i.e. $(\cal A^{(\mathfrak c)})_{{a}{b}}= \cal A_{{a}{b}}$ for ${a},{b} \ne \mathfrak c$. Correspondingly, we define the resolvent minor as %
\begin{align*}
G^{(\mathfrak c)}:&=\left[\left(H - \left( {\begin{array}{*{20}c}
   { zI_{p}} & 0  \\
   0 & { I_{n}}  \\
\end{array}} \right)\right)^{(\mathfrak c)}\right]^{-1} ,
\end{align*}
and define the partial traces $m^{(\mathfrak c)}$ and $m^{(\mathfrak c)}_k$, $k=0,1,2,$ by replacing $G$ with $G^{(\mathfrak c)}$ in equation \eqref{defm}. For convenience, we will adopt the convention that for the resolvent minor $G^{(\mathfrak c)}$ defined as above, $G^{(\mathfrak c)}_{{a}{b}} = 0$ if ${a} =\mathfrak c$ or ${b} =\mathfrak c$.
\end{definition}

Note that the resolvent minor $G^{(\mathfrak c)}$ is defined such that it is independent of the entries in the $\mathfrak c$-th row and column of $H$. One will see a crucial use of this fact in the heuristic proof below.

Notice that using equation \eqref{mainG}, we can write  equation \eqref{eigen2extra} as
\be\label{rewrite X as R} ((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}=n^{-1}\Sigma_2^{-1/2}V\cal G(0)V^\top\Sigma_2^{-1/2}.\ee
In the above definition, we have taken the argument of $\cal G$ to be a general complex number, because we will need to use $\cal G'(0)$ in the proof of Lemma \ref{lem_cov_derivative}, which requires a good estimate of $\cal G(z)$ for $z$ around the origin. Moreover, by Schur complement formula, we can obtain that
 \begin{equation} \label{green2}
 G (z):=  \left( {\begin{array}{*{20}c}
   { \cal G (z)} & \cal G(z)W  \\
   W^\top \cal G(z) & z \cal G_R
\end{array}} \right)^{-1},\quad \cal G_R:=(W^\top W - z)^{-1} ,
 \end{equation}
 where we abbreviated $W:=n^{-1/2}(\Lambda U^\top (Z^{(1)})^{\top}, V^\top (Z^{(2)})^\top)$, and
the subindex of $\cal G_R$ means the lower-right block. This shows that a control of $G(z)$ yields directly a control of $\mathcal G(z)$. On the other hand, $G(z)$ is a little more easier to use than $\cal G(z)$ in the proof.





\subsubsection{Asymptotic Limit of the Resolvent}\label{sec aymp_limit_G}







We now describe the asymptotic limit of $G(z)$ and $\cal G(z)$ as $n\to \infty$. First we define the deterministic limits of $(m_1(z), m_{2}(z))$, denoted by $(M_{1}(z),M_{2}(z))$
as the unique solution to the following system of equations
\begin{equation}\label{selfomega}
\begin{split}
& \frac1{M_{1}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2 r_1 M_{1} +r_2 M_{2}  } - 1 ,\  \ \frac1{M_{2}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1 M_{1} +  r_2 M_{2}  }- 1 ,
\end{split}
\ee
such that $(M_{1}(z), M_{2}(z))\in \C_+^2$ for $z\in \C_+$, where, for simplicity, we  introduced the following ratios
\be\label{ratios}
 \gamma_n :=\frac{p}{n}=\frac{1}{\rho_1+\rho_2},\quad r_1 :=\frac{n_1}{n}=\frac{\rho_1}{\rho_1+\rho_2},\quad r_2 :=\frac{n_2}{n}=\frac{\rho_2}{\rho_1+\rho_2}.
\ee
We then define the matrix limit of $G(z)$ as
\be \label{defn_piw}
\Pi(z) := \begin{pmatrix} -(z+r_1 M_{1}(z)\Lambda^2  +  r_2 M_{2}(z))^{-1} & 0 & 0 \\ 0 &  M_{1}(z)\id_{n_1} & 0 \\ 0 & 0 & M_{2}(z)\id_{n_2}  \end{pmatrix}.\ee
In particular, the matrix limit of $\cal G(z)$ is given by $-(z+r_1 M_{1}\Lambda^2 + r_2 M_{2})^{-1}$.

\vspace{5pt}
\noindent{\bf Proof overview:}
\HZ{Divide into several parts like sec 7 to improve readability.}
Now we give a heuristic derivation of the matrix limit when the entries of $Z^{(1)}$ and $Z^{(2)}$ are i.i.d. Gaussian. Note that in this case,
by the rotational invariance of the multivariate Gaussian distribution we have
\be\label{eq in Gauss} Z^{(1)} U\Lambda \stackrel{d}{=} Z^{(1)} \Lambda, \quad Z^{(2)} V \stackrel{d}{=} Z^{(2)},\ee
where ``$\stackrel{d}{=}$" means ``equal in distribution". Hence it suffices to consider the following resolvent
 \begin{equation} \nonumber
   G(z)= \left( {\begin{array}{*{20}c}
   { -z\id_{p} } & n^{-1/2}\Lambda (Z^{(1)})^{\top} & n^{-1/2} (Z^{(2)})^\top  \\
   {n^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1}} & 0 \\
   {n^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2}}
   \end{array}} \right)^{-1}.
 \end{equation}
Using Schur complement formula (see \alert{equation} (\ref{resolvent2}) \HZ{check other usage of equation ref}), we have that %
\begin{align}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu,\nu\in \mathcal I_1} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu} - \frac{1}{n} \sum_{\mu,\nu\in \mathcal I_2} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu} -2 \frac{\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu},  \ \ \text{for }\  i \in \cal I_0 , \label{0self_Gii}\\
\frac{1}{{G_{\mu\mu} }}&=  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}\lambda_i \lambda_j z_{\mu i}z_{\mu j} G^{\left(\mu\right)}_{ij}, \quad \frac{1}{{G_{\nu\nu} }}=  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}  z_{\nu i}z_{\nu j}  G^{\left(\nu\right)}_{ij},  \ \ \text{for }\  \mu \in \cal I_1, \ \nu\in \cal I_2, \label{0self_Gmu1}
\end{align}
where we recall the notations in equation \eqref{labelZ}. For the right-hand side of equation \eqref{0self_Gii}, $G^{(i)}$ is independent of the entries $z_{\mu i}$ and $z_{\nu i}$. Hence by the concentration inequalities in Lemma \ref{largedeviation}, we have that the  right-hand side of equation \eqref{0self_Gii} concentrates around the partial expectation over the entries $\{z_{\mu i}: \mu \in \cal I_1\cup \cal I_2\}$, i.e., with overwhelming probability,
\begin{align*}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu \in \mathcal I_1}  G^{\left( i \right)}_{\mu\mu} - \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} +\oo(1)= - z - \lambda_i^2 r_1 m_1^{(i)}(z)-  r_2m_2^{(i)}(z)+\oo(1),
\end{align*}
where we used the definition of $m_1^{(i)}$ and $m_2^{(i)}$ in equation \eqref{defm} with $G$ replaced by $G^{(i)}$. Intuitively, since we have only removed only one column and one row out of the $(p+n)$ columns and rows in $H$, $m_1^{(i)}$ and $m_2^{(i)}$ should be close to the original $m_1$ and $m_2$. Hence we obtain from the above equation that
\begin{align}\label{1self_Gii}
 G_{ii}  = -\frac{1}{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}.
\end{align}
Similarly, we can obtain from equation \eqref{0self_Gmu1} that for $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
\be\label{1self_Gmu} G_{\mu \mu }=-\frac{1}{1+\gamma_n m_0 + \oo(1)},\quad G_{\nu\nu}=-\frac1{1+\gamma_n m+\oo(1)},\ee
with overwhelming probability. Taking average we obtain that
\be\label{2self_Gmu} m_1= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu\mu}=-\frac{1}{1+\gamma_n m_0 + \oo(1)},\quad m_2=\frac{1}{n_2}\sum_{\nu \in \cal I_2}G_{\nu\nu}=-\frac{1}{1+\gamma_n m + \oo(1)},
\ee
with overwhelming probability. Together with the definition of $m$ and $m_0$ in equation \eqref{defm}, the two equations in equation \eqref{2self_Gmu} give that
\be\label{3self_Gmu}  \frac1{m_1}= -1- \frac{\gamma_n}{p} \sum_{i=1}^p \lambda_i^2 G_{ii}+ \oo(1),\quad \frac1{m_2}=-1-\frac{\gamma_n}{p} \sum_{i=1}^p G_{ii}  + \oo(1),\ee
with overwhelming probability. Plugging equation \eqref{1self_Gii} into equation \eqref{3self_Gmu}, we obtain that
\begin{align*}
& \frac1{m_1}= -1+ \frac{\gamma_n}{p} \sum_{i=1}^p \frac{\lambda_i^2 }{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}+ \oo(1),\\
& \frac1{m_2}=-1+\frac{\gamma_n}{p} \sum_{i=1}^p \frac{1}{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}  + \oo(1),
\end{align*}
with overwhelming probability, which give the approximate self-consistent equations for $(m_1,m_2)$. Compare them to the deterministic self-consistent equations in equation \eqref{selfomega}, one can observe that we should have $(m_1,m_2) =(M_{1}, M_{2})+\oo(1)$ with overwhelming probability. Inserting this approximate identity into equation \eqref{1self_Gii}-equation \eqref{2self_Gmu}, we see that for  $i \in \cal I_0$, $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
$$G_{ii}=-(z +\lambda_i^2 r_1  M_{1} + r_2 M_{2}+\oo(1))^{1/2},\quad G_{\mu\mu}=M_{1}+\oo(1),\quad G_{\nu\nu}=M_{2}+\oo(1),$$
with overwhelming probability. These explain the diagonal entries of $\Pi$ in equation \eqref{defn_piw}. For the off-diagonal entries, they are close to zero due to concentration. For example, for $i\ne j\in \cal I_1$, by Schur complement formula (see (\ref{resolvent3})), we have
$$G_{ij}=-G_{ii}\left(\frac{\lambda_i}{n^{1/2}}\sum_{\mu \in \cal I_1} z_{\mu i} G^{(i)}_{\mu j} + \frac{1}{n^{1/2}}\sum_{\mu \in \cal I_2} z_{\mu i} G^{(i)}_{\mu j} \right).$$
Using Lemma \ref{largedeviation}, we can show that $n^{-1/2}\sum_{\mu \in \cal I_1} z_{\mu i} G^{(i)}_{\mu j}$ and $n^{-1/2}\sum_{\mu \in \cal I_2} z_{\mu i} G^{(i)}_{\mu j}$ are both close to zero. The other off-diagonal entries can be bounded in the same way.

The above arguments are the core of the main proof. To have a rigorous proof, we need to estimate each error carefully, and extend the Gaussian case to the more general case where the entries of $Z^{(1)}$ and $Z^{(2)}$ only satisfy certain moment assumptions. These will make the real argument rather tedious, but the methods we used are standard in the random matrix literature \cite{erdos2017dynamical,Anisotropic}.

\fi



