\section{Toolbox}\label{app_tool}

\subsection{Algebraic Inequalities}

\begin{fact}\label{fact_proof_gA}
	The minimum singular value of a matrix $X^{\top}YX$ is at least the minimum singular value of $Y$ times the minimum singular value of $X^{\top}X$.
\end{fact}

The following simple resolvent identities are important tools for our proof. Recall that the resolvent minors have been defined in Definition \ref{defn_Minor}.
\begin{lemma}\label{lemm_resolvent}
We have the following resolvent identities.
\begin{itemize}
\item[(i)]
For $i\in \mathcal I_1$ and $\mu\in \mathcal I_1\cup \cal I_2$, we have
\begin{equation}
\frac{1}{G_{ii}} =  - z - \left( {AG^{\left( i \right)} A^\top} \right)_{ii} ,\quad  \frac{1}{{G_{\mu \mu } }} =  - 1  - \left( {A^\top  G^{\left( \mu  \right)} A} \right)_{\mu \mu }.\label{resolvent2}
\end{equation}

 \item[(ii)]
 For $i\in \mathcal I_1$, $\mu \in \mathcal I_1\cup \cal I_2$, $a\in \cal I\setminus \{i\}$ and $b\in \cal I\setminus \{ \mu\}$, we have
\begin{equation}
G_{ia}   = -G_{ii}  \left( AG^{\left( {i} \right)} \right)_{ia},\quad  G_{\mu b }  = - G_{\mu \mu }  \left( A^\top  G^{\left( {\mu } \right)}  \right)_{\mu b }. \label{resolvent3}
\end{equation}
%For $i\in \mathcal I_1$ and $\mu\in \mathcal I_2$, we have
%\begin{equation}\label{resolvent6}
%\begin{split}
% G_{i\mu } =  -G_{ii}  \left( WG^{\left( {i} \right)} \right)_{i\mu}, \quad G_{\mu i}= -G_{\mu\mu}\left( W^\top G^{(\mu)}\right)_{\mu i}. %\left( { - Y_{i\mu }  +  {\left( {YG^{\left( {i\mu } \right)} Y} \right)_{i\mu } } } \right) . %, \ \  G_{\mu i}  = G_{\mu \mu } G_{ii}^{\left( \mu  \right)} \left( { - Y_{\mu i}^\top  + \left( {Y^\top  G^{\left( {\mu i} \right)} Y^\top  } \right)_{\mu i} } \right).
%\end{split}
%\end{equation}

 \item[(iii)]
 For $a \in \mathcal I$ and $a_1,a_2 \in \mathcal I \setminus \{a\}$, we have
\begin{equation}
G_{a_1a_2}^{\left( a \right)}  = G_{a_1a_2}  - \frac{G_{a_1a} G_{aa_2}}{G_{aa}} .
%, \quad  \frac{1}{{G_{{b}{b}} }} = \frac{1}{{G_{{b}{b}}^{({a})} }} - \frac{{G_{{b}{a}} G_{{a}{b}} }}{{G_{{b}{b}} G_{{b}{b}}^{({a})} G_{{a}{a}} }}.
\label{resolvent8}
\end{equation}
%and
%\begin{equation}
%\frac{1}{{G_{ss} }} = \frac{1}{{G_{ss}^{(r)} }} - \frac{{G_{sr} G_{rs} }}{{G_{ss} G_{ss}^{(r)} G_{rr} }}.
%\label{resolvent9}
%\end{equation}
% \item[(iv)]
%All of the above identities hold for $G^{(\mathbb T)}$ instead of $G$ for $\mathbb T \subset \mathcal I$, and in the case where $A$ and $B$ are not diagonal.
\end{itemize}
\end{lemma}
\begin{proof}
All these identities can be proved directly using Schur's complement formula. The reader can also refer to, for example, \cite[Lemma 4.4]{Anisotropic}.
\end{proof}


\subsection{Random Variables with Bounded Moments}
The following lemma gives (almost) sharp concentration bounds for linear and quadratic forms of bounded supported random variables. Here we recall that the stochastic domination ``$\prec$" was defined in Definition \ref{stoch_domination}.
%Besides the proof of Lemma \ref{prop_entry}, we also use it in several other places of this paper, including the proofs in Section \ref{sec_proof_general} and Section \ref{app_iso_cov}.
%It constitutes the main difference between our proof and the one in \cite{KY2}, where the authors used a large deviation bound for random variables with arbitrarily high moments.

\begin{lemma}[Lemma 3.8 of \cite{EKYY1} and Theorem B.1 of \cite{Delocal}]\label{largedeviation}
Let $(x_i)$, $(y_j)$ be independent families of centered and independent random variables, and $(A_i)$, $(B_{ij})$ be families of deterministic complex numbers. Suppose the entries $x_i$ and $y_j$ have variances at most $1$, and $ x_i$ and $ y_j$ satisfy the bounded support condition (\ref{eq_support}) for a deterministic parameter $q$. %with $q\le n^{-\phi}$ for a small constant $\phi>0$. 
Then we have the following bounds:
%for any fixed $\xi>0$, the following bounds hold with $\xi$-high probability:
\begin{align}
\Big| \sum_{i=1}^n A_i x_i \Big\vert \prec  q \max_{i} \vert A_i \vert+ \Big(\sum_i |A_i|^2 \Big)^{1/2} , \quad &\Big\vert  \sum_{i,j=1}^n x_i B_{ij} y_j \Big\vert \prec q^2 B_d  + q n^{1/2}B_o +  \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}},\label{eq largedev10}  \\
\Big\vert  \sum_{i=1}^n (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec q n^{1/2}B_d   , \quad &\Big\vert  \sum_{1\le i\ne j\le n} \bar x_i B_{ij} x_j \Big\vert  \prec qn^{1/2}B_o +  \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} ,\label{eq largedev20} 
\end{align}
where $B_d:=\max_{i} |B_{ii} |$ and $B_o:= \max_{i\ne j} |B_{ij}|.$ Moreover, if all the moments of $ x_i$ and $ y_j$ exist in the sense of equation \eqref{assmAhigh2}, then we have stronger bounds
\begin{align}
\Big\vert \sum_i A_i x_i \Big\vert \prec  \Big(\sum_i |A_i|^2 \Big)^{1/2} , \quad  & \Big\vert \sum_{i,j} x_i B_{ij} y_j \Big\vert \prec  \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} ,\label{eq largedev1} \\
 \Big\vert  \sum_{i} (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec  \Big( \sum_i |B_{ii} |^2\Big)^{1/2}  ,\quad & \Big\vert  \sum_{i\ne j} \bar x_i B_{ij} x_j \Big\vert  \prec \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} .\label{eq largedev2}
\end{align}
%\begin{align}
%\Big\vert \frac1{\sqrt{n}}\sum_i A_i x_i \Big\vert \prec  \frac{1}{\sqrt{n}}\Big(\sum_i |A_i|^2 \Big)^{1/2} , \quad  & \Big\vert \frac1n\sum_{i,j} x_i B_{ij} y_j \Big\vert \prec  \frac{1}{n}\Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} ,\label{eq largedev1} \\
% \Big\vert  \frac1n \sum_{i} (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec \frac1{n}\Big( \sum_i |B_{ii} |^2\Big)^{1/2}  ,\quad & \Big\vert \frac1n\sum_{i\ne j} \bar x_i B_{ij} x_j \Big\vert  \prec  \frac{1}{n}\Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} .\label{eq largedev2}
%\end{align}
\end{lemma}

As a corollary, we can obtain the following concentration estimates for random variables with bounded moments as in equation \eqref{assmAhigh}. 

\begin{corollary} \label{cor_largedeviation}
Let $Z\in \R^{n\times p}$ be a random matrix as in Assumption \ref{assume_rm}. Then for any deterministic vector $v\in \R^p$, we have that
\be\label{Zv_cor}\left|\|Zv\|^2- n\|v\|^2\right|\le  n^{1-c_\varphi} \|v\|^2\ee
with high probability.
\end{corollary}
 \begin{proof}
We introduce the truncated matrices $\wt Z$ with entries
$ \wt Z_{ij}:= \mathbf 1\left( |Z_{ij}|\le q \right)\cdot Z_{ij} $
for $q= n^{1/2-\frac{\varphi - 4}{2\varphi}}\log n$. By equation \eqref{Ptrunc}, we have that
\begin{equation}\label{XneX222}
\mathbb P(\wt Z= Z) =1-\OO ( (\log n)^{-\varphi}).
\end{equation}
Furthermore, by equation \eqref{meanshif} we have that
\be\label{meanshif222}
|\mathbb E  \wt  Z_{ij}| =\OO(n^{-3/2}), \quad  \mathbb E |\wt  Z_{ij}|^2 =1+ \OO(n^{-1}).
\ee
%From the first estimate in equation \eqref{meanshif}, we can also get a bound on the operator norm:
%\be\label{EZ norm222}\|\E \wt Z^{(1)}\|=\OO(n^{-1/2}) .
%\ee
Then we can centralize and rescale $\wt Z$ as $ \wh Z:=\frac{\wt Z - \E \wt Z }{ (\E|\wt Z_{11}|^2)^{1/2}}.$ 
It suffices to show that 
\be\label{show_whZ}
 \left|\|\wh Zv\|^2- n\|v\|^2\right|\prec n^{1/2} q \|v\|^2.
\ee 
In fact, provided that \eqref{show_whZ} holds, using \eqref{meanshif222} it is easy to get that 
$$ \left|\|\wt Zv\|^2- n\|v\|^2\right|\prec n^{1/2} q \|v\|^2,$$
which implies \eqref{Zv_cor} for any fixed value $c_{\varphi}$ within $(0, \frac{\varphi - 4}{2\varphi})$.

For \eqref{show_whZ}, we first observe that for $1\le i \le n$, $(\wh Zv)_i=\sum_{1\le j \le p} \wh Z_{ij}v_j$ are i.i.d random variables of mean zero and variance $\|v\|^2$. Moreover, using \eqref{eq largedev10} we get that 
$$ |(\wh Zv)_i| \prec q \max_{1\le i\le p}|v_i| + \|v\| . $$
Together with the trivial $\max_{1\le i\le p}|v_i| \le \|v\|$, this implies that the random variables $\frac{(\wh Zv)_i}{\|v\|}$, $1\le i \le p$, are i.i.d random variables of mean zero, variance 1 and bounded support $q$. Then applying \eqref{eq largedev20}, we get that
$$ \left|\|\wh Zv\|^2 - n\|v\|^2\right|=\left|\sum_i \left(|(\wh Zv)_i|^2-\E|(\wh Zv)_i|^2\right)\right|\prec n^{1/2} q  \|v\|^2 .
 $$
This concludes \eqref{show_whZ}.
 \end{proof}