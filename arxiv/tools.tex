\section{Toolbox}\label{app_tool}

\subsection{Algebraic Inequalities}

\begin{fact}\label{fact_proof_gA}
	The minimum singular value of a matrix $X^{\top}YX$ is at least the minimum singular value of $Y$ times the minimum singular value of $X^{\top}X$.
\end{fact}

The following simple resolvent identities are important tools for our proof. Recall that the resolvent minors have been defined in Definition \ref{defn_Minor}.
\begin{lemma}\label{lemm_resolvent}
We have the following resolvent identities.
\begin{itemize}
\item[(i)]
For $i\in \mathcal I_1$ and $\mu\in \mathcal I_1\cup \cal I_2$, we have
\begin{equation}
\frac{1}{G_{ii}} =  - z - \left( {AG^{\left( i \right)} A^\top} \right)_{ii} ,\quad  \frac{1}{{G_{\mu \mu } }} =  - 1  - \left( {A^\top  G^{\left( \mu  \right)} A} \right)_{\mu \mu }.\label{resolvent2}
\end{equation}

 \item[(ii)]
 For $i\in \mathcal I_1$, $\mu \in \mathcal I_1\cup \cal I_2$, $a\in \cal I\setminus \{i\}$ and $b\in \cal I\setminus \{ \mu\}$, we have
\begin{equation}
G_{ia}   = -G_{ii}  \left( AG^{\left( {i} \right)} \right)_{ia},\quad  G_{\mu b }  = - G_{\mu \mu }  \left( A^\top  G^{\left( {\mu } \right)}  \right)_{\mu b }. \label{resolvent3}
\end{equation}
%For $i\in \mathcal I_1$ and $\mu\in \mathcal I_2$, we have
%\begin{equation}\label{resolvent6}
%\begin{split}
% G_{i\mu } =  -G_{ii}  \left( WG^{\left( {i} \right)} \right)_{i\mu}, \quad G_{\mu i}= -G_{\mu\mu}\left( W^\top G^{(\mu)}\right)_{\mu i}. %\left( { - Y_{i\mu }  +  {\left( {YG^{\left( {i\mu } \right)} Y} \right)_{i\mu } } } \right) . %, \ \  G_{\mu i}  = G_{\mu \mu } G_{ii}^{\left( \mu  \right)} \left( { - Y_{\mu i}^\top  + \left( {Y^\top  G^{\left( {\mu i} \right)} Y^\top  } \right)_{\mu i} } \right).
%\end{split}
%\end{equation}

 \item[(iii)]
 For $a \in \mathcal I$ and $a_1,a_2 \in \mathcal I \setminus \{a\}$, we have
\begin{equation}
G_{a_1a_2}^{\left( a \right)}  = G_{a_1a_2}  - \frac{G_{a_1a} G_{aa_2}}{G_{aa}} .
%, \quad  \frac{1}{{G_{{b}{b}} }} = \frac{1}{{G_{{b}{b}}^{({a})} }} - \frac{{G_{{b}{a}} G_{{a}{b}} }}{{G_{{b}{b}} G_{{b}{b}}^{({a})} G_{{a}{a}} }}.
\label{resolvent8}
\end{equation}
%and
%\begin{equation}
%\frac{1}{{G_{ss} }} = \frac{1}{{G_{ss}^{(r)} }} - \frac{{G_{sr} G_{rs} }}{{G_{ss} G_{ss}^{(r)} G_{rr} }}.
%\label{resolvent9}
%\end{equation}
% \item[(iv)]
%All of the above identities hold for $G^{(\mathbb T)}$ instead of $G$ for $\mathbb T \subset \mathcal I$, and in the case where $A$ and $B$ are not diagonal.
\end{itemize}
\end{lemma}
\begin{proof}
All these identities can be proved directly using Schur's complement formula. The reader can also refer to, for example, \cite[Lemma 4.4]{Anisotropic}.
\end{proof}


\subsection{Random Variables with Bounded Moments}
The following lemma gives (almost) sharp concentration bounds for linear and quadratic forms of bounded supported random variables. Here we recall that the stochastic domination ``$\prec$" was defined in Definition \ref{stoch_domination}.
%Besides the proof of Lemma \ref{prop_entry}, we also use it in several other places of this paper, including the proofs in Section \ref{sec_proof_general} and Section \ref{app_iso_cov}.
%It constitutes the main difference between our proof and the one in \cite{KY2}, where the authors used a large deviation bound for random variables with arbitrarily high moments.

\begin{lemma}[Lemma 3.8 of \cite{EKYY1} and Theorem B.1 of \cite{Delocal}]\label{largedeviation}
Let $(x_i)$, $(y_j)$ be independent families of centered and independent random variables, and $(A_i)$, $(B_{ij})$ be families of deterministic complex numbers. Suppose the entries $x_i$ and $y_j$ have variances at most $1$, and $n^{-1/2}x_i$ and $n^{-1/2}y_j$ satisfy the bounded support condition (\ref{eq_support}) with $q\le n^{-\phi}$ for a small constant $\phi>0$. Then we have the following bounds:
%for any fixed $\xi>0$, the following bounds hold with $\xi$-high probability:
\begin{align*}
\Big| \frac{1}{\sqrt{n}}\sum_i A_i x_i \Big\vert \prec  q \max_{i} \vert A_i \vert+ \frac{1}{\sqrt{n}}\Big(\sum_i |A_i|^2 \Big)^{1/2} , \quad &\Big\vert \frac1n\sum_{i,j} x_i B_{ij} y_j \Big\vert \prec q^2 B_d  + qB_o + \frac{1}{n}\Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}}, \\
\Big\vert \frac1n\sum_{i} (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec q B_d   , \quad &\Big\vert \frac1n\sum_{i\ne j} \bar x_i B_{ij} x_j \Big\vert  \prec qB_o + \frac{1}{n}\Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} ,
\end{align*}
where $B_d:=\max_{i} |B_{ii} |$ and $B_o:= \max_{i\ne j} |B_{ij}|.$ Moreover, if all the moments of $ x_i$ and $ y_j$ exist in the sense of equation \eqref{assmAhigh2}, then we have stronger bounds
\begin{align}
\Big\vert \frac1{\sqrt{n}}\sum_i A_i x_i \Big\vert \prec  \frac{1}{\sqrt{n}}\Big(\sum_i |A_i|^2 \Big)^{1/2} , \quad  & \Big\vert \frac1n\sum_{i,j} x_i B_{ij} y_j \Big\vert \prec  \frac{1}{n}\Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} ,\label{eq largedev1} \\
 \Big\vert  \frac1n \sum_{i} (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec \frac1{n}\Big( \sum_i |B_{ii} |^2\Big)^{1/2}  ,\quad & \Big\vert \frac1n\sum_{i\ne j} \bar x_i B_{ij} x_j \Big\vert  \prec  \frac{1}{n}\Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} .\label{eq largedev2}
\end{align}
\end{lemma}

