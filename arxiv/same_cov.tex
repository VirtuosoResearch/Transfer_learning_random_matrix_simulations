\section{Warmup: Bias-Variance Tradeoff in the Same Covariates Setting}


We begin by considering the case where all tasks have the same sample size and feature covariates, that is, $n_i = n$ and $X^{(i)} = X\in\real^{n\times p}$ for all $i = 1, \dots, t$.
We provide a sharp generalization error bound of hard parameter sharing estimators.
We show that the hard parameter sharing estimator finds a ``low-rank approximation of all tasks'', in the sense formalized below.
%This setting is prevalent in applications of multi-task learning to image classification, where there are multiple prediction labels/tasks for every image \cite{chexnet17,EA20}.
%We consider an arbitrary local minimum $B, W_1, \dots, W_2$ of the optimization objective.
%We extend the bias-variance decomposition from the two-task case to the multiple-task case.
%We observe that the expected prediction loss of $\hat{\beta}_t^{\MTL}$ conditional on $X$ consists of a bias and a variance equation as follows
%\begin{align}
%	\exarg{\varepsilon_1, \dots, \varepsilon_t}{L(\hat{\beta}_t^{\MTL}) \mid X}
%	=& \bignorm{\Sigma^{1/2} \bigbrace{B^{\star} \cW^{\top} (\cW \cW^{\top})^{-1} W_t - \beta_t}}^2 \label{eq_bias_multiple} \\
%	&+ \sigma^2 \cdot (W_t^{\top} (\cW \cW^{\top})^{-1} W_t) \cdot \bigtr{\Sigma (X^{\top} X)^{-1}} \label{eq_var_multiple}
%\end{align}
%One can see that equation \eqref{eq_bias_multiple} is the bias of the multi-task learning estimator and equation \eqref{eq_var_multiple} is its variance.
%Compared to the prediction loss of single-task learning (cf. equation \eqref{eq_var_stl}), we observe that the variance equation \eqref{eq_var_multiple} is always smaller because $W_t^{\top} (\cW \cW^{\top})^{-1} W_t \le 1$.
%On the other hand, the bias equation \eqref{eq_bias_multiple} is always larger because of the difference between the task models.
%We show the generalization error of hard parameter sharing estimators.
%Before stating the result, we define the following notations.
Let $B^\star := [{\beta}_1,{\beta}_2,\dots,{\beta}_{t}] \in \real^{p\times t}$ denote the ground truth model parameters.
Let $\Sigma$ denote the (same) population covariance matrix of all tasks.
Let $U_r U_r^{\top}$ denote the best rank-$r$ subspace approximation of ${B^{\star}}^\top\Sigma B^{\star}$, that is,
\[ U_r \define \argmin_{U\in\real^{t\times r} : U^{\top} U = \id_{r\times r}} \inner{U U^{\top}} {{B^{\star}}^{\top} \Sigma B^{\star}}. \]
For $i = 1,\dots, t$, let $v_i \in\real^r$ denote the $i$-th row of $U_r$.
Our main result in this section is stated as follows.

\begin{theorem}[Multiple tasks with the same covariates]\label{thm_many_tasks}
%Suppose $X=Z\Sigma^{1/2}\in \R^{n\times p}$ satisfy Assumption \ref{assm_secA1} with $\rho:=n/p>1$ being some fixed constant. Consider data models  $Y_i = X\beta_i + \varepsilon_i$, $i=1,2,\cdots, t$, where $\e_i\in \R^{n}$ are random vectors with i.i.d. entries with mean zero, variance $\sigma^2$ and all moments as in \eqref{assmAhigh}. Moreover, assume that $X$, $\beta_i$ and $\e_i$ are all independent of each other.
	%Let $n = c \cdot p$.
	%Let $X\in\real^{n\times p}$ and $Y_i = X\beta_i + \varepsilon_i$, for $i = 1,\dots,k$.
%	Consider $t$ data models $Y_i = X\beta_i + \varepsilon_i$, $i=1,2,\cdots, t$, where $X$ has covariance matrix $\Sigma$, and the entries of $\e_i$ are i.i.d. with mean zero and variance $\sigma^2$.gT
	%that satisfy Assumption \ref{assm_secA2} in the appendix.
	Consider the multiple-task case where all tasks have the same covariates $X = \Sigma^{1/2} Z \in \real^{n \times p}$.
	Assume that $Z$
	Assume that $\lambda_{\min}({B^{\star}}^\top\Sigma B^{\star})\gtrsim \sigma^2$.
	% Let $\delta$ be any fixed value such that $\delta \le \oo \left( \|B^\star\|^2 + \sigma^2\right)$.
	Then, for any task $i$ from $1$ to $t$, the prediction loss of the hard parameter sharing estimator $\hat{\beta}_i^{\MTL}$ satisfies that
	\begin{align}
		\bigabs{L(\hat{\beta}_i^{\MTL}) - \norm{\Sigma^{1/2} (B^{\star} U_r v_i - \beta_i)}^2 - \norm{v_i}^2 \cdot  \frac{\sigma^2  \cdot p}{n - p}} \lesssim p^{-\frac{c-4}{2c}}\bigbrace{\bigtr{{B^{\star}}^{\top}\Sigma B^{\star}} + \sigma^2}.
	\end{align}
	%\begin{itemize}
	%	\item \textbf{Positive transfer:} If $\left(1 - \norm{v_t}^2 \right)\frac{\sigma^2}{\rho - 1} -  > \delta$, then w.h.p over the randomness of $X, \varepsilon_1, \dots, \varepsilon_t$, we have that
	%	 \[ \te(\hat{\beta}_t^{\MTL}) < \te(\hat{\beta}_t^{\STL}). \]
	%	\item \textbf{Negative transfer:} If $\left(1 - \norm{v_t}^2\right)\frac{\sigma^2}{\rho - 1} - \norm{\Sigma^{1/2}(B^{\star} U_r v_t - \beta_t)}^2 < -\delta$, then w.h.p. over the randomness of $X, \varepsilon_1, \dots, \varepsilon_t$, we have that
	%	\[ \te(\hat{\beta}_t^{\MTL}) > \te(\hat{\beta}_t^{\STL}). \]
	%\end{itemize}
\end{theorem}
	Theorem \ref{thm_many_tasks} provides a sharp generalization error bound that is asymptotically tight as $p$ goes to infinity.
	The limiting loss of hard parameter sharing consists of two parts, a bias term that measures the error compared to the ground truth, and a variance term that scales with noise variance and sample size.
	Our result implies that the variance of hard parameter sharing always reduces compared to single-task learning, whose variance is equal to $\sigma^2 \cdot p / (n - p)$ (cf. Lemma \ref{lem_stl}).
	This is because $\norm{v_i}^2$ is at most one since the spectral norm of $U_r$, which is a projection matrix, is at most one.

	%First, we provide the steps for showing the bias-variance decomposition in equation \eqref{eq_bias_multiple} and \eqref{eq_var_multiple}.
	The key idea for proving Theorem \ref{thm_many_tasks} is a bias-variance decomposition of the prediction loss of hard parameter sharing.
	We obtain the decomposition as follows.
	Since all tasks have the same covariates, the optimization objective \eqref{eq_mtl} becomes
	\begin{align}
		f(B; A) = \sum_{i=1}^t \bignorm{X B a_i - Y_i}^2, \label{eq_mtl_same_cov}
	\end{align}
	where we recall that $B \in \real^{p \times r}$ and $a_1, a_2, \dots, a_t \in \R^r$. % for $1 < r < t$ by our assumption.
	Without loss of generality, we assume that $AA^{\top}$ is invertible.
	Otherwise, we can add a small amount of random perturbation to $A$ (e.g. an inverse exponential in $p$ amount of isotropic Gaussian noise), and the result still holds.
	Using the local optimality condition over $B$, that is, $\nabla_B f(\cdot) = 0$, we obtain $\hat{B}(A)$ that depends on the output layers as follows
	\begin{align*}
		\hat{B}(A) &= (X^{\top}X)^{-1} X^{\top} \bigbrace{\sum_{j=1}^t Y_i A_i^{\top}} (A  A^{\top})^{-1}. %\\
		%&= (B^\star A ^{\top}) (A A^{\top})^{-1} + (X^{\top}X)^{-1}X^{\top}   \bigbrace{\sum_{j=1}^t \varepsilon_i A_i^{\top}} (A  A^{\top})^{-1}.
	\end{align*}
	In the above, we use the fact that $AA^{\top}$ is invertible by our discussion above and $X^{\top}X$ is also invertible by Lemma ??.
	%Recall that $\hat{\beta}_i^{\MTL} = \hat{B} A_i$.
	The expected prediction loss over the random noise conditional on $X$ is equal to
	\begin{align}
		 & \exarg{\varepsilon_1, \dots, \varepsilon_t}{L(\hat{\beta}_i^{\MTL}) \mid X} \nonumber \\
		=& \exarg{\varepsilon_1, \dots, \varepsilon_t}{\bignorm{\Sigma^{1/2} \bigbrace{B^{\star} \cW^{\top} (\cW \cW^{\top})^{-1}W_i - \beta_i} + (X^{\top}X)^{-1} X^{\top} \bigbrace{\sum_{j=1}^t \varepsilon_j A_j^{\top}} (\cW \cW^{\top})^{-1} W_t}^2 \mid X} \nonumber \\
		=& \bignorm{\Sigma^{1/2} (B^{\star} \cW^{\top} (\cW \cW^{\top})^{-1} W_t - \beta_t)}^2
		+ \sum_{j=1}^t \bigtr{X (X^{\top}X)^{-1}\Sigma (X^{\top}X)^{-1}X^{\top} \exarg{\varepsilon_i}{\varepsilon_i \varepsilon_i^{\top}} (W_i^{\top} (\cW \cW^{\top})^{-1} W_t)^2} \nonumber \\
		=& \bignorm{\Sigma^{1/2}(B^{\star} \cW^{\top} (\cW \cW)^{-1} W_t - \beta_t)}^2
		+ \sigma^2 \cdot (W_t^{\top} (\cW \cW^{\top})^{-1} W_t) \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}. \label{eq_ex_pred}
	\end{align}
	The third step uses the fact that $\varepsilon_i$ consists of i.i.d. entries with mean zero and variance $\sigma^2$.
	The last step uses the fact that $\sum_{i=1}^t W_i W_i^{\top} = \cW \cW^{\top}$.
	One can see that the first part of the last equation is the bias and the second part is the variance.
	Hence the proof is complete.

	By plugging in the equation $\hat{B}$ back into equation \eqref{eq_mtl_same_cov}, we obtain the objective $g(\cW)$ that only depends on the output layers.
	\begin{align}
		g(\cal W) = \sum_{j=1}^t \bignorm{X B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j + U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j - Y_i}^2. \label{eq_mtl_output_layer}
	\end{align}
	We state a concentration error bound between $g(\cal W)$ and its expectation.

	\begin{lemma}\label{lem_error_same_cov}
		In the setting of Theorem \ref{thm_many_tasks}, we have that
		\[ g(\cW) = \ex{g(\cW)} \cdot (1 + o(1)) + \OO(\sigma^2 p^{1/2 + c}). \]
	\end{lemma}
	The proof of Lemma \ref{lem_error_same_cov} is via standard concentration bounds.
	The details can be found in Appendix \ref{app_proof_error_same_cov}.
	Based on the above result, we are ready to prove Theorem \ref{thm_many_tasks}.
	We use the notation $U_X U_X^{\top} = X (X^{\top} X)^{-1} X^{\top} \in\real^{n \times n}$ to denote the projection matrix to $X$, where $U_X\in\real^{n \times p}$.


\begin{proof}[Proof of Theorem \ref{thm_many_tasks}]
	Recall that we obtain $\hat{\cW}$ by minimizing $\cW$ in the above equation.
	In order to derive the minimizer, our proof involves two steps.
	First, we consider the expectation of $g(\cW)$ over $\varepsilon_1, \varepsilon_2, \dots, \varepsilon_t$, and $X$.
	We show that the minimizer of $\ex{g(\cW)}$ has a simple closed form solution similar to principal component analysis.
	Second, we show that the concentration error between $g(\cW)$ and $\ex{g(\cW)}$ is small provided with $n$ samples.
	Intuitively, this is because $\cW$ only has $r \times t$ parameters, which is much smaller than $n$ by our assumption.
	Hence we use standard concentration bounds and $\epsilon$-net arguments to show that the concentration error is small for $g(\cW)$.
	To facilitate the analysis, we divide $g(\cW)$ into three parts based on their degree of dependence on the random noise:
	\begin{align*}
		g_0(\cW) &= \sum_{j=1}^t \bignorm{XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j - X\beta_j}^2 = \bignorm{X B^{\star} (\cW^{\top} (\cW\cW^{\top})^{-1} \cW - \id_{t\times t})}^2, \\
		g_1(\cW) &= \sum_{j=1}^t \inner{XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - X\beta_j}{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j - \varepsilon_j} \\
		&= \sum_{j=1}^t \inner{-X B^{\star} \cW^{\top} (\cW \cW^{\top})^{-1} W_j + X\beta_j}{\varepsilon_j}, \\
		g_2(\cW) &= \sum_{j=1}^t \bignorm{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1}W_j - \varepsilon_j}^2 \\
		&= \bigbrace{\sum_{j=1}^t \norm{\varepsilon_j}^2} - \sum_{1\le i,j\le t} \bigbrace{W_i^{\top} (\cW \cW^{\top})^{-1} W_j} \cdot \bigbrace{\varepsilon_i^{\top} U_X U_X^{\top} \varepsilon_j}.
	\end{align*}
	For the second equation of $g_1(\cW)$, we use the fact that
		\[ \sum_{j=1}^t X B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j W_j^{\top} = X B^{\star} \cW^{\top} = \sum_{j=1}^t X \beta_j W_j^{\top}. \]
	For the second equation of $g_2(\cW)$, we observe that $\sum_{j=1}^t (\cW\cW^{\top})^{-1} W_j W_j^{\top} = \id_{r\times r}$ and rearrange the terms.
	%Now we switch $\hat{B}$ back into equation \eqref{eq_mtl_same_cov} to
	%Then as in \eqref{approxvalid}, we pick $N$ independent samples of the training set for each task with $N\ge n^{1-\e_0}$, and use the concentration result, Lemma \ref{largedeviation}, to get the validation loss as

	\medskip
	\noindent\textbf{The optimal solution of $\ex{g(\cW)}$.}
	We observe that $\ex{g(\cW)}$ admits a bias-variance decomposition as follows.
	First, we take expectation over $\varepsilon_1, \dots, \varepsilon_t$ conditional on $X$ and obtain the following:
	\begin{align}
		\exarg{\varepsilon_1, \dots, \varepsilon_t}{g(\cW) \mid X} - g_0(\cW)
		= \exarg{\varepsilon_1, \dots, \varepsilon_t}{g_2(\cW) \mid X}
		%		=& \sum_{j=1}^t \bigbrace{{\ex{\norm{\varepsilon_j}^2}} - \exarg{\varepsilon_1, \dots, \varepsilon_t}{\bignorm{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_t^{\top}} (\cW\cW^{\top})^{-1} W_j}^2 } }  \label{eq_empirical_2}\\
		= \sigma^2(n\cdot t - p\cdot r) \nonumber
	\end{align}
	%	\begin{align}
	%		\exarg{\varepsilon_1, \dots, \varepsilon_t}{g(\cW) \mid X}
	%		=& \bignorm{X B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} \cW - X B^{\star}}^2 \nonumber \\
	%		&+ \sum_{j=1}^t\bigbrace{\exarg{\varepsilon_1, \dots, \varepsilon_t}{\norm{\varepsilon_j}^2 - 2\inner{U_X U_X^{\top} \sum_{i=1}^t\bigbrace{\varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j}{\varepsilon_j}} } \nonumber \\
	%		&+ \sum_{j=1}^t \exarg{\varepsilon_1, \dots, \varepsilon_t}{\bignorm{U_X U_X^{\top}
	%\bigbrace{\sum_{i=1}^t  \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j}^2} \nonumber \\
	%		=& \norm{XB^{\star} (\cW^{\top}(\cW\cW^{\top})^{-1}\cW - \id)}^2	\label{eq_empirical_1}\\
	%			&+ \sum_{j=1}^t \bigbrace{{\ex{\norm{\varepsilon_j}^2}} - \exarg{\varepsilon_1, \dots, \varepsilon_t}{\bignorm{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_t^{\top}} (\cW\cW^{\top})^{-1} W_j}^2 } }  \label{eq_empirical_2}\\
	%		=& \bignorm{X B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} \cW - XB^{\star}}^2 + \sigma^2(n\cdot t - p\cdot r) \nonumber
	%	\end{align}
	The first equation uses the same fact the expectation of $g_1(\cW)$ over the random noise is zero.
	The second equation uses the fact that $\varepsilon_i$ and $\varepsilon_j$ are pairwise independent for any $i \neq j$, and the variance of every entry of $\varepsilon_i$ is $\sigma^2$ for every $i = 1,\dots, t$, $\tr(U_X U_X^\top)=p$, and
		\[ \sum_{1\le i\le t} \bigbrace{W_i^{\top} (\cW \cW^{\top})^{-1} W_i}
			= \id_{r\times r} = r. \]
	%\be\nonumber
	%g(\cal W)= g_0 + 2g_1 + g_2,
	%\ee
	%where
	%\begin{align*}
	%	g_0(\cal W) &= \sum_{j=1}^t \bignorm{XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j - X\beta_j}^2 = \bignorm{X B^{\star} (\cW^{\top} (\cW\cW^{\top})^{-1} \cW - \id_{t\times t})}^2, \\
	%	g_1(\cal W)  &= \sum_{j=1}^t \inner{XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - X\beta_j}{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j - \varepsilon_j} ,\\
	%			%&= \sum_{j=1}^t \inner{-X B^{\star} \cW^{\top} (\cW \cW^{\top})^{-1} W_j + X\beta_j}{\varepsilon_j}, \\
	%	g_2(\cal W)  &= \sum_{j=1}^t \bignorm{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1}W_j - \varepsilon_j}^2 .
	%				\end{align*}
	%Now we switch $\hat{B}$ back into equation \eqref{eq_mtl_same_cov} to
	%Then as in \eqref{approxvalid}, we pick $N$ independent samples of the training set for each task with $N\ge n^{1-\e_0}$, and use the concentration result, Lemma \ref{largedeviation}, to get the validation loss as
	%\medskip
	%\noindent\textbf{The optimal solution of $\ex{g(\cW)}$.}
	%We observe that $\ex{g(\cW)}$ admits a bias-variance decomposition as follows.
	%First, we take expectation over $\varepsilon_1, \dots, \varepsilon_t$ conditional on $X$ and obtain that
	%\begin{align*}
	%	\exarg{\varepsilon_1, \dots, \varepsilon_t}{g(\cW) \mid X} &= g_0(\cal W)
	%	+ \exarg{\varepsilon_1, \dots, \varepsilon_t}{g_2(\cal W)  \mid X} \\
	%	&=g_0(\cal W) + \exarg{\varepsilon_1, \dots, \varepsilon_t} {\bigbrace{\sum_{j=1}^t \norm{\varepsilon_j}^2} - \sum_{1\le i\le t} \bigbrace{W_i^{\top} (\cW \cW^{\top})^{-1} W_i} \cdot \bigbrace{\varepsilon_i^{\top} U_X U_X^{\top} \varepsilon_i}}\\
%		=& \sum_{j=1}^t \bigbrace{{\ex{\norm{\varepsilon_j}^2}} - \exarg{\varepsilon_1, \dots, \varepsilon_t}{\bignorm{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_t^{\top}} (\cW\cW^{\top})^{-1} W_j}^2 } }  \label{eq_empirical_2}\\
	%	&=g_0(\cal W) + \sigma^2(n\cdot t - p\cdot r), \nonumber
	%\end{align*}
%	\begin{align}
%		\exarg{\varepsilon_1, \dots, \varepsilon_t}{g(\cW) \mid X}
%		=& \bignorm{X B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} \cW - X B^{\star}}^2 \nonumber \\
%		&+ \sum_{j=1}^t\bigbrace{\exarg{\varepsilon_1, \dots, \varepsilon_t}{\norm{\varepsilon_j}^2 - 2\inner{U_X U_X^{\top} \sum_{i=1}^t\bigbrace{\varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j}{\varepsilon_j}} } \nonumber \\
%		&+ \sum_{j=1}^t \exarg{\varepsilon_1, \dots, \varepsilon_t}{\bignorm{U_X U_X^{\top}
%\bigbrace{\sum_{i=1}^t  \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j}^2} \nonumber \\
%		=& \norm{XB^{\star} (\cW^{\top}(\cW\cW^{\top})^{-1}\cW - \id)}^2	\label{eq_empirical_1}\\
%			&+ \sum_{j=1}^t \bigbrace{{\ex{\norm{\varepsilon_j}^2}} - \exarg{\varepsilon_1, \dots, \varepsilon_t}{\bignorm{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_t^{\top}} (\cW\cW^{\top})^{-1} W_j}^2 } }  \label{eq_empirical_2}\\
%		=& \bignorm{X B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} \cW - XB^{\star}}^2 + \sigma^2(n\cdot t - p\cdot r) \nonumber
%	\end{align}
	%where the first equation uses the fact the expectation of $Z_1$ over the random noise is zero;
	%the second equation uses the fact that $\varepsilon_i$ and $\varepsilon_j$ are pairwise independent for any $i \neq j$, and $\sum_{j=1}^t (\cW\cW^{\top})^{-1} W_j W_j^{\top} = \id$; the third equations uses that the variance of every entry of $\varepsilon_i$ is $\sigma^2$ for every $i = 1,\dots, t$, $\tr(U_X U_X^\top)=p$ and $\sum_{1\le i\le t} \bigbrace{W_i^{\top} (\cW \cW^{\top})^{-1} W_i}=\tr(\sum_i W_i W_i^{\top}(\cW \cW^{\top})^{-1})=r$.
	We further take expectation over $X$ for $g_0(\cW)$ and obtain the following:
	\begin{align*}
		\ex{g(\cW)} = n \bignorm{\Sigma^{1/2} B^{\star} (\cW^{\top} (\cW \cW^{\top})^{-1} \cW - \id_{t\times t})}^2 + \sigma^2 (n\cdot t - p \cdot r).
	\end{align*}
	%\be\label{eq_multival}g(\cal W)=  N\left[\val(\cal W) + t  \sigma^2 \right]\cdot \left( 1+\OO(p^{-(1-\e_0)/2+\e})\right).\ee
\iffalse
{\color{red}
\begin{align*}
	f(W_1, \dots, W_t) = \sum_{i=1}^t \bignorm{X \hat B W_i - Y_i}^2 = \val(\cal W) \cdot \left( 1+\OO(p^{-1/2+\e})\right),
\end{align*}
where
\begin{align*}
	 \val(\cal W) &= \sum_{i=1}^t  \bignorm{X\bigbrace{(B^\star \cal W^\top) (\cal W\cal W^{\top})^{-1} W_i - \beta_i}}^2 + \E_{\e_i:1\le i \le t}\sum_{i=1}^t\left\| X(X^{\top}X)^{-1}X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_i^{\top}} (\cal W \cal W^{\top})^{-1}W_i -\e_i\right\| \\
	 &= \sum_{i=1}^t  \bignorm{X\bigbrace{(B^\star \cal W^\top) (\cal W\cal W^{\top})^{-1} W_i - \beta_i}}^2 + \sigma^2 (nt-pr).
\end{align*}

}
\fi
%Here $\val(\cal W)$ is defined as
%it remains to consider minimizing the validation loss
%	$$\val( \cal W):=\exarg{\varepsilon_j, \forall 1\le j\le t}{ \sum_{i=1}^t \bignorm{\Sigma^{1/2}( \hat B W_i - \beta_i)}^2} =  \delta_{\bias}(\cal W) + \delta_{\vari} ,$$
%where the model shift bias term $\delta_{\bias}(\cal W) $ is given by
%	\begin{align*}
%		\delta_{\bias}(\cal W) :=\sum_{i=1}^t  \bignorm{\Sigma^{1/2}\bigbrace{(B^\star \cal W^\top) (\cal W\cal W^{\top})^{-1} W_i - \beta_i}}^2,
%	\end{align*}
%	and the variance term $\delta_{\vari} $ can be calculated as
%	\begin{align*}
%		\delta_{\vari} := \sigma^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}.
%	\end{align*}
	One can see that the above equation provides a bias-variance decomposition of $\ex{g(\cW)}$.
	The first part measures the bias of multi-task learning compared to single-task learning for all tasks.
	The second part measures the variance of multi-task learning and scales with $\sigma^2$.
	In order to minimize $\ex{g(\cW)}$, it suffices to minimize the bias equation over $\cal W$, since the variance equation does not depend on $\cal W$.
	%We denote $Q := \cal W^{\top} (\cal W\cal W^{\top})^{-1} \cal W \in\real^{t\times t}$, whose $(i,j)$-th entry is given by $W_i^{\top} (\cal W\cal W^{\top})^{-1} W_j$.
	%Let $B^{\star} = [\beta_1, \beta_2, \dots, \beta_k] \in\real^{p \times k}$ denote the true model parameters.
	%Now we can write $\delta_{\bias}(\cal W)$ succinctly as
	%\begin{align*}
	%	\delta_{\bias}(Q) \equiv \delta_{\bias}(\cal W) = \bignormFro{\Sigma^{1/2}B^{\star}  %\bigbrace{Q -\id}}^2 .
	%\end{align*}
	From the bias equation, we observe that the minimizer is equal to the best rank-$r$ (subspace) approximation to ${B^{\star}}^{\top} \Sigma B^{\star}$, which is denoted by $U_{r} U_r^{\top}$.


%	\[ \abs{Z_1} \le \sum_{j=1}^t \sigma \cdot \norm{X B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j - X \beta_j} \le \sigma \cdot \sqrt{t \cdot Z_0}, \]
%	where the second part follows by using Cauchy-Shwartz inequality.

	%We provide tights bounds on the concentration error from the randomness of $\varepsilon_1, \dots, \varepsilon_t$, and $X$, respectively.
	%\begin{align}
	%	g(\cW) &= \exarg{\varepsilon_1, \dots, \varepsilon_t}{g(\cW) \mid X} \cdot (1 \pm \OO(p^{-1/2 + c})), \text{ and} \label{approxvalid_1} \\
	%	\exarg{\varepsilon_1, \dots, \varepsilon_t}{g(\cW) \mid X} &= \ex{g(\cW)} \cdot(1 + o(1)) \label{approxvalid_2}
	%\end{align}
	%Together, they imply that $g(\cW) = \ex{g(\cW)} \cdot (1 + o(1))$. Therefore, next we focus on proving equation \eqref{approxvalid_1} and \eqref{approxvalid_2}.

	%For equation \eqref{approxvalid_1}, we observe that the summands of $g(\cW)$ w.r.t. $\varepsilon_1, \dots, \varepsilon_t$ belong to either of the following three types:
	%\begin{enumerate}
	%	\item[(i)] $\varepsilon_i^{\top} A \varepsilon_i$, for any $i = 1,\dots, t$ and a fixed $A\in\real^{n\times n}$ that is independent of $\varepsilon_i$;
	%	\item[(ii)] $\varepsilon_i^{\top} A \varepsilon_j$, for any $i \neq j$ and a fixed $A$ that is independent of both $\varepsilon_i$ and $\varepsilon_j$;
	%	\item[(iii)] $\varepsilon_i^{\top} A$, for any $i = 1,\dots, t$ and a fixed $A\in\real^n$ that is independent of $\varepsilon_i$.
	%\end{enumerate}
	%For all three types, using Lemma \ref{largedeviation} in Appendix \ref{sec_maintools} and the fact that all moments of $\varepsilon_i$ exist, we conclude that
	%\begin{enumerate}
	%	\item[(i)] $\varepsilon_i^{\top} A \varepsilon_i = \sigma^2 (1 + \OO(p^{-1/2 + c})) \normFro{A}^2$;
	%	\item[(ii)] $\abs{}$
	%	\item[(iii)]
	%\end{enumerate}

	%We use the fact that our random vectors have i.i.d. entries.
%Before doing that, we first need to fix the setting for the following discussions, because we want to keep track of the error rate carefully instead of obtaining an asymptotic result only.
	%Recall that $Y_i = X_i\beta_i + \varepsilon_i$ and $\wt Y_i = \wt X_i\beta_i + %\wt\varepsilon_i$, $i=1,2$, all satisfy Assumption \ref{assm_secA2}. Then we rewrite %\eqref{eq_mtl_2tasktilde} as
%$$	g( v) = \sum_{i=1}^2\left\| \wt X_i\wt\beta_i  - \wt \e_i\right\|^2 , \quad \wt\beta:=\hat %B w_i-\beta_i.$$
	%Since $ \wt X_i\wt\beta$ and $ \wt \e_i$ are independent random vectors with i.i.d. centered entries, we can use the concentration result,  to get that for any constant $\e>0$,
	%\begin{align*}
		%\left|\left\| \wt X_i\wt\beta_i  - \wt \e_i\right\|^2 -  \exarg{\wt X_i,\wt{\e}_i} {\left\| \wt X_i\wt\beta_i  - \wt \e_i\right\|^2} \right| & =\left|\left\| \wt X_i\wt\beta_i  %- \wt \e_i\right\|^2 - N_i (\wt\beta_i^\top \Sigma_i \wt\beta_i + \sigma_i^2) \right| \\
%&\le N_i^{1/2+\e} (\wt\beta_i^\top \Sigma_i \wt\beta_i + \sigma_i^2),
	%$\end{align*}
	%with high probability. Thus we obtain that
	%$$g(v)= \left[\sum_{i=1}^2 N_i\left\|\Sigma_i^{1/2}( \hat B w_i - \beta_i) \right\|^2 + (N_1\sigma^2_1+N_2\sigma^2_2)\right]\cdot \left( 1+\OO(p^{-(1-\e_0)/2+\e})\right),$$
%where we also used $N_i\ge p^{-1+\e_0}$. Inserting \eqref{hatB} into the above expression and using
	% again the concentration result, Lemma \ref{largedeviation}, we get that
	%$$ \sum_{i=1}^2 N_i\left\|\Sigma_i^{1/2}( \hat B w_1 - \beta_i) \right\|^2 = \val(v)\cdot \left( 1+\OO(p^{-1/2+\e})\right)$$
%with high probability.
%-----old-------
%Suppose that the entries of $\e_1$ and $\e_2$ have variance $\sigma^2$.  Using a validation set that is sub-sampled from the original training dataset, we get a validation loss as follows
%\begin{align}
%		&\val(\hat{B}; w_1, w_2):= \exarg{\varepsilon_1,\e_2} \sum_{i=1}^2 \left\|\Sigma_i^{1/2}( \hat B w_1 - \beta_i) \right\|^2 \\
%	&=  n_1 \cdot \bignorm{\Sigma_1^{1/2}\left(\frac{w_1^2}{w_2^2} X_1^{\top}X_1 + X_2^{\top}X_2\right)^{-1}X_2^{\top}X_2\left (\beta_s - \frac{w_1}{w_2}\beta_t\right)}^2 \nonumber \\
%		&+ n_1 \sigma^2 \cdot \frac{w_1^2}{w_2^2} \bigtr{\left(\frac{w_1^2}{w_2^2}  X_1^{\top}X_1 + X_2^{\top}X_2\right)^{-1}\Sigma_1} \nonumber \\
%		&+ n_2 \cdot \frac{w_1^2}{w_2^2}\bignorm{\Sigma_2^{1/2}\left(\frac{w_1^2}{w_2^2} X_1^{\top}X_1 + X_2^{\top}X_2\right)^{-1}X_1^{\top}X_1\left(\beta_s - \frac{w_1}{w_2}\beta_t\right)}^2 \nonumber \\
%		&+ n_2 \sigma^2 \cdot \bigtr{\left(\frac{w_1^2}{w_2^2} X_1^{\top}X_1 + X_2^{\top}X_2\right)^{-1}\Sigma_2}. \label{eq_val_mtl}
%\end{align}
%\nc
%------------------
%Thus we conclude the proof.

	\medskip
	\noindent\textbf{Dealing with the empirical minimizer.}
	%eet $\hat {\cal W}$ be the minimizer of $g$, and denote $\hat Q:= \hat{\cal W}^{\top} (\hat{\cal W}\hat{\cal W}^{\top})^{-1} \hat{\cal W} $.
	We show that $\norm{\hat{\cW}^{\top} (\hat{\cW}\hat{\cW}^{\top})^{-1} \cW - U_r U_r^{\top}} \le o(1)$ w.h.p.
	%\be\label{Q-Q}\|Q_0^{-1}\hat Q - \id\|_F = \oo(1) \quad \text{w.h.p.}\ee
	Since $\hat{\cW}$ is the minimizer of $g(\cdot)$, we have that
	\begin{align*}
		g(U_r) - g(\hat{\cW}) = \bigabs{g(\hat{\cW}) - g(U_r)} &= n \cdot \bigabs{\inner{{B^{\star}}^{\top} \Sigma B^{\star}}{U_r U_r^{\top} - \hat{\cW}^{\top} (\hat{\cW}\hat{\cW}^{\top})^{-1} \hat{\cW}}}\\
		&\ge n \cdot \sigma_{\min}({B^{\star}}^{\top} \Sigma B^{\star}) \cdot \bignormFro{U_r U_r^{\top} - \hat{\cW}^{\top} (\hat{\cW}\hat{\cW}^{\top})^{-1} \hat{\cW}}.
	\end{align*}
	On the other hand, by equation \eqref{eq_est_g} and triangle inequality, we have that
	\begin{align*}
		g(U_r) - g(\hat{\cW}) &\le \bigabs{g(U_r) - \ex{g(U_r)}} + (\ex{g(U_r)} - \ex{g(\hat{\cW})}) + \bigabs{g(\hat{\cW}) - \ex{g(\hat{\cW})}} \\
		&\le \bigabs{g(U_r) - \ex{g(U_r)}} +\bigabs{g(\hat{\cW}) - \ex{g(\hat{\cW})}} \\
		&\le o(n) \cdot \sigma_{\max}({B^{\star}}^{\top} \Sigma B^{\star}) + O(\sigma^2 p^{1/2 +\e}).
	\end{align*}
	Combined together, and using the assumption that the condition number of ${B^{\star}}^{\top}\Sigma B^{\star}$ does not grow with $p$ and $\sigma_{\min}({B^{\star}}^{\top} \Sigma B^{\star}) \ge \sigma^2 / p^{1/2}$, we conclude that the distance between the two subspaces $\hat{\cW}^{\top}(\hat{\cW}\hat{\cW}^{\top})^{-1}\hat{\cW}$ and $U_r U_r^{\top}$ diminish to $o(1)$ as $p$ grows.
	%The second step uses the fact that $U_r U_r^{\top}$ is the global minimum of $\ex{g(\cdot)}$.
	%In fact, if \eqref{Q-Q} does not hold, then using the condition $\lambda_{\min}((B^{\star})^\top\Sigma B^{\star})\gtrsim \sigma^2$ and that $\delta_{\vari}(\cal W)=\OO(\sigma^2)$ by  Lemma \ref{lem_minv}, we obtain that
	%$$   \val(\hat {Q}) + t  \sigma^2 > (\val( Q_0) + t \sigma^2 )\cdot (1+\oo(1)) \ \Rightarrow \ g( \hat{\cal W})>g( {\cal W}_0),$$
	%where $\cal W_0\in \R^{r\times t}$ is a matrix such that  $ \cal W_0^{\top} (\cal W_0\cal W_0^{\top})^{-1} \cal W_0=Q_0$. Hence $\hat {\cal W}$ is not a minimizer, which leads to a contradiction.

	\medskip
	\noindent\textbf{Putting all parts together.}
	%In sum, we have solved that $\hat{\beta}_i^{\MTL}=B^{\star}\left( U_r v_i +\oo(1)\right)$.
	Now we are ready to finish the proof.
	Similar to the proof of equation \eqref{eq_est_g1} and \eqref{eq_est_g2}, we have that
	\begin{align}
		L(\hat{\beta}_t^{\MTL}) = \exarg{\varepsilon_1, \dots, \varepsilon_t}{L(\hat{\beta}_t^{\MTL})} (1 + O(p^{-1/2+c})) + O(\sigma^2 \cdot p^{-1/2 + c}). \label{eq_multiple_last1}
	\end{align}
	Using equation \eqref{eq_ex_pred}, we have that
	\begin{align}
		\exarg{\varepsilon_1, \dots, \varepsilon_t}{\hat{\beta}_t^{\MTL}} &= \bignorm{\Sigma^{1/2} \left((B^\star \hat{\cal W}^\top) (\hat{\cal W}\hat{\cal W}^{\top})^{-1} \hat W_t - \beta_t \right) }^2
		+ \sigma^2  \hat W_t^{\top} (\hat{\cal W}\hat{\cal W}^{\top})^{-1} \hat W_t \cdot \bigtr{\Sigma (X^{\top}X)^{-1}} \nonumber \\
		&= \bignorm{\Sigma^{1/2} \bigbrace{B^{\star} U_r v_t-\beta_t}}^2 + \oo\left(\bigtr{{B^{\star}}^{\top} \Sigma B^{\star}}\right) + \sigma^2(\norm{v_t}^2 + o(1)) \bigtr{\Sigma (X^{\top}X)^{-1}} \cdot (1+\oo(1)) \nonumber \\
		&= \bignorm{\Sigma^{1/2} \bigbrace{B^{\star} U_r v_t-\beta_t}}^2 + \frac{\sigma^2}{\rho-1}\norm{v_t}^2 + \oo \left( \bigtr{{B^{\star}}^{\top} \Sigma B^{\star}} + \sigma^2\right), \label{eq_multiple_last2}
	\end{align}
	In the second step, we use that $\norm{\hat{\cW}^{\top} (\cW \cW^{\top})^{-1} \cW - U_r U_r^{\top}} \le o(1)$ w.h.p., which implies that
	\[ \bignorm{U_r v_t - \hat{\cW}^{\top} (\cW \cW^{\top})^{-1} \hat{\cW}_t} \le o(1) \quad \text{ and } \quad \bigabs{\hat{\cW}_t^{\top} (\hat{\cW} \hat{\cW}^{\top})^{-1} \hat{W}_t - \norm{v_t}^2} \le o(1). \]
  In the last step, we use Lemma \ref{lem_minv}, that is, $\bigtr{\Sigma (X^{\top} X)^{-1}} = \sigma^2 \cdot (1 + o(1))/ (\rho - 1)$.
	Combining equation \eqref{eq_multiple_last1}, \eqref{eq_multiple_last2}, and $\te(\hat{\beta}_t^{\STL})=\frac{\sigma^2}{\rho-1} \cdot \left( 1+\oo(1)\right)$, we conclude the proof.
\end{proof}
%From the above we can obtain three conceptual insights that are consistent with Section \ref{sec_denoise} and \ref{sec_insight}.
%\begin{itemize}
%	\item The de-noising effect of multi-task learning.
%	\item Multi-task training vs single-task training can be either positive or negative.
%	\item Transfer learning is better than the other two. And the improvement over multi-task training increases as the model distances become larger.
%\end{itemize}






