\section{A Bias-variance Decomposition for Multiple Tasks}\label{sec_same}

%We begin by considering the case where all tasks have the same sample size and feature covariates, that is, $n_i = n$ and $X^{(i)} = X\in\real^{n\times p}$ for all $i = 1, \dots, t$.
%We provide a sharp generalization error bound of hard parameter sharing estimators.
We show that the prediction loss of hard parameter sharing admits a clean bias-variance decomposition, when all tasks have the same sample size and feature covariates.
%This setting is prevalent in applications of multi-task learning to image classification, where there are multiple prediction labels/tasks for every image \cite{chexnet17,EA20}.
%We consider an arbitrary local minimum $B, W_1, \dots, W_2$ of the optimization objective.
%We extend the bias-variance decomposition from the two-task case to the multiple-task case.
%We observe that the expected prediction loss of $\hat{\beta}_t^{\MTL}$ conditional on $X$ consists of a bias and a variance equation as follows
%\begin{align}
%	\exarg{\varepsilon_1, \dots, \varepsilon_t}{L(\hat{\beta}_t^{\MTL}) \mid X}
%	=& \bignorm{\Sigma^{1/2} \bigbrace{B^{\star} \cW^{\top} (\cW \cW^{\top})^{-1} W_t - \beta_t}}^2 \label{eq_bias_multiple} \\
%	&+ \sigma^2 \cdot (W_t^{\top} (\cW \cW^{\top})^{-1} W_t) \cdot \bigtr{\Sigma (X^{\top} X)^{-1}} \label{eq_var_multiple}
%\end{align}
%One can see that equation \eqref{eq_bias_multiple} is the bias of the multi-task learning estimator and equation \eqref{eq_var_multiple} is its variance.
%Compared to the prediction loss of single-task learning (cf. equation \eqref{eq_var_stl}), we observe that the variance equation \eqref{eq_var_multiple} is always smaller because $W_t^{\top} (\cW \cW^{\top})^{-1} W_t \le 1$.
%On the other hand, the bias equation \eqref{eq_bias_multiple} is always larger because of the difference between the task models.
%We show the generalization error of hard parameter sharing estimators.
%Before stating the result, we define the following notations.
Let $\Sigma$ denote the (same) population covariance matrix of all tasks.
Let $A^{\star} {A^{\star}}^{\top}$ denote the best rank-$r$ subspace approximation of ${B^{\star}}^\top\Sigma B^{\star}$, that is,
\[ A^{\star} \define \argmin_{U\in\real^{t\times r} : U^{\top} U = \id_{r\times r}} \inner{U U^{\top}} {{B^{\star}}^{\top} \Sigma B^{\star}}. \]
For $i = 1,\dots, t$, let $a_i^{\star} \in\real^r$ denote the $i$-th column of $A^{\star}{A^{\star}}^{\top}$.
Our main result shows that hard parameter sharing essentially approximates all tasks through the rank-$r$ matrix $B^{\star} A^{\star} {A^{\star}}^{\top}$.

\begin{theorem}[Bias-variance decomposition for prediction loss]\label{thm_many_tasks}
%Suppose $X=Z\Sigma^{1/2}\in \R^{n\times p}$ satisfy Assumption \ref{assm_secA1} with $\rho:=n/p>1$ being some fixed constant. Consider data models  $Y_i = X\beta_i + \varepsilon_i$, $i=1,2,\cdots, t$, where $\e_i\in \R^{n}$ are random vectors with i.i.d. entries with mean zero, variance $\sigma^2$ and all moments as in \eqref{assmAhigh}. Moreover, assume that $X$, $\beta_i$ and $\e_i$ are all independent of each other.
	%Let $n = c \cdot p$.
	%Let $X\in\real^{n\times p}$ and $Y_i = X\beta_i + \varepsilon_i$, for $i = 1,\dots,k$.
%	Consider $t$ data models $Y_i = X\beta_i + \varepsilon_i$, $i=1,2,\cdots, t$, where $X$ has covariance matrix $\Sigma$, and the entries of $\e_i$ are i.i.d. with mean zero and variance $\sigma^2$.gT
	%that satisfy Assumption \ref{assm_secA2} in the appendix.
	Consider the multiple-task case where all tasks have the same covariate $X\in\real^{n\times p}$ and population covariance matrix $\Sigma\in\real^{p\times p}$.
	Suppose that $X$ satisfies Assumption \ref{assume_rm} and the sample size $n$ is greater than $\rho \cdot p$ for an absolute constant $\rho$ greater than one.
	Recall that $c_{\varphi}$ is any fixed value within $(0, \frac{\varphi-4}{2\varphi})$.
	% Let $\delta$ be any fixed value such that $\delta \le \oo \left( \|B^\star\|^2 + \sigma^2\right)$.
	Let $L(B^{\star}a_i^{\star}): = \norm{\Sigma^{1/2} (B^{\star} a_i^{\star}- \beta^{(i)})}^2$.
	Let $C_1: = \frac{\rho\cdot \normFro{\Sigma^{1/2} B^{\star}}^2}{(\sqrt{\rho} - 1)^2\lambda_{\min}^2(\Sigma^{1/2} B^{\star}) + \sigma^2}$ and $C_2 :=  C_1\cdot \norm{\Sigma^{1/2} B^{\star}}$ \FY{why not use $C_1: = \frac{ \normFro{\Sigma^{1/2} B^{\star}}^2}{ \lambda_{\min}^2(\Sigma^{1/2} B^{\star}) + \sigma^2}$? $C_2$ does not seem to be correct because the dimension does not match. I will check the proof to see whether $C_2 =  C_1\cdot \norm{\Sigma^{1/2} B^{\star}}^2$ or $C_1\cdot \sigma\norm{\Sigma^{1/2} B^{\star}}$?}, both of which do not grow with $p$.
	Then, for any task $i = 1, 2, \dots, t$, the prediction loss of the hard parameter sharing estimator $\hat{\beta}_i^{\MTL}$ satisfies that with high probability,
	\begin{align}
		\bigabs{L(\hat{\beta}_i^{\MTL}) - L(B^{\star}a_i^{\star}) - \sigma^2  \cdot \frac{p \cdot \norm{a_i^{\star}}^2}{n - p}} \le  p^{-c_{\varphi}}\bigbrace{C_1 L(B^{\star}a_i^{\star}) + C_1 \sigma^2 \frac{p \cdot{\norm{a_i^{\star}}^2}}{n-p} + C_2}. \label{eq_thm_many}
	\end{align}
	
	%\begin{itemize}
	%	\item \textbf{Positive transfer:} If $\left(1 - \norm{v_t}^2 \right)\frac{\sigma^2}{\rho - 1} -  > \delta$, then w.h.p over the randomness of $X, \varepsilon_1, \dots, \varepsilon_t$, we have that
	%	 \[ \te(\hat{\beta}_t^{\MTL}) < \te(\hat{\beta}_t^{\STL}). \]
	%	\item \textbf{Negative transfer:} If $\left(1 - \norm{v_t}^2\right)\frac{\sigma^2}{\rho - 1} - \norm{\Sigma^{1/2}(B^{\star} U_r v_t - \beta_t)}^2 < -\delta$, then w.h.p. over the randomness of $X, \varepsilon_1, \dots, \varepsilon_t$, we have that
	%	\[ \te(\hat{\beta}_t^{\MTL}) > \te(\hat{\beta}_t^{\STL}). \]
	%\end{itemize}
\end{theorem}
	Theorem \ref{thm_many_tasks} provides a sharp generalization error bound that is asymptotically tight when $p$ goes to infinity.
	The limiting loss of hard parameter sharing consists of two parts, a bias term $L(B^{\star} a_i^{\star})$ that measures the error of $B^{\star} a_i^{\star}$, and a variance term that scales with noise variance $\sigma^2$.
%	Our result implies that the variance of hard parameter sharing is always smaller than single-task learning.
%	This is because	STL's variance is equal to $\frac{\sigma^2 \cdot p} {n - p}$ by Fact \ref{lem_minv}, and $\norm{a_i^{\star}}^2 \le 1$ since the spectral norm of $U_r$, which is a projection matrix, is at most one.


We introduce several standard assumptions on the random matrices we study.
\begin{assumption}\label{assume_rm}
	Let $\varphi > 4$ be a fixed value.
	Let $\tau > 0$ be a small enough constant.
	Let $X = Z \Sigma^{1/2} \in\real^{n\times p}$ be a random matrix where $Z \in \real^{n\times p}$ consists of i.i.d. entries with zero mean and unit variance and $\Sigma \in \real^{p\times p}$ is a positive semidefinite matrix.
	We assume the following standard conditions.
	\begin{enumerate}
		\item For every entry of $Z$, we assume that its $\varphi$-th moment exists, that is, there exist a fixed constant $C > 0$ such that
			\begin{align}\label{assmAhigh}
				\ex{\abs{Z_{i,j}}^{\varphi}} \le C, \text{ for any } 1\le i \le n \text{ and } 1\le j \le p.
			\end{align}
		\item The sample size $n$ grows to infinity proportionally with the dimension $p$, i.e. $n / p \rightarrow \rho \in (0, 1/\tau)$ as $p$ goes infinity.
	\end{enumerate}
\end{assumption}

Under the above assumption, we state several well-known facts regarding the random matrix $X$.

\begin{fact}\label{lem_minv}
	Let $c_{\varphi}$ be any fixed value within $(0, \frac{\varphi - 4}{2\varphi})$.
	%Let $X  \in \real^{n\times p}$ be a random matrix that satisfies Assumption \ref{assume_rm}.
	%Let $\Sigma\in\real^{p\times p}$ denote the population covariance matrix of $X$.
	With high probability over the randomness of $X$, we have that
	\begin{enumerate}
		\item When $n / p$ converges to $\rho > 1$, the sample covariance matrix $\frac{X^{\top}X}{n}$ is full rank.
		\item $\bigtr{\Sigma (X^{\top} X)^{-1}} = \frac{p}{n - p} + \OO(p^{-c_{\varphi}})$, cf. Theorem 2.4 in \citet{isotropic}.
		\item The singular values of $Z^{\top}Z$ are greater than $(\sqrt{n} - \sqrt{p})^2 - n \cdot p^{-c_{\varphi}}$ and less than $(\sqrt{n} + \sqrt{p})^2 + n \cdot p^{-c_{\varphi}}$, cf. \citet[Theorem 2.10]{isotropic} and \citet[Lemma 3.12]{DY}.
	\end{enumerate}
\end{fact}

Finally, for the random noise component, we assume that all of its moments exist.
More precisely, there exists a fixed function $C(\cdot) : \mathbb{Z} \rightarrow \real^+$ such that for any $a = 1, 2, \dots, \infty$, we have that
\begin{align}\label{assmAhigh2}
	\ex{\abs{\varepsilon_{j}^{(i)}}^a} \le C(a), \text{ for any } 1\le i\le t \text{ and } 1\le j\le n_i.
\end{align}
Hence, for any value $\varphi > 4$, we get that Fact \ref{lem_minv} holds for $\varepsilon^{(i)}$, for all $i = 1, 2, \dots, t$.
Let $\e$ be a small enough fixed value and let $c_{\infty}$ be any fixed value within $(0, 1/2-\e)$.
We have that Fact \ref{lem_minv} holds for $\varepsilon^{(i)}$ where $c_{\varphi}$ becomes $c_{\infty}$ instead.

\paragraph{Random-effects model.}
A natural special case of the linear model is when every $\beta_i$ consists of two random components, one that is shared among all tasks and one that is task-specific.
Thus, each task contributes a certain amount to the shared component and injects a task-specific bias.
Let $\beta_0$ denote the shared component whose entries are sampled i.i.d. from an isotropic Gaussian distribution of mean zero and variance $p^{-1}\kappa^2$.
Let the task-specific component be a random Gaussian vector whose coordinates all have mean zero and variance $p^{-1} d^2 / 2$.
Thus, for any two different $\beta_i$ and $\beta_j$, their distance is roughly $d^2$.
%	The labels are $Y_i = X_i\beta_i + \varepsilon_i$, where $\e_i$ consists of i.i.d. entries with mean zero and variance $\sigma^2$.
For our purpose, it is enough to think of the order of $d$ being $1/\sqrt{p}$ and $pd^2/\sigma^2$ being constant.
%The more precise conditions on the relations between $d^2$, $\sigma^2$ and $\kappa^2$ are given in  \eqref{choiceofpara}.
We assume that all the random variables have finite moments up to any order as in equation \eqref{assmAhigh2}.


\begin{example}[Sample efficiency]\label{ex_same_cov}
We illustrate the above result in the random-effects model described in Section \ref{sec_hps}.
We further assume that $\Sigma$ is isotropic for illustration.

For the bias term, we show that the average prediction loss of $B^{\star} a_i^{\star}$ satisfies that
\[ \frac{1}{t} \sum_{i=1}^t L(B^{\star} a_i^{\star}) = \normFro{B^{\star} A^{\star} {A^{\star}}^{\top} - B^{\star}}^2 = (t - r) d^2. \]
Recall that $A^{\star} {A^{\star}}^{\top}$ is the best rank-$r$ approximation of ${B^{\star}}^{\top}\Sigma B^{\star} = {B^{\star}}^{\top} B^{\star}$, and the $(i, j)$-th entry of this matrix is roughly equal to
\begin{align*}
	\beta_i^{\top} \beta_j \approx \norm{\beta_0}^2 + \begin{cases}
																								0, \text{ if } i \neq j \\
																								d^2, \text{ if } i = j.
	\end{cases}
\end{align*}
which follows from the definition of the random-effects model.
Note that $\norm{\beta_0}^2$ is roughly $\kappa^2$.
One can verify that the top singular value of ${B^{\star}}^{\top} B^{\star}$ is approximately $p \kappa^2 + d^2$ and the rest of its singular values are all equal to $d^2$.
Therefore, by taking a rank-$r$ approximation of ${B^{\star}}^{\top} B^{\star}$, we get the average prediction loss equation above.

For the variance term, since $A^{\star}$ has rank-$r$, we have that $\sum_{i=1}^t \norm{a_i^{\star}}^2 = r$.
Combined together, the average prediction loss of hard parameter sharing is as follows
\[ \frac{1}{t}\sum_{i=1}^t L(\hat{\beta}_i^{\MTL}) = \bigbrace{1 - \frac{r}{t}} d^2 + \frac{r}{t} \cdot \frac{\sigma^2 p}{n - p} \pm \OO(p^{-c_{\varphi}}). \]
We compare hard parameter sharing to single-task learning.
Using Fact \ref{lem_minv}, the average prediction loss of single-task learning is $\sigma^2\cdot \bigtr{\Sigma (X^{\top} X)^{-1}} = \frac{\sigma^2 p}{n - p} \pm \OO(p^{-{c_{\varphi}}})$.
	Suppose that $p$ is sufficiently large so that $p^{-c_{\varphi}}$ is negligible.
First, the prediction loss of hard parameter sharing is less than single-task learning if and only if $d^2 < \frac{\sigma^2 p}{n - p}$: the ``task-specific variance'' of $\beta_i$ is smaller than the ``noise variance''.
Second, when $d^2 < \frac{\sigma^2 p}{n - p}$, hard parameter sharing performs the best when the rank $r$ is one.
	In this case, hard parameter sharing requires only $p + \frac{n - p}{t - (t - 1)\frac{d^2 (n - p)}{\sigma^2 p}}$ samples, which is less than $n$, to achieve comparable loss to single-task learning.
\end{example}


\paragraph{Proof overview.} The key idea for proving Theorem \ref{thm_many_tasks} is a bias-variance decomposition of the prediction loss.
	Since all tasks have the same covariates, the optimization objective \eqref{eq_mtl} becomes
	\begin{align}
		f(A, B) = \sum_{j=1}^t \bignorm{X B A_j - Y^{(j)}}^2, \label{eq_mtl_same_cov}
	\end{align}
	where we recall that $B \in \real^{p \times r}$ and $A_1, A_2, \dots, A_t \in \R^r$.
	Using the local optimality condition over $B$, that is, $\frac{\partial f}{\partial B} = 0$, we obtain $\hat{B}$ as a function of the output layers as follows
	\begin{align} \label{eq_Bhat}
		\hat{B} \define (X^{\top}X)^{-1} X^{\top} \bigbrace{\sum_{j=1}^t Y^{(j)} A_j^{\top}} (A  A^{\top})^{+}
		= (X^{\top} X)^{-1} X^{\top} Y A^{\top} (AA^{\top})^{+}.
	\end{align}
	Above, we have used that $X^{\top}X$ is invertible by Fact \ref{lem_minv}, and $Y$ was defined as $Y=[Y^{(1)},Y^{(2)},\cdots, Y^{(t)}]$.
	%\FY{Is $\dag$ a standard notation? It is a bad notation at least for me because $\dag$ is more often used as Hermitian conjugate. Wiki page uses $(AA^{\top})^{+}$ for pseudo-inverse.}
	Plugging $\hat{B}$ into equation \eqref{eq_mtl_same_cov}, we obtain the following objective that only depends on the output layer:
	\begin{align}
		g(A) \define \sum_{j=1}^t \bignorm{X (X^{\top}X)^{-1}X^{\top} Y A^{\top} (AA^{\top})^{+} A_j - Y^{(j)}}^2. \label{eq_mtl_output_layer}
	\end{align}

	\begin{claim}[Characterizing global optimum]\label{lem_exp_opt}
		In the setting of Theorem \ref{thm_many_tasks}, we have that
		\begin{align}
			\exarg{\set{\varepsilon^{(j)}}_{j=1}^t, X}{g(A)} = n \bignormFro{\Sigma^{1/2} B^{\star} \bigbrace{A^{\top} (AA^{\top})^{\dagger} A - \id_{t\times t}}}^2 + \sigma^2 (n\cdot t - p \cdot r). \label{eq_gA}
		\end{align}
		As a result, the minimum of $\ex{g(A)}$, denoted by $A^{\star}{A^\star}^\top$, is the best rank-$r$ approximation of ${B^{\star}}^{\top}\Sigma B^{\star}$.
	\end{claim}

	 One can see that the expected prediction loss admits a nice bias-variance decomposition in equation \eqref{eq_gA}.
%	The first part measures the bias of hard parameter sharing and the second part measures the variance of hard parameter sharing and scales with $\sigma^2$.
	Furthermore, its minimizer only depends on the bias term since the variance term is fixed.
	%We denote $Q := \cal W^{\top} (\cal W\cal W^{\top})^{-1} \cal W \in\real^{t\times t}$, whose $(i,j)$-th entry is given by $W_i^{\top} (\cal W\cal W^{\top})^{-1} W_j$.
	%Let $B^{\star} = [\beta_1, \beta_2, \dots, \beta_k] \in\real^{p \times k}$ denote the true model parameters.
	%Now we can write $\delta_{\bias}(\cal W)$ succinctly as
	%\begin{align*}
	%	\delta_{\bias}(Q) \equiv \delta_{\bias}(\cal W) = \bignormFro{\Sigma^{1/2}B^{\star}  %\bigbrace{Q -\id}}^2 .
	%\end{align*}
	The minimizer of the bias term is precisely $A^{\star} {A^{\star}}^{\top}$.

	Recall that $(\hat{A}, \hat{B})$ is the global minimizer of $f(A, B)$, where $\hat{B}$ is equal to $\hat{B}(\hat{A})$ given by equation \eqref{eq_Bhat}.
	Furthermore, $\hat{A}$ is a global minimizer of $g(A)$ in equation \eqref{eq_mtl_output_layer}.
	Our next claim shows that the subspace spanned by the rows of $\hat{A}$ is close to that of $A^{\star}$.
	\begin{claim}\label{claim_opt_dist}
		Let $U_{\hat{A}} U_{\hat{A}}^{\top} \in\real^{t\times t}$ denote the subspace $\hat{A}^{\top} (\hat{A}\hat{A}^{\top})^{+} \hat{A}$.
		In the setting of Theorem \ref{thm_many_tasks}, we have that
		\[ \bignormFro{U_{\hat{A}} U_{\hat{A}}^{\top} - A^{\star} {A^{\star}}^{\top}}
				\le  p^{-c_{\varphi}} \cdot C_1. \]
	\end{claim}
	The above claim shows that as $p$ goes to infinity, the distance between the column subspace of $U_{\hat{A}}$ and the column subspace of ${A^{\star}}$ goes to zero.
	Finally, we show that the prediction loss of hard parameter sharing satisfies the following claim.
	\begin{claim}\label{claim_pred_err}
		In the setting of Theorem \ref{thm_many_tasks},
		let $\hat{a}_i = \hat{A}^{\top} (\hat{A}\hat{A}^{\top})^{+} \hat{A}_i$.
		We have that the prediction loss of $\hat{\beta}_i^{\MTL} = \hat{B} \hat{A}_i$ satisfies that
		\begin{align*}
			\bigabs{L(\hat{\beta}_i^{\MTL}) - L(B^{\star} \hat{a}_i) - \norm{\hat{a}_i}^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}}
			\le  2\sigma \cdot p^{\e} \cdot \bigbrace{L(B^{\star} \hat{a}_i) + \sigma \cdot p^{\e} \cdot \norm{\hat{a}_i}^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}}.
		\end{align*}
	\end{claim}
	The proof of Claim \ref{lem_exp_opt}, Claim \ref{claim_opt_dist}, and Claim \ref{claim_pred_err} can be found in Appendix \ref{app_proof_error_same_cov}.
	Provided with these results, we are ready to prove Theorem \ref{thm_many_tasks}.
	\begin{proof}[Proof of Theorem \ref{thm_many_tasks}]
		Using Claim \ref{claim_pred_err}, we get that the prediction loss of $\hat{\beta}_i^{\MTL}$ is within an $\OO(p^{-c_{\infty}})$ fraction of $L(B^{\star}\hat{a}_i)+\norm{\hat{a}_i}^2 \bigtr{\Sigma(X^{\top}X)^{-1}}$.
		For the latter, we use Claim \ref{claim_opt_dist} to upper bound the difference between $\norm{\hat{a}_i}^2$ and $\norm{a_i^{\star}}^2$, and use Fact \ref{lem_minv} (ii) to get a precise estimate of $\bigtr{\Sigma (X^{\top}X)^{-1}}$.
		For $L(B^{\star}\hat{a}_i)$, we again use Claim \ref{claim_opt_dist} to upper bound the distance between $\hat{a}_i$ and $a_i^{\star}$.
		Combined together, we have shown that equation \eqref{eq_thm_many} holds for any fixed value $c_{\varphi}$ within $(0, \frac{\varphi-4}{2\varphi})$, and the proof is complete.
	\end{proof}
