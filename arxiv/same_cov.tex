\section{A Bias-variance Decomposition for Multiple Tasks}\label{sec_same}

%We begin by considering the case where all tasks have the same sample size and feature covariates, that is, $n_i = n$ and $X^{(i)} = X\in\real^{n\times p}$ for all $i = 1, \dots, t$.
%We provide a sharp generalization error bound of hard parameter sharing estimators.
We show that the prediction loss of hard parameter sharing admits a clean bias-variance decomposition, when all tasks have the same sample size and feature covariates. \FY{Is it should be "same covariate" or "same covariates"?}
%This setting is prevalent in applications of multi-task learning to image classification, where there are multiple prediction labels/tasks for every image \cite{chexnet17,EA20}.
%We consider an arbitrary local minimum $B, W_1, \dots, W_2$ of the optimization objective.
%We extend the bias-variance decomposition from the two-task case to the multiple-task case.
%We observe that the expected prediction loss of $\hat{\beta}_t^{\MTL}$ conditional on $X$ consists of a bias and a variance equation as follows
%\begin{align}
%	\exarg{\varepsilon_1, \dots, \varepsilon_t}{L(\hat{\beta}_t^{\MTL}) \mid X}
%	=& \bignorm{\Sigma^{1/2} \bigbrace{B^{\star} \cW^{\top} (\cW \cW^{\top})^{-1} W_t - \beta_t}}^2 \label{eq_bias_multiple} \\
%	&+ \sigma^2 \cdot (W_t^{\top} (\cW \cW^{\top})^{-1} W_t) \cdot \bigtr{\Sigma (X^{\top} X)^{-1}} \label{eq_var_multiple}
%\end{align}
%One can see that equation \eqref{eq_bias_multiple} is the bias of the multi-task learning estimator and equation \eqref{eq_var_multiple} is its variance.
%Compared to the prediction loss of single-task learning (cf. equation \eqref{eq_var_stl}), we observe that the variance equation \eqref{eq_var_multiple} is always smaller because $W_t^{\top} (\cW \cW^{\top})^{-1} W_t \le 1$.
%On the other hand, the bias equation \eqref{eq_bias_multiple} is always larger because of the difference between the task models.
%We show the generalization error of hard parameter sharing estimators.
%Before stating the result, we define the following notations.
Let $\Sigma\in \R^{p\times p}$ denote the population covariance matrix of all tasks.
Suppose we have $t$ datasets whose features are all equal to $X$, and the label (vector) of the $i$-th task is equal to $Y^{(i)} = X \beta^{(i)} + \varepsilon^{(i)}$.
We assume that: $X = Z \Sigma^{1/2}$ and every entry of $Z \in \real^{n \times p}$ is drawn independently from a one dimensional distribution with mean zero, unit variance, and constant $\varphi$-th moment for a fixed $\varphi > 4$;
every entry of $\varepsilon^{(i)} \in \real^{n \times t}$ is drawn indepdently from a one dimensional ditribution with mean zero, variance $\sigma^2$, and bounded moment up to any order.\footnote{More precisely, there exists a fixed function $C: \mathbb{N} \rightarrow \real^+$ such that for any $k \in \N$, the $k$-th moment is bounded by $C(k)$.}
Let $A^{\star} {A^{\star}}^{\top}$ denote the best rank-$r$ subspace approximation of ${B^{\star}}^\top\Sigma B^{\star}$, that is,
\[ A^{\star} \define \argmin_{U\in\real^{t\times r} : U^{\top} U = \id_{r\times r}} \inner{U U^{\top}} {{B^{\star}}^{\top} \Sigma B^{\star}}. \]
To ensure that $A^{\star}$ is unique, we assume that $\lambda_{r+1}({B^\star}^\top \Sigma B^\star)$ is strictly smaller than $\lambda_{r}({B^\star}^\top \Sigma B^\star)$.
For $i = 1,\dots, t$, let $a_i^{\star} \in\real^r$ denote the $i$-th column of $A^{\star}{A^{\star}}^{\top}$. 
Our main result shows that hard parameter sharing essentially approximates all tasks through the rank-$r$ matrix $B^{\star} A^{\star} {A^{\star}}^{\top}$.

\begin{theorem}[Bias-variance decomposition for prediction loss]\label{thm_many_tasks}
%Suppose $X=Z\Sigma^{1/2}\in \R^{n\times p}$ satisfy Assumption \ref{assm_secA1} with $\rho:=n/p>1$ being some fixed constant. Consider data models  $Y_i = X\beta_i + \varepsilon_i$, $i=1,2,\cdots, t$, where $\e_i\in \R^{n}$ are random vectors with i.i.d. entries with mean zero, variance $\sigma^2$ and all moments as in \eqref{assmAhigh}. Moreover, assume that $X$, $\beta_i$ and $\e_i$ are all independent of each other.
	%Let $n = c \cdot p$.
	%Let $X\in\real^{n\times p}$ and $Y_i = X\beta_i + \varepsilon_i$, for $i = 1,\dots,k$.
%	Consider $t$ data models $Y_i = X\beta_i + \varepsilon_i$, $i=1,2,\cdots, t$, where $X$ has covariance matrix $\Sigma$, and the entries of $\e_i$ are i.i.d. with mean zero and variance $\sigma^2$.gT
	%that satisfy Assumption \ref{assm_secA2} in the appendix.
	Assume that $n > \rho p$ for a fixed constant $\rho > 1$.
	Let $c_{\varphi}$ be any fixed value within $(0, \frac{\varphi-4}{2\varphi})$.
	Let $L(B^{\star}a_i^{\star}): = \norm{\Sigma^{1/2} (B^{\star} a_i^{\star}- \beta^{(i)})}^2$.
	For any task $i = 1, 2, \dots, t$, the prediction loss of the estimator $\hat{\beta}_i^{\MTL}$ satisfies that with high probability,
	\begin{align*}
		\bigabs{L(\hat{\beta}_i^{\MTL}) - L(B^{\star}a_i^{\star}) - \sigma^2 \norm{a_i^{\star}}^2 \bigtr{\Sigma (X^{\top}X)^{-1}}}
		\le n^{-c_{\varphi}/2} \cdot C_1\bigbrace{ \norm{\Sigma^{1/2} B^{\star}}^2+  \sigma^2}  ,
	\end{align*}
	where $C_1: = \frac{\bignormFro{\Sigma^{1/2} B^{\star}}^2 + \sigma^2 t}{\lambda_r ({B^\star}^\top \Sigma B^\star)- \lambda_{r+1}({B^\star}^\top \Sigma B^\star)}$. %and $C_2 :=  C_1\cdot \norm{\Sigma^{1/2} B^{\star}}$.
\end{theorem}

%\FY{why not use $C_1: = \frac{ \normFro{\Sigma^{1/2} B^{\star}}^2}{ \lambda_{\min}^2(\Sigma^{1/2} B^{\star}) + \sigma^2}$? $C_2$ does not seem to be correct because the dimension does not match. I will check the proof to see whether $C_2 =  C_1\cdot \norm{\Sigma^{1/2} B^{\star}}^2$ or $C_1\cdot \sigma\norm{\Sigma^{1/2} B^{\star}}$?}
Theorem \ref{thm_many_tasks} provides a sharp generalization error bound that is asymptotically tight when $n$ goes to infinity.
The limiting loss of hard parameter sharing consists of two parts, a bias term $L(B^{\star} a_i^{\star})$ that measures the error of $B^{\star} a_i^{\star}$, and a variance term that scales with noise variance $\sigma^2$.
%	Our result implies that the variance of hard parameter sharing is always smaller than single-task learning.
%	This is because	STL's variance is equal to $\frac{\sigma^2 \cdot p} {n - p}$ by Fact \ref{lem_minv}, and $\norm{a_i^{\star}}^2 \le 1$ since the spectral norm of $U_r$, which is a projection matrix, is at most one.
We can compare how well hard parameter sharing performs vs. single-task learning, whose prediction loss is equal to $\sigma^2 \tr[\Sigma (X^{\top} X)^{-1}]$, for every task $i$.
On one hand, hard parameter sharing helps by reducing variance, since $\norm{a_i^{\star}}^2 \le 1$.
On the other hand, it hurts by increasing bias.


\paragraph{How does hard parameter sharing scale with sample size $n$?}
We first consider the variance term.
Intuitively, variance reduces as $n$ increases.
To make this precise, we introduce the high-dimensional setting that assumes $n / p$ increases at a fixed ratio.
\begin{assumption}\label{assume_rm}
	Let $\tau > 0$ be a small enough constant.
%	Let $X = Z \Sigma^{1/2} \in\real^{n\times p}$ be a random matrix where $Z \in \real^{n\times p}$ consists of i.i.d. entries with zero mean and unit variance and $\Sigma \in \real^{p\times p}$ is a positive semidefinite matrix.
	In the high-dimensional setting,
%		\item For every entry of $Z$, we assume that its $\varphi$-th moment exists, that is, there exist a fixed constant $C > 0$ such that
%			\begin{align}\label{assmAhigh}
%				\ex{\abs{Z_{i,j}}^{\varphi}} \le C, \text{ for any } 1\le i \le n \text{ and } 1\le j \le p.
%			\end{align}
  the sample size $n$ grows to infinity proportionally with the dimension $p$, i.e. $n / p \rightarrow \rho \in (\tau, 1/\tau)$ as $p$ goes to infinity.
%	\end{enumerate}
\end{assumption}
Under the above assumption, we state a well-known fact for $\tr[\Sigma (X^{\top} X)^{-1}] = \tr[(Z^{\top} Z)^{-1}]$.

\begin{fact}[cf. Theorem 2.4 in \citet{isotropic}]\label{fact_tr}
	%Let $X  \in \real^{n\times p}$ be a random matrix that satisfies Assumption \ref{assume_rm}.
	%Let $\Sigma\in\real^{p\times p}$ denote the population covariance matrix of $X$.
	With high probability over the randomness of $Z$, we have that $\bigtr{\Sigma (X^{\top} X)^{-1}} = \frac{p}{n - p} + \OO(n^{-c_{\varphi}})$.
\end{fact}

%Finally, for the random noise component, we assume that all of its moments exist.
%More precisely, there exists a fixed function $C(\cdot) : \mathbb{Z} \rightarrow \real^+$ such that for any $a = 1, 2, \dots, \infty$, we have that
%\begin{align}\label{assmAhigh2}
%	\ex{\abs{\varepsilon_{j}^{(i)}}^a} \le C(a), \text{ for any } 1\le i\le t \text{ and } 1\le j\le n_i.
%\end{align}
%Hence, for any value $\varphi > 4$, we get that Fact \ref{lem_minv} holds for $\varepsilon^{(i)}$, for all $i = 1, 2, \dots, t$.
%Let $\e$ be a small enough fixed value and let $c_{\infty}$ be any fixed value within $(0, 1/2-\e)$.
%We have that Fact \ref{lem_minv} holds for $\varepsilon^{(i)}$ where $c_{\varphi}$ becomes $c_{\infty}$ instead.

Next, we consider the bias $L(B^{\star} a^{\star}_i)$.
We illustrate through a random-effects model, which has been studied for a single task case \cite{dobriban2020wonder}.
Suppose every $\beta_i$ consists of two random components, one that is shared among all tasks and one that is task-specific.
Thus, each task contributes a certain amount to the shared component and injects a task-specific bias.
Let $\beta_0$ denote the shared component whose entries are sampled i.i.d. from an isotropic Gaussian distribution of mean zero and variance $p^{-1}\kappa^2$.
Let the task-specific component be a random Gaussian vector with i.i.d. entries of mean zero and variance $p^{-1} d^2 / 2$.
Thus, for any two different $\beta_i$ and $\beta_j$, their distance is roughly $d^2$.
%	The labels are $Y_i = X_i\beta_i + \varepsilon_i$, where $\e_i$ consists of i.i.d. entries with mean zero and variance $\sigma^2$.
Concretely, we can think of $\kappa = 1$ and $d^2/\sigma^2 = \OO(1)$.
%The more precise conditions on the relations between $d^2$, $\sigma^2$ and $\kappa^2$ are given in  \eqref{choiceofpara}.
%We assume that all the random variables have finite moments up to any order as in equation \eqref{assmAhigh2}.

\begin{example}[Sample efficiency]\label{ex_same_cov}
In the random-effects model described above, we further assume that $\Sigma$ is isotropic for illustration. We show that when the rank of hard parameter sharing is one, the bias $L(B^{\star} a_i^{\star})$ satisfies that with high probability, 
\[ \frac{1}{t} \sum_{i=1}^t L(B^{\star} a_i^{\star}) = \frac{1}{t}\normFro{B^{\star} A^{\star} {A^{\star}}^{\top} - B^{\star}}^2 \approx \left(1 - \frac{1}{t}\right) \frac{ d^2}{2}  . \]
Since $A^{\star} {A^{\star}}^{\top}$ is the best rank-$1$ approximation of ${B^{\star}}^{\top}\Sigma B^{\star} = {B^{\star}}^{\top} B^{\star}$, and the $(i, j)$-th entry of this matrix is roughly equal to
%\begin{align*}
%	\beta_i^{\top} \beta_j \approx \norm{\beta_0}^2 + \frac{d^2}{2}\delta_{ij},\quad 1\le i,j \le t,
%\end{align*}
\begin{align*}
	\beta_i^{\top} \beta_j \approx \norm{\beta_0}^2 + \begin{cases}
																								0, &\text{ if } i \neq j \\
																								d^2/2, &\text{ if } i = j
	\end{cases}
\end{align*}
which follows from the definition of the random-effects model.
Note that $\norm{\beta_0}^2$ is roughly $\kappa^2$.
One can verify that the top eigenvalue of ${B^{\star}}^{\top} B^{\star}$ is approximately $t \kappa^2 + d^2/2$ and the rest of its eigenvalues are all equal to $d^2/2$.
Therefore, by taking a rank-$1$ approximation of ${B^{\star}}^{\top} B^{\star}$, we get the average prediction loss as above.

For the variance term, since $A^{\star}$ has rank-$1$, we have that $\sum_{i=1}^t \norm{a_i^{\star}}^2 = 1$. Moreover, we can use Fact \ref{fact_tr} to calculate $\bigtr{\Sigma (X^{\top} X)^{-1}}$. 
Combined together, the average prediction loss of hard parameter sharing is as follows
\[ \frac{1}{t}\sum_{i=1}^t L(\hat{\beta}_i^{\MTL}) = \bigbrace{1 - \frac{1}{t}} \frac{d^2}{2} + \frac{1}{t} \cdot \frac{\sigma^2 p}{n - p} \pm \OO(n^{-c_{\varphi} / 2}) .\] %\pm \OO(n^{-c_\varphi} (\sigma^2 +\kappa^2 +d^2)). \]
%In fact, the error for this approximate equation is at most $\OO(n^{-c_\varphi/2} (\sigma^2 +\kappa^2 +d^2))$.
\end{example}
We compare hard parameter sharing to single-task learning.
Using Fact \ref{fact_tr} above, the average prediction loss of single-task learning is $\sigma^2\cdot \bigtr{\Sigma (X^{\top} X)^{-1}} = \frac{\sigma^2 p}{n - p}  \pm \OO(n^{-c_\varphi} \sigma^2)$ with high probability.
	Suppose $n$ is sufficiently large so that the error is negligible.
\begin{enumerate}
	\item The prediction loss of hard parameter sharing is less than single-task learning if and only if $\frac{d^2}{2} < \frac{\sigma^2 p}{n - p}$, that is, the ``task-specific variance'' of $\beta_i$ is smaller than the ``noise variance''.
	\item When $\frac{d^2}{2} < \frac{\sigma^2 p}{n - p}$, increasing $r$ does not help.
	To see this, one can verify what when $r$ increases by one, bias reduces by $\frac{d^2}{2}$, but variance increases by $\frac{\sigma^2 p}{n-1}$ (details omitted).
	\item Hard parameter sharing requires at most $p + \frac{n - p}{t - (t - 1)\frac{d^2 (n - p)}{2\sigma^2 p}} < n$ samples to get comparable loss to single-task learning.
	This follows by using this sample size in the average prediction loss equation above.
\end{enumerate}
%\FY{I think these two facts probably need more clarification?}


\paragraph{Proof overview.} The key idea for proving Theorem \ref{thm_many_tasks} is a characterization of $f(A, B)$'s global minimizer.
	Since all tasks have the same covariates, the optimization objective \eqref{eq_mtl} becomes
	\begin{align}
		f(A, B) = \sum_{j=1}^t \bignorm{X B A_j - Y^{(j)}}^2, \label{eq_mtl_same_cov}
	\end{align}
	where we recall that $B \in \real^{p \times r}$ and $A_1, A_2, \dots, A_t \in \R^r$.
	Using the local optimality condition over $B$, that is, $\frac{\partial f}{\partial B} = 0$, we obtain $\hat{B}$ as a function of the output layers as follows
	\begin{align} \label{eq_Bhat}
		\hat{B}(A) \define (X^{\top}X)^{-1} X^{\top} \bigbrace{\sum_{j=1}^t Y^{(j)} A_j^{\top}} (A  A^{\top})^{+}
		= (X^{\top} X)^{-1} X^{\top} Y A^{\top} (AA^{\top})^{+}.
	\end{align}
	Here we have used that $X^{\top}X$ is invertible since $n > \rho p$ (cf. Fact \ref{fact_minv}).
	%\FY{Is $\dag$ a standard notation? It is a bad notation at least for me because $\dag$ is more often used as Hermitian conjugate. Wiki page uses $(AA^{\top})^{+}$ for pseudo-inverse.}
	Plugging $\hat{B}(A)$ into equation \eqref{eq_mtl_same_cov}, we obtain the following objective that only depends on the output layer:
	\begin{align}
		g(A) \define \sum_{j=1}^t \bignorm{X (X^{\top}X)^{-1}X^{\top} Y A^{\top} (AA^{\top})^{+} A_j - Y^{(j)}}^2. \label{eq_mtl_output_layer}
	\end{align}
	Let $\hat{A}$ be the global minimizer of $g(A)$. Then  $(\hat{A}, \hat{B}(\hat A))$ is the global minimizer of $f(A, B)$. 
	%Recall that $(\hat{A}, \hat{B})$ is the global minimizer of $f(A, B)$, where $\hat{B}$ is equal to $\hat{B}(\hat{A})$ given by equation \eqref{eq_Bhat}.
	%Furthermore, $\hat{A}$ is a global minimizer of $g(A)$ in equation \eqref{eq_mtl_output_layer}.
	Our next claim shows that the subspace spanned by the rows of $\hat{A}$ is close to that of $A^{\star}$.
	\begin{claim}\label{claim_opt_dist}
		Let $U_{\hat{A}} U_{\hat{A}}^{\top} \in\real^{t\times t}$ denote the subspace projection $\hat{A}^{\top} (\hat{A}\hat{A}^{\top})^{+} \hat{A}$.
		In the setting of Theorem \ref{thm_many_tasks}, we have that
		\[ \bignormFro{U_{\hat{A}} U_{\hat{A}}^{\top} - A^{\star} {A^{\star}}^{\top}}^2
				\le  p^{-c_{\varphi}} \cdot C_1. \]
	\end{claim}
	The proof of the above claim is based on the following characterization.

	\begin{claim}\label{lem_exp_opt}
		In the setting of Theorem \ref{thm_many_tasks}, we have that
		\begin{align}
			\exarg{\set{\varepsilon^{(j)}}_{j=1}^t, X}{g(A)} = n \bignormFro{\Sigma^{1/2} B^{\star} \bigbrace{A^{\top} (AA^{\top})^{\dagger} A - \id_{t\times t}}}^2 + \sigma^2 (n\cdot t - p \cdot r). \label{eq_gA}
		\end{align}
		As a result, the minimum of $\ex{g(A)}$, denoted by $A^{\star}{A^\star}^\top$, is the best rank-$r$ approximation of ${B^{\star}}^{\top}\Sigma B^{\star}$.
	\end{claim}

	 One can see that the expected optimization objective also admits a nice bias-variance decomposition.
	Furthermore, its minimizer only depends on the bias term since the variance term is fixed, and the minimizer of the bias term is precisely $A^{\star} {A^{\star}}^{\top}$.

	The next piece of our proof deals with the prediction loss of hard parameter sharing.
	\begin{claim}\label{claim_pred_err}
		In the setting of Theorem \ref{thm_many_tasks},
		let $\hat{a}_i = \hat{A}^{\top} (\hat{A}\hat{A}^{\top})^{+} \hat{A}_i$.
		We have that the prediction loss of $\hat{\beta}_i^{\MTL} := \hat{B} \hat{A}_i$ satisfies that
		\begin{align*}
			\bigabs{L(\hat{\beta}_i^{\MTL}) - L(B^{\star} \hat{a}_i) - \sigma^2 \norm{\hat{a}_i}^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}}
			\le  n^{-c_\infty} \left( L(B^{\star} \hat{a}_i)+ \sigma^2  \cdot\|\hat a_i\|^2\right) .
		\end{align*}
	\end{claim}
	The proof of Claim \ref{claim_opt_dist}, Claim \ref{lem_exp_opt}, and Claim \ref{claim_pred_err} can be found in Appendix \ref{app_proof_error_same_cov}.
	Provided with these results, we are ready to prove Theorem \ref{thm_many_tasks}.
	\begin{proof}[Proof of Theorem \ref{thm_many_tasks}]
		Using Claim \ref{claim_pred_err}, we get that the prediction loss of $\hat{\beta}_i^{\MTL}$ is within an $\OO(n^{-c_{\infty}})$ fraction of $L(B^{\star}\hat{a}_i)+\sigma^2\norm{\hat{a}_i}^2\cdot \bigtr{\Sigma(X^{\top}X)^{-1}}$. Moreover, Claim \ref{claim_opt_dist} gives directly an upper bound on $\|\hat a_i - a_i^\star\|^2$. With this estimate, we can bound the difference 
		$$L(B^{\star}\hat{a}_i)+\sigma^2\norm{\hat{a}_i}^2\cdot \bigtr{\Sigma(X^{\top}X)^{-1}} - L(B^{\star} {a}^\star_i)-\sigma^2\norm{{a}^\star_i}^2\cdot \bigtr{\Sigma(X^{\top}X)^{-1}}.$$
		Combined together, our proof is complete. We omit the details.
%		For the latter, we use Claim \ref{claim_opt_dist} to upper bound the difference between $\norm{\hat{a}_i}^2$ and $\norm{a_i^{\star}}^2$.
%		For $L(B^{\star}\hat{a}_i)$, we again use Claim \ref{claim_opt_dist} to upper bound the distance between $\hat{a}_i$ and $a_i^{\star}$.
%		Combined together, our proof is complete.
	\end{proof}
