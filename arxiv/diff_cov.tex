\section{The Different Covariates Setting}\label{sec_diff}

When the covariates across tasks are different, we can no longer obtain an optimal solution for minimizing equation \eqref{eq_mtl} as in Claim \ref{lem_exp_opt}.
This is because the minimization problem is non-convex with respect to $A$ and $B$.
Therefore, we consider an arbitrary local minimum of equation \eqref{eq_mtl} and provide generalization bounds that scale with key properties of task data such as sample sizes and covariate shifts.
We focus on the two-task case to better understand the impact of having different sample sizes and different covariates in hard parameter sharing.
We will develop the key technical results in this section, and describe their implications in the next section.

We first provide a key technical tool that we derived using recent developments in random matrix theory \cite{??}.
To motivate our study, we consider a specialized setting where the output layer weights $A_1, A_2$ are both equal to one.
Without loss of generality, we consider the prediction loss of hard parameter sharing for task two and the same analysis applies to task one.
By solving $B$ in equation \eqref{eq_mtl}, we obtain the hard parameter sharing estimator $\hat{\beta}_2^{\MTL} = B A_2 = B$ as follows:
\begin{align*}
	\hat{\beta}_2^{\MTL} = {\hat{\Sigma}}^{-1} ({X^{(1)}}^{\top} Y^{(1)} + {X^{(2)}}^{\top} Y^{(2)}), \text{ where } \hat{\Sigma} = {X^{(1)}}^{\top} X^{(1)} + {X^{(2)}}^{\top} X^{(2)}.
\end{align*}
For the prediction loss, the empirical loss is close to the expected loss similar to what we have shown in Claim \ref{claim_pred_err}.
Hence, we consider the expected loss conditional on the covariates as follows:
\begin{align}\label{eq_diff_cov}
	\exarg{\cE}{L(\hat{\beta}_2^{\MTL}) \mid X^{(1)}, X^{(2)}} =
	\bignorm{(\Sigma^{(2)})^{1/2} \hat{\Sigma}^{-1} {X^{(1)}}^{\top} X^{(1)} (\beta^{(1)} - \beta^{(2)})}^2
	+ \sigma^2 \bigtr{\Sigma^{(2)}\hat{\Sigma}^{-1}}.
\end{align}

Interestingly, the prediction loss admits a bias-variance decomposition similar to the same covariates setting.
The variance equation scales with the sample covariance matrice of both tsks, whereas the bias equation scales additionally with the distance between the beta vectors.
In both the bias and variance equations, the matrix inverse $\hat{\Sigma}^{-1}$ is a crucial quantity.
Intuitively, this matrix inverse combines the sample covariance matrix of both tasks, and it is not hard to see that the expectation of $\hat{\Sigma}$ is equal to a mixture of their population covariance matrices, with mixing proportions determined by the sample sizes.
However, since the two tasks have different covariates, the scaling of $\hat{\Sigma}^{-1}$ not only depends on the tasks' sample sizes, but also depends on the covariate shift between the two tasks.
Building on the intuition, We introduce a key result that shows tight asymptotic convergence rate of the bias and variance as $p$ goes to infinity.
We define a key quantity $M \define \Sigma_1^{1/2}\Sigma_2^{-1/2}$ that captures the covariate shift between task $1$ and task $2$.
Let $U\Lambda V^\top$ denote the singular value decomposition of $M$, where $\Lambda\in\real^{p\times p}$ consists of $M$'s singular values $\lambda_1, \lambda_2, \dots, \lambda_p$ as its diagonal entries in descending order.

\begin{theorem}\label{thm_main_RMT}
	Suppose that $X_1$ and $X_2$ are both random matrices that satisfy Assumption \ref{assume_rm}.
	With high probability over the randomness of $X_1$ and $X_2$, we have the following limits:
	\begin{enumerate}
		\item[i)]  The variance equation converges to the following asymptotic limit
			\begin{align}\label{lem_cov_shift_eq}
				\bigabs{\bigtr{\Sigma^{(2)} \bigbrace{(n_1 + n_2){\hat{\Sigma}^{-1}} - (a_1 \Sigma^{(1)} + a_2 \Sigma^{(2)})^{-1}}}} \lesssim p^{-c_{\varphi}}\norm{\Sigma^{(2)}},
			\end{align}
			where $a_1$ and $a_2$ are the solutions of the following two equations:
			\begin{align}
				a_1 + a_2 = 1- \frac{p}{n_1 + n_2},\quad a_1 + \frac1{n_1 + n_2}\cdot \bigbrace{\sum_{i=1}^p \frac{\lambda_i^2 a_1}{\lambda_i^2 a_1 + a_2}} = \frac{n_1}{n_1 + n_2}. \label{eq_a12extra}
			\end{align}
		\item[ii)] For any fixed vector $w\in\real^p$ with unit norm, the bias equation converges to the following asymptotic limit
			\begin{align}\label{lem_cov_derv_eq}
				\bigabs{w^{\top} \Sigma^{(1)} \bigbrace{(n_1+n_2)^2\hat{\Sigma}^{-1} \Sigma^{(2)} \hat{\Sigma}^{-1} - {\Sigma^{(2)}}^{-1/2} V {\frac{a_3 \Lambda^2 + (a_4 + 1)\id}{(a_1 \Lambda + a_2\id)^2}} V^{\top} {\Sigma^{(2)}}^{-1/2}} \Sigma^{(1)} w} \lesssim p^{-c_{\varphi}},
			\end{align}
				where $a_{3}$ and $a_4$ are the solutions of the following two equations with $b_k = \frac1{p}\sum_{i=1}^p \frac{\lambda_i^{2k}} {(\lambda_i^2 a_1 + a_2)^2}$, for $k = 0, 1, 2$:
			\begin{align}\label{eq_a34extra}
				\left(\frac{\rho_1}{a_1^{2}} -  b_2  \right)\cdot  a_3 -  b_1 \cdot  a_4 = b_1,\quad \left(\frac{\rho_2}{a_2^{2}}-  b_0\right)\cdot  a_4 - b_1 \cdot  a_3
				= b_0.
			\end{align}
	\end{enumerate}
\end{theorem}
\noindent{\textbf{Remark.}} Our result extends a well-known result on the inverse of sample covariance matrices.
In particular, by setting $X_1 = 0$, we obtain the second result in Fact \ref{lem_minv}.
When $X_1=0$, it is not hard to solve $a_1 = 0$ and $a_2 = (n_2-p) / n_2$ from equation \eqref{eq_a12extra}, and our result reduces to the case of a single sample covariance matrix.
For the single-task case of a multivariate Gaussian random matrix, the result follows from the classical result for the mean of inverse Wishart distribution \cite{anderson1958introduction}.
For a non-Gaussian random matrix, the result can be obtained using the Stieltjes transform method (cf. Lemma 3.11 of \citet{bai2009spectral}).

By combining the bias and variance asymptotics, we obtain the generalization error of $\hat{\beta}_2^{\MTL}$.
Since the proof is similar to the same covariates setting, we describe the key differences and omit the rest of the details.
First, we will rescale $X_1$ by $A_1 / A_2$ before appling Theorem \ref{thm_main_RMT}.
Second, we approximate ${X^{(1)}}^{\top}X^{(1)}$ with $n_1 \Sigma^{(1)}$ in equation \eqref{eq_diff_cov} using Fact \ref{lem_minv}.
For the rest of this section, we present the main ideas in the proof of Theorem \ref{thm_main_RMT}.
%We will give a simple (although not totally rigorous) derivation of the equations \eqref{eq_a12extra} and \eqref{eq_a34extra}.
The full proof can be found in Section \ref{sec_maintools}.

\paragraph{Proof overview.} The central quantify of interest is the matrix inverse $\hat{\Sigma}^{-1}$.
To study this quantity, we use the Stieltjes transform or the resolvent method in random matrix theory \cite{bai2009spectral,tao2012topics,erdos2017dynamical}.
For any probability measure $\mu$ supported on $[0,\infty)$, the Stieltjes transform of $\mu$ is given by
$$m_\mu(z):= \int_0^\infty \frac{\dd\mu(x)}{x-z}, \text{ for any complex number } z\in \C\setminus [0,\infty).$$
%Conversely, given a Stieltjes transform $m_{\mu}(z)$, we can recover the measure $\mu$ through the inverse formula \HZ{add a ref to the equation below}
%$$ \frac{\dd \mu(x)}{\dd x} = \frac{1}{\pi}\lim_{\eta \downarrow 0}\im m_{\mu}(x+\ii \eta).$$
%where $\rho(x)$ is the density for $\mu$, i.e. $\dd \mu(x)=\rho(x)\dd x$.
Hence the Stieltjes transform method reduces the study of a probability measure to the study of a complex function.
To study the matrix inverse $\hat{\Sigma}^{-1}$, we consider the Stieltjes transform of its empirical spectral distribution.
More precisely, it suffices to study the spectra of another matrix as follows
\begin{align}\label{eigen2extra}
	\hat{\Sigma}^{-1}= \Sigma_2^{-1/2}V \frac{Y^{-1}}{n_1 + n_2} V^\top\Sigma_2^{-1/2}. \text{ where } Y \define \frac{1}{n_1 + n_2}(\Lambda U^\top (Z^{(1)})^\top Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V).
\end{align}
It is not hard to see that $\tr[\Sigma^{(2)} \hat{\Sigma}^{-1}] = \bigtr{Y^{-1}} / (n_1 + n_2)$.
Let $\mu=p^{-1}\sum_{i} \delta_{\lambda_i}$ denote the empirical spectral distribution of $Y$.
%The scaling $(n_1+n_2)^{-1}$ ensures such that the support of $\mu$ is bounded.
The Stieltjes transform of $\mu$ is as follows
 \[ m_{\mu}(z) \define \frac{1}{p}\sum_{i=1}^p \frac{1}{\sigma_i - z}= p^{-1}\tr\left[(Y-z\id)^{-1}\right]. \]
In the above equation, $(Y - z\id)^{-1}$ is known as the resolvent matrix or Green's function of $Y$.
%When $p$ goes to infinity, it is well-known that $m_{\mu}(z)$ converges to a fixed distribution governed by a set of self-consistent equations.
%These self-consistent equations give the asymptotic limit of the trace of $\hat{\Sigma}^{-1}$.
%The above approach applies when $\Sigma^{(2)}$ is isotropic in Theorem \ref{thm_main_RMT}.
%Since our goal is to show the limit of $\tr\left[ (Y - z\id)^{-1}V^{\top}{\Sigma^{(2)}}^{-1}V \right]$ as shown in equation \eqref{eigen2extra}, we study the  $(Y-z\id)^{-1}$.
%Compared to the Stieltjes transform, the resolvent also applies to random matrices.
We shall prove the convergence of the resolvent matrix of $Y$ using the so-called ``local laws'' with a sharp convergence rate \cite{isotropic,erdos2017dynamical,Anisotropic}.
%Recent developments in the random matrix literature have shown the convergence of the resolvent matrix using the so-called ``local laws'' or ``deterministic equivalents'' (cf. \cite{Hachem2007deterministic,DS18}).
More precisely, given a deterministic $p\times p$ matrix $R(z)$, we say that $R(z)$ and $(Y-z\id)^{-1}$ are equivalent if for any sequence of deterministic matrices $B$ with uniformly bounded operator norms,
$$\bigtr{((Y-z\id)^{-1}-R(z))B}\to 0.$$
When the above equation holds for large enough $p$, we say that $(Y-z\id)^{-1}$ converges to the matrix limit $R(z)$.
%For our purpose, we use a convenient linearization trick in linear algebra, that is, the SVD of a rectangular matrix $A$ is equivalent to the study of the eigendecomposition of the symmetric block matrix
%$$H(A):=\begin{pmatrix}0 & A \\ A^\top & 0\end{pmatrix},$$
%which is linear in $A$. This trick has been used in many random matrix literatures, such as \cite{Anisotropic, AEK_Gram, XYY_circular,DY20201}.
We observe that the matrix $Y$ can also be written as $A A^{\top}$ for a $p$ by $n_1 + n_2$ matrix defined as follows.
	\[ A = (n_1+ n_2)^{-1/2} [\Lambda U^\top (Z^{(1)})^\top,V^\top (Z^{(2)})^\top]. \]
%}\HZ{what does this trick mean? use less technical words},
%This idea dates back at least to Girko, see e.g., the works \cite{girko1975random,girko1985spectral} and references therein.
Consider the following symmetric and square matrix whose dimension is $p + n_1 + n_2$.
%which is a linear function of $(Z^{(1)})$ and $(Z^{(2)})$:
%\begin{definition}[Linearizing block matrix]\label{def_linearHG}%definiton of the Green function
%We define the $(n+N)\times (n+N)$ block matrix
 \begin{equation}\label{linearize_block}
    H \define \left( {\begin{array}{*{20}c}
   0 & A  \\
   A^{\top} & 0
%   {Z^{(2)}V} & 0 & 0
   \end{array}} \right).
 \end{equation}
%The resolvent of $H$ is defined as follows.
%\begin{definition}[Resolvents]
The resolvent of $H$ is given by $G(z) \define (H - z\id)^{-1}$, for any complex value $z\in \mathbb C$.  %\left[H -\left( {\begin{array}{*{20}c}
%   { z\id_{p}} & 0 & 0 \\
%   0 & { \id_{n_1}}  & 0\\
%      0 & 0  & { \id_{n_2}}\\
%\end{array}} \right)\right]^{-1} , \quad z\in \mathbb C ,
%It is easy to verify that the eigenvalues $\lambda_1(H)\ge \ldots \ge \lambda_{n+N}(H)$ of $H$ are related to the ones of $\mathcal Q_1$ through
%\begin{equation}\label{Heigen}
%\lambda_i(H)=-\lambda_{n+N-i+1}(H)=\sqrt{\lambda_i\left(\mathcal Q_2\right)}, \ \ 1\le i \le n\wedge N, \quad \text{and}\quad \lambda_i(H)=0, \ \ n\wedge N + 1 \le i \le n\vee N.
%\end{equation}
%and
%$$\lambda_i(H)=0, \ \ n\wedge N + 1 \le i \le n\vee N.$$
%where we used the notations $n\wedge N:=\min\{N,M\}$ and $n\vee N:=\max\{N,M\}$.
%\begin{definition}[Index sets]\label{def_index}
%Likewise, the resolvent of $Y$ is given by $\cal G(z) \define (Y - z\id)^{-1}$, for any $z\in \mathbb C$.
%Let $m_1(z):= \frac{1}{n_1}\sum_{i = p+1}^{p + n_1} G_{i, i}(z)$ and $m_2(z):= \frac{1}{n_2}\sum_{i = p + n_1 + 1}^{p + n_1 + n_2} G_{i, i}(z)$.
Since $H$ has a specific block structure, it is not hard to verify that %\HZ{Has $G(z)$ been defined before?}
	\begin{equation*} %\label{green2}
	  G(z) =  \left( {\begin{array}{*{20}c}
			(Y - z\id)^{-1} & (Y - z\id)^{-1} A  \\
      A^\top (Y - z\id)^{-1} & (A^\top A - z\id)^{-1}
		\end{array}} \right).%\quad \cal G_R:=(W^\top W - z)^{-1} ,
  \end{equation*}
%This shows that a control of $G(z)$ yields directly a control of $\mathcal G(z)$ as the upper-left block. On the other hand, $G(z)$ is a little  easier to use than $\cal G(z)$, and obviously contains more information.
%\noindent\textbf{Asymptotic limits of resolvents.}

\paragraph{Matrix limit.}
In Theorem \ref{LEM_SMALL}, we show that for $z$ in a small neighborhood around $0$, when $p$ goes to infinity, $G(z)$ converges to a matrix limit as follows:
\be \label{defn_piw}
	\Gi(z) \define \begin{pmatrix} (a_{1}(z)\Lambda^2  +  (a_{2}(z)- z)\id)^{-1} & 0 & 0 \\ 0 & - \frac{\rho_1+\rho_2}{\rho_1} a_{1}(z)\id_{n_1} & 0 \\ 0 & 0 & -\frac{\rho_1+\rho_2}{\rho_2}a_{2}(z)\id_{n_2}  \end{pmatrix},\ee
%In particular, the matrix limit of $\cal G(z)$ is given by
%\be\label{matrix limit}(-z\id_p+a_{1}(z)\Lambda^2 + a_{2}(z)\id_p)^{-1},\ee
where $a_1(z)$ and $a_2(z)$ are the unique solutions to the following system of equations
\begin{align}
	&a_1(z) + a_2(z) = 1 - \frac{1}{n_1 + n_2} \bigbrace{\sum_{i=1}^p \frac{\lambda_i^2 a_1(z) + a_2(z)}{\lambda_i^2 a_1(z) + a_2(z) - z}}, \nonumber\\
	&a_1(z) + \frac{1}{n_1 + n_2}\bigbrace{\sum_{i=1}^p \frac{\lambda_i^2 a_1(z)}{\lambda_i^2 a_1(z) + a_2(z) - z}} = \frac{n_1}{n_1 + n_2}. \label{selfomega_a}
% \frac{\rho_1}{a_{1}(z)} = \frac{1}{p}\sum_{i=1}^p \frac{\lambda_i^2}{ - z+\lambda_i^2 a_{1}(z) +a_{2} (z) } + (\rho_1+\rho_2),\  \frac{\rho_2}{a_{2}(z)} = \frac{1}{p}\sum_{i=1}^p \frac{1 }{  -z+\lambda_i^2 a_{1}(z) +  a_{2}(z)  }+ (\rho_1+\rho_2) .
\end{align}
The existence and uniqueness of the solution to the above equation system are shown in Lemma \ref{lem_mbehaviorw}.
%First, we define the deterministic limits of $(m_1(z), m_{2}(z))$ by $\left(-\frac{\rho_1+\rho_2}{\rho_1}a_{1}(z),-\frac{\rho_1+\rho_2}{\rho_2}a_{2}(z)\right)$, where
%satisfying that $\im a_{1}(z)< 0$ and $\im a_{2}(z)<0$ for $z\in \C_+$ with $\im z$.
%The existence and uniqueness of the solution $(a_1(z), a_2(z))$ for $z$ around 0 will be proved in Section \ref{sec contract}.
%Here, for simplicity of notations, we introduced the following ratios
%\be\label{ratios}
% \gamma_n :=\frac{p}{n}=\frac{1}{\rho_1+\rho_2},\quad r_1 :=\frac{n_1}{n}=\frac{\rho_1}{\rho_1+\rho_2},\quad r_2 :=\frac{n_2}{n}=\frac{\rho_2}{\rho_1+\rho_2}.
%\ee
Given this result, we show that when $z = 0$, the matrix limit $\Gi(0)$ implies the variance asymptotic shown in equation \eqref{lem_cov_shift_eq}.
First, we have that $a_1 = a_1(0)$ and $a_2 = a_2(0)$ since equation \eqref{selfomega_a} reduces to equation \eqref{eq_a12extra}.
Second, since $Y^{-1}$ is the upper-left block matrix of $G(0)$, we have that $Y^{-1}$ converges to $(a_1\Lambda^2 + a_2)^{-1}$.
Using equation \eqref{eigen2extra}, we have that when $p$ goes to infinity, the trace of $\Sigma^{(2)} \hat{\Sigma}$ converges to \[ \bigtr{(a_1 \Lambda^2 + a_2\id)^{-1} V^{\top} {\Sigma^{(2)}}^{-1} V} = \bigtr{(a_1 \Sigma^{(1)} + a_2 \Sigma^{(2)})^{-1}}. \]
%\noindent{\bf Variance asymptotics.} Using definition \eqref{mainG}, we can write equation \eqref{eigen2extra} as
%\be\label{rewrite X as R} [(X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}]^{-1}=n^{-1}\Sigma_2^{-1/2}V\cal G(0)V^\top\Sigma_2^{-1/2}.\ee
%When $z=0$, it is easy to check that , which means that we actually have $a_1(0)=a_1$ and $a_2(0)=a_2$. Hence the matrix limit of $\cal G(0)$ is given by $(a_{1}\Lambda^2 + a_{2}\id_p)^{-1}$. Then inserting this limit into equation \eqref{rewrite X as R}, we can write the left-hand side of equation \eqref{lem_cov_shift_eq} as
%\begin{align}
%&\bigtr{\left( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}\right)^{-1} \Sigma}\approx n^{-1}\bigtr{\Sigma_2^{-1/2}V\cal (a_{1}\Lambda^2 + a_{2}\id_p)^{-1}V^\top\Sigma_2^{-1/2}\Sigma}  \nonumber\\
%&=n^{-1}\bigtr{\Sigma_2^{-1/2}\cal (a_{1}\Sigma_2^{-1/2}\Sigma_1\Sigma_2^{-1/2} + a_{2}\id_p)^{-1}\Sigma_2^{-1/2}\Sigma}  = n^{-1}\bigtr{\cal (a_{1} \Sigma_1  + a_{2}\Sigma_2)^{-1}\Sigma}  ,\label{Gi00}
%\end{align}
where in the second step we used $V \Lambda^2 V^\top=M^\top M=\Sigma_2^{-1/2}\Sigma_1\Sigma_2^{-1/2}$.
The concentration error of the above limit is shown in Appendix \ref{appendix RMT}.%The rigorous proof of equation \eqref{lem_cov_shift_eq} will be given in Section \ref{sec pf RMTlemma}.

For the bias asymptotic in equation \eqref{lem_cov_derv_eq}, We can write the left-hand side of equation \eqref{lem_cov_derv_eq} using the derivative of $\cal G$ with respect to $z$ at $z=0$. More precisely, using equation \eqref{eigen2extra} we can obtain that
\begin{align}
&n^2\bignorm{\Sigma_2^{1/2} \bigbrace{ (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)} }^{-1} \Sigma_1^{1/2} w}^2 \nonumber\\
&=n^2 w^\top \Sigma_1^{1/2}\Sigma_2^{-1/2}V\left(   \Lambda U^\top (Z^{(1)})^\top Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^{-2}V^\top\Sigma_2^{-1/2}\Sigma_1^{1/2}w \nonumber\\
&=  w^\top \Sigma_1^{1/2}\Sigma_2^{-1/2}V\cal G'(0)V^\top\Sigma_2^{-1/2}\Sigma_1^{1/2}w,\label{calculate G'}
\end{align}
where we used equation \eqref{rewrite X as R} in the second step. Since the matrix limit of $\cal G(z)$ is given by equation \eqref{matrix limit}, it is natural to guess that the matrix limit of $\cal G'(0)$ is given by
\be\label{cal G'0}\cal G'(0) \approx \left.\frac{\dd}{\dd z}\right|_{z=0}(-z\id_p+a_{1}(z)\Lambda^2 + a_{2}(z)\id_p)^{-1} = \frac{\id_p- a_1'(0)\Lambda^2 - a_2'(0)\id_p}{(a_{1}(0)\Lambda^2 + a_{2}(0)\id_p)^2}.\ee
If we let $a_3:=-a_1'(0)$ and $a_4:=-a_2'(0)$, then taking implicit differentiation of equation \eqref{selfomega_a} we can check that $(a_3,a_4)$ satisfies equation \eqref{eq_a34extra}. Then inserting \eqref{cal G'0} into \eqref{calculate G'}, we obtain that
\begin{align}
& n^2\bignorm{\Sigma_2^{1/2} \bigbrace{ (X^{(1)})^{\top}(X^{(1)}) + (X^{(2)})^{\top}(X^{(2)}) }^{-1} \Sigma_1^{1/2} w}^2 \nonumber\\
&\approx  w^\top \Sigma_1^{1/2}\Sigma_2^{-1/2}V\frac{a_3\Lambda^2 +(1+ a_4)\id_p}{(a_{1}\Lambda^2 + a_{2}\id_p)^2}V^\top \Sigma_2^{-1/2}\Sigma_1^{1/2}w= w^{\top} \Pi_\bias w, \label{calculatePibias}
\end{align}
where in the last step we used $M = \Sigma_1^{1/2}\Sigma_2^{-1/2}$ and $V \Lambda^2 V^\top=M^\top M$. This concludes equation \eqref{lem_cov_derv_eq}.
Note that in order to have the approximate identity for $\cal G'(0)$ in equation \eqref{cal G'0}, we not only need to know the asymptotics of $\cal G(0)$, but also need to know the asymptotics of $\cal G(z)$ for general $z$ around $z=0$. This is the main reason why we need to take a general $z$ in the definition of resolvents. The rigorous proof of equation \eqref{lem_cov_derv_eq} will be given in Section \ref{sec pf RMTlemma}, where we justify the approximate identity in equation \eqref{cal G'0}.
%In the above definition, we have taken the argument of $\cal G$ to be a general complex number, because we will need to use $\cal G'(0)$ in the proof of Lemma \ref{lem_cov_derivative}, which requires a good estimate of $\cal G(z)$ for $z$ around the origin.
%For the variance asymptotic limit, we study the resolvent
%	\[ R(z):= \bigbrace{\Sigma_2^{-1/2}( (X^{(1)})^{\top}(X^{(1)}) + (X^{(2)})^{\top}(X^{(2)}))\Sigma_2^{-1/2} - z \id}^{-1}, \text{ for any } z\in \C \text{ around } z=0. \]
%Using the techniques from \citet{Anisotropic} and \citet{yang2019spiked}, we find the asymptotic limit of $R(z)$ for any $z$ as $p$ goes to infinity, denoted by $R_\infty(z)$, with an almost optimal convergence rate of $p$.
%In particular, when $z=0$, the asymptotic limit of equation \eqref{lem_cov_shift_eq} is given by
%	\[ \tr[\Sigma_2^{-1/2} \Sigma \Sigma_2^{-1/2}R_\infty(0)]. \]
%
%For the bias asymptotic limit, we show in \todo{where?} that
%$$\bignorm{\Sigma_2^{1/2} ((X^{(1)})^{\top}(X^{(1)}) + (X^{(2)})^{\top}(X^{(2)})^{-1} w}^2= w^\top \Sigma_2^{-1/2}R'(0)\Sigma_2^{-1/2} w.$$
%Hence its limit can be calculated through $R_\infty'(z)$, which gives the expression in \eqref{lem_cov_derv_eq}.
%We leave the full proof of Lemma \ref{lem_cov_shift} and Lemma \ref{lem_cov_derivative} to Appendix \ref{sec_maintools}.
%Combining the above two results, we provide the proof of Theorem \ref{thm_main_informal} in Section \ref{app_proof_main_thm}.


The above arguments are the core of the main proof. To have a rigorous proof, we need to estimate each error carefully, and extend the Gaussian case to the more general case where the entries of $Z^{(1)}$ and $Z^{(2)}$ only satisfy certain moment assumptions. These will make the real argument rather tedious, but the methods we used are standard in the random matrix literature \cite{erdos2017dynamical,Anisotropic}. For the full rigorous proof, we refer the reader to Section \ref{appendix RMT}.


