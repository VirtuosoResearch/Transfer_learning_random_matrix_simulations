\section{The Different Covariates Setting}\label{sec_diff}

The different covariates setting differs from the same covariates setting in two key aspects.
First, different tasks may have different sample sizes. In extreme scenarios, one task may have much less labeled data compared to another task.
Second, the features of different tasks may have different shapes, a phenomenon that is often called ``covariate shift''.
In this section, we show how to deal with the the above two aspects.

When the covariates across tasks are different, we can no longer obtain an optimal solution for minimizing equation \eqref{eq_mtl} as in Claim \ref{lem_exp_opt}.
This is because the minimization problem is non-convex with respect to $A$ and $B$.
Therefore, we consider an arbitrary local minimum of equation \eqref{eq_mtl} and provide generalization bounds that scale with key properties of task data.
We focus on the two-task case to better understand the impact of having different sample sizes and different covariates in hard parameter sharing.

We develop a key technical result using recent developments in random matrix theory.
To motivate our study, we consider a specialized setting where the output layer weights $A_1, A_2$ are both equal to one.
Without loss of generality, we consider the prediction loss of hard parameter sharing for task two and the same analysis applies to task one.
By solving $B$ in equation \eqref{eq_mtl}, we obtain the hard parameter sharing estimator $\hat{\beta}_2^{\MTL} = B A_2 = B$ as follows:
\begin{align*}
	\hat{\beta}_2^{\MTL} = {\hat{\Sigma}}^{-1} ({X^{(1)}}^{\top} Y^{(1)} + {X^{(2)}}^{\top} Y^{(2)}), \text{ where } \hat{\Sigma} = {X^{(1)}}^{\top} X^{(1)} + {X^{(2)}}^{\top} X^{(2)}.
\end{align*}
For the prediction loss, the empirical loss is close to the expected loss similar to what we have shown in Claim \ref{claim_pred_err}.
Hence, we consider the expected loss conditional on the covariates as follows:
\begin{align}\label{eq_diff_cov}
	\exarg{\cE}{L(\hat{\beta}_2^{\MTL}) \mid X^{(1)}, X^{(2)}} =
	\bignorm{(\Sigma^{(2)})^{1/2} \hat{\Sigma}^{-1} {X^{(1)}}^{\top} X^{(1)} (\beta^{(1)} - \beta^{(2)})}^2
	+ \sigma^2 \bigtr{\Sigma^{(2)}\hat{\Sigma}^{-1}}.
\end{align}

Interestingly, the prediction loss admits a bias-variance decomposition similar to the same covariates setting.
The variance equation scales with the sample covariance matrice of both tsks, whereas the bias equation scales additionally with the distance between the beta vectors.
In both the bias and variance equations, the matrix inverse $\hat{\Sigma}^{-1}$ is a crucial quantity.
Intuitively, this matrix inverse combines the sample covariance matrix of both tasks, and it is not hard to see that the expectation of $\hat{\Sigma}$ is equal to a mixture of their population covariance matrices, with mixing proportions determined by the sample sizes.
However, since the two tasks have different covariates, the scaling of $\hat{\Sigma}^{-1}$ not only depends on the tasks' sample sizes, but also depends on the covariate shift between the two tasks.
Building on the intuition, We introduce a key result that shows tight asymptotic convergence rate of the bias and variance as $p$ goes to infinity.
We define a key quantity $M \define \Sigma_1^{1/2}\Sigma_2^{-1/2}$ that captures the covariate shift between task $1$ and task $2$.
Let $U\Lambda V^\top$ denote the singular value decomposition of $M$, where $\Lambda\in\real^{p\times p}$ consists of $M$'s singular values $\lambda_1, \lambda_2, \dots, \lambda_p$ as its diagonal entries in descending order.

\begin{theorem}\label{thm_main_RMT}
	Suppose that $X_1$ and $X_2$ are both random matrices that satisfy Assumption \ref{assume_rm}.
	Assume that: a) $M$'s singular values are all greater than $\tau$ and less than $1/\tau$; b) task one's sample size is greater than $\tau p$ and task two's sample size is greater than $(1 + \tau) p$.
	With high probability over the randomness of $X_1$ and $X_2$, we have the following limits:

	\noindent (i) The variance equation converges to the following asymptotic limit
			\begin{align}\label{lem_cov_shift_eq}
				\bigabs{\bigtr{\Sigma^{(2)} \bigbrace{(n_1 + n_2){\hat{\Sigma}^{-1}} - (a_1 \Sigma^{(1)} + a_2 \Sigma^{(2)})^{-1}}}} \lesssim p^{-c_{\varphi}}\norm{\Sigma^{(2)}},
			\end{align}
			where $a_1$ and $a_2$ are the solutions of the following two equations:
			\begin{align}
				a_1 + a_2 = 1- \frac{p}{n_1 + n_2},\quad a_1 + \frac1{n_1 + n_2}\cdot \bigbrace{\sum_{i=1}^p \frac{\lambda_i^2 a_1}{\lambda_i^2 a_1 + a_2}} = \frac{n_1}{n_1 + n_2}. \label{eq_a12extra}
			\end{align}
	\noindent (ii) For any fixed vector $w\in\real^p$ with unit norm, the bias equation converges to the following asymptotic limit
			\begin{align}\label{lem_cov_derv_eq}
				\bigabs{w^{\top} \Sigma^{(1)} \bigbrace{(n_1+n_2)^2\hat{\Sigma}^{-1} \Sigma^{(2)} \hat{\Sigma}^{-1} - {\Sigma^{(2)}}^{-1/2} V {\frac{a_3 \Lambda^2 + (a_4 + 1)\id}{(a_1 \Lambda^2 + a_2\id)^2}} V^{\top} {\Sigma^{(2)}}^{-1/2}} \Sigma^{(1)} w} \lesssim p^{-c_{\varphi}},
			\end{align}
				where $a_{3}$ and $a_4$ are the solutions of the following two equations % with $b_k = \frac1{p}\sum_{i=1}^p \frac{\lambda_i^{2k}} {(\lambda_i^2 a_1 + a_2)^2}$, for $k = 0, 1, 2$:
			\begin{align}\label{eq_a34extra}
				a_3 + a_4 = \frac{1}{n_1 + n_2}\sum_{i=1}^p \frac{1}{\lambda_i^2 a_1 + a_2},
				a_3 + \frac{1}{n_1 + n_2} \sum_{i=1}^p \frac{\lambda_i^2 (a_1 a_4 - a_2 a_3)}{(\lambda_i^2 a_1 + a_2)^2} = \frac{1}{n_1 + n_2} \sum_{i=1}^p \frac{\lambda_i^2 a_1}{(\lambda_i^2 a_1 + a_2)^{2}}.
%				\left(\frac{\rho_1}{a_1^{2}} -  b_2  \right)\cdot  a_3 -  b_1 \cdot  a_4 = b_1,\quad \left(\frac{\rho_2}{a_2^{2}}-  b_0\right)\cdot  a_4 - b_1 \cdot  a_3
%				= b_0.
			\end{align}
\end{theorem}
Our result extends a well-known result on the inverse of sample covariance matrices.
If $X_1$ is zero, then we obtain Fact \ref{lem_minv}(ii) as a special case.
To see this, we solve equation \eqref{eq_a12extra} and obtain $a_1 = 0$ and $a_2 = (n_2-p) / n_2$, and apply these solutions to the variance limit.
\HZ{go to intro}
For a multivariate Gaussian random matrix, this result follows from the classical result for the mean of inverse Wishart distribution \cite{anderson1958introduction}.
For a non-Gaussian random matrix, this result can be obtained using the well-known Stieltjes transform method (cf. Lemma 3.11 of \citet{bai2009spectral}).

%As mentioned in the beginning of this section, different tasks may have different sample sizes and different feature covariance matrices.
%Under these scenarios, Theorem \ref{thm_main_RMT} shows the bias and variance asymptotics for the two-task case.
One can see that both the variance limit and the bias limit depend heavily on both tasks' samples sizes and the covariate shift matrix $M$.
Next, we use our result to study how varying sample sizes and covariate shifts affects hard parameter sharing, respectively.

\begin{example}[Sample ratio]\label{ex_sample_ratio}
	We first consider the impact of varying sample sizes.
	Consider the random-effects model from Section \ref{sec_hps}, with both tasks having an isotropic population covariance matrix.

	In applying Theorem \ref{thm_main_RMT} to the above setting, we first solve the self-consistent equations \eqref{eq_a12extra} by using $\lambda_i = 1$ for all $1\le i\le p$.
	This gives us the variance limit $\tr[(a_1\Sigma^{(1)} + a_2\Sigma^{(2)})^{-1}] / (n_1 + n_2) = \frac{p}{n_1 + n_2 - p}$, since we have that $a_1 + a_2 = 1 - \frac{p}{n_1 + n_2 - p}$.
	Secondly, we solve the self-consistent equations \eqref{eq_a34extra} given that we already know $a_1$ and $a_2$.
	This gives us the bias limit.
	Combined together, we obtain the following corollary, which requires natural assumptions over the random-effects model.

\begin{corollary}\label{cor_MTL_loss}
	In the setting of Example \ref{ex_sample_ratio}, assume that
	%a) the sample sizes $n_1$ and $n_2$ are greater than $(1 + \tau) p$, b) $\Sigma_1=\Sigma_2=\id_p$, and c) %there exists a small constant $c_0>0$ such that
	(i) task one's sample size is at least $n_1 \ge \min(\frac{17}{\varepsilon^2}, 50) \cdot p$ for a fixed positive value $\varepsilon$, and task two's sample size is at least $n_2 \ge \frac{3}{2}p$;
	(ii) the noise variance is smaller than the signal strength: $\sigma^2 \lesssim  \kappa^2$;
%	\be\label{choiceofpara0}
%	p^{-1/2+c_0}\sigma^2 + p^{c_0}d^2\le \kappa^2\le p^{1-c_0} (\sigma^2 +d^2)  .  	\ee
	%\be\label{choiceofpara0}
%	(ii) the task-specific variance of $\beta_i$ is much smaller than the signal strength {\color{red}$d^2 = \oo( {\kappa^2})$}; \HZ{what does $\ll$ mean exactly?}
%	(iii) the sample sizes $n_1$ and $n_2$ are greater than $(1 + \tau) p$.
	(iii) the task-specific variance of $\beta_i$ is much smaller than the shared signal strength: $d^2 \le p^{-c_{\varphi}}{\kappa^2}$.
	The prediction loss of hard parameter sharing for task two satisfies that
	\begin{align}\label{cor_MTL_error}
	%-\left[1- \left( 1-\frac{1}{\sqrt{\rho_1}}\right)^4\right] pd^2\cdot \frac{\rho_1^2 (\rho_1+\rho_2)}{(\rho_1 + \rho_2 - 1)^3} +\OO(p^{-c}\sigma^2)  \le
	  \left|L(\hat{\beta}_2^{\MTL}) - \frac{d^2 n_1^2 (n_1 + n_2)}{(n_1 + n_2 - p)^3} -\frac{\sigma^2 p}{n_1 + n_2 - p}  \right| \le \varepsilon \cdot \frac{d^2 n_1^2 (n_1 + n_2)}{(n_1 + n_2 - p)^3} + p^{-c_{\varphi}} \cdot \OO(d^2 + \sigma^2) +{\color{red} p^{-c_\infty}\cdot \OO(\kappa^2)}.
	%\left[\left( 1+\frac{1}{\sqrt{\rho_1}}\right)^4-1\right] d^2\cdot \frac{\rho_1^2 (\rho_1+\rho_2)}{(\rho_1 + \rho_2 - 1)^3} \\
	%& +C \left[(p^{-c_\varphi}+p^{-c_\infty/2})(\sigma^2 +d^2)+p^{-c_\infty}\kappa^2 + %\frac{d^4+\sigma^2 d^2}{\kappa^2}\right],\nonumber
	 \end{align}
	 {\color{red}[FY: there is an extra $\kappa^2$ term]}
%	 for a large constant $C>0$.
\end{corollary}


%It is well-known since the seminal work of Caruana \cite{C97} that how well multi-task learning performs depends on task relatedness. We formalize this connection in the above isotropic setting, where we can perform explicit calculations. The prediction loss of MTL has been given in Corollary \ref{cor_MTL_loss}. We can also calculate the prediction loss of STL easily using Fact \ref{lem_minv} (ii). The proof of the following claim will be given in Section \ref{app_iso_cov}.
%\begin{claim} \label{claim_STL_loss}
%Under the above isotropic setting for task two, we have that with high probability,
%\begin{align*}
%	 L(\hat{\beta}_2^{\STL}) = \frac{\sigma^2}{ \rho_2-1}  +\OO(p^{-\e}\sigma^2 )
%	 \end{align*}
%	 for any constant $\e\in (0,1/2)$.
%\end{claim}
%We now discuss two implications of Corollary \ref{cor_MTL_loss} and Claim \ref{claim_STL_loss} regarding the information transfer in MTL: the transitions from positive transfer to negative transfer with respect to varying model distance and varying sample ratio, respectively.

%\noindent{\bf Varying model distance.}
%With Corollary \ref{cor_MTL_loss} and Claim \ref{claim_STL_loss}, it is not hard to see
%One can observe that as we increase the distance $pd^2$ between $\beta_1$ and $\beta_2$, there is a transition from positive transfer to negative transfer in MTL. More precisely, the bias term $  pd^2\cdot \frac{\rho_1^2 (\rho_1+\rho_2)}{(\rho_1 + \rho_2 - 1)^3}$ of MTL increases as the distance between $\beta_1$ and $\beta_2$ increases, while variance reduction from STL to MTL is $\frac{\sigma^2}{ \rho_2-1}  -\frac{\sigma^2}{\rho_1+\rho_2-1}$.
%Therefore, while the variance of MTL still reduces compared to STL,
%If the bias increases more than the amount of variance reduced, we will observe negative transfer.

%We apply Corollary \ref{cor_MTL_loss} to the parameter setting of Figure \ref{fig_model_shift} (the details are left to Appendix \ref{app_synthetic}). We can see that our result is able to predict positive or negative transfer  accurately and matches the empirical curve.
%There are several unexplained observations near the transition threshold $0$, which are caused by the concentration error on the right-hand side of \eqref{cor_MTL_error}.
%We fix the target task and vary the source task, in particular the parameter $d$ which determines $\norm{\beta_1 - \beta_2}$.
%Figure \ref{fig_model_shift} shows the result.
%We observe that Proposition \ref{prop_dist_transition} explains most of the observations in Figure \ref{fig_model_shift}.

%The proof of Proposition \ref{prop_dist_transition} involves two parts.
%First, in equation \eqref{eq_te_var}, the positive variance reduction effect scales with $n_1 = \rho_1 p$, the number of source task data points.
%Second, we show that the negative effect of model-shift bias scales with $pd^2$, which is the expectation of $\norm{\beta_1 - \beta_2}^2$.
%The proof of Proposition \ref{prop_dist_transition} can be found in Appendix \ref{app_proof_31}.
%A key part of the analysis shows that $\hat{W}_1 / \hat{W}_2$ is roughly equal to one in the isotropic model,
%thus simplifying the general condition in Theorem \ref{thm_main_informal}.

%In classical Rademacher or VC based theory of multi-task learning, the generalization bounds are usually presented for settings where the sample sizes are equal for all tasks \cite{B00,M06,MPR16}.
%More generally, such results are still applicable when all task data are being added simultaneously.
%On the other hand, uneven sample sizes between different tasks (or even dominating tasks) have been empirically observed as a cause of negative transfer \cite{YKGLHF20}.
%For such settings, we have also observed that adding more labeled data from one task does not always help.
%Our theory accurately predicts a curious phenomenon, where increasing the sample size of the source task results in negative transfer!
%On the other hand, we have observed that adding more labeled data does not always improve performance in multi-task learning.

Our result shows that the prediction loss of hard parameter sharing consists of a bias term that scales with task-specific variance $d^2$ and a noise variance term that scales with $\sigma^2$.
In the following, we explain an interesting phenomenon, where adding task one's samples helps task two initially, but hurts eventually.
Consider the prediction loss if we fix $n_2$ and increase $n_1$.
First, the noise variance term in Corollary \ref{cor_MTL_loss} always reduces as $n_1$ increases.
Second, the bias term always increases as $n_1$ increases, which can be verified by calculating the derivative of the bias term.
Combined together, we observe that the \textit{optimal} sample ratio between $n_1$ and $n_2$ should be among one of following three cases:
(i) $n_1 = 0$, in which case the prediction loss reduces to $\frac{\sigma^2 p}{n_2 - p}$;
(ii) $n_1 = \infty$, in which case the prediction loss reduces to $d^2$;
(iii) the critial point where the derivative of the bias and the variance terms w.r.t. $n_1$ equals zero.

Based on this observation, we have the following trend when we fix $n_2$ and increase $n_1$:
\begin{itemize}
	\item If $d^2$ is smaller than the prediction loss of STL for task two $\frac{\sigma^2 p}{n_2 - p}$, then the prediction loss of hard parameter sharing is always lower than STL. That is, task one always transfers positively to task two.
	\item Otherwise, task one transfers positively to task two initially, but transfers negatively eventually.
\end{itemize}
%Therefore, while the variance of MTL still reduces compared to STL,
%If the bias increases more than the amount of variance reduced, we observe negative transfer.
%of MTL increases and converges to $pd^2$ as the sample ratio $\rho_1/\rho_2$ increases, and the variance reduction $\frac{\sigma^2}{ \rho_2-1}  -\frac{\sigma^2}{\rho_1+\rho_2-1}$ also increases and converges to $\frac{\sigma^2}{\rho_2-1}$ as $\rho_1/\rho_2$ increases. The tradeoff between these two terms depends on $\rho_1$ in a nonlinear way that is more complicated than the one in the case of varying model distance. But from our results, one can check the following phenomena. If $\frac{\sigma^2}{\rho_2-1} > pd^2$, then the transfer is always positive. On the other hand, if $\frac{\sigma^2}{\rho_2-1} < pd^2$, then there is a transition from the positive transfer to negative transfer as $\rho_1/\rho_2$ increases.
%Therefore, while the variance of MTL still reduces compared to STL,
%If the bias increases more than the amount of variance reduced, we will observe negative transfer.
%Figure \ref{fig_size} provides a simulation result for such a setting (described in Appendix \ref{app_synthetic}).
%We observe that as $n_1 / n_2$ increases, there is a transition from positive to negative transfer.
%There are several unexplained observations near $y = 0$ caused by the concentration error on the right-hand side of \eqref{cor_MTL_error}.
\end{example}

%\begin{proposition}[Source/target sample ratio]\label{prop_data_size}
%	In the isotropic model, suppose that $\rho_1 > 40$ and $\rho_2 > 110$ are fixed constants, and $\Psi(\beta_1, \beta_2) > 2/(\rho_2 - 1)$.
%	Then we have that
%	\begin{itemize}
%		\item \textbf{Positive transfer:} If $\frac{n_1}{n_2} = \frac{\rho_1}{\rho_2} < \frac{1}{\nu} \cdot \frac{1 - 2\rho_2^{-1}}{\Psi(\beta_1, \beta_2) (\rho_2 - 1) - \nu^{-1}}$, then w.h.p. $$\te(\hat{\beta}_2^{\MTL}) < \te(\hat{\beta}_2^{\STL}).$$
%		\item \textbf{Negative transfer:} If $\frac{n_1}{n_2} = \frac{\rho_1}{\rho_2} > {\nu} \cdot \frac{1 - 2\rho_2^{-1}}{\Psi(\beta_1, \beta_2) (\rho_2 - 3/2) - \nu}$, then w.h.p. $$\te(\hat{\beta}_2^{\MTL}) > \te(\hat{\beta}_2^{\STL}).$$
%	\end{itemize}
%\end{proposition}
%Proposition \ref{prop_data_size} describes the bias-variance tradeoff in terms of the sample ratio $\rho_1 / \rho_2$.


%We apply the result to the setting of Figure \ref{fig_size} (described in Appendix \ref{app_synthetic}).
%There are several unexplained observations near $y = 0$ caused by $\nu$.
%The proof of Proposition \ref{prop_data_size} can be found in Appendix \ref{app_proof_32}.

\begin{example}[Covariate shift]\label{sec_covshift}
%So far we have considered the isotropic model where $\Sigma_1 = \Sigma_2$.
%This setting is relevant for settings where different tasks share the same input features such as multi-class image classification.
%In general, the covariance matrices of the two tasks may be different such as in text classification.
Our second example studies the impact of varying covariate shifts on the variance limit.
%As we are going to show later, covariate shift is accurately captured by the spectrum of $\Sigma^{1/2}\Sigma^{-1/2}$.
Recall that the matrix $M$ captures the geometry of covariate shifts.
Suppose that half of the singular values of $M$ are equal to a parameter $\lambda > 0$ and the other half are equal to $\lambda^{-1}$.
We observe the following dichotomy on the impact of covariate shifts.
\begin{itemize}
	\item If $n_1 \ge n_2$, then the variance (limit) of hard parameter sharing is smallest when $\lambda = 1$ or there is no covariate shift.
	\item If $n_1 < n_2$, then having covariate shifts ($\lambda \neq 1$) always helps reduce variance (limit).
\end{itemize}
To see this, we first write the variance limit for different values of $\lambda$ as follows
\begin{align*}
	\bigtr{\Sigma^{(2)} (a_1 \Sigma^{(1)} + a_2 \Sigma^{(2)})^{-1}}
	= \bigtr{(a_1 M^{\top} M + a_2 \id)^{-1}}
	= \frac{1}{2}\bigbrace{(\lambda^{-2} a_1 + a_2)^{-1} + (\lambda^2 a_1 + a_2)^{-1}}.
\end{align*}
Denote the above function by $f(\lambda)$.
We compare $f(\lambda)$ with $f(1)$, and observe that
\[ 2f(\lambda) - 2f(1) = {(\lambda^2 a_1 + \frac{n_1 + n_2 - p}{n_1 + n_2} - a_1})^{-1}
	+ (\lambda^{-2} a_1 + \frac{n_1 + n_2 - p}{n_1 + n_2} - a_1)^{-1}  - \frac{2(n_1 + n_2)}{n_1 + n_2 - p}, \]
where we used the first part of equation \eqref{lem_cov_shift_eq}.
Next, one can verify that the above equation is equal to $(2a_1 - \frac{n_1 + n_2 - p} {n_1 + n_2}) g(\lambda, a_1)$, for a fixed and positive function $g(\lambda, a_1)$ (details omitted).
Hence, whether or not $f(\lambda)$ is small than $f(1)$ is decided by $a_1$.
To solve $a_1$, we present an informal argument that $a_1 \ge \frac{(n_1 + n_2 - p)}{2(n_1 + n_2)}$ if and only if $n_1 \ge n_2$.
Intuitively, with the current spectrum of $M$, $a_1$ and $a_2$ have a symmetric role in the self-consistent equations.
Hence, when $n_1 \ge n_2$, we expect that $a_1 \ge a_2$, hence $a_1 \ge \frac{1}{2}(1 - \frac{p}{n_1 + n_2})$.
Vice serva when $n_1 < n_2$.
A formal proof requires solving the self-consistent equations \eqref{lem_cov_shift_eq} by replacing half of the singular with $\lambda$ and the other half with $\lambda^{-1}$.
We omit the details.
%We denote the test error as $\te(\hat \beta_{2}^{\MTL},\lambda)$ in the setting where $M$ has $p/2$ singular values that are equal to $\lambda$ and $p/2$ singular values that are equal to $1 / \lambda$. Then equations in equation \eqref{eq_a12extra} become
%\begin{align*}
%	a_1 + \frac{p}{2(n_1 + n_2)}\cdot \bigbrace{\frac{a_1}{a_1 + \lambda^2 a_2} + \frac{a_1}{a_1 + \lambda^{-2} a_2}} = \frac{n_1}{n_1 + n_2}.
%\end{align*}
%It's not hard to verify that there is only one valid solution $(a_1,a_2)$ to equation \eqref{compleeq}.
%After solving these, with equation \eqref{gvar_extra} we get that w.h.p.,
%\be\label{testcomple}
% \te(\hat \beta_{2}^{\MTL},\lambda)= \frac{\sigma^2}{2(\rho_1 + \rho_2)}(1+\OO(p^{-\e}))\cdot f(\lambda) ,\quad f(\lambda): = \frac{1}{\lambda^{-2}{a_1} + a_2} + \frac{1}{\lambda^2a_1 + a_2}.\ee
%First we notice that the curves in Figure  \ref{fig_model_shift_phasetrans} (c) all cross at the point $n_1=n_2$. In fact, if $n_1=n_2$, then it is easy to observe that $a_1=a_2=(1-\gamma)/2$ is the solution to equation \eqref{compleeq}, where we denote $ \gamma=p/(n_1+n_2)$. Then for any $\lambda$, the test error in equation \eqref{testcomple} takes the value
%$$te(\lambda)= \frac{\gamma}{2}\frac{1}{(1-\gamma)/2}=\frac{p}{n_1+n_2-p}.$$
%Second, from Figure \ref{fig_te_complement} we observe that the complementary cases with $\lambda>1$ is better than the case without covariate shift (i.e. $M=\id$ case) when $n_1<n_2$. On the other hand, if we have enough source task data such that $n_1>n_2$, then it is always better to have no covariate shift.
%We now study the behavior of $f$ as $\lambda$ changes.
%This phenomenon can be also explained using our theory.
%We abbreviate $\gamma:=(\rho_1 + \rho_2)^{-1}$. Then with equation \eqref{compleeq}, we can rewrite
%$$f(\lambda)= \frac{1}{\lambda^{-2}{a_1} + (1-\gamma - a_1)} + \frac{1}{\lambda^2a_1 + (1-\gamma - a_1)}.$$
%Then we can compute that
%\begin{align*}
%f(\lambda) - f(1)&= \frac{ \lambda^2-1}{1-\gamma} a_1\cdot \bigbrace{  \frac{1}{ -a_1(\lambda^2-1)+(1-\gamma)\lambda^2 } - \frac{1}{a_1(\lambda^2-1) + (1-\gamma)}} \\
%&= \frac{(\lambda^2-1)^2}{1-\gamma}  a_1\cdot  \frac{2a_1 - (1-\gamma) }{[-a_1(\lambda^2-1)+(1-\gamma)\lambda^2 ][a_1(\lambda^2-1) + (1-\gamma)]} .
%\end{align*}
Thus, we conclude that if $n_1 \ge n_2$, then $f(\lambda) > f(1)$; if $n_1< n_2$, then $f(\lambda)< f(1)$. % which gives $\te(\hat \beta_{2}^{\MTL},\lambda)<\te(\hat \beta_{2}^{\MTL},1)$.
%\item[(iii)] If $n_1=n_2$, we have $f(\lambda)=f(1)=2/(1-\gamma)$, which means $\te(\hat \beta_{2}^{\MTL},\lambda)$ and $\te(\hat \beta_{2}^{\MTL},1)$ are roughly the same. %, which explains why the curves in Figure  \ref{fig_model_shift_phasetrans} (c) all cross at the point $n_1=n_2$.
%\end{itemize}
%This also partially justifies Proposition \ref{prop_covariate}. %Theorem  the observations in Figure \ref{fig_model_shift_phasetrans} (c),

%We compare two cases: (i) when $M = \id_{p\times p}$; (ii) when $M$ has $p/2$ singular values that are equal to $\lambda$ and $p/2$ singular values that are equal to $1 / \lambda$.
%Hence, $\lambda$ measures the severity of the covariate shift.
%	Figure \ref{fig_covariate} shows a simulation of this setting by varying $\lambda$.
%	We observe that as source/target sample ratio increases, the performance gap between the two cases increases.


%By applying Lemma \ref{lem_cov_shift_informal}, we find that when $n_1 / n_2$ is large, having no covariate shift is the optimal choice provided that the determinant of $M^{\top}M$ is bounded.
%We compare different choices of $M$ that belong to the following bounded set.
%Let $\lambda_i$ be the $i$-th singular value of $M$.
%Let $\mu_{\min} < \mu < \mu_{\max}$ be fixed values that do not grow with $p$.
%{\begin{align*}
%		\cS_{\mu}\define\bigset{M \left| \prod_{i=1}^p \lambda_i \le \mu^p, \mu_{\min} \le \lambda_i\le \mu_{\max}, \text{ for all } 1\le i\le p\right.}.
%\end{align*}}
%	We assume that $\beta_1$ and $\beta_2$ are generated following the isotropic model with $d = 0$.
%\begin{corollary}[Derived from Theorem \ref{thm_main_RMT}]\label{prop_covariate}
%	In the setting described above, assume further that a random-effects model holds for both tasks.
%	The prediction loss of $\hat{\beta}_2^{\MTL}$ for task two satisfies that
%	\begin{align*}
%		L(\hat{\beta}_2^{\MTL})
%	\end{align*}
%	Let $g(M)$ denote the prediction loss of $\hat{\beta}_t^{\MTL}$ when $M = \Sigma_1^{1/2}\Sigma_2^{-1/2} \in\cS_{\mu}$.
%	We have that
%		\[ g(\mu\id) \le \bigbrace{1+ \bigo{{\rho_2}/{\rho_1}  }} \min_{M\in\cS_{\mu}} g(M). \]
%\end{corollary}
%Proposition \ref{prop_covariate} shows that when the sample ratio is large, having no covariate shift gives the optimal performance for multi-task learning.
%The proof of Proposition \ref{prop_covariate} can be found in Appendix \ref{app_proof_33}.
\end{example}

As a remark, by combining the bias and variance asymptotics, we can obtain the generalization error of hard parameter sharing for any local minimum solutions $(A, B)$.
Since the proof is similar to the same covariates setting and Example \ref{cor_MTL_loss}, we describe the main differences and omit the rest of the details.
First, we will rescale $X_1$ by $A_1 / A_2$ before applying Theorem \ref{thm_main_RMT}.
Second, we approximate ${X^{(1)}}^{\top}X^{(1)}$ by $n_1 \Sigma^{(1)}$ in equation \eqref{eq_diff_cov} using Fact \ref{lem_minv}.
For the rest of this section, we present the main ideas in the proof of Theorem \ref{thm_main_RMT}.
The full proof can be found in Section \ref{sec_maintools}.
%Proposition \ref{prop_covariate} implies that when $\rho_1\gg \rho_2$, having no covariate shift is the optimal choice for choosing the source task.
%This provides evidence that covariate shift is unfavorable when there are many source task datapoints,

%\todo{} To complement the result, we show an example when the statement is not true if $n_1 \le n_2$.

%We ask: is it better to have $M$ as being close to identity, or should $M$ involve varying levels of singular values?
%Understanding this question has implications for applying normalization methods in multi-task learning \cite{LV19,CBLR18,YKGLHF20}.
%We show that if $n_1$ is much larger than $n_2$, then the optimal $M$ matrix should be proportional to identity, under certain assumptions on its range of singular values (to be formulated in Proposition \ref{prop_covariate}).
%On the other hand, if $n_1$ is comparable or even smaller than $n_2$, we show an example where having ``complementary'' covariance matrices is better performing than having the same covariance matrices.


\paragraph{Proof overview.} The central quantify of interest is the matrix inverse $\hat{\Sigma}^{-1}$.
To study this quantity, we use the Stieltjes transform or the resolvent method in random matrix theory \cite{bai2009spectral,tao2012topics,erdos2017dynamical}.
For any probability measure $\mu$ supported on $[0,\infty)$, the Stieltjes transform of $\mu$ is given by
$$m_\mu(z):= \int_0^\infty \frac{\dd\mu(x)}{x-z}, \text{ for any complex number } z\in \C\setminus [0,\infty).$$
%Conversely, given a Stieltjes transform $m_{\mu}(z)$, we can recover the measure $\mu$ through the inverse formula \HZ{add a ref to the equation below}
%$$ \frac{\dd \mu(x)}{\dd x} = \frac{1}{\pi}\lim_{\eta \downarrow 0}\im m_{\mu}(x+\ii \eta).$$
%where $\rho(x)$ is the density for $\mu$, i.e. $\dd \mu(x)=\rho(x)\dd x$.
Hence the Stieltjes transform method reduces the study of a probability measure to the study of a complex function.
To study the matrix inverse $\hat{\Sigma}^{-1}$, we consider the Stieltjes transform of its empirical spectral distribution.
More precisely, it suffices to study another matrix
\begin{align}\label{eigen2extra}
	Y \define \frac{1}{n_1 + n_2}(\Lambda U^\top (Z^{(1)})^\top Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V), \text{ since } (n_1 + n_2)\hat{\Sigma}^{-1}= \Sigma_2^{-1/2} VY^{-1}V^\top \Sigma_2^{-1/2}.
\end{align}
It is not hard to verify that $\tr[\Sigma^{(2)} \hat{\Sigma}^{-1}] = \bigtr{Y^{-1}} / (n_1 + n_2)$.
Let $\mu=p^{-1}\sum_{i} \delta_{\lambda_i}$ denote the empirical spectral distribution of $Y$, where $\delta_{\lambda_i}$ is the indicator measure.
%The scaling $(n_1+n_2)^{-1}$ ensures such that the support of $\mu$ is bounded.
The Stieltjes transform of $\mu$ is as follows
 \[ m_{\mu}(z) \define \frac{1}{p}\sum_{i=1}^p \frac{1}{\sigma_i - z}= p^{-1}\tr\left[(Y-z\id)^{-1}\right]. \]
In the above equation, $(Y - z\id)^{-1}$ is known as the resolvent matrix or Green's function of $Y$.
%When $p$ goes to infinity, it is well-known that $m_{\mu}(z)$ converges to a fixed distribution governed by a set of self-consistent equations.
%These self-consistent equations give the asymptotic limit of the trace of $\hat{\Sigma}^{-1}$.
%The above approach applies when $\Sigma^{(2)}$ is isotropic in Theorem \ref{thm_main_RMT}.
%Since our goal is to show the limit of $\tr\left[ (Y - z\id)^{-1}V^{\top}{\Sigma^{(2)}}^{-1}V \right]$ as shown in equation \eqref{eigen2extra}, we study the  $(Y-z\id)^{-1}$.
%Compared to the Stieltjes transform, the resolvent also applies to random matrices.
We shall prove the convergence of the resolvent matrix of $Y$ using the so-called ``local laws'' with a sharp convergence rate \cite{isotropic,erdos2017dynamical,Anisotropic}.
%Recent developments in the random matrix literature have shown the convergence of the resolvent matrix using the so-called ``local laws'' or ``deterministic equivalents'' (cf. \cite{Hachem2007deterministic,DS18}).
More precisely, given a deterministic $p\times p$ matrix $R(z)$, we say that $R(z)$ and $(Y-z\id)^{-1}$ are equivalent if for any sequence of deterministic matrices $B$ with uniformly bounded operator norms,
$$\bigtr{((Y-z\id)^{-1}-R(z))B}\to 0.$$
When the above equation holds for large enough $p$, we say that $(Y-z\id)^{-1}$ converges to the matrix limit $R(z)$.
%For our purpose, we use a convenient linearization trick in linear algebra, that is, the SVD of a rectangular matrix $A$ is equivalent to the study of the eigendecomposition of the symmetric block matrix
%$$H(A):=\begin{pmatrix}0 & A \\ A^\top & 0\end{pmatrix},$$
%which is linear in $A$. This trick has been used in many random matrix literatures, such as \cite{Anisotropic, AEK_Gram, XYY_circular,DY20201}.
We observe that the matrix $Y$ can also be written as $A A^{\top}$ for a $p$ by $n_1 + n_2$ matrix defined as follows.
	\[ A = (n_1+ n_2)^{-1/2} [\Lambda U^\top (Z^{(1)})^\top,V^\top (Z^{(2)})^\top]. \]
%}\HZ{what does this trick mean? use less technical words},
%This idea dates back at least to Girko, see e.g., the works \cite{girko1975random,girko1985spectral} and references therein.
Consider the following symmetric and square matrix whose dimension is $p + n_1 + n_2$.
%which is a linear function of $(Z^{(1)})$ and $(Z^{(2)})$:
%\begin{definition}[Linearizing block matrix]\label{def_linearHG}%definiton of the Green function
%We define the $(n+N)\times (n+N)$ block matrix
 \begin{equation}\label{linearize_block}
    H \define \left( {\begin{array}{*{20}c}
   0 & A  \\
   A^{\top} & 0
%   {Z^{(2)}V} & 0 & 0
   \end{array}} \right).
 \end{equation}
The resolvent of $H$ is given by $G(z) \define (H - z\id)^{-1}$, for any complex value $z\in \mathbb C$.
Since $H$ has a specific block structure, it is not hard to verify that
	\begin{equation*} %\label{green2}
	  G(z) =  \left( {\begin{array}{*{20}c}
			(Y - z\id)^{-1} & (Y - z\id)^{-1} A  \\
      A^\top (Y - z\id)^{-1} & (A^\top A - z\id)^{-1}
		\end{array}} \right).%\quad \cal G_R:=(W^\top W - z)^{-1} ,
  \end{equation*}

\paragraph{Matrix limit.}
In Theorem \ref{LEM_SMALL}, we show that for $z$ in a small neighborhood around $0$, when $p$ goes to infinity, $G(z)$ converges to a matrix limit as follows:
\be \label{defn_piw}
	\Gi(z) \define \begin{pmatrix} (a_{1}(z)\Lambda^2  +  (a_{2}(z)- z)\id)^{-1} & 0 & 0 \\ 0 & - \frac{\rho_1+\rho_2}{\rho_1} a_{1}(z)\id_{n_1} & 0 \\ 0 & 0 & -\frac{\rho_1+\rho_2}{\rho_2}a_{2}(z)\id_{n_2}  \end{pmatrix},\ee
where $a_1(z)$ and $a_2(z)$ are the unique solutions to the following system of equations
\begin{align}
	&a_1(z) + a_2(z) = 1 - \frac{1}{n_1 + n_2} \bigbrace{\sum_{i=1}^p \frac{\lambda_i^2 a_1(z) + a_2(z)}{\lambda_i^2 a_1(z) + a_2(z) - z}}, \nonumber\\
	&a_1(z) + \frac{1}{n_1 + n_2}\bigbrace{\sum_{i=1}^p \frac{\lambda_i^2 a_1(z)}{\lambda_i^2 a_1(z) + a_2(z) - z}} = \frac{n_1}{n_1 + n_2}. \label{selfomega_a}
% \frac{\rho_1}{a_{1}(z)} = \frac{1}{p}\sum_{i=1}^p \frac{\lambda_i^2}{ - z+\lambda_i^2 a_{1}(z) +a_{2} (z) } + (\rho_1+\rho_2),\  \frac{\rho_2}{a_{2}(z)} = \frac{1}{p}\sum_{i=1}^p \frac{1 }{  -z+\lambda_i^2 a_{1}(z) +  a_{2}(z)  }+ (\rho_1+\rho_2) .
\end{align}
The existence and uniqueness of the solution to the above equation system are shown in Lemma \ref{lem_mbehaviorw}.
%First, we define the deterministic limits of $(m_1(z), m_{2}(z))$ by $\left(-\frac{\rho_1+\rho_2}{\rho_1}a_{1}(z),-\frac{\rho_1+\rho_2}{\rho_2}a_{2}(z)\right)$, where
%satisfying that $\im a_{1}(z)< 0$ and $\im a_{2}(z)<0$ for $z\in \C_+$ with $\im z$.
%\be\label{ratios}
% \gamma_n :=\frac{p}{n}=\frac{1}{\rho_1+\rho_2},\quad r_1 :=\frac{n_1}{n}=\frac{\rho_1}{\rho_1+\rho_2},\quad r_2 :=\frac{n_2}{n}=\frac{\rho_2}{\rho_1+\rho_2}.
%\ee
Given this result, we show that when $z = 0$, the matrix limit $\Gi(0)$ implies the variance asymptotic shown in equation \eqref{lem_cov_shift_eq}.
First, we have that $a_1 = a_1(0)$ and $a_2 = a_2(0)$ since equation \eqref{selfomega_a} reduces to equation \eqref{eq_a12extra}.
Second, since $Y^{-1}$ is the upper-left block matrix of $G(0)$, we have that $Y^{-1}$ converges to $(a_1\Lambda^2 + a_2)^{-1}$.
Using equation \eqref{eigen2extra}, we have that when $p$ goes to infinity, the trace of $\Sigma^{(2)} \hat{\Sigma}$ converges to
\[ \bigtr{(a_1 \Lambda^2 + a_2\id)^{-1}} = \bigtr{(a_1 M^{\top}M + a_2 \id)^{-1}} = \bigtr{\Sigma^{(2)} (a_1 \Sigma^{(1)} + a_2 \Sigma^{(2)})^{-1}}. \]
%\noindent{\bf Variance asymptotics.} Using definition \eqref{mainG}, we can write equation \eqref{eigen2extra} as
%\be\label{rewrite X as R} [(X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}]^{-1}=n^{-1}\Sigma_2^{-1/2}V\cal G(0)V^\top\Sigma_2^{-1/2}.\ee
%When $z=0$, it is easy to check that , which means that we actually have $a_1(0)=a_1$ and $a_2(0)=a_2$. Hence the matrix limit of $\cal G(0)$ is given by $(a_{1}\Lambda^2 + a_{2}\id_p)^{-1}$. Then inserting this limit into equation \eqref{rewrite X as R}, we can write the left-hand side of equation \eqref{lem_cov_shift_eq} as
%\begin{align}
%&\bigtr{\left( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}\right)^{-1} \Sigma}\approx n^{-1}\bigtr{\Sigma_2^{-1/2}V\cal (a_{1}\Lambda^2 + a_{2}\id_p)^{-1}V^\top\Sigma_2^{-1/2}\Sigma}  \nonumber\\
%&=n^{-1}\bigtr{\Sigma_2^{-1/2}\cal (a_{1}\Sigma_2^{-1/2}\Sigma_1\Sigma_2^{-1/2} + a_{2}\id_p)^{-1}\Sigma_2^{-1/2}\Sigma}  = n^{-1}\bigtr{\cal (a_{1} \Sigma_1  + a_{2}\Sigma_2)^{-1}\Sigma}  ,\label{Gi00}
%\end{align}
where we note that the SVD of $M^\top M = (\Sigma^{(2)})^{-1/2} \Sigma^{(1)} (\Sigma^{(2)})^{-1/2}$ is equal to $V^{\top}\Lambda^2 V$.
%For the asymptotic limit, its concentration error is shown in Appendix \ref{appendix RMT}.

For the bias asymptotic in equation \eqref{lem_cov_derv_eq}, we show that it is governed by the derivative of the resolve of $Y$ with respect to $z$ at $z = 0$.
More precisely, we have that
\begin{align}\label{calculate G'}
	(n_1 + n_2)^2 \hat{\Sigma}^{-1}\Sigma^{(2)}\hat{\Sigma}^{-1} = {\Sigma^{(2)}}^{-1/2} V Y^{-2} V^{\top} {\Sigma^{(2)}}^{-1/2}.
\end{align}
Let $\cal G(z)$ denote the resolvent of $Y$.
Our key observation is that $\frac{\dd{\cal G(z)}}{\dd z} = - \cal G^2(z)$.
Given the matrix limit of $\cal G(z)$ from equation \eqref{defn_piw} above, we have that the matrix limit of $\frac{\dd{\cal G(0)}}{\dd z}$ is given by
\begin{align}\label{cal G'0}
	\frac{\dd \cal G(z)}{\dd z} = \frac{-\frac{\dd a_1(z)}{\dd z}\Lambda^2 - (\frac{\dd a_2(z)}{\dd z} - 1)\id}{(a_{1}(z)\Lambda^2 + a_{2}(z)\id_p)^2}.
\end{align}
As for the derivative of $a_1(z)$ and $a_2(z)$, we solve them by taking the derivative on both sides of the equation system \eqref{selfomega_a} with respect to $z$.
%\begin{align*}
%	\frac{\dd a_1(z)}{\dd z} + \frac{\dd a_2(z)}{\dd z} = -\frac{1}{n_1 + n_2} \sum_{i=1}^p \frac{1}{\lambda_i^2 a_1 + a_2},
%	\frac{\dd a_1(z)}{\dd z} + \frac{1}{n_1 + n_2}\sum_{i=1}^p \frac{\lambda_i^2 (a_1'(z) a_2 - a_2'(z) a_1)}{(\lambda_i^2 a_1 + a_2)^2} = -\frac{1}{n_1 + n_2} \sum_{i=1}^p \frac{\lambda_i^2 a_1}{(\lambda_i^2 a_1 + a_2)^2}
%\end{align*}
Let $a_3$ be the derivative of $-a_1(z)$ and $a_4$ be the derivative of $-a_2(z)$, both evaluated at $z = 0$.
%then taking implicit differentiation of equation \eqref{selfomega_a}
We have that $a_3$ and $a_4$ satisfy the equation system \eqref{eq_a34extra}. Applying equation \eqref{cal G'0} into equation \eqref{calculate G'}, we obtain that the asymptotic limit of the bias equation.
%\begin{align}
%& n^2\bignorm{\Sigma_2^{1/2} \bigbrace{ (X^{(1)})^{\top}(X^{(1)}) + (X^{(2)})^{\top}(X^{(2)}) }^{-1} \Sigma_1^{1/2} w}^2 \nonumber\\
%&\approx  w^\top \Sigma_1^{1/2}\Sigma_2^{-1/2}V\frac{a_3\Lambda^2 +(1+ a_4)\id_p}{(a_{1}\Lambda^2 + a_{2}\id)^2}V^\top \Sigma_2^{-1/2}\Sigma_1^{1/2}w= w^{\top} \Pi_\bias w, \label{calculatePibias}
%\end{align}
%where in the last step we used $M = \Sigma_1^{1/2}\Sigma_2^{-1/2}$ and $V \Lambda^2 V^\top=M^\top M$. This concludes equation \eqref{lem_cov_derv_eq}.
Note that for $\frac{\dd \cal G(z)}{\dd z}$ to be close to its limit at $z = 0$, we not only need the asymptotic limit of $\cal G(0)$, but also the asymptotic limit of $\cal G(z)$ for $z$ within a small neighborhood of $z = 0$.
This is why consider the resolvent matrix of $Y$ as opposed to the Stieljes transform of its empirical spectral distribution.

Next, we briefly describe how to derive the matrix limit $\Gi(z)$.
First, we consider the case where $Z^{(1)}$ and $Z^{(2)}$ are both multivariate Gaussian random matrices.
By rotation invariance, we have that in the matrix $A$,  $Z^{(1)} U$ and $Z^{(2)} V$ are both multivariate Gaussian random matrices.
We use Schur complement to obtain the entries of $G(z)$, and show that for the diagonal entries, they satisfy a set of self-consistent equations in the limit, which are stated above.
On the other hand, for the off-diagonal entries, using standard concentration bounds, we show that they are approximately zero.
Second, we extend our result for the Gaussian case to the more general case where the entries of $Z^{(1)}$ and $Z^{(2)}$ satisfy the bounded moment assumption.
We prove an anisotropic local law using standard proof techniques in the random matrix literature \cite{erdos2017dynamical,Anisotropic}.
The full proof of Theorem \ref{thm_main_RMT} can be found in Appendix \ref{sec pf RMTlemma}.
