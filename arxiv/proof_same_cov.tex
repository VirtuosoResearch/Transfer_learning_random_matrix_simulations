\section{Missing Proof of Theorem \ref{thm_many_tasks}}\label{app_proof_error_same_cov}

	\paragraph{Proof of Claim \ref{lem_exp_opt}.}
	To facilitate the analysis, we consider the following matrix notations.
	Denote
		%\[ \cE A^{\top} := {\sum_{k=1}^t \varepsilon^{(k)} A_k^{\top}}, \]
\[\cE  :=[\epsilon^{(1)},\epsilon^{(2)},\cdots, \epsilon^{(t)}],  \quad \text{ and } \quad \cW \define X(X^{\top} X)^{-1} X^{\top} \cE A^{\top} (AA^{\top})^{\dag}. \]
	For any $j = 1,2,\dots, t$, let
	\begin{align*}
		H_j &\define B^{\star} A^{\top} (AA^{\top})^{+} A_j - \beta^{(j)}, \quad \text{ and } \quad E_j \define \cW A_j - \varepsilon^{(j)}.
	\end{align*}
	Then, we can write the function $g(A)$ conveniently as
	\[ g(A) = \sum_{j=1}^t \bignorm{X H_j + E_j}^2. \]
	We will divide $g(A)$ into three parts.
	We will use matrix notations in the proof since they are more compact.
	That is, stacking $[H_j]_j$ gives matrix $B^{\star} A^{\top} (AA^{\top}) A - B^{\star}$,
	and stacking $[E_j]_j$ gives $\cW A - \cE$.

	\paragraph{Part 1:} The first part is the square of $XH_j$,
	\begin{align}\label{eq_gA_p1}
		\sum_{j=1}^t \bignorm{X H_j}^2
		= \bignormFro{X (B^{\star} A^{\top} (AA^{\top}) A - B^{\star})}^2
		= \bignormFro{X (B^{\star} U_A U_A^{\top} - B^{\star})}^2,
	\end{align}
	where $U_{A} U_{A}^{\top} \in\real^{t\times t}$ denotes the subspace $ {A}^{\top} ( {A}{A}^{\top})^{+}  {A}$.
	Taking expectation of equation \eqref{eq_gA_p1} over $X$, we get
		\[ \bignorm{\Sigma^{1/2}(B^{\star} U_A U_A^{\top} - B^{\star})}^2. \]

	\paragraph{Part 2:} The second part is the cross term, which is equal to the following using the matrix notations
	\begin{align}\label{eq_gA_p2}
		\sum_{j=1}^t\inner{XH_j}{E_j} = \inner{X(B^{\star} U_A U_A^{\top} - B^{\star})}{\cW A - \cE}
		= - \inner{X (B^{\star} U_A U_A^{\top} - B^{\star})}{\cE},
	\end{align}
	which is zero in expectation over $\cE$.

	\paragraph{Part 3:} The last part is the square of $E_j$,
	\begin{align}\label{eq_gA_p3}
		\sum_{j=1}^t \norm{E_j}^2 &= \bignormFro{\cW A - \cE}^2
		= \bignormFro{\cE}^2 - \inner{\cW A}{\cE},
	\end{align}
	which is because $\norm{\cW A}^2 = \inner{\cW A}{\cE}$ by algebraic calculation.
	Hence, it suffices to show that the expectation of equation \eqref{eq_gA_p3} is equal to $\sigma^2 (n\cdot t - p\cdot r)$.
	First, we have that $\ex{\|\cE\|_F^2} = \sigma^2 \cdot n \cdot t$.
%	Conditional on $X$, we have that
%	\begin{align*}
%		\exarg{\cE}{\norm{E_j}^2}
%		= \exarg{\cE}{\sum_{j=1}^t \bigbrace{\bignorm{\cW A_j}^2 - 2\inner{\cW A_j}{\varepsilon^{(j)}} }} + \sigma^2 \cdot n \cdot t.
%	\end{align*}
%	We consider both terms in the above equation one by one.
	Second, notice that for $U_XU_X^\top:=X(X^{\top} X)^{-1} X^{\top}$ and $1\le i, j \le t$,
	%\begin{align}
	%	\sum_{j=1}^t \norm{\cW A_j}^2 = \sum_{j=1}^t \bigtr{\cW A_j A_j^{\top} \cW^{\top}} = \bigtr{\cW AA^{\top} \cW^{\top}} = \bigtr{U_X U_X^{\top} \cE A^{\top} (AA^{\top})^{-1} A \cE^{\top}}. \label{eq_proof_same_cov_1}
	%\end{align}
	%We observe that
	\begin{align*}
			\exarg{\cE}{(\cE^{\top}U_XU_X^\top \cE)_{ij} }
		&= \exarg{\cE}{{\varepsilon^{(i)}}^\top U_X U_X^\top  \varepsilon^{(j)} } = \sigma^2 \cdot \tr\left( U_XU_X^\top \right) \cdot \delta_{ij} = p\sigma^2 \cdot \delta_{ij},
	\end{align*}
	which implies that $\exarg{\cE}{\cE^{\top}U_XU_X^\top \cE }=p\sigma^2 \cdot \id_{t\times t}$.
	In the above derivation, the second step used the fact that $\E(\varepsilon^{(i)}_k \varepsilon^{(i)}_l )=\sigma^2\cdot \delta_{ij}\delta_{kl}$ for $1\le k,l \le n$, and the third step used the fact that $\tr(U_XU_X^\top) =\tr(U_X^\top U_X) =p$.
	Therefore, we have that
%	\begin{align*}
%		\inner{\cW A}{\cE} = \sigma^2 \cdot r \cdot \bigtr{U_X U_X^{\top}} = \sigma^2 \cdot r \cdot p,
%	\end{align*}
%because $U_X$ has rank $p$ by Fact \ref{lem_minv}.
	\begin{align*}
		\exarg{\cE}{ \inner{\cW A}{\cE}} = p\sigma^2 \cdot \bigtr{\id_{t\times t}U_AU_A^\top} =  p \sigma^2 \cdot r ,
			\end{align*}
   because $U_AU_A^\top$ has rank $r$.	
	%Next, note that
	%\[ \sum_{j=1}^t \inner{\cW A_j}{\varepsilon^{(j)}} = \bigtr{\cW A\cE^{\top}}, \]
	%which is equal to equation \eqref{eq_proof_same_cov_1} by the definition of $\cW$.
	Hence the proof is complete.
	
	
	\iffalse
	Second, notice that
	%\begin{align}
	%	\sum_{j=1}^t \norm{\cW A_j}^2 = \sum_{j=1}^t \bigtr{\cW A_j A_j^{\top} \cW^{\top}} = \bigtr{\cW AA^{\top} \cW^{\top}} = \bigtr{U_X U_X^{\top} \cE A^{\top} (AA^{\top})^{-1} A \cE^{\top}}. \label{eq_proof_same_cov_1}
	%\end{align}
	%We observe that
	\begin{align*}
			\exarg{\cE}{\cE A^{\top} (AA^{\top})^{+} A \cE^{\top}}
		&= \exarg{\cE}{\sum_{j=1}^t \sum_{k=1}^t \varepsilon^{(j)} A_j^{\top} (AA^{\top})^{+} A_k {\varepsilon^{(k)}}^{\top}} \\
		&= \exarg{\cE}{\sum_{j=1}^t \varepsilon^{(j)} A_j^{\top} (AA^{\top})^{+} A_j {\varepsilon^{(j)}}^{\top}} \\
		&= \sigma^2 \cdot r \cdot \id_{n\times n}.
	\end{align*}
	In the above derivation, the second step used the fact that for any $j\neq k$, $\varepsilon^{(j)}$ and $\varepsilon^{(k)}$ are pairwise independent.
	The third step used the fact that $\sum_{j=1}^t A_j^{\top} (AA^{\top})^{+} A_j = \bigtr{\id_{r\times r}} = r$, and $\ex{\varepsilon^{(j)} {\varepsilon^{(j)}}^{\top}} = \sigma^2 \cdot \id_{n\times n}$.
	Therefore, we have that
%	\begin{align*}
%		\inner{\cW A}{\cE} = \sigma^2 \cdot r \cdot \bigtr{U_X U_X^{\top}} = \sigma^2 \cdot r \cdot p,
%	\end{align*}
%because $U_X$ has rank $p$ by Fact \ref{lem_minv}.
	\begin{align*}
		\exarg{\cE}{ \inner{\cW A}{\cE}} = \sigma^2 \cdot r \cdot \bigtr{X(X^{\top} X)^{-1} X^{\top}} =  \sigma^2 \cdot r \cdot \bigtr{ X^{\top}X(X^{\top} X)^{-1}}  = \sigma^2 \cdot r \cdot p.
	\end{align*}
   because $X^\top X$ is a $p\times p$ matrix.	
	%Next, note that
	%\[ \sum_{j=1}^t \inner{\cW A_j}{\varepsilon^{(j)}} = \bigtr{\cW A\cE^{\top}}, \]
	%which is equal to equation \eqref{eq_proof_same_cov_1} by the definition of $\cW$.
	Hence the proof is complete.
\fi

	\paragraph{Proof of Claim \ref{claim_opt_dist}.}
	In order to show that the subspace spanned by the rows of $\hat{A}$ is close to $U_r$, we first show that $g(A)$ and its expectation is close as follows.
	\begin{align}\label{eq_gA_err}
		\bigabs{g(A) - \exarg{\cE, X}{g(A)}} \lesssim p^{-c_{\varphi}} \cdot n \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id)}^2 + p^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
	\end{align}
	To show the above result, we consider the concentration error of each part of $g(A)$.

	For equation \eqref{eq_gA_p1}, % $g_0(\cal W)= \sum_{j=1}^t\left\|Z v_j \right\|^2$.
%	$$g_0(\cal W)= \sum_{j=1}^t\left\|Z v_j \right\|^2,\quad v_j:= \Sigma^{1/2}\left(B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j -  \beta_j\right).$$
%	note that $X H_j = Z \Sigma^{1/2} H_j \in \real^n$ is a random vector with i.i.d. entries of mean zero, variance $\|\Sigma^{1/2}H_j\|^2$, and finite $\varphi$-th moment by Assumption \eqref{assume_rm}.
%	Hence by the law of large numbers, we have that with high probability	
	applying Corollary \ref{cor_largedeviation} to $X H_j = Z \Sigma^{1/2} H_j$, we obtain that 
	${\left\|Z \Sigma^{1/2} H_j \right\|^2 } = { n \|\Sigma^{1/2} H_j\|^2} \cdot (1 + \OO(p^{-c_\varphi}))$ with high probability. 
	%, for a small enough constant $\e$,
	This implies that
	\begin{align}
		\abs{\sum_{j=1}^t \|X H_j\|^2 - \exarg{X}{\sum_{j=1}^t \norm{X H_j}^2}} \lesssim p^{-c_\varphi} \cdot n \bignorm{\Sigma^{1/2} B^{\star} (U_{A}U_A^{\top} - \id_{t\times t})}^2. \label{eq_gA_err1}
	\end{align}

	For equation \eqref{eq_gA_p2}, 
	%using Lemma \ref{largedeviation} in Appendix \ref{app_tool} and the fact that all moments of $\varepsilon_i$ exist by Assumption \eqref{assmAhigh2}, 
	using Corollary \ref{cor_calE}, we obtain that the following holds with high probability:
	\begin{align}
		|\inner{XB^{\star} (U_AU_A^{\top} - \id)}{\cE}| &\le p^\e \cdot \sigma \cdot \bignormFro{XB^{\star} (U_AU_A^{\top} - \id)} \nonumber \\
		&\le p^{\e} \cdot \sigma \cdot \norm{Z} \cdot \bignormFro{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id)} \nonumber \\
		&\lesssim p^{\e} \cdot \sigma \cdot \sqrt{n}\cdot \sqrt{t}\bignorm{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id)}, \label{eq_gA_err2}
	\end{align}
	which is bounded by the right hand side of equation \eqref{eq_gA_err} by AM-GM inequality.
	In the second step, we used the fact that $X=Z\Sigma^{1/2}$.
	In the third step, we used Fact \ref{lem_minv} (iii) to bound the operator norm $\|Z\|$ by $\OO(\sqrt{n})$, and used $\bignormFro{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id)}\le \sqrt{t} \bignorm{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id)}$ since the matrix $\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id)$ has rank at most $t$.

	For equation \eqref{eq_gA_p3}, using Corollary \ref{cor_calE},
	%Lemma \ref{largedeviation} and the fact that all moments of $\varepsilon_i$ exist, 
	we obtain that with high probability,
	\begin{align}
		\abs{\normFro{\cE}^2 - \ex{\normFro{\cE}^2}} \le p^{c} \sigma^2 \| \id_{n\times n}\|_F = p^{c} \cdot \sqrt{n} \cdot \sigma^2 \le p^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t, \label{eq_gA_err3}
	\end{align}
	as long as the constant $c$ is taken sufficiently small. 
	For the inner product between $\cW A$ and $\cE$, we have that with high probability,
	\begin{align}
		\bigabs{\inner{\cW A}{\cE} - \exarg{\cE}{\inner{\cW A}{\cE}} } &= \bigabs{\bigtr{\left(\cE^\top U_X U_X^{\top}\cE  - p\sigma^2 \cdot  \id_{t\times t} \right)  U_AU_A^{\top} }} \nonumber  \\
		&\le \bignormFro{U_{A} U_A^{\top}} \cdot \bignormFro{\cE^\top U_X U_X^{\top} \cE - p \sigma^2 \cdot  \id_{t\times t}} \nonumber \\
		&\le \sqrt{r} \cdot (p^{\e}\sigma^2\cdot \|U_X U_X^\top\|_F) \nonumber \\
		&\le p^{1/2+c} \cdot \sigma^2 \cdot \sqrt{r} \nonumber\\
		& \le  p^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t .\label{eq_gA_err4}
	\end{align}

%	\begin{align}
%		\bigabs{\inner{\cW A}{\cE} - \ex{\inner{\cW A}{\cE}}} &= \bigabs{\bigtr{U_X U_X^{\top} (\cE U_AU_A^{\top} \cE^{\top} - \sigma^2 \cdot r \id_{n\times n})}} \nonumber  \\
%		&\le \bignormFro{U_{X} U_X^{\top}} \cdot \bignorm{\cE U_A U_A^{\top} \cE^{\top} - \sigma^2 \cdot r \id_{n\times n}} \nonumber \\
%		&\le p^{1/2} \cdot (\sigma^2 r\cdot p^{\e}). \label{eq_gA_err4}
%	\end{align}
	%\begin{align*}
	%	%|\inner{U_X U_X^{\top} \bigbrace{\varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1}W_j}{\varepsilon_j}|
	%	\abs{W_i^{\top} (\cW\cW^{\top})^{-1} W_j \cdot \bigbrace{\varepsilon_i^{\top} U_X U_X^{\top} \varepsilon_j}}
	%	&\le |W_i^{\top} (\cW\cW^{\top})^{-1}W_j| \cdot \sigma^2 p^{\e} \bigtr{(U_X U_X^{\top})^2}^{1/2} \le \sigma^2 p^{1/2+\e}
  % &\left|\left(1-\mathop{\mathbb{E}}_{\varepsilon_j} \right)\left[\inner{U_X U_X^{\top} \bigbrace{\varepsilon_j W_j^{\top}} (\cW\cW^{\top})^{-1}W_j}{\varepsilon_j}\right]\right| \le |W_j^{\top} (\cW\cW^{\top})^{-1}W_j| \cdot \sigma^2 p^{\e} \tr \left[(U_X U_X^{\top})^2\right]\le \sigma^2 p^{1/2+\e},\\
  % &\left|\left(1-\mathop{\mathbb{E}}_{\varepsilon_j} \right)\left[\inner{U_X U_X^{\top} \bigbrace{\varepsilon_j W_j^{\top}} (\cW\cW^{\top})^{-1}W_j}{U_X U_X^{\top} \bigbrace{\varepsilon_j W_j^{\top}} (\cW\cW^{\top})^{-1}W_j}\right]\right|\\
  % &\le |W_j^{\top} (\cW\cW^{\top})^{-1}W_j|^2 \cdot \sigma^2 p^{\e} \tr \left[(U_X U_X^{\top})^4\right]\le \sigma^2 p^{1/2+\e},
	%\end{align*}
	%Combining equation \eqref{eq_gA_}, we get that with high probability
	%\begin{align}
	%	\left|g_2(\cal W)-\sigma^2(n\cdot t - p\cdot r)\right|=\left|g_2(\cal W)- \exarg{\varepsilon_1, \dots, \varepsilon_t} {g_2(\cal W)}\right| \lesssim \sigma^2 p^{1/2+\e}. \label{eq_gA_err}
	%\end{align}
	Combining equations \eqref{eq_gA_err1}, \eqref{eq_gA_err2}, \eqref{eq_gA_err3}, and \eqref{eq_gA_err4}, we obtain equation \eqref{eq_gA_err}.

	\bigskip
	Next, we use equation \eqref{eq_gA_err} to prove the claim.
	Using triangle inequality, we upper bound the gap between $g(A^{\star})$ and $g(\hat{A})$
	\begin{align}
		g(A^{\star}) - g(\hat{A}) &\le \bigabs{g(A^{\star}) - \ex{g(A^{\star})}} + (\ex{g(A^{\star})} - \ex{g(\hat{A})}) + \bigabs{g(\hat{A}) - \ex{g(\hat{A})}} \nonumber \\
		&\le \bigabs{g(A^{\star}) - \ex{g(A^{\star})}} +\bigabs{g(\hat{A}) - \ex{g(\hat{A})}} \nonumber \\
		&\le p^{-c} \cdot n \bignormFro{\Sigma^{1/2} B^{\star}}^2 + p^{-1/2 + \e}\cdot \sigma^2 n \cdot t. \label{eq_g_gap}
	\end{align}
	The second equation uses the fact that $A^{\star}$ is a global minimum of $\ex{g(\cdot)}$, hence $\ex{g(A^{\star})} \le \ex{g(\hat{A})}$.
	The third equation uses equation \eqref{eq_gA_err} and the fact that the spectral norm of $U_A U_A^{\top} - \id$ is at most one.

	We can also derive a lower bound on the gap as follows.
	We simplify $g(A)$ as
	\begin{align*}
		g(A)  &= \normFro{X (X^{\top}X)^{-1} X^{\top} Y U_A U_A^{\top} - Y}^2 \\
					&= \normFro{Y}^2 - \inner{U_A U_A^{\top}}{Y^{\top} X (X^{\top} X)^{-1} X^{\top} Y}.
	\end{align*}
	We denote $H = Y^{\top} X (X^{\top} X)^{-1} X^{\top} Y$, which appears in the equation above.
	Since $\hat{A}$ is a global minimum of $g(A)$, we have that $g(A^{\star}) \ge g(\hat{A})$.
	Hence,
	\begin{align}
		g(A^{\star}) - g(\hat{A}) &= \abs{g(A^{\star}) - g(\hat{A})} = \abs{\inner{H}{A^{\star} {A^{\star}}^{\top} - U_{\hat{A}} U_{\hat{A}}^{\top}}} \nonumber \\
												&\ge \lambda_{\min}(H) \cdot \normFro{U_r U_r^{\top} - U_{\hat{A}} U_{\hat{A}}^{\top}}. \label{eq_g_gap1}
	\end{align}
	Recall that $Y = X B^{\star} + \cE$ in matrix notation.
	We bound the minimum singular value of $H$ as follows
	\begin{align}
		\lambda_{\min}(H) &= \lambda_{\min}({B^{\star}}^{\top} X^{\top} X B^{\star} + \cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}X^{\top}X B^{\star}) + \lambda_{\min}(\cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}\Sigma B^{\star}) \cdot \lambda_{\min}(Z^{\top}Z) + \lambda_{\min}(\cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}\Sigma B^{\star}) \cdot ((\sqrt n - \sqrt p)^2 - n \cdot p^{-c}) + \sigma^2 \cdot p \cdot (1 - p^{-1/2 + \e}). \label{eq_g_gap2}
	\end{align}
	The second equation uses the fact that the minimum singular value of the sum of two PSD matrices is greater than the sum of the minimum singular value of each matrix.
	The third equation uses the fact that $X = Z \Sigma^{1/2}$ and Fact \ref{fact_proof_gA} in Appendix \ref{app_tool}.
	The last equation uses Fact \ref{lem_minv} for the minimum singular value of $Z^{\top}Z$ and Lemma \ref{largedeviation} for $\cE\cE^{\top}$.

	Combining equation \eqref{eq_g_gap}, \eqref{eq_g_gap1}, and \eqref{eq_g_gap2}, we conclude that
	\begin{align*}
		\bignormFro{U_r U_r^{\top} - U_{\hat{A}} U_{\hat{A}}^{\top}}
		&\le \frac{p^{-c} \cdot n \bignormFro{\Sigma^{1/2} B^{\star}}^2 + p^{-1/2 + c} \cdot \sigma^2 nt} {\lambda_{\min}(H)} \\
		&\le \frac{p^{-c} \rho \normFro{\Sigma^{1/2} B^{\star}}^2 + p^{-1/2 + \e} \cdot \sigma^2 \rho t}{\lambda_{\min}^2(\Sigma^{1/2} B^{\star}) ((\sqrt{\rho} - 1)^2 - \rho \cdot p^{-c}) + \sigma^2 (1 - p^{-1/2 + \e})}
	\end{align*}
	Hence for large enough $p$, we obtain that Claim \ref{claim_opt_dist} holds.
	The proof is complete.

	\paragraph{Proof of Claim \ref{claim_pred_err}.}
	We apply similar arguments from Claim \ref{lem_exp_opt} and \ref{claim_opt_dist} for this proof.
	The prediction loss of hard parameter sharing for task $i$ is equal to
	\begin{align*}
		L(\hat{\beta}_i^{\MTL}) &= \bignorm{\Sigma^{1/2} (\hat{B} \hat{A}_i - \beta_i)}^2 \\
		&= \bignorm{\Sigma^{1/2} ((X^{\top} X)^{-1} X^{\top} Y \hat{A}^{\top} (\hat{A} \hat{A}^{\top})^{+} \hat{A}_i - \beta_i)}^2 \\
		&= \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i + E_i)}^2,
	\end{align*}
	where we denote $E_i = (X^{\top} X)^{-1} X^{\top} \cE \hat{a}_i$.
	We divide the prediction loss to three parts.

	\paragraph{Part 1:} The first part is the bias term:
	$\norm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i)}^2 = L(B^{\star} \hat{a}_i)$.

	\paragraph{Part 2:} The second part is the cross term, whose expectation over $\cE$ is zero. The concentration error is
	\begin{align*}
		\bigabs{\inner{\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)}{\Sigma^{1/2}E_i}}
		\le \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i)} \cdot \bignorm{\Sigma^{1/2} E_i}.
	\end{align*}
	Next, we use the fact that the spectral norm of $\cE$ is at most $\sigma \cdot p^{\e}$.
	Hence, the spectral norm of $\Sigma^{1/2} E_i$ is at most
		\[ \sigma \cdot p^{\e} \cdot \norm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}} \cdot \norm{\hat{a}_i}. \]
	Therefore, the cross term is bounded by $\sigma \cdot p^{\e}$ times
	\begin{align*}
			& \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i)} \cdot \norm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}} \cdot \norm{\hat{a}_i} \\
		\le & \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i)}^2 + \norm{\hat{a}_i}^2 \cdot \bignorm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}}^2 \tag{by Cauchy-Shwartz inequality} \\
		\le & \bignorm{\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)} + \norm{\hat{a}_i}^2 \cdot \bigtr{\Sigma (X^{\top} X)^{-1}}.
	\end{align*}

	\paragraph{Part 3:} The final part is the square of $E_i$. One can verify that the expectation of $\norm{\Sigma^{1/2} E_i}^2$ is equal to $\sigma^2 \norm{\hat{a}_i}^2 \cdot \tr[\Sigma (X^{\top} X)^{-1}]$.
	Next, we bound the concentration error.
	Let $A = X (X^{\top} X)^{-1} \Sigma (X^{\top} X)^{-1} X^{\top}$.
	We have that
	\begin{align*}
		\bigabs{\bignorm{\Sigma^{1/2} E_i}^2 - \exarg{\cE}{\bignorm{\Sigma^{1/2} E_i}^2}}
		&= \bigabs{\inner{A}{\cE \hat{a}_i \hat{a}_i^{\top} \cE^{\top} - \exarg{\cE}{\cE \hat{a}_i \hat{a}_i^{\top} \cE}}}.
	\end{align*}
	In the above equation, we note that the trace of $A$ is precisely the trace of $\Sigma (X^{\top} X)^{-1}$.
	On the other hand, the concentration error of the spectral norm of $\cE \hat{a}_i \hat{a}_i^{\top} \cE^{\top}$ is at most $\sigma^2 \e $.

	Combining the three parts together, we conclude the proof of Claim \ref{claim_pred_err}.

	%\paragraph{Proof of Theorem \ref{thm_many_tasks}.}
	%In sum, we have solved that $\hat{\beta}_i^{\MTL}=B^{\star}\left( U_r v_i +\oo(1)\right)$.
	%Now we are ready to finish the proof.
	%Similar to the proof of equation \eqref{eq_est_g1} and \eqref{eq_est_g2}, we have that
	%\begin{align}
	%	L(\hat{\beta}_t^{\MTL}) = \exarg{\varepsilon_1, \dots, \varepsilon_t}{L(\hat{\beta}_t^{\MTL})} (1 + O(p^{-1/2+c})) + O(\sigma^2 \cdot p^{-1/2 + c}). \label{eq_multiple_last1}
	%\end{align}
	%Using equation \eqref{eq_ex_pred}, we have that
	%\begin{align}
	%	\exarg{\varepsilon_1, \dots, \varepsilon_t}{\hat{\beta}_t^{\MTL}} &= \bignorm{\Sigma^{1/2} \left((B^\star \hat{\cal W}^\top) (\hat{\cal W}\hat{\cal W}^{\top})^{-1} \hat W_t - \beta_t \right) }^2
	%	+ \sigma^2  \hat W_t^{\top} (\hat{\cal W}\hat{\cal W}^{\top})^{-1} \hat W_t \cdot \bigtr{\Sigma (X^{\top}X)^{-1}} \nonumber \\
	%	&= \bignorm{\Sigma^{1/2} \bigbrace{B^{\star} U_r v_t-\beta_t}}^2 + \oo\left(\bigtr{{B^{\star}}^{\top} \Sigma B^{\star}}\right) + \sigma^2(\norm{v_t}^2 + o(1)) \bigtr{\Sigma (X^{\top}X)^{-1}} \cdot (1+\oo(1)) \nonumber \\
	%	&= \bignorm{\Sigma^{1/2} \bigbrace{B^{\star} U_r v_t-\beta_t}}^2 + \frac{\sigma^2}{\rho-1}\norm{v_t}^2 + \oo \left( \bigtr{{B^{\star}}^{\top} \Sigma B^{\star}} + \sigma^2\right), \label{eq_multiple_last2}
	%\end{align}
	%In the second step, we use that $\norm{\hat{\cW}^{\top} (\cW \cW^{\top})^{-1} \cW - U_r U_r^{\top}} \le o(1)$ w.h.p., which implies that
	%\[ \bignorm{U_r v_t - \hat{\cW}^{\top} (\cW \cW^{\top})^{-1} \hat{\cW}_t} \le o(1) \quad \text{ and } \quad \bigabs{\hat{\cW}_t^{\top} (\hat{\cW} \hat{\cW}^{\top})^{-1} \hat{W}_t - \norm{v_t}^2} \le o(1). \]
  %In the last step, we use Lemma \ref{lem_minv}, that is, $\bigtr{\Sigma (X^{\top} X)^{-1}} = \sigma^2 \cdot (1 + o(1))/ (\rho - 1)$.
	%Combining equation \eqref{eq_multiple_last1}, \eqref{eq_multiple_last2}, and $\te(\hat{\beta}_t^{\STL})=\frac{\sigma^2}{\rho-1} \cdot \left( 1+\oo(1)\right)$, we conclude the proof.
	%The second step uses the fact that $U_r U_r^{\top}$ is the global minimum of $\ex{g(\cdot)}$.
	%In fact, if \eqref{Q-Q} does not hold, then using the condition $\lambda_{\min}((B^{\star})^\top\Sigma B^{\star})\gtrsim \sigma^2$ and that $\delta_{\vari}(\cal W)=\OO(\sigma^2)$ by  Lemma \ref{lem_minv}, we obtain that
	%$$   \val(\hat {Q}) + t  \sigma^2 > (\val( Q_0) + t \sigma^2 )\cdot (1+\oo(1)) \ \Rightarrow \ g( \hat{\cal W})>g( {\cal W}_0),$$
	%where $\cal W_0\in \R^{r\times t}$ is a matrix such that  $ \cal W_0^{\top} (\cal W_0\cal W_0^{\top})^{-1} \cal W_0=Q_0$. Hence $\hat {\cal W}$ is not a minimizer, which leads to a contradiction.
	%with high probability
	%\begin{align}
	%g(\cal W)&= n \bignorm{\Sigma^{1/2} B^{\star} (\cW^{\top} (\cW \cW^{\top})^{-1} \cW - \id_{t\times t})}^2 \cdot (1+\oo(1)) + \sigma^2 (n\cdot t - p \cdot r)+\OO(\sigma^2 p^{1/2+\e}) \nonumber \\
	%&=\ex{g(\cW)} \cdot (1+\oo(1)) + \OO(\sigma^2 p^{1/2+\e}). \label{eq_est_g}
	%\end{align}

%		Applying the above estimates into $g_1(\cW)$, we get that
%	First, for $g_1(\cW)$, using Lemma \ref{largedeviation} in Appendix \ref{sec_maintools} and the fact that all moments of $\varepsilon_i$ exist, we obtain that for a sufficiently small constant $\e>0$, the following holds with high probability:
%	\[ \text{for any } 1\le j \le t, |\inner{XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - X\beta_j}{\varepsilon_j}| \le p^\e \sigma\|XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - X\beta_j\|. \]
%		Applying the above estimates into $g_1(\cW)$, we get that
%	\begin{align}
%	  \abs{g_1(\cal W)}    &\le p^\e \sigma \sum_{j=1}^t \|  {XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - X\beta_j}\| \nonumber \\
%	&\le  p^\e \sigma \sum_{j=1}^t \bignorm{  \Sigma^{1/2}\left(B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - \beta_j\right)} \cdot \|Z\| \nonumber\\
%	&\lesssim p^{1/2+\e} \sigma \sum_{j=1}^t \bignorm{  \Sigma_1^{1/2}\left(B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - \beta_j\right)} \nonumber\\
%	& \le p^{1/2+\e} \bignorm{\Sigma^{1/2} B^{\star} (\cW^{\top} (\cW \cW^{\top})^{-1} \cW - \id_{t\times t})}^2 + t\sigma^2 p^{1/2+\e}.  \label{eq_est_g1}
%	\end{align}
%	In the second step, we use the fact that $X=Z\Sigma^{1/2}$.
%	In the third step, we use equation \eqref{eq_isometric} to bound the operator norm $\|Z\|$ by $\OO(\sqrt{p})$.
%	In the last step, we use the AM-GM inequality.


	%\paragraph{Proof of Lemma \ref{lem_error_same_cov}.} Next, we show that the concentration errors of $g_0(\cW), g_1(\cW), g_2(\cW)$ are all lower order terms compared to $\ex{g(\cW)}$.


	%and
	%\begin{align*}  &|\inner{XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - X\beta_j}{U_X U_X^{\top} \bigbrace{\varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j }| \\
	%&\le p^\e \sigma |W_i^{\top} (\cW\cW^{\top})^{-1} W_j|\cdot \|U_X U_X^{\top}( {XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - X\beta_j})\|
	%\end{align*}



	%	\begin{align*}
	%	g_0(\cal W) &= \sum_{j=1}^t \bignorm{XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j - X\beta_j}^2 = \bignorm{X B^{\star} (\cW^{\top} (\cW\cW^{\top})^{-1} \cW - \id_{t\times t})}^2, \\
	%	g_1(\cal W)  &= \sum_{j=1}^t \inner{XB^{\star} \cW^{\top} (\cW\cW^{\top})^{-1}W_j - X\beta_j}{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1} W_j - \varepsilon_j} ,\\
	%			&= \sum_{j=1}^t \inner{-X B^{\star} \cW^{\top} (\cW \cW^{\top})^{-1} W_j + X\beta_j}{\varepsilon_j},\\
	%	g_2(\cal W)  &= \sum_{j=1}^t \bignorm{U_X U_X^{\top} \bigbrace{\sum_{i=1}^t \varepsilon_i W_i^{\top}} (\cW\cW^{\top})^{-1}W_j - \varepsilon_j}^2 .
	%				\end{align*}
