\section{Missing Proof of the Different Covariates Setting}\label{sec_maintools}

\subsection{Random Matrix Theory Results}\label{appendix RMT}

We provide the proof of Theorem \ref{thm_main_RMT}.
We begin with a warm up analysis when the entries of $Z^{(1)}$ and $Z^{(2)}$ are drawn i.i.d. from an isotropic Gaussian distribution.
By the rotational invariance of the multivariate Gaussian distribution, we have that the entries of $Z^{(1)} U$ and $Z^{(2)}$ also follow an isotropic Gaussian distribution.
Hence it suffices to consider the following resolvent
 \begin{equation} \label{resolv Gauss1}
   G(z)= \left( {\begin{array}{*{20}c}
   { -z\id_{p} } & n^{-1/2}\Lambda (Z^{(1)})^\top & n^{-1/2} (Z^{(2)})^\top  \\
   {n^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1}} & 0 \\
   {n^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2}}
   \end{array}} \right)^{-1}.
 \end{equation}
We show how to derive the matrix limit $\Gi(z)$ and the self-consistent equation system \eqref{selfomega_a}.
We first introduce several notations.
Define the index sets
$$\cal I_0:=\llbracket 1,p\rrbracket, \quad  \cal I_1:=\llbracket p+1,p+n_1\rrbracket, \quad \cal I_2:=\llbracket p+n_1+1,p+n_1+n_2\rrbracket ,\quad \cal I:=\cal I_0\cup \cal I_1\cup \cal I_2  .$$
%We will consistently use the latin letters $i,j\in\sI_{0}$ and greek letters $\mu,\nu\in\sI_{1}\cup \sI_{2}$.
%Correspondingly, the indices of the matrices $Z^{(1)}$ and $Z^{(2)}$ are labelled as
%	\be\label{labelZ}
% Z^{(1)}= [Z^{(1)}_{\mu i}:i\in \mathcal I_0, \mu \in \mathcal I_1], \quad Z^{(2)}= [Z^{(2)}_{\nu i}:i\in \mathcal I_0, \nu \in \mathcal I_2].\ee
We will study the following partial traces of the resolve $G(z)$:
\be\label{defm}
\begin{split}
m(z) :=\frac1p\sum_{i\in \cal I_0} G_{ii}(z) ,\quad & m_0(z):=\frac1p\sum_{i\in \cal I_0} \lambda_i^2 G_{ii}(z),\\
 m_1(z):= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu\mu}(z) ,\quad & m_2(z):= \frac{1}{n_2}\sum_{\nu\in \cal I_2}G_{\nu\nu}(z).
\end{split}
\ee
To deal with the matrix inverse, we consider the following resolvent minors of $G(z)$.
\begin{definition}[Resolvent minors]\label{defn_Minor}
	Let $X \in \real^{(p + n_1 + n_2)\times (p + n_1 + n_2)}$ and $i = 1, 2, \dots, p + n_1 + n_2$.
	The minor of $X$ after removing the $i$-th row and column of $X$ is denoted by $X^{(i)} := [X_{a_1,a_2}:a_1, a_2 \in \mathcal I\setminus \{i\}]$ as a square matrix with dimension $p + n_1 + n_2 - 1$.
	For the indices of $X^{(i)}$, we use $X^{(i)}_{a_1, a_2}$ to denote $ X_{a_1, a_2}$ when $a_1$ and $a_2$ both not equal to $i$, and $X^{(i)}_{a_1, a_2}$ to be zero when $a_1 = i$ or $a_2 = i$.
	The resolvent minor of $G(z)$ after removing the $i$-th row and column is defined as
	\begin{align*}
		G^{(i)}(z) := \left[ \left( {\begin{array}{*{20}c}
		  { -z\id_{p} } & n^{-1/2}\Lambda (Z^{(1)})^\top & n^{-1/2} (Z^{(2)})^\top  \\
      {n^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1}} & 0 \\
			{n^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2}}
    \end{array}} \right)^{(i)}\right]^{-1}.
	\end{align*}
\end{definition}
As a remark, we denote the partial traces $m^{(i)}(z)$, $m_0^{(i)}(z)$, $m_1^{(i)}(z)$, and $m_2^{(i)}(z)$ by replacing $G(z)$ with $G(z)^{(i)}$ in equation \eqref{defm}.

\paragraph{Self-consistent equations.}
We briefly describe the ideas for deriving the equation set \eqref{selfomega_a}.
A complete full can be found in Lemma \ref{lemm_selfcons_weak}.
We show that with high probability, the following equations hold approximately:
\be\label{approximate m1m2}
\begin{split}
& m_1^{-1}(z) = -1+ \frac{1}{n} \sum_{i=1}^p \frac{\lambda_i^2 }{ z +\lambda_i^2 \frac{n_1}{n_1+ n_2} m_1(z) +  \frac{n_2}{n_1+n_2}m_2(z)+\oo(1)}+ \oo(1),\\
& m_2^{-1}(z) = -1+\frac{1}{n} \sum_{i=1}^p \frac{1}{ z +\lambda_i^2 \frac{n_1}{n_1 + n_2} m_1(z) +  \frac{n_2}{n_1+n_2}m_2(z)+\oo(1)}  + \oo(1).
\end{split}
\ee
These are the self-consistent equations that we stated in equation \eqref{selfomega_a}.
More precisely, we have that $m_1(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_1} a_1(z) $ and $m_2(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_2} a_2(z)$.

The core idea is to study $G(z)$ using the Schur complement formula.
First, we consider the diagonal entries of $G(z)$ for each block in $\cal I_0$, $\cal I_1$, and $\cal I_2$.
For any $i$ in $\cal I_0$, any $\mu$ in $\cal I_1$, and any $\nu$ in $\cal I_2$, we have that
\begin{align*}
	&G_{i,i}^{-1}(z) = -z - \frac{\lambda_i^2}{n} \sum_{\mu,\nu\in \mathcal I_1} Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}G^{\left( i \right)}_{\mu,\nu}(z) - \frac{1}{n} \sum_{\mu,\nu\in \mathcal I_2} Z^{(2)}_{\mu i}Z^{(2)}_{\nu i} G^{\left( i \right)}_{\mu,\nu}(z) -\frac{2\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu,\nu}(z) \\
	&G_{\mu,\mu}^{-1}(z) =  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}\lambda_i \lambda_j Z^{(1)}_{\mu i}Z^{(1)}_{\mu j} G^{\left(\mu\right)}_{i,j}(z) \\
	&G_{\nu,\nu}^{-1}(z) =  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}  Z^{(2)}_{\nu i}Z^{(2)}_{\nu j}  G^{\left(\nu\right)}_{i,j}(z).
\end{align*}
For the first equation, we expand the Schur complement formula $G_{ii}^{-1}(z) = -z - H_i G^{(i)}(z) H_{i}^\top$, where $H_i$ is the $i$-th row of $H$ with the $(i,i)$-th entry removed.
The second and third equation follow by similar calculations.

%\be\label{approx m12 add}
%(m_1,m_2) =\left(-\frac{\rho_1+\rho_2}{\rho_1}a_{1}(z),-\frac{\rho_1+\rho_2}{\rho_2}a_{2}(z)\right)+\oo(1) \quad \text{with overwhelming probability. }
%\ee

Next, we apply standard concentration bounds to simplify the above results.
For $G^{-1}_{i, i}(z)$, recall that the resolvent minor $G^{(i)}$ is defined such that it is independent of the $i$-th row and column of $Z^{(1)}$ and $Z^{(2)}$.
Hence by standard concentration inequalities, we have that the cross terms are approximately zero.
%is approximately equal to the expectation over $\{Z^{(1)}_{\mu i}: \mu \in \cal I_1 \}\cup \{Z^{(2)}_{\nu i}: \nu \in \cal I_2\}$.
As shown in Lemma \ref{lemm_selfcons_weak}, we have that with high probability the following holds
\begin{align*}
	G_{i,i}^{-1}(z) &= -z - \frac{\lambda_i^2}{n} \sum_{\mu \in \mathcal I_1}  G^{\left( i \right)}_{\mu\mu} - \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} +\oo(1) \\
	&= - z - \frac{\lambda_i^2 \cdot n_1}{n_1 + n_2} m_1^{(i)}(z)-  \frac{n_2}{n_1 + n_2} m_2^{(i)}(z)+\oo(1)
\end{align*}
by our definition of the partial traces $m_1^{(i)}(z)$ and $m_2^{(i)}(z)$ with respect to the resolvent minor $G^{(i)}(z)$.
Since we removed only one column and one row from $H(z)$, $m_1^{(i)}(z)$ and $m_2^{(i)}(z)$ should be approximately equal to $m_1(z)$ and $m_2(z)$.
Hence we obtain that
\begin{align}\label{1self_Gii}
	G_{i,i}(z)  = -\left(z + \frac{\lambda_i^2 \cdot n_1}{n_1 + n_2} m_1(z) +  \frac{n_2}{n_1 + n_2} m_2(z) + \oo(1)\right)^{-1}.
\end{align}
For the other two blocks $\cal I_1$ and $\cal I_2$, using similar ideas we have that the following holds with high probability
\begin{align*}
	G_{\mu,\mu}(z) &= -\left(1+\frac{p}{n_1 + n_2} m_0(z) + \oo(1)\right)^{-1} \\
	G_{\nu,\nu}(z) &= -\left(1+\frac{p}{n_1 + n_2} m(z) + \oo(1)\right)^{-1}.
\end{align*}
By averaging the above results over $\mu \in \cal I_1$ and $\nu \in \cal I_2$, we obtain that with high probability
\begin{align*}
	m_1(z) &= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu,\mu}(z) = -\left(1+\frac{p}{n_1 + n_2} m_0(z) + \oo(1)\right)^{-1} \\
	m_2(z) &= \frac{1}{n_2}\sum_{\nu \in \cal I_2}G_{\nu,\nu}(z) = -\left(1+\frac{p}{n_1 + n_2} m(z) + \oo(1)\right)^{-1}.
\end{align*}
Furthermore, we obtain that for $\mu \in \cal I_1$ and $\nu\in \cal I_2$, with high probability
$G_{\mu,\mu}(z) = m_1(z) +\oo(1)$ and $G_{\nu, \nu}(z) = m_2+\oo(1)$.
In other words, both block matrices within $\cal I_1$ and $\cal I_2$ are approximately a scaling of the identity matrix.
The above results for $m_1(z)$ and $m_2(z)$ imply that
\begin{align*}
	m_1^{-1}(z) = -1- \frac{1}{n_1 + n_2} \sum_{i=1}^p \lambda_i^2 G_{i,i}(z)+ \oo(1) \\
	m_2^{-1}(z) = -1- \frac{1}{n_1 + n_2} \sum_{i=1}^p G_{i,i}(z)  + \oo(1).
\end{align*}
where we used the definition of $m(z)$ and $m_0(z)$.
By applying equation \eqref{1self_Gii} for $G_{i, i}(z)$, we obtain the self-consistent equations \ref{approximate m1m2}.
In Lemma \ref{lem_stabw}, we show that the self-consistent equations are stable, that is, a small perturbation of the equations leads to a small perturbation of the solution.

\paragraph{Matrix limit.}
Finally, we derive the matrix limit $\Gi(z)$.
Applying our solution for $m_1(z)$ and $m_2(z)$ into equation \eqref{1self_Gii}, we get that for $i$ in $\cal I_0$,
$G_{i, i}(z) = (-z +\lambda_i^2 a_{1}(z) + a_2(z)+\oo(1))^{-1}$.
Similarly, for $\mu$ in $\cal I_1$, we have that $G_{\mu, \mu}(z) = -\frac{n_1 + n_2}{n_1}a_{1}(z)+\oo(1)$, and for $\nu$ in $\cal I_2$, we have that $G_{\nu, \nu}(z) = -\frac{n_1 + n_2}{n_2}a_{2}(z)+\oo(1)$.
Hence we have derived the diagonal entries of $\Gi(z)$.
In Lemma \ref{Z_lemma}, we show that the off-diagonal entries are close to zero.
For example, for $i\ne j\in \cal I_1$, by Schur complement, we have that
$$G_{i,j}(z) = -G_{i,i}(z)\cdot {n^{-1/2}}\Big({\lambda_i}\sum_{\mu \in \cal I_1} Z^{(1)}_{\mu i} G^{(i)}_{\mu, j}(z) + \sum_{\mu \in \cal I_2} Z^{(2)}_{\mu, i} G^{(i)}_{\mu, j}(z) \Big).$$
Using standard concentration inequalities, we show that $\sum_{\mu \in \cal I_1} Z^{(1)}_{\mu i} G^{(i)}_{\mu, j}(z)$ and $\sum_{\mu \in \cal I_2} Z^{(2)}_{\mu i} G^{(i)}_{\mu, j}(z)$ are both close to zero.
The other off-diagonal entries are bounded similarly.
%The bound on the off-diagonal entries will be proved rigorously .


\paragraph{Notations.}
We introduce several notations for the proof of Theorem \ref{thm_main_RMT}.
We say that an event $\Xi$ holds with overwhelming probability if for any constant $D>0$, $\mathbb P(\Xi)\ge 1- n^{-D}$ for large enough $n$.
Moreover, we say $\Xi$ holds with overwhelming probability in an event $\Omega$ if for any constant $D>0$, $\mathbb P(\Omega\setminus \Xi)\le n^{-D}$ for large enough $n$.
The following notion of stochastic domination, which was first introduced in \cite{Average_fluc}, is commonly used in the study of random matrices.

\begin{definition}[Stochastic domination]\label{stoch_domination}
Let $\xi\equiv \xi^{(n)}$ and $\zeta\equiv \zeta^{(n)}$ be two $n$-dependent random variables.
%\HZ{what is $n$-dependent rv?}[{\cor Most variables in our paper are $n$-dependent implicitly, such as resolvents. Furthermore, the random matrix entries can also depend on $n$ if we want. One important example is Bernoulli random variable with sparsity $p=n^{-\theta}$. Furthermore, this also includes the random variables that do not depend on $n$ as a special case.}]
We say that $\xi$ is stochastically dominated by $\zeta$, denoted by $\xi\prec \zeta$ or $\xi=\OO_\prec(\zeta)$, if for any small constant $\e > 0$ and any large constant $D > 0$, there exists a function $n_0(\e, D)$ such that for any $n > n_0(\e, D)$,
\[ \bbP\left(|\xi| >n^\e |\zeta|\right)\le n^{-D}. \]
In case $\xi(u)$ and $\zeta(u)$ is a function of $u$ supported in $\cal U$, then we say $\xi(u)$ is stochastically dominated by $\zeta(u)$ uniformly in $\cal U$ if % for any constants $\e, D>0$,
\[ \sup_{u\in \cal U}\bbP\left(|\xi(u)|>n^\e |\zeta(u)|\right)\le n^{-D}. \]
\end{definition}

%For example, we say that $\xi$ is stochastically dominated by $\zeta$ with overwhelming probability if $\zeta$ is greater than $\xi$ up to a small power of $n^{\e}$.
We make several remarks.
First, since we allow an $n^\e$ factor in stochastic domination, we can ignore log factors without loss of generality since $(\log n)^C\prec 1$ for any constant $C>0$.
Second, given a random variable $\xi$ whose moments exist up to any order, we have that $|\xi|\prec 1$.
This is because by Markov's inequality, let $k$ be greater than $D/ \e$, then we have that
$$ \P(|\xi|\ge n^{\e})\le n^{-k\e}{\E |\xi|^k}\le n^{-D}.$$
This implies that a Gaussian random variable $\xi$ with unit variance satisfies that $|\xi|\prec 1$.

The following fact collects several basic properties that are often used in the proof.
\begin{fact}[Lemma 3.2 in \cite{isotropic}]\label{lem_stodomin}
Let $\xi$ and $\zeta$ be two families of nonnegative random variables depending on some parameters $u\in \cal U$ or $v\in \cal V$.
\begin{itemize}
\item[(i)] Suppose that $\xi (u,v)\prec \zeta(u,v)$ uniformly in $u\in \cal U$ and $v\in \cal V$. If $|\cal V|\le n^C$ for some constant $C>0$, then $\sum_{v\in \cal V} \xi(u,v) \prec \sum_{v\in \cal V} \zeta(u,v)$ uniformly in $u$.

\item[(ii)] If $\xi_1 (u)\prec \zeta_1(u)$ and $\xi_2 (u)\prec  \zeta_2(u)$ uniformly in $u\in \cal U$, then $\xi_1(u)\xi_2(u) \prec \zeta_1(u) \zeta_2(u)$ uniformly in $u\in \cal U$.

\item[(iii)] Suppose that $\Psi(u)\ge n^{-C}$ is a family of deterministic parameters, and $\xi(u)$ satisfies $\mathbb E\xi(u)^2 \le n^C$. If $\xi(u)\prec \Psi(u)$ uniformly in $u$, then we also have $\mathbb E\xi(u) \prec \Psi(u)$ uniformly in $u$.
\end{itemize}
\end{fact}

%In this section, we shall prove a more fundamental random matrix result in a little more general setting than the one in Section \todo{add reference}; see Theorem \ref{LEM_SMALL} below. %\ref{sec_prelim}.
%Then Theorem \ref{thm_main_RMT} will follow from this result as immediate corollaries. We first state the exact assumptions needed for our purpose.

Next, we introduce the bounded support assumption for a random matrix.
We say that a random matrix $Z \in\real^{n \times p}$ satisfies the {\it{bounded support condition}} with $q$ or $Z$ has support $q$ if
\begin{equation}
	\max_{1\le i \le n, 1 \le j \le p}\vert Z_{i, j} \vert \prec q. \label{eq_support}
\end{equation}
As shown in the example above, if the entries of $Z$ have finite moments up to any order, then $Z$ has bounded support $1$.
More generally, if the entries of $Z$ have finite $\varphi$-th moment, then using Markov's inequality and a simple union bound we get that %\HZ{I don't fully get the equation below}
\begin{align}
	\P\left(\max_{1\le i\le n, 1\le j \le p}|Z_{i, j}|\ge (\log n) n^{\frac{2}{\varphi}}\right) &\le \sum_{i=1}^n \sum_{j=1}^p \P\left(|Z_{i, j}|\ge (\log n) n^{\frac{2}{\varphi}}\right)  \nonumber\\
	&\le \sum_{i=1}^n \sum_{j=1}^p \frac{C}{\left[(\log n) n^{\frac{2}{\varphi}}\right]^\varphi} = \OO((\log n)^{-\varphi}).\label{Ptrunc}
	\end{align}
In other words, $Z$ has bounded support $q=n^{\frac{2}{\varphi}}$ with high probability.
%This leads to the exponent $\frac{\varphi-4}{2\varphi}$ in Fact \ref{lem_minv}. %This fact will be used in the proof of Corollary \ref{main_cor} below.
%The above discussion shows that the bounded support condition provides great flexibility in dealing with finite moment conditions.
%\begin{assumption}%\label{assm_big1}
%As in Section \todo{add reference}, we consider random matrices $X^{(1)}=Z^{(1)}\Sigma_1^{1/2}$ and $X^{(2)}=Z^{(2)}\Sigma_2^{1/2}$, where $Z^{(1)}$ and $Z^{(2)}$ are independent $n_1\times p$ and $n_2\times p$ random matrices with i.i.d. entries of zero mean and unit variance. Recall that $\rho_1= n_1/p$ and $\rho_2=n_2/p$. We assume that for a small constant $0<\tau<1$,
%\HZ{We have defined $\rho_1,\rho_2$ already in sec 2. Consider using ``Recall that $\rho_1 =, \rho_2 =$.''}
%\be\label{assm2}
%0\le \rho_1 \le \tau^{-1}, \quad 1+\tau \le \rho_{2} \le \tau^{-1}.
%\ee
%Recall that the singular value decomposition of $M = \Sig_1^{1/2} \Sig_2^{-1/2}$ is $U \Lambda V^{\top}$. We assume that
%\begin{equation}\label{assm32}
%\tau \le \lambda_p \le \lambda_1 \le \tau^{-1} .%, \quad \max\left\{\pi_A^{(n)}([0,\tau]), \pi_B^{(n)}([0,\tau])\right\} \le 1 - \tau .
%\end{equation}
%Finally, recalling the notations in equation \eqref{labelZ}, we assume that $Z^{(1)}$ and $Z^{(2)}$ satisfy
%\begin{equation}
%n^{-1/2}\max_{i\in \cal I_0,\mu \in \cal I_1}\vert Z^{(1)}_{\mu i}\vert \prec q,\quad n^{-1/2}\max_{i\in \cal I_0,\nu \in \cal I_2}\vert Z^{(2)}_{\nu i}\vert \prec q, \label{eq_support}
%\end{equation}
%for some deterministic parameter $q$ satisfying $ n^{-{1}/{2}} \leq q \leq n^{- \tau} $ for a small constant $\tau>0$.
%Here we recall the notations in \eqref{labelZ}.
 %and (\ref{assm2}). We assume that $T$ is an $M\times M$ deterministic diagonal matrix satisfying (\ref{simple_assumption}) and (\ref{assm3}).
%\end{assumption}
%\HZ{This is restating what's stated above again. Remove redundant statements (unless there's something critical missing in Sec 2).} [\cor FY: The setting in sec 1 is too vague for the purpose of this section in the sense that it is hard to find the exact properties we need. In this section, I want to state rigorous statements with a reference to an exact assumption. Moreover, the above assumption contains more.\nc]


%\begin{remark}
%The lower bound $\rho_2\ge 1+\tau$ in equation \eqref{assm2} is to ensure that the sample covariance matrix $(X^{(2)})^\top X^{(2)}$ is non-singular with overwhelming probability. Moreover, we have allowed that $\rho_1=0$, because we want to state a general result that also covers the special case with no source task.
%\end{remark}


%\begin{definition}\label{defn_support}
%We say a (rescaled) random matrix $\cal M$ satisfies the {\it{bounded support condition}} with $q$, if
%\begin{equation}
%\max_{i,j}\vert \cal M_{ij}\vert \prec q. \label{eq_support}
%\end{equation}
%%Here $q\equiv q(n)$ is a deterministic parameter and usually satisfies $ n^{-{1}/{2}} \leq q \leq n^{- \phi} $ for some (small) constant $\phi>0$.
%Whenever (\ref{eq_support}) holds, we say that $M$ has support $q$.
%%Moreover, if the entries of $X$ satisfy (\ref{size_condition}), then $X$ trivially satisfies the bounded support condition with $q=N^{-\phi}$.
%\end{definition}
%\begin{example}
%\todo{Talk about the Gaussian case and Bernoulli case.}

%\end{example}



%Recall that $n:=n_1+n_2$, which is the total number of data in the two tasks.
%Before giving the main proof, we first introduce some notations and tools.
%Following the notations in \cite{EKYY,EKYY1}, we will use the following definition to characterize events of overwhelming probability.
%
%\begin{definition}[overwhelming probability event] \label{high_prob}
%Define
%\begin{equation}\label{def_phi}
%\varphi:=(\log N)^{\log \log N}.
%\end{equation}
%We say that an $N$-dependent event $\Omega$ holds with {\it{$\xi$-overwhelming probability}} if there exist constant $c,C>0$ independent of $N$, such that
%\begin{equation}
%\mathbb{P}(\Omega) \geq 1-N^{C} \exp(- c\varphi^{\xi}),  \label{D25}
%\end{equation}
%for all sufficiently large $N$. For simplicity, for the case $\xi=1$, we just say {\it{overwhelming probability}}. Note that if (\ref{D25}) holds, then $\mathbb P(\Omega) \ge 1 - \exp(-c'\varphi^{\xi})$ for any constant $0\le c' <c$.
%\end{definition}
%\begin{remark}
%For any $c, C>0$, there exists a $0< c^{\prime}<c$, such that $N^{C} \exp(-c \varphi^{\xi}) \leq \exp(-c^{\prime} \varphi^{\xi})$. Hence, if
%\begin{equation}
%\mathbb{P}(\Omega) \geq 1- \exp(-c \varphi^{\xi})
%\end{equation}
%$\Omega$ is also a $\xi$-overwhelming probability event.
%\end{remark}

