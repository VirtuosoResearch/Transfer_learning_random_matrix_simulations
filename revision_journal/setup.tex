\section{Preliminaries}\label{sec_HPS}
In this section, we define our model for the setting of transfer learning, and introduce several transfer learning estimators that will be considered in this paper.


\subsection{Data model}

We describe the data model and the underlying assumptions.
Suppose we have two regression datasets.
We denote their sample sizes by $n_1$ and $n_2$, respectively.
Consider either $i = 1$ or $i = 2$.
Let $X^{(i)} \in \real^{n_i \times p}$ denote dataset $i$'s feature covariates.
We assume that the label vector $Y^{(i)} \in \real^{n_i}$ for the feature covariates $X^{(i)}$ follows a linear model plus random noise $\varepsilon^{(i)}$:
$Y^{(i)}= X^{(i)}\beta^{(i)} + \varepsilon^{(i)}.$
Both $\beta^{(1)}$ and $\beta^{(2)}$ are two arbitrary deterministic or random vectors that are independent of the feature covariates and the noise vector.
Throughout the paper, we make the following assumptions on  $X^{(i)}$ and  $\varepsilon^{(i)}$, which are standard in recent  random matrix theory literature (see e.g. \citet{tulino2004random,bai2010spectral}). %We remark that all these assumptions are natural in high-dimensional statistics.

First, we assume the row vectors of $X^{(i)}$ are i.i.d. centered random vectors with  population covariance matrix $\Sigma^{(i)} \in \real^{p \times p}$. More precisely, let
\be\label{XofZ}
    X^{(i)} = Z^{(i)} (\Sigma^{(i)})^{1/2} \in \real^{n_i\times p},
\ee
where $\Sigma^{(i)}$ is a deterministic positive definite symmetric matrices, $Z^{(i)}=(z^{(i)}_{j,k})$ is a $n_i\times p$ random matrix with real-valued independent entries having zero mean and unit variance.
%\begin{equation}\label{assm1}
%\mathbb{E} z^{(1)}_{ij}=\mathbb{E} z^{(2)}_{ij} =0, \ \quad \ \mathbb{E} \vert z^{(1)}_{ij} \vert^2=\mathbb{E} \vert z^{(2)}_{ij} \vert^2 =1,
%\end{equation}
Let $\tau>0$ be a small constant.
We assume that for some constant $\varphi>4$, the $\varphi$-th moment of each entry $z^{(i)}_{j, k}$ is bounded from above by $1/\tau$:
\be \label{conditionA2}
\ex{\vert z^{(i)}_{j, k} \vert^\varphi}  \le \frac 1{\tau}. %,\quad \mathbb{E} \vert  z^{(2)}_{ij} \vert^\varphi   \le \tau^{-1}
\ee
 
Second, we assume that $\varepsilon^{(i)} \in \real^{n_i}$ is a random noise vector with i.i.d entries having mean zero, variance $\sigma^2$, and bounded moment up to any order: for any fixed $k\in \N$, there exists a constant $C_k>0$ such that
\be\label{eq_highmoments}
\ex{| \ve^{(i)}_{j} |^k} \le C_k.
\ee

%Moreover, we assume that
%\be\label{eq_SNR}
%\tau\le \frac{\|\beta^{(1)}\|^2}{\|\beta^{(2)}\|^2} \le \tau^{-1},\quad  \frac{\|\beta^{(1)}\|^2}{\sigma^2} \ge \tau,
%\ee
%for a small constant $\tau >0$. The first condition means that the norms of the two model parameters $\beta^{(1)}$ and $\beta^{(2)}$ are comparable, while the second condition means that the singal-to-noise ratio cannot be too small.

Finally, we assume a high-dimensional setting when the sample sizes are proportional to the dimension up to a constant factor.
Denote by $\rho_i = n_i / p$, we require that
\be\label{assm2}
1+\tau \le \rho_i \le p^{\tau^{-1}}.
\ee
The first inequality ($1 + \tau \le \rho_i$) ensures that the sample covariance matrices $(X^{(1)})^\top X^{(1)}$ and $(X^{(2)})^\top X^{(2)}$ are non-singular with high probability.
%Thus, the ordinary least squares estimator is well-defined for the linear regression problem on each (single) task.
%This setting encompasses many 
The second inequality ($\rho_i \le p^{\tau^{-1}}$) is without loss of generality.
Otherwise, standard concentration results such as the central limit theorem already imply accurate estimates of the linear model under the setting defined by equation \eqref{assm2}.

To summarize, the underlying assumptions in our data model are as follows.
\begin{assumption}[Data model]\label{assm_big1}
Let $\tau$ be a small constant.
Let $\Sig^{(1)}$ and $\Sig^{(2)}$ are deterministic positive definite symmetric matrices whose eigenvalues are bounded between $\tau$ and $1 / \tau$.
\begin{enumerate}
\item  $X^{(1)}$ and $X^{(2)}$ take the form of equation \eqref{XofZ}, where $Z^{(1)}$ and $Z^{(2)}$ are both random matrices with i.i.d. entries having zero mean, unit variance, and bounded moments as in equation \eqref{conditionA2}.

\item $\varepsilon^{(1)} \in \real^{n}$ and $\varepsilon^{(2)} \in \real^{n}$ are random vectors independent from $X^{(1)}$ and $X^{(2)}$, and with i.i.d entries of mean zero, variance $\sigma^2$, and bounded moments as in equation \eqref{eq_highmoments}.

\item $\rho_{1}$ and $\rho_{2}$ satisfy equation \eqref{assm2}. Furthermore, $\tau \le \frac{\rho_1}{\rho_2}\le \tau^{-1}$.
\end{enumerate}
\end{assumption}


%\item $\beta^{(1)}$ and $\beta^{(2)} $ are independent of $X^{(1)}$, $X^{(2)}$, $\varepsilon^{(1)}$ and $\varepsilon^{(2)} $. %, and satisfy \eqref{eq_SNR}.

\todo{
We assume that $\Sig^{(i)}$ has eigendecomposition
\be\label{eigen}
\Sig^{(i)}= O_i\Lambda_i O_i^\top, \quad  \Lambda_1=\text{diag}(\si_1^{(i)}, \ldots, \si^{(i)}_n),  \quad i=1,2,
\ee
where $O_i$ is the eigenmatrix and the eigenvalues satisfy that
\begin{equation}\label{assm3}
 \tau \le  \si^{(i)}_p \le\cdots\le \si^{(i)}_2 \le \si^{(i)}_1 \le \tau^{-1} \quad i=1,2.
\ee}
%for a small constant $\tau>0$.


\subsection{Estimators}

We describe our estimators that combine the information from both datasets.
We learn a two-layer linear neural network with parameters $A \in \real^{p\times r}$, $B_1 \in \real^{r}$, and $B_2 \in \real^r$ by minimizing the following optimization objective:
\begin{align}\label{eq_hps}
    f(A, B_1, B_2) = \bignorm{X^{(1)} A B_1 - Y^{(1)}}_2^2 + \bignorm{X^{(2)} A B_2 - Y^{(2)}}_2^2.
\end{align}
The above two-layer neural network (also called hard parameter sharing \cite{C97}) captures the intuition that both datasets share the feature space (i.e. the subspace spanned by the columns of $A$) while each having a separate prediction head.
Suppose that $(\hat{A}, \hat{B_1}, \hat{B_2})$ is a minimizer of $f(\cdot)$.
We define the hard parameter sharing estimator for task $i$ as
\[ \hat{\beta}_i^{\MTL} \define \hat{B} \hat{A}_i. \]
Our study of such estimators is motivated by the fact that hard paratemer sharing is widely used in empirical studies of multi-task learning \cite{R17}.
\todo{Later on in Section \ref{sec_diff}, we will further compare the HPS estimator to other natural transfer learning estimators.} %$L(\hat{\beta}_2^{\STL})$ and $L(\hat{\beta}^{\AV}(\lambda))$ for some properly chosen $\lambda\in [0,1]$.}



We measure the performance of $\hat{\beta}_i^{\MTL}$ through the out-of-sample predication risk (or test loss).
Consider a datapoint $(x,y)$ generated from the same model as dataset $i$: $y= x^\top \beta^{(i)} + \ve$. %, where $x \in \R^p$ and $\ve\in \R$ are independent of $X^{(1)}$, $X^{(2)}$, $\varepsilon^{(1)}$ and $\varepsilon^{(2)}$, and only $x$ is observable.
%We want to use $x^\top \hat{\beta}_2^{\MTL} (\hat a) $ to predict $y$, and we measure
The expected predication risk (under mean squared error) is given by
$$\exarg{x, \varepsilon}{\left\|y-x^\top \hat{\beta}_i^{\MTL} \right\|^2}= \left\|\big(\Sigma^{(i)}\big)^{1/2} \left(\hat{\beta}_i^{\MTL} - \beta^{(i)}\right)\right\|^2 + \sigma^2.$$
%Since $\sigma^2$ is a constant that does not depend on the model, we ignore it and define the predication loss as
The excess risk is defined without the constant term $\sigma^2$:
\be\label{HPS_loss}
    L(\hat{\beta}_i^{\MTL}):= \left\| \big(\Sigma^{(i)}\big)^{1/2} \left(\hat{\beta}_i^{\MTL} - \beta^{(i)}\right)\right\|^2 .
\ee

We focus on the setting where the hidden dimension $r = 1$.
Otherwise, the excess risk of the HPS estimator and that of the ordinary least squares (OLS) estimator are equal to each other \cite{WZR20}.
\begin{proposition}\label{prop_large_r}
    Suppose that Assumption \ref{assm_big1} holds.
    Let $\hat{\beta}_i^{\STL} = \big( {X^{(i)}}^{\top} X^{(i)} \big)^{-1} {X^{(i)}}^{\top} Y^{(i)}$ denote the OLS estimator, for either $i = 1$ or $i = 2$.
    Then, for any $r \ge 2$, we have that $L(\hat{\beta}_i^{\MTL}) = L(\hat{\beta}_i^{\STL})$.
\end{proposition}

\begin{proof}
    Since both $n_1$ and $n_2$ are at least $(1 + \tau) p$, the inverse of both ${X^{(1)}}^{\top} X^{(1)}$ and ${X^{(2)}}^{\top} X^{(2)}$ exists.
    Thus, both $\hat{\beta}_1^{\STL}$ and $\hat{\beta}_2^{\STL}$ are well-defined.
    The global minimum of each summand of equation \eqref{eq_hps} is achieved if and only if $\hat{\beta}_1^{\STL}$ and $\hat{\beta}_2^{\STL}$ are both in the column subspace of $A$.
    In this case, $\hat{\beta}_1^{\MTL} = A B_1$ must be equal to $\hat{\beta}_1^{\STL}$ (same for $\hat{\beta}_2^{\MTL}$).
\end{proof}
%Our goal is to characterize the 
%$The minimizers of $f(\cdot)$
%\FY{Introduce some popular transfer learning estimators: total loss estimator, and model averaging estimator.}

When $r = 1$, both $B_1$ and $B_2$ reduce to scalars.
In particular, the case when $B_1$ and $B_2$ are both equal to $1$ reduces to a pooling estimator, which treats both datasets as if they are drawn independently from the same distribution.
Such pooling estimators have been shown to be effective on e-commence and healthcare datasets \cite{bastani2020predicting}.
Additionally, they have been used as a preprocessing step of minimax optimal estimators, under certain conditions of the data \cite{li2020transfer} (see also Section \ref{sec_related} for further discussion of both works).
%Another commonly used transfer learning estimator is defined as the minimizer of the total loss function \cite{, li2020transfer}:
%$$\hat \beta^{\rm{TL}} := \argmin\left[ \norm{X^{(1)} \beta - Y^{(1)}}^2 + \norm{X^{(2)} \beta - Y^{(2)}}^2\right].$$

%Given a solution from minimizing $f(A, B)$, denoted by   (which we will specify below), let  denote the HPS estimator for task $i$.
%The critical questions are:
%(i) How well does the estimator work? In particular, how does the performance of the estimator scale with sample size?
%(ii) For datasets with different sample sizes and covariate shifts, how do they affect the estimator?




%$L(\hat{\beta}_2^{\STL})$ of the single-task OLS estimator  as a baseline.
\subsection{Bias and variance}

Proposition \ref{prop_large_r} suggests that the two-layer neural network provides information transfer only when $r = 1$.
Next, we provide the bias-variance decomposition of $L(\hat{\beta}^{\MTL}_2)$ under this setting.
As a remark, for general $r$, problem \eqref{eq_hps} is non-convex since $f(\cdot)$ is a degree-four polynomial.
Nevertheless, the setting when $r = 1$ remain tractable since we can write a closed-form solution of $A$ given any (scalar) $B_1$ and $B_2$.


\begin{proposition}\label{lem_HPS_loss}
     Under Assumption \ref{assm_big1}, for any small constant $\e>0$ and large constant $C>0$, there exists a high probability event $\Xi$, on which the following estimates hold uniformly in $a\in \R$ and $\lambda\in [0,1]$:
    \begin{align}
    L(\hat{\beta}_2^{\MTL}(a)) &=\left[ 1+ \OO(p^{-1/2+c})\right]\cdot \left[L_{\bias} (a) + L_{\var}(a)\right] \nonumber\\
    &+ \OO \left[ p^{-C}\left( \|\beta^{(1)} \|^2  +  \|\beta^{(2)} \|^2\right)   \right], \label{L_HPS_simple}
    \end{align}
    \begin{align}
    L(\hat{\beta}^{\AV}(\lambda) ) &= \left[ 1+ \OO(p^{-1/2+c})\right] \cdot \lambda^2 \left\| (\Sigma^{(2)})^{1/2}(\beta^{(1)}-\beta^{(2)})\right\|^2 \nonumber \\
    &+  \left[ 1+ \OO(p^{-1/2+c})\right] \cdot \sigma^2   \lambda^2 \bigtr{\Sigma^{(2)} [(X^{(1)})^\top X^{(1)}]^{-1}  }\nonumber\\
    & + \left[ 1+ \OO(p^{-1/2+c})\right] \cdot \sigma^2  (1-\lambda)^2 \bigtr{\Sigma^{(2)} [(X^{(2)})^\top X^{(2)}]^{-1}  }  . \label{L_AVE_simple}
    \end{align}
    and
    \begin{align}
    L(\hat{\beta}_2^{\STL} ) &= \left[ 1+ \OO(p^{-1/2+c})\right] \cdot \sigma^2  \bigtr{\Sigma^{(2)} [(X^{(2)})^\top X^{(2)}]^{-1}  }  . \label{L_STL_simple}
    \end{align}
    %Moreover, there exists a constant $C_0>0$ such that with high probability,
    %\be\label{loss_large}
    %g(a) > g(0) \quad \text{for all $a$ such that }|a|\ge C_0.
    %\ee
    %with high probability for any small constant $\e>0$.
\end{proposition}

\begin{proof}[Proof sketch]
    Plugging \eqref{HPS_est} into \eqref{HPS_loss}, we get
    \begin{align}
    L(\hat{\beta}_2^{\MTL}(\hat a))  =  &\left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat a)^{-1} (X^{(1)})^\top X^{(1)} (\hat a\beta^{(1)}-\hat a^2\beta^{(2)}) \right. \nonumber\\
    &\left. +(\Sigma^{(2)})^{1/2}\hat \Sigma(\hat a)^{-1} \left[ (X^{(2)})^\top \epsilon^{(2)} + \hat a  (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2.\label{Lbeta_HPS}
    \end{align}
    Note that if we replace $\hat a$ with a fixed number $a$ in \eqref{Lbeta_HPS}, then taking the expectation over $\ve^{(1)}$ and $\ve^{(2)}$, we can get a clean bias-variance decomposition of the predication loss:
    \be\label{partial-L} \exarg{\ve^{(1)},\ve^{(2)}}{L(\hat{\beta}_2^{\MTL}(a)) } = L_{\bias} (a) + L_{\var}(a),\ee
    where
    \be\label{Lbias} L_{\bias} (a) := \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(a)^{-1} (X^{(1)})^\top X^{(1)} \left(a\beta^{(1)}- a^2\beta^{(2)}\right) \right\|^2 \ee
    is called the bias term, which depends on the model bias between task 1 and task 2, and
    \be\label{Lvar}  L_{\var}(a):=\sigma^2  \bigtr{{\Sigma^{(2)}\hat \Sigma(a)^{-1}  }} \ee
    is called the variance term, which depends on the noise variance. Using concentration of the noise vectors $\ve^{(1)}$ and $\ve^{(2)}$, we can show that $L(\hat{\beta}_2^{\MTL}(a))$ is close to \eqref{partial-L} up to a small error as in the next lemma. In this paper, we say that an event $\Xi$ holds \emph{with high probability} (w.h.p.) if $\mathbb P(\Xi)\to 1$ as $p\to \infty$.
    %predication loss for the HPS and OLS estimators.
\end{proof}


Since \eqref{L_HPS_simple} holds uniformly for all $a\in \R$, we can also apply it to $\hat{\beta}_2^{\MTL}(\hat a)$, where $\hat a$ is a random variable that may depend on $\ve^{(1)}$ and $\ve^{(2)}$. In practice, the parameter $ a$  is up to one's choice and may not be the global minimizer, so we have stated the result for an completely arbitrary $a$. (In fact, the optimization objective $g(a)$ is generally non-convex, so in some cases one can only find a local minimizer.) For a fixed $a\in \R$, the proof of \eqref{L_HPS_simple} is based on the sharp concentration bounds in Lemma \ref{largedeviation} of the supplement \cite{MTL_suppl}. To extend uniformly to all $a\in \R$, we will use a standard $\ve$-net argument, which leads to a small error $ p^{-C}\left( \|\beta^{(1)} \|^2  +  \|\beta^{(2)} \|^2\right) $. Note that this error is negligible unless $\beta^{(1)}$ and $ a\beta^{(2)}$ cancel each other almost exactly, and the noise variance $\sigma$ is very small. The proof of Lemma \ref{lem_HPS_loss} will be given in Appendix  \ref{app_firstpf} of the supplement \cite{MTL_suppl}.

%In this section, we show that the prediction loss of hard parameter sharing admits a clean bias-variance decomposition, when all tasks have the same features.




For the single-task predication loss \eqref{L_STL_simple}, we can calculate its asymptotic limit exactly using the following classical result in multivariate statistics.

\begin{lemma}[Theorem 2.4 of \cite{isotropic} and Theorem 3.14 of \cite{DY}]\label{fact_tr}
	%Let $X  \in \real^{n\times p}$ be a random matrix that satisfies Assumption \ref{assume_rm}.
	%Let $\Sigma\in\real^{p\times p}$ denote the population covariance matrix of $X$.
	Under Assumption \ref{assm_big1}, we have that for any deterministic $p\times p$ matrix $\Sigma$,
		\[ \bigtr{\Sigma \frac1{(X^{(2)})^\top X^{(2)} }  } %= \bigtr{ \frac1{(Z^{(2)})^\top Z^{(2)} }  }
		= \frac{1}{n_2 - p}\bigtr{\Sigma (\Sigma^{(2)})^{-1}} +  \OO\left( \frac{p^{\e}}{\sqrt{n_2 p}}\cdot \frac{p}{n_2}\|A\| \right) \]
		with high probability for any small constant $\e>0$.
\end{lemma}
 %The above result has a long history in random matrix theory.
If the entries of $Z^{(2)}$ are i.i.d. Gaussian, then this result follows from the classical result for %the mean of
the inverse Wishart distribution \cite{anderson1958introduction}. For a general non-Gaussian $Z^{(2)}$, this result can be obtained using the well-known Stieltjes transform method (cf. Lemma 3.11 of \cite{bai2009spectral}). Here we have presented the results from \cite{isotropic,DY}, which give an almost sharp convergence rate. With Lemma \ref{fact_tr}, we obtain that w.h.p.,
\begin{align}
L(\hat{\beta}_2^{\STL} ) &= \left[ 1+ \OO(p^{-1/2+c})\right] \cdot \frac{p \sigma^2 }{n_2-p}   , \label{L_STL_simple01}
\end{align}
and
\begin{align}
L(\hat{\beta}^{\AV}(\lambda) ) &= \left[ 1+ \OO(p^{-1/2+c})\right] \cdot \lambda^2 \left\| (\Sigma^{(2)})^{1/2}(\beta^{(1)}-\beta^{(2)})\right\|^2 \nonumber \\
&+  \left[ 1+ \OO(p^{-1/2+c})\right] \cdot \sigma^2 \left[\frac{ \lambda^2}{n_1-p}\bigtr{\Sigma^{(2)}(\Sigma^{(1)})^{-1}} + \frac{(1-\lambda)^2p  }{n_2-p}\right] . \label{L_AVE_simple01}
 \end{align}

On the other hand, much less is known about the predication loss of the HPS estimator. In particular, its exact dependence on the model parameters, including the model bias, noise variance, sample sizes, and population covariance matrices, is not well-understood so far. %\FY{What is known so far about this topic?}
In Section \ref{sec_HPS_loss}, we give a rigorous analysis of the bias term \eqref{Lbias} and variance term \eqref{Lvar} in two different settings: (i) independent $X^{(1)}$ and $X^{(2)}$ with different sample sizes and same population covariance matrices $\Sigma^{(1)}=\Sigma^{(2)}$; (ii) independent $X^{(1)}$ and $X^{(2)}$ with different sample sizes and different population covariance matrices. In case (i), we provide the exact asymptotics for both the bias and variance terms, together with almost sharp convergence rates. We will use these results to illustrate the effects of the \emph{bias-variance tradeoff} and \emph{varying sample sizes}. In case (ii), we will prove an exact asymptotic variance limit, and employ the result to show the intricate effect of the covariate shift. On the other hand, in case (ii), we will provide an estimate on the bias term, which becomes exact only when $n_1\gg p$. %Here case (ii) is actually more general than (i)
%In particular, we will use our results to analyze the effect of the following factors: \emph{bias-variance tradeoff}, \emph{sample sizes}, and \emph{covariate shift}.
In Section \ref{sec_same}, we will consider a simple extension to a multi-task learning setting with more than two tasks and  the same covariates for all tasks. We will give the asymptotic limit of the predication loss for the HPS estimator, which provides useful insights into the more rich multi-task learning theory. %Due to length constraint, we will explore the multi-task learning setting in greater details in other works.

%{\cor discuss about the bias-variance trade-off}
%However, to the best of our knowledge, the asymptotic limit of $L(\hat{\beta}_2^{\MTL}(a))$ has never been calculated exactly in any high-dimensional setting.

\todo{clarify overparametrize}
As a clar,  following the work of \citet{hastie2019surprises} (see also \citet{bartlett2020benign} and the references therein)
