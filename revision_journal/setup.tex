\section{Preliminaries}\label{sec_HPS}
We begin by formally defining the data model involving large dimensional random matrices together with the underling assumptions.
We define the covariate shift and model shift settings.
Then, we describe the HPS estimators for these settings using two-layer linear neural networks.
Finally, we present a bias-variance decomposition of the HPS estimators.
We explain why this decomposition is crucial for analyzing information transfer and the need for new techniques in the analysis.
Our setup in this section concerns the case of a single data source.
Later on in Section \ref{sec_same}, we will extend the setup to the case of multiple data sources.
%In Section \ref{sec_same}, we will consider a simple extension to a multi-task learning setting with more than two tasks and  the same covariates for all tasks. We will give the asymptotic limit of the predication loss for the HPS estimator, which provides useful insights into the more rich multi-task learning theory. %Due to length constraint, we will explore the multi-task learning setting in greater details in other works.

\subsection{Data model}\label{sec_data}

We describe the data model and the underlying assumptions.
Suppose we have two regression datasets (or tasks).
We denote their sample sizes by $n_1$ and $n_2$, respectively.
Consider either $i = 1$ or $i = 2$.
Let $X^{(i)} \in \real^{n_i \times p}$ denote dataset $i$'s feature covariates.
We assume that the label vector $Y^{(i)} \in \real^{n_i}$ for the feature covariates $X^{(i)}$ follows a linear model plus random noise $\varepsilon^{(i)}$:
$Y^{(i)}= X^{(i)}\beta^{(i)} + \varepsilon^{(i)}.$
Both $\beta^{(1)}$ and $\beta^{(2)}$ are two arbitrary deterministic or random vectors that are independent of the feature covariates and the noise vector.
Throughout the paper, we make the following assumptions on  $X^{(i)}$ and  $\varepsilon^{(i)}$, which are standard in recent  random matrix theory literature (see e.g. \citet{tulino2004random,bai2010spectral}). %We remark that all these assumptions are natural in high-dimensional statistics.

First, we assume the row vectors of $X^{(i)}$ are i.i.d. centered random vectors with  population covariance matrix $\Sigma^{(i)} \in \real^{p \times p}$. More precisely, let
\be\label{XofZ}
    X^{(i)} = Z^{(i)} (\Sigma^{(i)})^{1/2} \in \real^{n_i\times p},
\ee
where $\Sigma^{(i)}$ is a deterministic positive definite symmetric matrices, $Z^{(i)}=(z^{(i)}_{j,k})$ is a $n_i\times p$ random matrix with real-valued independent entries having zero mean and unit variance.
%\begin{equation}\label{assm1}
%\mathbb{E} z^{(1)}_{ij}=\mathbb{E} z^{(2)}_{ij} =0, \ \quad \ \mathbb{E} \vert z^{(1)}_{ij} \vert^2=\mathbb{E} \vert z^{(2)}_{ij} \vert^2 =1,
%\end{equation}
Let $\tau>0$ be a small constant.
We assume that for some constant $\varphi>4$, the $\varphi$-th moment of each entry $z^{(i)}_{j, k}$ is bounded from above by $1/\tau$:
\be \label{conditionA2}
\ex{\vert z^{(i)}_{j, k} \vert^\varphi}  \le \frac 1{\tau}. %,\quad \mathbb{E} \vert  z^{(2)}_{ij} \vert^\varphi   \le \tau^{-1}
\ee
%$Denote by $\Sig^{(i)} = O_i\Lambda_i O_i^\top$ its eigendecomposition, where $\Lambda_1=\text{diag}(\si_1^{(i)}, \ldots, \si^{(i)}_n)$.
%\be\label{eigen}
%\Sig^{(i)}, \quad  ,  \quad i=1,2,
%\ee
Additionally, We assume that the eigenvalues of $\Sigma^{(i)}$ are bounded between $\tau$ and $1 / \tau$.
%\begin{equation}\label{assm3}
%    \tau \le  \si^{(i)}_p \le\cdots\le \si^{(i)}_2 \le \si^{(i)}_1 \le \tau^{-1} \quad i=1,2.
%\end{equation}
 
Second, we assume that $\varepsilon^{(i)} \in \real^{n_i}$ is a random noise vector with i.i.d entries having mean zero, variance $\sigma^2$, and bounded moment up to any order: for any fixed $k\in \N$, there exists a constant $C_k>0$ such that
\be\label{eq_highmoments}
\ex{| \ve^{(i)}_{j} |^k} \le C_k.
\ee

%Moreover, we assume that
%\be\label{eq_SNR}
%\tau\le \frac{\|\beta^{(1)}\|^2}{\|\beta^{(2)}\|^2} \le \tau^{-1},\quad  \frac{\|\beta^{(1)}\|^2}{\sigma^2} \ge \tau,
%\ee
%for a small constant $\tau >0$. The first condition means that the norms of the two model parameters $\beta^{(1)}$ and $\beta^{(2)}$ are comparable, while the second condition means that the singal-to-noise ratio cannot be too small.

Finally, we assume a high-dimensional setting when the sample sizes are proportional to the dimension up to a constant factor.
Denote by $\rho_i = n_i / p$, we require that
\be\label{assm2}
1+\tau \le \rho_i \le p^{\tau^{-1}}.
\ee
The first inequality ($1 + \tau \le \rho_i$) ensures that the sample covariance matrices $(X^{(1)})^\top X^{(1)}$ and $(X^{(2)})^\top X^{(2)}$ are non-singular with high probability.
%Thus, the ordinary least squares estimator is well-defined for the linear regression problem on each (single) task.
%This setting encompasses many 
The second inequality ($\rho_i \le p^{\tau^{-1}}$) is without loss of generality.
Otherwise, standard concentration results such as the central limit theorem already imply accurate estimates of the linear model under the setting defined by equation \eqref{assm2}.

To summarize, the underlying assumptions in our data model are as follows.
\begin{assumption}[Data model]\label{assm_big1}
Let $\tau$ be a small constant.
Let $\Sig^{(1)}$ and $\Sig^{(2)}$ are deterministic positive definite symmetric matrices whose eigenvalues are bounded between $\tau$ and $1 / \tau$.
\begin{enumerate}
\item  $X^{(1)}$ and $X^{(2)}$ take the form of equation \eqref{XofZ}, where $Z^{(1)}$ and $Z^{(2)}$ are both random matrices with i.i.d. entries having zero mean, unit variance, and bounded moments as in equation \eqref{conditionA2}.

\item $\varepsilon^{(1)} \in \real^{n}$ and $\varepsilon^{(2)} \in \real^{n}$ are random vectors independent from $X^{(1)}$ and $X^{(2)}$, and with i.i.d entries of mean zero, variance $\sigma^2$, and bounded moments as in equation \eqref{eq_highmoments}.

\item $\rho_{1}$ and $\rho_{2}$ satisfy equation \eqref{assm2}. Furthermore, $\tau \le \frac{\rho_1}{\rho_2}\le \tau^{-1}$.
\end{enumerate}
\end{assumption}

Having defined the data model for each task, next, we describe typical scenarios in transfer learning under our setup.
We define two settings as follows:
\begin{itemize}
    \item \textbf{Covariate shift:} independent $X^{(1)}$ and $X^{(2)}$ with the same population covariance matrices but different models.
    \item \textbf{Model shift:} independent $X^{(1)}$ and $X^{(2)}$ with the same population covariance matrices (i.e. $\Sigma^{(1)}=\Sigma^{(2)}$) but different models (i.e. $\beta^{(1)} \neq \beta^{(2)}$).
\end{itemize}
One interesting special case of capturing different models is the following random-effect model, which has been studied in several recent works concerning high-dimensional (single-task) ridge regression \cite{dobriban2018high} and distributed regression \cite{dobriban2020wonder}.
 %{How does hard parameter sharing scale with sample sizes and covariate shift $M$?} One can see that the variance limit depends intricately on both tasks' samples sizes and covariate shift. Next, we illustrate how varying them impact the prediction loss.
%\begin{example}%[Sample sizes]
%\label{ex_sample_ratio}
	%We first consider the impact of varying sample sizes.
	%Consider the random-effect model from Section \ref{sec_same}, with both tasks having an isotropic population covariance matrix.
	%We again consider the random effect model in Example \ref{ex_same_cov} with $t=2$.
Suppose each $\beta^{(i)}$ consists of two random components, one that is shared among the two tasks and one that is task-specific. More precisely, let
\begin{align}
    \beta^{(i)}=\beta_0 +\wt \beta^{(i)},\quad i=1,2, \label{eq_re}
\end{align}
where $\beta_0$ denotes the shared component, %whose entries are i.i.d. Gaussian random variables of mean zero and variance $p^{-1}\kappa^2$,
and $\wt \beta^{(i)}$ denotes the $i$-th
%Let $\beta^{(i)}$ be equal to $\beta_0$ plus a
task-specific component whose entries are i.i.d. Gaussian random variables of mean zero and variance $p^{-1} \mu^2$.

%\item $\beta^{(1)}$ and $\beta^{(2)} $ are independent of $X^{(1)}$, $X^{(2)}$, $\varepsilon^{(1)}$ and $\varepsilon^{(2)} $. %, and satisfy \eqref{eq_SNR}.

\subsection{Estimators}

%We describe our estimators that combine the information from both datasets.
We learn a two-layer linear neural network with parameters $A \in \real^{p\times r}$, $B_1 \in \real^{r}$, and $B_2 \in \real^r$ by minimizing the following optimization objective:
\begin{align}\label{eq_hps}
    f(A, B_1, B_2) = \bignorm{X^{(1)} A B_1 - Y^{(1)}}_2^2 + \bignorm{X^{(2)} A B_2 - Y^{(2)}}_2^2.
\end{align}
The above two-layer neural network (also called hard parameter sharing \cite{C97}) captures the intuition that both datasets share the feature space (i.e. the subspace spanned by the columns of $A$) while each having a separate prediction head.
Suppose that $(\hat{A}, \hat{B_1}, \hat{B_2})$ is a minimizer of $f(\cdot)$.
We define the hard parameter sharing estimator for task $i$ as
\[ \hat{\beta}_i^{\MTL} \define \hat{B} \hat{A}_i. \]
Our study of such estimators is motivated by the fact that hard parameter sharing is widely used in empirical studies of multi-task learning \cite{R17}.
Later on in Section \ref{sec_diff}, we will evaluate the performance of HPS estimators against several natural transfer learning estimators.

We measure the performance of $\hat{\beta}_i^{\MTL}$ over samples $(x,y)$ generated from the $i$-th data model: $y= x^\top \beta^{(i)} + \ve$.
The expected prediction risk (under mean squared loss) is given by
$$\exarg{x, \varepsilon}{\left\|y-x^\top \hat{\beta}_i^{\MTL} \right\|^2}= \left\|\big(\Sigma^{(i)}\big)^{1/2} \left(\hat{\beta}_i^{\MTL} - \beta^{(i)}\right)\right\|^2 + \sigma^2.$$
The excess risk is the difference between the above risk and the expected risk of the population risk optimizer (which is equal to $\sigma^2$):
\be\label{HPS_loss}
    L(\hat{\beta}_i^{\MTL}):= \left\| \big(\Sigma^{(i)}\big)^{1/2} \left(\hat{\beta}_i^{\MTL} - \beta^{(i)}\right)\right\|^2 .
\ee

We focus on the setting where the hidden dimension $r = 1$.
Otherwise, the excess risk of the HPS estimator and that of the ordinary least squares (OLS) estimator are equal to each other \cite{WZR20}.
\begin{proposition}\label{prop_large_r}
    Suppose that Assumption \ref{assm_big1} holds.
    Let $\hat{\beta}_i^{\STL} = \big( (X^{(i)})^{\top} X^{(i)} \big)^{-1} (X^{(i)})^{\top} Y^{(i)}$ denote the OLS estimator, for either $i = 1$ or $i = 2$.
    Then, for any $r \ge 2$, we have that $L(\hat{\beta}_i^{\MTL}) = L(\hat{\beta}_i^{\STL})$.
\end{proposition}

\begin{proof}
    Since both $n_1$ and $n_2$ are at least $(1 + \tau) p$, the inverse of both ${X^{(1)}}^{\top} X^{(1)}$ and ${X^{(2)}}^{\top} X^{(2)}$ exists.
    Thus, both $\hat{\beta}_1^{\STL}$ and $\hat{\beta}_2^{\STL}$ are well-defined.
    The global minimum of each summand of equation \eqref{eq_hps} is achieved if and only if $\hat{\beta}_1^{\STL}$ and $\hat{\beta}_2^{\STL}$ are both in the column subspace of $A$.
    In this case, $\hat{\beta}_1^{\MTL} = A B_1$ must be equal to $\hat{\beta}_1^{\STL}$ (same for $\hat{\beta}_2^{\MTL}$).
\end{proof}

When $r = 1$, both $B_1$ and $B_2$ reduce to scalars.
In particular, the case when $B_1$ and $B_2$ are both equal to $1$ reduces to a pooling estimator, which treats both datasets as if they are drawn independently from the same distribution.
Such pooling estimators have been shown to be effective on e-commence and healthcare datasets \cite{bastani2020predicting}.
Additionally, they have been used as a preprocessing step of minimax optimal estimators, under certain conditions of the data \cite{li2020transfer} (see also Section \ref{sec_related} for further discussion of both works).



\subsection{Bias and variance}

%Proposition \ref{prop_large_r} suggests that the two-layer neural network provides information transfer only when $r = 1$.

Next, we provide the bias-variance decomposition of the excess risk under this setting.
Without loss of generality, we focus on task two's excess risk $L(\hat{\beta}^{\MTL}_2)$.
As a remark, for general $r$, problem \eqref{eq_hps} is non-convex since $f(\cdot)$ is a degree-four polynomial.
Nevertheless, the setting when $r = 1$ remains tractable since we can write a closed-form solution of $A$ given any (scalar) $B_1$ and $B_2$.
Since $B_1$ and $B_2$ are both scalars, let $a = B_1 / B_2$.
Let $\hat{\Sigma}(a) \define a^2 (X^{(1)})^{\top} X^{(1)} + (X^{(2)})^{\top} X^{(2)}$ be a weighted combination of the sample covariance matrices.
We present a bias-variance decomposition of $L(\hat{\beta}^{\MTL}_2)$ for the setting of $r = 1$ as follows.
We say that an event $\Xi$ holds \emph{with high probability} (w.h.p.) if $\mathbb P(\Xi)\to 1$ as $p\to \infty$.

\begin{proposition}[Bias-variance decomposition]\label{lem_HPS_loss}
     Under Assumption \ref{assm_big1}, for any small constant $\e>0$ and large constant $C>0$, there exists an event $\Xi$, on which the following estimates hold uniformly in $a\in \R$ and $\lambda\in [0,1]$ w.h.p.:
    \begin{align}
        L(\hat{\beta}_2^{\MTL}) =& \left( 1+ \OO(p^{-1/2+c})\right)\cdot \left(L_{\bias} (a) + L_{\var}(a)\right) %\nonumber\\
        + \OO \left( p^{-C}\big( \|\beta^{(1)} \|^2  +  \|\beta^{(2)} \|^2\big)   \right), \label{L_HPS_simple}
    \end{align}
    where the bias and variance equations are defined as follows:
    \begin{align}
        L_{\bias} (a) &\define \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(a)^{-1} (X^{(1)})^\top X^{(1)} \left(a\beta^{(1)}- a^2\beta^{(2)}\right) \right\|^2,  \label{Lbias} \\
        L_{\var}(a)   &\define \sigma^2  \bigtr{{\Sigma^{(2)}\hat \Sigma(a)^{-1}  }}.  \label{Lvar}
    \end{align}
\end{proposition}



Equation \eqref{L_HPS_simple} shows that the excess risk of the HPS estimator admits a clean bias-variance decomposition plus lower-order terms that decrease to zero with the dimension $p$.
Since equation \eqref{L_HPS_simple} holds uniformly for all $a\in \R$, we can apply the result to $\hat a = \hat{B}_1 / \hat{B}_2$, for any $\hat{B}_1$ and $\hat{B}_2$ that may depend on $\ve^{(1)}$ and $\ve^{(2)}$ (after solving problem \eqref{eq_hps}).
%The proof of Proposition \ref{lem_HPS_loss} involves
We briefly describe why Proposition \ref{lem_HPS_loss} holds.
First, suppose that $a$ is an arbitrary value that is independent of the random noise vectors $\varepsilon^{(1)}$ and $\varepsilon^{(2)}$.
By taking the expectation over both of them, we get the following result:
\be\label{partial-L}
    \exarg{\ve^{(1)},\ve^{(2)}}{L(\hat{\beta}_2^{\MTL}) } = L_{\bias} (\hat a) + L_{\var}(\hat a).
\ee
Secondly, using concentration of the noise vectors $\ve^{(1)}$ and $\ve^{(2)}$, we can show that $L(\hat{\beta}_2^{\MTL}(a))$ is close to \eqref{partial-L} up to a small error as in the next lemma.
For a fixed $a\in \R$, the proof of \eqref{L_HPS_simple} is based on the sharp concentration bounds in Lemma \ref{largedeviation} of Section \ref{??}.
To extend uniformly to all $a\in \R$, we will use a standard $\ve$-net argument, which leads to a small error $ p^{-C}\left( \|\beta^{(1)} \|^2  +  \|\beta^{(2)} \|^2\right) $. Note that this error is negligible unless $\beta^{(1)}$ and $ a\beta^{(2)}$ cancel each other almost exactly, and the noise variance $\sigma$ is very small.
The proof of Proposition \ref{lem_HPS_loss} can be found in Section  \ref{app_firstpf}.

Equation \eqref{partial-L} suggests that the bias and variance of HPS both depend on the scalar $\hat a$.
We provide some intuition about this scalar in the setting of random-effect models.
We show that under natural assumptions, the global minimizer $\hat a = \hat B_1 / \hat B_2$ is approximately equal to $1$.
The proof of Proposition \ref{lem_hat_v} can be found in Section \ref{app_iso_cov_prop}.

\begin{proposition}[Random-effect model]\label{lem_hat_v}
    Suppose Assumption \ref{assm_big1} and the random-effect model under equation \eqref{eq_re} holds. 
    Suppose that for a constant $c_0>0$,
	\be\label{para_rel}
	    \|\beta_0\|^2 \ge p^{c_0}\mu^2 +p^{-1/2+c_0}\sigma^2.
	\ee
    Then we have that for any small constant $c>0$ and large constant $ C>0$,
	%Let $c$ be a sufficiently small fixed constant.
	%In the setting of Corollary \ref{cor_MTL_loss},
%There exists a constant $C>0$ such that
	\be\label{hatw_add1}
	 \hat a =1+ \OO\left(\frac{\mu^2}{\|\beta_0\|^2} + p^{-\frac 1 4 + c} \frac{\mu+\sigma}{\|\beta_0\|} + p^{-C}\right) \quad \text{w.h.p.}
	\ee
\end{proposition}
\todo{change $d$ to $\mu$ because a reviewer complained}
%Suppose every $\beta^{(i)}$ consists of two random components, one that is shared among all tasks and one that is task-specific.
%%Thus, each task contributes a certain amount to the shared component and injects a task-specific bias.
%More precisely, %we have
%$$\beta^{(i)}=\beta_0 +\wt \beta^{(i)},\quad i=1,2,\cdots, t,$$
%where $\beta_0$ denotes the shared component, %whose entries are i.i.d. Gaussian random variables of mean zero and variance $p^{-1}\kappa^2$,
%and $\wt \beta^{(i)}$ denotes the $i$-th
%%Let $\beta^{(i)}$ be equal to $\beta_0$ plus a
%task-specific component whose entries are i.i.d. Gaussian random variables of mean zero and variance $p^{-1} d^2$.
%We emphasize that unlike Theorem \ref{cor_MTL_loss}, for Proposition \ref{lem_hat_v} we do not assume $\Sigma^{(1)}=\Sigma^{(2)}$ and the Gaussian distributions of the entries of $Z^{(1)}$ and $Z^{(2)}$.
%In particular, its exact dependence on the model parameters, including the model bias, noise variance, sample sizes, and population covariance matrices, is not well-understood so far. %\FY{What is known so far about this topic?}

%In particular, we will use our results to analyze the effect of the following factors: \emph{bias-variance tradeoff}, \emph{sample sizes}, and \emph{covariate shift}.


%{\cor discuss about the bias-variance trade-off}
%However, to the best of our knowledge, the asymptotic limit of $L(\hat{\beta}_2^{\MTL}(a))$ has never been calculated exactly in any high-dimensional setting.
%Another commonly used transfer learning estimator is defined as the minimizer of the total loss function \cite{, li2020transfer}:
%$$\hat \beta^{\rm{TL}} := \argmin\left[ \norm{X^{(1)} \beta - Y^{(1)}}^2 + \norm{X^{(2)} \beta - Y^{(2)}}^2\right].$$

%Given a solution from minimizing $f(A, B)$, denoted by   (which we will specify below), let  denote the HPS estimator for task $i$.
%The critical questions are:
%(i) How well does the estimator work? In particular, how does the performance of the estimator scale with sample size?
%(ii) For datasets with different sample sizes and covariate shifts, how do they affect the estimator?





\paragraph{Connection to information transfer.} To be precise, we say that task one provides a \textit{positive transfer} to task two, if the excess risk $L(\hat{\beta}_2^{\MTL})$ is smaller than $L(\hat{\beta}_2^{\STL})$.
Otherwise, we say that task one provides a \textit{negative transfer} to task two.
We now illustrate that the bias-variance decomposition of Proposition \ref{lem_HPS_loss} characterizes whether or not the information transfer is positive.

First, we compare the bias. Since $n_1 \ge (1 + \tau) p$ by Assumption \ref{assm_big1}, the bias of $\hat{\beta}_2^{\STL}$ is zero.
Thus, the bias of $\hat{\beta}_2^{\MTL}$ is always higher than that of $\hat{\beta}_2^{\STL}$.
Second, we compare the variance. By a similar argument as in Proposition \ref{lem_HPS_loss}, the variance of $\hat{\beta}_2^{\STL}$ is equal to $\sigma^2 \bigtr{\Sigma^{(2)} ((X^{(2)})^{\top} X^{(2)})^{-1}}$.
Then, a straightforward algebraic calculation (e.g. using the Woodbury matrix identity) shows that the variance of $\hat{\beta}_2^{\MTL}$ is always higher than that of $\hat{\beta}_2^{\STL}$:
\begin{align*}
    \bigtr{\Sigma^{(2)} \big(\hat{\Sigma}(a)\big)^{-1}} \le \bigtr{\Sigma^{(2)} \big((X^{(2)})^{\top} X^{(2)}\big)^{-1}}.
\end{align*}
Combining both comparisons, we conclude that HPS affects the excess risk in two ways: the bias always increases while the variance always decreases.
Thus, whether or not task one provides a positive transfer to task two depends on which effect is more significant.

%The bias equation \eqref{Lbias} captures the 
%Plugging \eqref{HPS_est} into \eqref{HPS_loss}, we get
%\begin{align}
%    L(\hat{\beta}_2^{\MTL}(\hat a))  =  &\left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat a)^{-1} (X^{(1)})^\top X^{(1)} (\hat a\beta^{(1)}-\hat a^2\beta^{(2)}) \right. \nonumber\\
%    &\left. +(\Sigma^{(2)})^{1/2}\hat \Sigma(\hat a)^{-1} \left[ (X^{(2)})^\top \epsilon^{(2)} + \hat a  (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2.\label{Lbeta_HPS}
%\end{align}



%In practice, the parameter $ a$  is up to one's choice and may not be the global minimizer, so we have stated the result for an completely arbitrary $a$. (In fact, the optimization objective $g(a)$ is generally non-convex, so in some cases one can only find a local minimizer.) 

Building on the above connection, we apply recent developments from random matrix theory to analyze information transfer.
As a warmup, we describe the following classical result in multivariate statistics that characterizes the excess risk (or equivalently, the variance) of the OLS estimator.

\begin{lemma}[Variance estimates of OLS estimators, cf. Theorem 2.4 of \citet{isotropic} and Theorem 3.14 of \citet{DY}]\label{fact_tr}
	%Let $X  \in \real^{n\times p}$ be a random matrix that satisfies Assumption \ref{assume_rm}.
	%Let $\Sigma\in\real^{p\times p}$ denote the population covariance matrix of $X$.
	Under Assumption \ref{assm_big1}, we have that for any deterministic  matrix $\Sigma\in\real^{p\times p}$,
		\[ \bigtr{\Sigma \Big((X^{(2)})^\top X^{(2)} \Big)^{-1}  } %= \bigtr{ \frac1{(Z^{(2)})^\top Z^{(2)} }  }
		= \frac{1}{n_2 - p}\bigtr{\Sigma (\Sigma^{(2)})^{-1}} +  \OO\left( \frac{p^{\e}}{\sqrt{n_2 p}}\cdot \frac{p}{n_2}\|\Sigma\|_{op} \right) \]
		with high probability for any small constant $\e>0$.
\end{lemma}
\todo{the scaling $\sqrt{n_2}$ above doesn't seem correct}

 %The above result has a long history in random matrix theory.
If the entries of $Z^{(2)}$ are i.i.d. Gaussian, then this result follows from the classical result for %the mean of
the inverse Wishart distribution \citet{anderson1958introduction}. For a general non-Gaussian $Z^{(2)}$, this result can be obtained using the well-known Stieltjes transform method (cf. Lemma 3.11 of \citet{bai2009spectral}). Here we have presented the results from \citet{isotropic} and \citet{DY}, who give an almost sharp convergence rate.

On the other hand, characterizing the risk of the HPS estimator requires dealing with several challenges.
First, characterizing bias (cf. equation \eqref{Lbias}) introduces the issue of \textit{model shift}, when $\beta^{(1)}$ differs from $\beta^{(2)}$.
Second, characterizing variance (cf. equation \eqref{Lvar}) introduces  the issue of \textit{covariate shift}, when $\Sigma^{(1)}$ differs from $\Sigma^{(2)}$.
Our main results in the next section provide precise bias-variance asymptotics under (either or both) covariate and model shifts.