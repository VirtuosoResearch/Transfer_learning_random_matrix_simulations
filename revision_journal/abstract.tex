\begin{abstract}
    We consider the problem of transfer learning---gaining knowledge from one task and applying it to a different but related target task.
    A fundamental question in transfer learning is when combining the data of both tasks works better than using only the target task's data (equivalently, when there is a ``positive information transfer'').
    We study this question formally in a linear regression setting where a certain two-layer linear neural network estimator is used to combine both tasks' data.
    Under various settings involving covariate and model shifts, the estimator reproduces several interesting behaviors regarding positive (and negative) information transfer observed on real-world data.
    
    We then show the precise asymptotic limit of risk of the estimator in terms of geometries of the data in a high-dimensional setting, where the sample sizes increase with the feature dimension proportionally at a fixed ratio.
    We also find that the asymptotics is accurate for finite dimensions and apply them to analyze the exact conditions for positive information transfer.
    This analysis leads to several novel insights regarding covariate and model shifts.
    For example, the risk curve is non-monotone under model shift, thus motivating a more efficient procedure for progressively adding data.
    The main ingredient of our analysis is showing the asymptotic limit of certain combinations of independent sample covariance matrices under covariate shift and different sample sizes, which may be of independent interest.
    
%	We develop new techniques and establish a number of new results in the high-dimensional setting, where the sample size and feature dimension increase at a fixed ratio.  Second, we characterize the asymptotic bias-variance limit for two tasks, even when they have arbitrarily different sample size ratios and covariate shifts. We also demonstrate that these limiting estimates for the empirical loss are incredibly accurate in moderate dimensions. Finally, we explain an intriguing phenomenon where increasing one task's sample size helps another task initially by reducing variance but hurts eventually due to increasing bias. This suggests progressively adding data for optimizing hard parameter sharing, and we validate its efficiency in text classification tasks.
\end{abstract}
