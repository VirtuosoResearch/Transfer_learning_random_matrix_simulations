\section{Empirical results}\label{sec_exp}

This section complements our theoretical analysis of HPS with empirical evaluations.
First, we evaluate the performance of HPS estimators against several natural transfer learning estimators.
We show that HPS achieves superior performance under various settings of covariate and model shifts.
Second, we provide empirical implications that are inspired by our theoretical insights for mitigating covariate and model shifts.
One concerns a certain covariate alignment procedure for mitigating covariate shift.
We show that such alignment procedures provide greater gains for larger regimes of $n_1 / n_2$.
The other is a progressive training procedure for mitigating model shift.
We show that progressively increasing the sample sizes of data sources significantly reduces the computational cost of learning HPS neural networks while achieving the same accuracy for predicting the target task.

\subsection{Evaluating HPS under covariate and model shifts}\label{sec_diff}

\begin{figure}[!t]
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{figures/compare_risk_covariate_shift.eps}
		\caption{Covariate shift}
		\label{fig_sec5_covariate}
	\end{subfigure}\hfill%
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{figures/compare_risk_model_shift.eps}
		\caption{Model shift}
		\label{fig_sec5_model}
	\end{subfigure}\hfill%
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{figures/compare_risk_cov_model_shift.eps}
		\caption{Covariate and model shift}
		\label{fig_sec5_cov_model}
	\end{subfigure}
	\caption{We compare the empirical excess risk of HPS and several natural transfer learning estimators. We find that HPS achieves the lowest excess risk for predicting task two under (either or both) covariate and model shift. This simulation uses $p = 100, n_1 = n_2 = 200, \sigma = 1/2$. Figure \ref{fig_sec5_cov_model} uses $\lambda = 4$ for generating covariate-shifted features.}
	\label{fig_sec51}
\end{figure}

We show that HPS estimators enjoy superior empirical performance compared to several natural transfer learning estimators.
We consider the following estimators for this comparative study:
\begin{itemize}
    \item {\bf Averaging estimator (AVG):} given two tasks, take a convex combination of their OLS estimators $b \cdot \hat{\beta}^{\STL}_1 + (1 - b) \cdot \hat{\beta}^{\STL}_2$.
    \item {\bf STL estimator (STL):} simply use the OLS estimator of task two $\hat{\beta}^{\STL}_2$ without using task one's data.
    \item {\bf Transfer learning estimator (TL):} simply use the OLS estimator of task one $\hat{\beta}^{\STL}_1$ without using task two's data.
\end{itemize}
The parameter $b$ in the averaging estimator is optimized using a validation set of the same size as the training set.
Additionally, we extend the two-layer neural network formulation of HPS (cf. equation \eqref{eq_hps}) to a weighted objective:
$b \cdot \bignorm{X^{(1)} B A_1 - Y^{(1)}}^2 + (1 - b) \cdot  \bignorm{X^{2} B A_2 - Y^{(2)}}^2$, and optimize $b$ using the same validation set.

Figure \ref{fig_sec51} shows the result under (either or both) covariate and model shifts.
Figure \ref{fig_sec5_covariate} uses the same data generating process as Figure \ref{fig_sec31}.
Figure \ref{fig_sec5_model} uses the same data generating process as Figure \ref{fig_sec3_model_shift}.
Figure \ref{fig_sec5_cov_model} combines both by generating covariate-shifted features and different linear models for each dataset.
For reach run, we average the result over 100 random seeds because of high variance from the small sample sizes.
We find that HPS outperforms all the other estimators in this simulation (the result for the TL estimator is worse than STL and is omitted from the figure).

%\subsection{Implications for regression methods}


\subsection{Implications for neural networks}

\begin{wrapfigure}[12]{r}{0.3\textwidth}
	\vspace{-0.35in}
	\begin{center}
		\includegraphics[width=0.27\textwidth]{figures/mtl_model_arch.pdf}
	\end{center}
	\vspace{-0.28in}
	\caption{A hard parameter sharing neural network.}
	\label{fig_intro_arch}
\end{wrapfigure}
We conduct further studies of HPS for text classification tasks.
We consider six datasets for predicting movie review sentiment (MR and SST), sentence subjectivity (SUBJ), customer review sentiment (CR), question types (TREC), and polarity of a phrase (MPQA).\footnote{The datasets can be downloaded at \url{https://github.com/harvardnlp/sent-conv-torch/tree/master/data}. Further statistics of each dataset can also be found following the link.}
%\cite{pang2005seeing,socher2013recursive}
%\cite{pang2004sentimental}
%\cite{pang2004sentimental}
%\cite{pang2004sentimental}
%\cite{pang2004sentimental}
% \cite{lei2018simple}
%The question is to predict positive or negative sentiment expressed in the text.
We learn a hard parameter sharing model that consists of a word embedding layer using GloVe \cite{pennington2014glove}, followed by a shared feature representation layer.\footnote{The GloVe word vectors can be downloaded at \url{https://nlp.stanford.edu/projects/glove/}.}
A separate output layer is used for predicting the labels of each dataset.
We evaluate on three possibilities of feature representation layers including LSTM, MLP, and CNN, as implemented by PyTorch.\footnote{\url{pytorch} Before the MLP layer, we apply an average pooling layer over the embedding of every word in the sentence.}
See Figure \ref{fig_intro_arch} for an illustration of the network architecture.

\paragraph{Covariate alignment for mitigating covariate shift.}
We apply our theoretical insights to conduct detailed analysis of a covariate alignment procedure proposed by \citet{WZR20}.
The idea of this procedure is to insert an ``alignment'' module (or matrix) between every input $X_i$ and the shared module $B$.
During training, the entire network is optimized together with this alignment module (see \citet{WZR20} for more details about the implementation).

Our hypothesis is that as $n_1 / n_2$ increases, performing covariate alignment leads to larger accuracy improvement over the baseline HPS.
Recall from Claim \ref{prop_covariate} that covariate shift between the data source and the target task worsens the performance of HPS.
The effect is further exacerbated when the sample size of the data source is larger than the target task.
%This observation highlights the need for mitigating covariate shift when $n_1 / n_2$ increases.
To verify the hypothesis, we conduct multi-task training over all 15 pairwise tasks (among the six datasets).
We measure the average accuracy improvement from performing covariance alignment vs. HPS (trained using stochastic gradient descent (SGD)) over the 15 task pairs.
Figure \ref{fig_ab_cov} confirms our hypothesis.
We observe that covariate alignment achieves up to 4\% accuracy improvement as $n_1 / n_2$ increases.
%We fix task two's sample size at $1,000$, and increase task one's sample size from $1,000$ to $3,000$.


%\textit{Predicting transfer effect via STL results.}
%We show that the single-task based metric proposed in Section \ref{sec_similarity} can predict positive or negative transfer in MTL.
%A common challenge in the study of MTL is that the results can be hard to understand.
%It is difficult to predict when MTL performs well without running extensive trials.
%Our insight is that we can use STL results to help understand MTL results.
%Table \ref{tab:mtl_better_than_stl} shows the result on both the sentiment analysis and the ChestX-ray14 tasks.
%We find that using a threshold of $\tau = 0.1$, the STL results correctly predict positive or negative transfer with $75.6\%$ precision and $38.8\%$ recall among $30$ times $5$ (random seeds) task pairs!
%We observe similar results for $91$ task pairs from the ChestX-ray14 dataset.
%The results show that STL results are indicative of MTL results.

\paragraph{Progressive training for mitigating model shift.}
Inspired by our theoretical analysis, we propose a progressive training procedure that reduces the computational cost of learning HPS networks.
To motivate this procedure, we first conduct an experiment similar to Figure \ref{fig_sec3_model_transition}) but using the text classification datasets instead.
In Figure \ref{fig_ab_data}, we find that for multiple pairs of datasets, increasing $n_1$ improves task two's test accuracy initially, but hurts eventually.

These examples and the ones in Figure \ref{fig_sec3_model_transition} suggest a natural progressive training procedure that increases $n_1$ progressively until performance drops.
Concretely, we first divide the training data into $S$ batches.
Then, we progress in $S$ stages during training. During each stage, we progressively add one more minibatch of data from the data source(s).
During each stage, we run SGD for $T$ epochs using only the available minibatches of data.
We terminate once task two's validation accuracy drops compared to the previous round's result or reaches a desired threshold $\tau$.
See Algorithm \ref{alg_inc_train} for the complete procedure.
As an example, this procedure will terminate at the optimal value of $n_1$ if applied to the settings of Figures  \ref{fig_sec3_model_transition} and \ref{fig_ab_data}.
In light of these observations, our hypothesis is that this procedure requires a lower computational cost compared to SGD.
%For example, if the procedure terminates at 30\% of all batches, then SGD only passes over 30\% of its data, whereas standard round-robin training passes over 100\% of task one's data.

\begin{figure}[!t]
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\vspace{0pt}
		\includegraphics[width=0.8\textwidth]{figures/fig3b.pdf}
		\caption{Covariate alignment under varying sample sizes}
		\label{fig_ab_cov}
	\end{subfigure}\hfill
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\vspace{0pt}
		\includegraphics[width=0.8\textwidth]{figures/fig3a.pdf}
		\caption{Information transfer under varying sample sizes}
		\label{fig_ab_data}
	\end{subfigure}
	\caption{
	We extend some of our theoretical observations to text classification tasks using HPS neural networks.
	Figure \ref{fig_ab_cov} shows that covariance alignment improves the task two's test accuracy more for larger sample size ratio $n_1 / n_2$.
 	Figure \ref{fig_ab_data} shows that task one provides a positive transfer to task two in HPS only for a restricted range of $n_1$ (similar to Figure \ref{fig_sec3_model_transition}).
    Both experiments fixes $n_2 = 1000$ while varying $n_1$ from $100$ to $3000$.
	The $y$-axis measures task two's test accuracy using HPS subtracted by its test accuracy without using task one's data.}
	\label{fig_text}
\end{figure}


\begin{algorithm}[!h]
	\caption{Progressive training of hard parameter sharing networks}
	\label{alg_inc_train}
	\begin{algorithmic}[1]
		\Input Two tasks $(X^{(1)}, Y^{(1)})$ and $(X^{(2)}, Y^{(2)})$, divided into a training, validation, and test set.
		\Param A shared module $B$, and output layers $A_1, A_2$.
		\Req Number of batches $S$, epochs $T$, a threshold $\tau\in(0,1)$.
		\Output The trained modules $B, A_2$ optimized for task two.
		\State Divide the training set of $(X^{(1)}, Y^{(1)})$ randomly into $S$ batches: $(x^{(1)}, y^{(1)}), \dots, (x^{(S)}, y^{(S)})$.
		\For{$i = 1,\dots, S$}
		\For{$j = 1,\dots, T$}
		\State Minimize the cross-entropy loss of $B, A_1, A_2$ on  $\set{(x^{(k)}, y^{(k))}}_{k=1}^i$ and the training set of $(X^{(2)}, Y^{(2)})$ using SGD.
		\EndFor
		\State Let $a_i$ be the current validation accuracy for task two.
		\If{$a_i < a_{i-1}$ or $a_i > \tau$}
		\State \textbf{break}
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

\smallskip
\begin{wrapfigure}[10]{r}{0.26\textwidth}
	\vspace{-0.15in}
	\centering
	\begin{tabular}{c c c}
		\toprule
		Models & Relative cost \\
		\midrule
		{\bf CNN}  & 30\% \\
		{\bf LSTM} & 35\% \\
		{\bf MLP}  & 31\%  \\
		\bottomrule
		\end{tabular}
		\captionof{table}{The computational cost of running Algorithm \ref{alg_inc_train} relative to the cost of running SGD.}
		\label{tab:taskonomy}
	\label{fig_intro_arch}
\end{wrapfigure}
We evaluate Algorithm \ref{alg_inc_train} in two scenarios.
First, we consider transferring from single data source to a target task.
We evaluate over all $15$ two-task pairs.
We find that when averaged over all the pairs, our procedure requires less than $35\%$ of the computational cost relative to SGD while reaching the same level of test accuracy for predicting task two.
Second, we consider transferring from multiple data sources.
We use all six datasets as data sources to help predict every other task---that is, we measure the average accuracy of predicting all six tasks.
We find that a similar progressive training style procedure requires less than $35\%$ of the computational cost (relative to the cost of running SGD) for reaching the same average test accuracy of SGD.

\medskip
\noindent\textit{Further details.}
Algorithm \ref{alg_inc_train} uses SGD during the minimization step.
We randomly shuffle the data of both tasks and apply SGD on the shuffled data.
For the two-task experiment, we set $\tau$ to be task two's (STL) test accuracy trained without using any other task's data.
%We include all of task two's data and progressively add task one's data using the procedure described above.
%As a result, once Algorithm \ref{alg_inc_train} terminates, task two's test accuracy is at least $\tau$.
For the six-task experiment, We set $\tau$ to be the average test accuracy of all six tasks trained using all tasks' data.
We include the data from all tasks except SST. For SST, we progressively increasing its sample size similar to Algorithm \ref{alg_inc_train}.



%\begin{figure}[!t]
%	\centering
%	\begin{subfigure}[b]{0.5\textwidth}
%		\centering
%		\includegraphics[width=0.5\textwidth]{figures/ratio_alignment_norm_diff_all.pdf}
%		\caption{Averaged over all 16 task pairs}
%		\label{fig_ab_cov}
%	\end{subfigure}\hfill
%	\begin{subfigure}[b]{0.5\textwidth}
%		\centering
%		\includegraphics[width=0.5\textwidth]{figures/ratio_alignment_norm_trec_cr_lstm.pdf}
%		\caption{An example task pair}
%		\label{fig_cov_a}
%	\end{subfigure}
%	\caption{ (TREC and CR)}
%\end{figure}

%\textbf{Further results of the covariance alignment procedure.}
%Our results in Figure \ref{fig_ab_cov} are averaged over all the task pairs.
%In Figure \ref{fig_covariate_app}, we show two task pairs as examples.
%In Figure \ref{fig_cov_a}, we observe that for the particular task pair, covariance alignment provides more significant gains when the sample ratio is large.
%In Figure \ref{fig_cov_b}, we observe that covariance alignment does not always improve over the baseline multi-task learning model.
%One explanation is that MR and SST are similar tasks, hence adding the alignment module is unnecessary.
%An interesting question is to understand when adding the alignment module benefits the multi-task learning model.
%We leave this question for future work.
%Note: For text classification tasks, the source task training data size ranges from 500 to 1,500 and target task training data size is 1000; For ChestX-ray14,

%\begin{figure}[!h]
%	\centering
%	\begin{subfigure}[b]{0.48\textwidth}
%		\centering
%		\includegraphics[width=0.7\textwidth]{figures/ratio_alignment_norm_trec_cr_lstm.pdf}
%		\caption{Task pair TREC and CR}
%		\label{fig_cov_a}
%	\end{subfigure}\hfill
%		\begin{subfigure}[b]{0.48\textwidth}
%		\centering
%		\includegraphics[width=0.7\textwidth]{figures/ratio_alignment_mr_sst_lstm.pdf}
%		\caption{Task pair MR and SST}
%			\label{fig_cov_b}
%	\end{subfigure}
%	\caption{(a) For the task pair TREC and CR, adding the covariance alignment procedure provides more improvement when the source/target sample ratio is large.
%	(b) For the task pair MR and SST, adding the covariance alignment procedure hurts performance.
%	One explanation is that MR and SST are similar tasks, hence adding the alignment module is unnecessary.}
%	\label{fig_covariate_app}
%\end{figure}