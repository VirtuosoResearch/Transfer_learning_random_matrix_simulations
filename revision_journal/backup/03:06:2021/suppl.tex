\documentclass[aos,preprint]{imsart}
\usepackage{xr}
	\externaldocument{./manuscript}
%% Packages

%% Packages
\def\shownotes{0}
%\usepackage{natbib}
%\setcitestyle{authoryear,open={(},close={)}}
%\renewcommand{\cite}{\citep}
\usepackage[export]{adjustbox}
%\usepackage[margin=1in]{geometry}
%\usepackage{fullpage}
\usepackage{subfloat}
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{macro_math}
\usepackage{macro_fan}
\usepackage{stmaryrd}
\usepackage{multirow}
\usepackage[us,12hr]{datetime}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{caption,subcaption}
%\usepackage{breqn}
\renewcommand{\thesection}{\Alph{section}}

\newcommand{\Qa}{\mathcal Q^{(1)}}
\newcommand{\Qb}{\mathcal Q^{(2)}}
\newcommand{\mua}{\mu^{(1)}}
\newcommand{\mub}{\mu^{(2)}}
\newcommand{\fm}{\mathfrak m}

\usepackage{dsfont}


\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothezis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\theoremstyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlocaldefs

\begin{document}

\begin{frontmatter}
\title{Supplement to ``Hard Parameter Sharing in High-dimensional Linear Regression"}
%\title{A sample article title with some additional note\thanksref{t1}}
\runtitle{Hard Parameter Sharing in High-dimensional Linear Regression}
%\thankstext{T1}{A sample additional note to the title.}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%Only one address is permitted per author. %%
%%Only division, organization and e-mail is %%
%%included in the address.                  %%
%%Additional information can be included in %%
%%the Acknowledgments section if necessary. %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Fan} \snm{Yang}\ead[label=e1]{fyang75@wharton.upenn.edu} \thanks{Fan Yang and Hongyang R. Zhang contributed equally.}}
\author[B]{\fnms{Hongyang R.} \snm{Zhang}\ead[label=e2,mark]{hrzhang@northeastern.edu}\samethanks} 
\author[C]{\fnms{Sen} \snm{Wu}\ead[label=e3,mark]{senwu@stanford.edu}} 
\author[A]{\fnms{Weijie J.} \snm{Su}\ead[label=e4,mark]{suw@wharton.upenn.edu} }
\and
\author[C]{\fnms{Christopher} \snm{R\'e}\ead[label=e5,mark]{chrismre@cs.stanford.edu}} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Department of Statistics,
University of Pennsylvania,
\printead{e1,e4}}
\address[B]{Khoury College of Computer Sciences, Northeastern University,
\printead{e2}}
\address[C]{Department of Computer Science, Stanford University,
\printead{e3,e5}}

\end{aug}

\begin{abstract}
In this supplementary file, we provide the proofs of some technical results in the main manuscript, including Lemma \ref{lem_HPS_loss}, Theorem \ref{thm_many_tasks}, Theorem \ref{cor_MTL_loss}, Proposition \ref{lem_hat_v}, Theorem \ref{thm_main_RMT} and Proposition \ref{prop_main_RMT}.
\end{abstract}

\begin{keyword}[class=MSC2020]
\kwd[Primary ]{62J05}
\kwd{60B20}
\kwd[; secondary ]{62E20, 62H10}
\end{keyword}

\begin{keyword}
\kwd{Hard parameter sharing, high-dimensional linear regression, random matrix theory, sample covariance matrices}
\end{keyword}

\end{frontmatter}


\section{Basic tools}\label{app_tool}
 In this section, we collect some basic tools that will be used in the proof.
%We state several concentration results for dealing with random matrices with bounded moments.
%Recall from Section \ref{sec_same} that we consider random matrices $Z \in \real^{n\times p}$ whose entries are i.i.d. with zero mean, unit variance, and bounded $\varphi$-th moment.
%We say that an event $\Xi$ holds with overwhelming probability if for any constant $D>0$, $\mathbb P(\Xi)\ge 1- n^{-D}$ for large enough $n$. Moreover, we say $\Xi$ holds with overwhelming probability in an event $\Omega$ if for any constant $D>0$, $\mathbb P(\Omega\setminus \Xi)\le n^{-D}$ for large enough $n$.
First, for our proof it is convenient to introduce the following notation.
\begin{definition}[Overwhelming probability]
 We say $\Xi$ holds \emph{with overwhelming probability} (w.o.p.) if for any constant $D>0$, $\mathbb P(\Xi)\ge 1- p^{-D}$ for large enough $p$. Moreover, we say $\Xi$ holds with overwhelming probability in an event $\Omega$ if for any constant $D>0$, $\mathbb P(\Omega\setminus \Xi)\le p^{-D}$ for large enough $p$.
\end{definition}

The following notion of stochastic domination, which was first introduced in \cite{Average_fluc}, is commonly used in the study of random matrices.

\begin{definition}[Stochastic domination]\label{stoch_domination}
Let $\xi\equiv \xi^{(n)}$ and $\zeta\equiv \zeta^{(n)}$ be two $n$-dependent random variables.
%\HZ{what is $n$-dependent rv?}[{\cor Most variables in our paper are $n$-dependent implicitly, such as resolvents. Furthermore, the random matrix entries can also depend on $n$ if we want. One important example is Bernoulli random variable with sparsity $p=n^{-\theta}$. Furthermore, this also includes the random variables that do not depend on $n$ as a special case.}]
We say that $\xi$ is stochastically dominated by $\zeta$, denoted by $\xi\prec \zeta$ or $\xi=\OO_\prec(\zeta)$, if for any small constant $\e > 0$ and any large constant $D > 0$, there exists a function $n_0(\e, D)$ such that for all $n > n_0(\e, D)$,
\[ \bbP\left(|\xi| >n^\e |\zeta|\right)\le n^{-D}. \]
In other word, $\xi\prec \zeta$ if $|\xi| \le n^\e |\zeta|$ with overwhelming probability for any small constant $\e>0$. If $\xi(u)$ and $\zeta(u)$ are functions of $u$ supported in $\cal U$, then we say $\xi(u)$ is stochastically dominated by $\zeta(u)$ uniformly in $\cal U$ if % for any constants $\e, D>0$,
\[ \sup_{u\in \cal U}\bbP\left(|\xi(u)|>n^\e |\zeta(u)|\right)\le n^{-D}. \]
Given any event $\Omega$, we say $\xi \prec \zeta$ on $\Omega$ if $\mathbf 1_{\Omega}\xi \prec \zeta$.
%for any constant $D>0$, $\mathbb P(\Omega\setminus \Xi)\le n^{-D}$ for large enough $n$.
\end{definition}


%The following notion of stochastic domination, which was first introduced in \cite{Average_fluc}, is commonly used in the study of random matrices.
%
%\begin{definition}[Stochastic domination]\label{stoch_domination}
%Let $\xi\equiv \xi^{(n)}$ and $\zeta\equiv \zeta^{(n)}$ be two $n$-dependent random variables.
%%\HZ{what is $n$-dependent rv?}[{\cor Most variables in our paper are $n$-dependent implicitly, such as resolvents. Furthermore, the random matrix entries can also depend on $n$ if we want. One important example is Bernoulli random variable with sparsity $p=n^{-\theta}$. Furthermore, this also includes the random variables that do not depend on $n$ as a special case.}]
%We say that $\xi$ is stochastically dominated by $\zeta$, denoted by $\xi\prec \zeta$ or $\xi=\OO_\prec(\zeta)$, if for any small constant $\e > 0$ and any large constant $D > 0$, there exists a function $n_0(\e, D)$ such that for all $n > n_0(\e, D)$,
%\[ \bbP\left(|\xi| >n^\e |\zeta|\right)\le n^{-D}. \]
%In case $\xi(u)$ and $\zeta(u)$ is a function of $u$ supported in $\cal U$, then we say $\xi(u)$ is stochastically dominated by $\zeta(u)$ uniformly in $\cal U$ if % for any constants $\e, D>0$,
%\[ \sup_{u\in \cal U}\bbP\left(|\xi(u)|>n^\e |\zeta(u)|\right)\le n^{-D}. \]
%\end{definition}

%For example, we say that $\xi$ is stochastically dominated by $\zeta$ with overwhelming probability if $\zeta$ is greater than $\xi$ up to a small power of $n^{\e}$.
\begin{remark}We make several simple remarks.
First, since we allow for an $n^\e$ factor in stochastic domination, we can ignore $\log$ factors without loss of generality since $(\log n)^C\prec 1$ for any constant $C>0$.
Second, given a random variable $\xi$ with unit variance and finite moments up to any order as in \eqref{eq_highmoments}, we have that $|\xi|\prec 1$.
This is because by Markov's inequality, we have that
$$ \P(|\xi|\ge n^{\e})\le n^{-k\e}{\E |\xi|^k}\le n^{-D},$$
as long as $k$ is taken to be larger than $D/ \e$.
%As a special case, this implies that a Gaussian random variable $\xi$ with unit variance satisfies that $|\xi|\prec 1$.
\end{remark}

The following lemma collects several basic properties of  stochastic domination that will be used tacitly in the proof. Roughly speaking, it says that the stochastic domination ``$\prec$" can be treated as  the conventional less-than sign ``$<$" in some sense.


\begin{lemma}[Lemma 3.2 in \cite{isotropic}]\label{lem_stodomin}
Let $\xi$ and $\zeta$ be two families of nonnegative random variables depending on some parameters $u\in \cal U$ or $v\in \cal V$.
\begin{itemize}
\item[(i)] {\bf Sum.} Suppose that $\xi (u,v)\prec \zeta(u,v)$ uniformly in $u\in \cal U$ and $v\in \cal V$. If $|\cal V|\le n^C$ for some constant $C>0$, then $\sum_{v\in \cal V} \xi(u,v) \prec \sum_{v\in \cal V} \zeta(u,v)$ uniformly in $u$.

\item[(ii)] {\bf Product.} If $\xi_1 (u)\prec \zeta_1(u)$ and $\xi_2 (u)\prec  \zeta_2(u)$ uniformly in $u\in \cal U$, then $\xi_1(u)\xi_2(u) \prec \zeta_1(u) \zeta_2(u)$ uniformly in $u\in \cal U$.

\item[(iii)] {\bf Expectation.} Suppose that $\Psi(u)\ge n^{-C}$ is a family of deterministic parameters, and $\xi(u)$ satisfies $\mathbb E\xi(u)^2 \le n^C$. If $\xi(u)\prec \Psi(u)$ uniformly in $u$, then we also have $\mathbb E\xi(u) \prec \Psi(u)$ uniformly in $u$.
\end{itemize}
\end{lemma}

 

%In this section, we shall prove a more fundamental random matrix result in a little more general setting than the one in Section \todo{add reference}; see Theorem \ref{LEM_SMALL} below. %\ref{sec_prelim}.
%Then Theorem \ref{thm_main_RMT} will follow from this result as immediate corollaries. We first state the exact assumptions needed for our purpose.

%Next, we introduce the bounded support assumption for a random matrix.
We say that a random matrix $Z \in\real^{n \times p}$ satisfies the {\it{bounded support condition}} with $Q$ or $Z$ has support $Q$ if
\begin{equation}
	\max_{1\le i \le n, 1 \le j \le p}\vert Z_{i j} \vert \prec Q. \label{eq_support}
\end{equation}
As shown in the example above, if the entries of $Z$ have finite moments up to any order, then $Z$ has bounded support $1$.
More generally, if the entries of $Z$ have finite $\varphi$-th moment, then using Markov's inequality and a simple union bound we get that %\HZ{I don't fully get the equation below}
\begin{align}
	\P\left(\max_{1\le i\le n, 1\le j \le p}|Z_{i  j}|\ge (\log n) n^{\frac{2}{\varphi}}\right) &\le \sum_{i=1}^n \sum_{j=1}^p \P\left(|Z_{i j}|\ge (\log n) n^{\frac{2}{\varphi}}\right)  \nonumber\\
	&\lesssim \sum_{i=1}^n \sum_{j=1}^p  \left[(\log n) n^{\frac{2}{\varphi}}\right]^{-\varphi} = \OO((\log n)^{-\varphi}).\label{Ptrunc}
	\end{align}
In other words, $Z$ has bounded support $Q=n^{\frac{2}{\varphi}}$ with high probability.


The following lemma gives sharp concentration bounds for linear and quadratic forms of random variables with bounded support. %We recall that the stochastic domination ``$\prec$" has been defined in Definition \ref{stoch_domination}.
%Besides the proof of Lemma \ref{prop_entry}, we also use it in several other places of this paper, including the proofs in Section \ref{sec_proof_general} and Section \ref{app_iso_cov}.
%It constitutes the main difference between our proof and the one in \cite{KY2}, where the authors used a large deviation bound for random variables with arbitrarily high moments.
\begin{lemma}[Lemma 3.8 of \cite{EKYY1} and Theorem B.1 of \cite{Delocal}]\label{largedeviation}
Let $(x_i)$, $(y_j)$ be independent families of centered and independent random variables, and $(A_i)$, $(B_{ij})$ be families of deterministic complex numbers. Suppose the entries $x_i$ and $y_j$ have variance at most $1$, and satisfy the bounded support condition (\ref{eq_support}) for a deterministic parameter $Q \ge 1$. %with $q\le n^{-\phi}$ for a small constant $\phi>0$.
Then we have the following results:
%for any fixed $\xi>0$, the following bounds hold with $\xi$-high probability:
\begin{align}
& \Big| \sum_{i=1}^n A_i x_i \Big\vert \prec Q \max_{i} \vert A_i \vert+ \Big(\sum_i |A_i|^2 \Big)^{1/2} ,\label{eq largedev10}  \\ 
& \Big\vert  \sum_{i,j=1}^n x_i B_{ij} y_j \Big\vert \prec Q^2 B_d  + Q n^{1/2}B_o +  \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}},\label{eq largedev11}  \\
& \Big\vert  \sum_{i=1}^n (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec Q n^{1/2}B_d   ,\label{eq largedev20}\\ 
&\Big\vert  \sum_{1\le i\ne j\le n} \bar x_i B_{ij} x_j \Big\vert  \prec Qn^{1/2}B_o +  \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} ,\label{eq largedev21}
\end{align}
where we denote $B_d:=\max_{i} |B_{ii} |$ and $B_o:= \max_{i\ne j} |B_{ij}|.$ Moreover, if the moments of $ x_i$ and $ y_j$ exist up to any order, then we have the following stronger results:
\begin{align}
& \Big\vert \sum_i A_i x_i \Big\vert \prec  \Big(\sum_i |A_i|^2 \Big)^{1/2} , \label{eq largedev0} \\
&  \Big\vert \sum_{i,j} x_i B_{ij} y_j \Big\vert \prec  \Big(\sum_{i, j} |B_{ij}|^2\Big)^{{1}/{2}}, \label{eq largedev1} \\
& \Big\vert  \sum_{i} (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec  \Big( \sum_i |B_{ii} |^2\Big)^{1/2}  ,\label{eq largedev2} \\
&   \Big\vert  \sum_{i\ne j} \bar x_i B_{ij} x_j \Big\vert  \prec \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}}.\label{eq largedev3}
\end{align}
\end{lemma}

It is well-known that the limiting eigenvalue distributions of $(Z^{(1)})^\top Z^{(1)}$ and $(Z^{(2)})^\top Z^{(2)}$ satisfy the Marchenko-Pastur (MP) law \cite{MP}. Moreover, their eigenvalues are all inside the support of the MP %law $[(1-\sqrt{d_1})^2 , (1+\sqrt{d_1})^2]$ (resp. $[(1-\sqrt{d_2})^2 , (1+\sqrt{d_2})^2]$) 
with high probability \cite{No_outside}. In the proof, we shall need a slightly stronger result that holds with overwhelming probability as given by the following lemma. 
%Denote the nonzero eigenvalues of $Z_1^\top Z_1$ and $Z_2^\top Z_2$ by $\lambda_1 (Z_1^\top Z_1) \ge \cdots \ge \lambda_{p\wedge n_1} (Z_1^\top Z_1)$ and $\lambda_1 (Z_2^\top Z_2) \ge \cdots \ge \lambda_p (Z_2^\top Z_2)$.
 
\begin{lemma}\label{SxxSyy}
Suppose $Z\in \R^{n\times p}$ is an $n\times p$ random matrix satisfying the same assumptions as $Z^{(1)}$ and $Z^{(2)}$ in Assumption \ref{assm_big1}, and $\rho:=n/p$ satisfies \eqref{assm2}. %(i) for $Z_1,Z_2$ satisfy 
Moreover, suppose $Z$ satisfies the bounded support condition \eqref{eq_support} for a deterministic parameter $Q$ satisfying $ 1\le Q \leq n^{ 1/2 - c_Q} $ for a small constant $c_Q>0$.  Then for any constant $\e>0$, we have that %with overwhelming probability,
%\be\label{op rough1} %(1-\sqrt{d_1})^2 - \e \le  \lambda_p(Z_1^\top Z_1)   \le  
%\lambda_1(Z_1^\top Z_1) \le (1+\sqrt{d_1})^2 + \e ,
%\ee
%and
\be\label{op rough2} (\sqrt{n}-\sqrt{p})^2 - \OO_\prec(n\cdot Q) \le  \lambda_p (Z^\top Z)  \le  \lambda_1(Z^\top Z) \le (\sqrt{n}-\sqrt{p})^2 + \OO_\prec(n\cdot Q)  .
\ee
\end{lemma}
\begin{proof}
When $Q$ is of order 1, this lemma follows from \cite[Theorem 2.10]{isotropic}. %although the authors considered the case with $q \prec n^{-1/2}$ only. 
The estimate for the case with larger $Q$ satisfying $ 1\le Q \leq n^{ 1/2 - c_Q} $ follows from \cite[Lemma 3.11]{DY}. 
%but only the bounds for the largest eigenvalues are given there in order to avoid the issue with the smallest eigenvalue when $d_{2}$ is close to 1. However, under the assumption \eqref{assm2}, the lower bound for the smallest eigenvalue follows from the exactly the same arguments as in \cite{DY}. Hence we omit the details. 
\end{proof}

Using a standard cut-off argument, we can extend Lemma \ref{largedeviation} and Lemma \ref{SxxSyy} to the random matrices whose entries only satisfy certain moment assumptions but not necessarily the bounded support condition.


\begin{corollary}\label{fact_minv}
Suppose $Z\in \R^{n\times p}$ is an $n\times p$ random matrix satisfying the same assumptions as $Z^{(1)}$ and $Z^{(2)}$ in Assumption \ref{assm_big1}, and $\rho:=n/p$ satisfies \eqref{assm2}. Then \eqref{op rough2} holds on a high probability event with $Q=n^{2/\varphi}$, where $\varphi$ is the constant in \eqref{conditionA2}.
%	Suppose Assumption \ref{assume_rm} holds. With probability $1- 1 / \poly(n)$ over the randomness of $Z$, we have:
%	\begin{enumerate}
%		\item When $n / p$ converges to a fixed $\rho > 1$, the sample covariance matrix ${X^{\top}X} / n$ has full rank $p$.
%		\item The singular values of $Z^{\top}Z$ are greater than $(\sqrt{n} - \sqrt{p})^2 - n \cdot p^{-c_{\varphi}}$ and less than $(\sqrt{n} + \sqrt{p})^2 + n \cdot p^{-c_{\varphi}}$, cf. \cite[Theorem 2.10]{isotropic} and \cite[Lemma 3.12]{DY}.
%	\end{enumerate}
\end{corollary}
\begin{proof}
For $Q=n^{2/\varphi}$, we introduce a truncated matrix $\wt Z$ with entries 
\be\label{truncateZ} \wt Z_{ij}:= \mathbf 1\left( |Z_{ij}|\le Q \log n\right)\cdot Z_{ij}.\ee
%Then $\wt Z=Z$ when all the entries of $Z$ are smaller than $Q$. 
From equation \eqref{Ptrunc}, we get 
\begin{equation}\label{XneX222}
\mathbb P(\wt Z= Z) = 1- \P\left(\max_{i,j}|Z_{i  j}| > (\log n) n^{\frac{2}{\varphi}}\right)=1-\OO ( (\log n)^{-\varphi}).
\end{equation}
By definition, we have 
\be \label{EwtZ}
\begin{split}
&\E  \wt  Z_{ij} = - \mathbb E \left[ \mathbf 1\left( |Z_{ij}|> Q \log n \right)Z_{ ij}\right] ,\\ 
&\E  |\wt  Z_{ ij}|^2  = 1 - \mathbb E \left[ \mathbf 1\left( |Z_{ ij}|> Q \log n \right)|Z_{ ij}|^2\right] .
\end{split}
\ee
Using the formula for expectation in terms of the tail probabilities, we can check that
\begin{align*}
&  \mathbb E \left| \mathbf 1\left( |Z_{ ij}|> Q\log n \right)Z_{ ij}\right| = \int_0^\infty \P\left( \left| \mathbf 1\left(  |Z_{ ij}|> Q\log n \right)Z_{ ij}\right| > s\right)\dd s \\
& = \int_0^{Q\log n}\P\left( |Z_{ ij}|> Q\log n \right)\dd s +\int_{Q\log n}^\infty \P\left(|Z_{ ij}| > s\right)\dd s  \\
& \lesssim \int_0^{Q\log n}\left(Q\log n \right)^{-\varphi}\dd s +\int_{Q\log n}^\infty s^{-\varphi}\dd s \le n^{-2(\varphi-1)/\varphi},
\end{align*}
where in the third step we used the finite $\varphi$-th moment condition \eqref{conditionA2} for $Z_{ij}$ %\eqref{assmAhigh} \HZ{check; I don't see which one} 
and Markov's inequality. Similarly, we can obtain that
\begin{align*}
&  \mathbb E \left| \mathbf 1\left( |Z_{ ij}|> Q \log n \right)Z_{ ij}\right|^2  =  2\int_0^\infty s \P\left( \left| \mathbf 1\left( |Z_{ij}|>Q\log n \right)Z_{ij}\right| > s\right)\dd s \\
&=  2\int_0^{Q\log n} s \P\left( |Z_{ ij}|> Q\log n \right)\dd s +2\int_{Q\log n}^\infty s\P\left(|Z_{ ij}| > s\right)\dd s  \\
& \lesssim  \int_0^{Q\log n}s\left(Q\log n \right)^{-\varphi}\dd s +\int_{Q\log n}^\infty s^{-\varphi+1}\dd s \le n^{-2(\varphi-2)/\varphi}.
\end{align*}
Plugging the above two estimates into equation \eqref{EwtZ} and using $\varphi>4$, we get that
%Using (\ref{condition_4e}) and integration by parts, it is easy to verify that %we can get that
%\begin{align*}
%\mathbb E  |z^{(\al)}_{ij}|1_{|z^{(\al)}_{ij}|> q} =\OO(n^{-2-\e}), \quad \mathbb E |z^{(\al)}_{ij}|^2 1_{|z^{(\al)}_{ij}|> q} =\OO(n^{-2-\e}), \quad \al=1,2,
%\end{align*}
%which imply that
\be\label{meanshif}
|\mathbb E  Z_{ij}| =\OO(n^{-3/2}), \quad  \mathbb E |Z_{ij}|^2 =1+ \OO(n^{-1}).
\ee
From the first estimate in equation \eqref{meanshif}, we can also get a bound on the operator norm:
\be\label{EZ norm}\|\E Z\|=\OO(n^{-1/2}) .
\ee
We centralize and rescale $\wt Z$ as 
\be\label{center_truncateZ} \wh Z:=(\E|\wt Z_{11}|^2)^{-1/2} (\wt Z - \E \wt Z).\ee 
Now $\wh Z$ satisfies the assumptions of Lemma \ref{SxxSyy} with bounded support $Q$, hence we get
\be\nonumber (\sqrt{n}-\sqrt{p})^2 - \OO_\prec(n\cdot Q) \le  \lambda_p (\wh Z^\top \wh Z)  \le  \lambda_1(\wh Z^\top \wh Z) \le (\sqrt{n}-\sqrt{p})^2 + \OO_\prec(n\cdot Q)  .
\ee
Combining this estimate with \eqref{meanshif} and \eqref{EZ norm}, it is easy to show that \eqref{op rough2} holds for the eigenvalues of $\wt Z^\top \wt Z$, which concludes the proof by \eqref{XneX222}. 
\end{proof}
 
%Next, we state a concentration result for $Z$.

\begin{corollary} \label{cor_largedeviation}
Suppose $Z\in \R^{n\times p}$ is an $n\times p$ random matrix satisfying Assumption \ref{assm_big1}. Then there exists a high probability event, on which for any deterministic vector $v\in \R^p$, %we have that w.h.p.
\be\label{Zv_cor}\left|\|Zv\|^2- n\|v\|^2\right|\prec n^{1/2}Q \|v\|^2,\quad \text{for} \ \ Q=n^{2/\varphi}.\ee
\end{corollary}
\begin{proof}
%	Let $Q= n^{ \frac{2}{\varphi}}\log n$.
%	We introduce a truncated matrix $\wt Z$ with entries $\wt Z_{ij}:= \mathbf 1\left( |Z_{ij}|\le Q \right)\cdot Z_{ij}$. Then $\wt Z$ is equal to $Z$ when all the entries of $Z$ are smaller than $Q$. Using Markov's inequality and a simple union bound (cf. equation \eqref{Ptrunc}), this happens with probability 
%\begin{equation}\label{XneX222}
%\mathbb P(\wt Z= Z) =1-\OO ( (\log n)^{-\varphi}).
%\end{equation}
%Furthermore, using the finite $\varphi$-th moment condition and the tail probabilities, we can show that the mean and variance of $Z-\wt Z$ are small, which gives that (cf. equation \eqref{meanshif}) %, we have that
%\be\label{meanshif222}
%	|\mathbb E  \wt  Z_{ij}| =\OO(n^{-3/2}), \quad  \mathbb E |\wt  Z_{ij}|^2 =1+ \OO(n^{-1}).
%\ee
%%From the first estimate in equation \eqref{meanshif}, we can also get a bound on the operator norm:
%%\be\label{EZ norm222}\|\E \wt Z^{(1)}\|=\OO(n^{-1/2}) .
%%\ee
%We centralize and rescale $\wt Z$ as $ \wh Z:=\frac{\wt Z - \E \wt Z }{ (\E|\wt Z_{11}|^2)^{1/2}}.$
As in the proof of Corollary \ref{fact_minv}, we truncate $Z$ as in \eqref{truncateZ} and define $\wh Z$ as in \eqref{center_truncateZ}. By \eqref{meanshif} and \eqref{EZ norm}, we see that to conclude \eqref{Zv_cor}, it suffices to show 
%Let $\e$ be a sufficiently small fixed constant.
%To prove the result, it suffices to show that
\be\label{show_whZ}
 \left|\|\wh Zv\|^2- n\|v\|^2\right| \le n^{1/2 + \e} Q\|v\|^2.
\ee 
%This is because provided with equation \eqref{meanshif222} and \eqref{show_whZ}, we can get that
%$$ \left|\|\wt Zv\|^2- n\|v\|^2\right| \le n^{1/2 + \e} Q\|v\|^2,$$
%which implies the desired our (recall that $c_{\varphi} < 1/2 - \e$).
To prove equation \eqref{show_whZ}, we first notice that %for any $i = 1, 2, \dots, n$, 
%$$(\wh Zv)_i=\sum_{1\le j \le p} \wh Z_{ij}v_j$ is at most $2n^{\e} Q$.
$\wh{Z} v \in \R^n$ is a random vector with i.i.d. entries of mean zero and variance $\|v\|^2$. Furthermore, using equation \eqref{eq largedev10} from Lemma \ref{largedeviation}, we get that %with overwhelming probability
$$ |(\wh Zv)_i| \prec  Q\max_{1\le i\le p}|v_i| +  \|v\| \le 2 Q \|v\|. $$
Hence, $ {(\wh Zv)}/{\|v\|}$ consists of i.i.d random variables with zero mean, unit variance, and bounded support $Q$. Then applying equation \eqref{eq largedev20}, we get that
$$ \left|\|\wh Zv\|^2 - n\|v\|^2\right|= \|v\|^2\left|\sum_i \left(\frac{|(\wh Zv)_i|^2}{\|v\|^2}-\E\frac{|(\wh Zv)_i|^2}{\|v\|^2}\right)\right|\prec Q n^{1/2}  \|v\|^2 .
 $$
Hence the proof is complete.
 \end{proof}
 
%Next, we provide several concentration results for $\cal E=[\epsilon^{(1)},\epsilon^{(2)},\cdots, \epsilon^{(t)}]\in \R^{n\times t}$, which consists of i.i.d. random variables with mean zero, variance $\sigma^2$ and bounded moments up to any order.

From Lemma \ref{largedeviation}, we immediately obtain the following concentration results for the noise vectors.
 
\begin{corollary}\label{cor_calE}
Suppose $\ve^{(1)},\cdots,\ve^{(t)}\in \R^{n}$ are independent random vectors satisfying Assumption \ref{assm_big1}.  
%Let $\e>0$ be a sufficiently small fixed constant.
For any deterministic vector $v\in \R^n$, we have that  
\be\label{vcalE}
	|v^\top \epsilon^{(i)}| \prec  \sigma \|v\| ,\quad i=1,\cdots, t. %, \ \ \text{ and }\ \ \|v^\top \cal E\| \le n^{c} \cdot \sigma  \|v\|.
\ee
For any deterministic matrix $B \in \real^{n\times n}$, we have that  
\begin{align}\label{vcalA2}
	\left|(\epsilon^{(i)})^\top B \epsilon^{(j)} - \delta_{ij}\cdot \sigma^2 \tr(B)\right| \prec \sigma^2 \|B\|_F,\quad   i=1,\cdots, t.  
	%\ \ \text{ and }\ \ \left\| \cal E^\top B \cal E - \sigma^2 \tr[B]\cdot \id_{t\times t}\right\|_F \le n^{c} \cdot  \sigma^2\|B\|_F.
\end{align}
%\be\label{vcalAE3}
%,
%\ee
\end{corollary}
\begin{proof}
Note that $\epsilon^{(i)}/\sigma$ is a random vector with i.i.d. entries of zero mean, unit variance, and bounded moments up to any order. Then equation \eqref{vcalE} is an immediate consequence of \eqref{eq largedev0}.
%Using the first estimate in equation \eqref{eq largedev1} of Lemma \ref{largedeviation} (recall that the stochastic domination notation means the inequality holds with a multiplicative factor of $p^{\e}$ on the right), we obtain that equation \eqref{vcalE} holds (the second result is a consequence of the first).
Using equation \eqref{eq largedev1}, we obtain that for $i\ne j$,
 \begin{align*}
\left|(\epsilon^{(i)})^\top B \epsilon^{(j)}\right| = \Big|\sum_{k,l=1}^n \epsilon^{(i)}_k\epsilon^{(j)}_l B_{kl} \Big| \prec \sigma^2 \Big(\sum_{k,l=1}^n|B_{kl}|^2\Big)^{1/2}=\sigma^2 \|B\|_F.
 \end{align*}
Using the two estimates \eqref{eq largedev2} and \eqref{eq largedev3}, we obtain that 
\begin{align*}
\left|(\epsilon^{(i)})^\top B \epsilon^{(i)} - \sigma^2 \tr[B]\right| &\le \Big| \sum_{k}  \left(|\epsilon^{(i)}_k|^2-\E|\epsilon^{(i)}_k|^2\right)B_{ii} \Big|+\Big| \sum_{ k\ne l } \epsilon^{(i)}_k \epsilon^{(i)}_l B_{kl} \Big| \\
&\prec \sigma^2 \Big( \sum_{k}|B_{kk}|^2\Big)+ \sigma^2 \Big( \sum_{k\ne l}|B_{kl}|^2\Big)^{1/2}\lesssim \sigma^2 \|B\|_F.
\end{align*}
Hence, we have shown equation \eqref{vcalA2}. %(the second result is again a consequence of the first result).
\end{proof}


\section{Proof of Lemma \ref{lem_HPS_loss}}\label{app_firstpf}
 In this section, we prove Lemma \ref{lem_HPS_loss} using the estimates in Appendix \ref{app_tool}.
 For a fixed $a\in \R,$ we expand $L(\hat{\beta}_2^{\MTL}(a)) $ as
 \begin{align}
 &L(\hat{\beta}_2^{\MTL}(a)) =L_{\bias}(a)+2h_1(a) +2 h_2(a) +h_3(a)  +h_4(a)  +2h_5(a)  , 
 \end{align}
 where
 \begin{align*}
& h_1(a) := a (\epsilon^{(1)})^\top X^{(1)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1} (X^{(1)})^\top X^{(1)} ( a\beta^{(1)}- a^2\beta^{(2)})  ,\\
& h_2(a) :=   (\epsilon^{(2)})^\top X^{(2)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1} (X^{(1)})^\top X^{(1)} ( a\beta^{(1)}- a^2\beta^{(2)})  ,\\
& h_3(x):=  a^2 (\epsilon^{(1)})^\top X^{(1)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1}(X^{(1)})^\top \epsilon^{(1)}   , \\
& h_4(x):= (\epsilon^{(2)})^\top X^{(2)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1}  (X^{(2)})^\top \epsilon^{(2)} , \\
&h_5(x):=a  (\epsilon^{(1)})^\top X^{(1)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1}(X^{(2)})^\top \epsilon^{(2)} .
\end{align*}
Next, we estimate each term using Corollary \ref{cor_calE}. In the proof, we will use the following estimates, which can be proved easily using \eqref{assm3} and Corollary \ref{fact_minv}: on a high probability event $\Xi_1$,
%that will be commonly used in the proof. First, by Corollary \ref{fact_minv}, we  
\be\label{Op_norm}
\|X^{(1)}\| \lesssim \sqrt{n_1}, \quad \|X^{(2)}\|  \lesssim  \sqrt{n_2},\quad \left\|\hat\Sigma(a)^{-1}\right\|  \lesssim (a^2n_1+n_2)^{-1},
\ee
and 
\be\label{Op_norm2}
L_{\bias}(a) \sim \frac{n_1^2}{(a^2n_1+n_2)^2} \left\| a\beta^{(1)}- a^2\beta^{(2)}  \right\|^2, \quad L_{\var}(a)\sim  \frac{p\sigma^2}{a^2n_1 + n_2}.
\ee
Throughout the following proof, we assume that event $\Xi_1$ holds.

%By Fact \ref{fact_minv} (ii), we have that w.h.p. the operator norm of $X^{(1)}$ and $X^{(2)}$ are both bounded by $O(\sqrt{n})$.
%Furthermore, the operator norm of $\hat\Sigma(x)^{-1}$ is bounded by $(x^2 + 1)^{-1} O(n_1 + n_2) = (x^2 + 1)^{-1} O(p)$.
 % \be\label{op_X12}
%\|X^{(1)}\|\le \sqrt{(\sqrt{n_1} + \sqrt{p})^2 + n_1 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p}, \quad \|X^{(2)}\|\le \sqrt{(\sqrt{n_2} + \sqrt{p})^2 + n_2 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p},
% \ee
% and
% \be\label{op_Sig1}
%\| \hat\Sigma(x)^{-1}\|\le \frac1{x^2[(\sqrt{n_1} - \sqrt{p})^2 - n_1 \cdot p^{-c_{\varphi}}]+[(\sqrt{n_2} - \sqrt{p})^2 - n_2 \cdot p^{-c_{\varphi}}] }\lesssim \frac{1}{(x^2+1)p},
% \ee
%where we used that $3\le n_1/p\le \tau^{-1}$ and $3\le n_2/p\le \tau^{-1}$ for a small constant $\tau>0$.

For $h_1(a)$, using \eqref{vcalE} we obtain that 
\begin{align}
	 \left|h_1(a) \right| &\prec  \sigma |a|  \left\|  X^{(1)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1} (X^{(1)})^\top X^{(1)} ( a\beta^{(1)}- a^2\beta^{(2)})\right\|  \nonumber\\
&\lesssim  \sigma |a| \left\|   a\beta^{(1)}- a^2\beta^{(2)} \right\|\cdot \left\|  X^{(1)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1} (X^{(1)})^\top X^{(1)}  \right\| \nonumber \\
&	\lesssim \frac{\sigma |a| \left\|   a\beta^{(1)}- a^2\beta^{(2)} \right\| n_1^{3/2}}{(a^2 n_1 + n_2)^2} \le \frac{ p^{-1/4} n_1 \left\|   a\beta^{(1)}- a^2\beta^{(2)} \right\|  }{ a^2 n_1 + n_2 }\cdot \frac{p^{1/4} \sigma  }{(a^2 n_1 + n_2)^{1/2}} \nonumber\\ % \label{eq_hA1}
&\le \frac{ p^{-1/2} n_1^2 \left\|   a\beta^{(1)}- a^2\beta^{(2)} \right\|^2  }{ (a^2 n_1 + n_2)^2 }+ \frac{p^{1/2}  \sigma^2 }{a^2 n_1 + n_2} \lesssim p^{-1/2} \left[ L_{\bias}(a) + L_{\var}(a)\right],\nonumber
\end{align}
where we use \eqref{assm3} and \eqref{Op_norm}  in the third step, the AM-GM inequality in the 
fifth step, and \eqref{Op_norm2} in the last step. Similarly, we can show that 
\begin{align}
	 \left|h_2(a) \right|  \prec p^{-1/2} \left[ L_{\bias}(a) + L_{\var}(a)\right]. \nonumber
\end{align}
For $h_3(a)$, using \eqref{vcalA2} we obtain that   
\begin{align*}
	 &\left|h_3(a) -\sigma^2 a^2 \bigtr{X^{(1)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1}(X^{(1)})^\top } \right| \\ 
	 &\prec  \sigma^2 a^2  \left\| X^{(1)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1}(X^{(1)})^\top \right\|_F \lesssim  \frac{\sigma^2 a^2}{(a^2 n_1+ n_2)^2}  \left\| X^{(1)}  (X^{(1)})^\top \right\|_F \\
	 &  \lesssim \frac{\sigma^2 a^2 p^{1/2}n_1}{(a^2 n_1+ n_2)^2}    \le  \frac{\sigma^2  p^{1/2} }{ a^2 n_1+ n_2 } \lesssim    p^{-1/2}   L_{\var}(a).
\end{align*}
where we use \eqref{assm3} and \eqref{Op_norm}  in the second step, and $ \left\| X^{(1)}  (X^{(1)})^\top \right\|_F \le p^{1/2} \left\| X^{(1)}  (X^{(1)})^\top \right\| \lesssim p^{1/2}n_1$ in the last step. Similarly, we can show that  
\begin{align*}
	 &\left|h_4(a) -\sigma^2   \bigtr{X^{(2)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1}  (X^{(2)})^\top} \right| \prec   p^{-1/2}   L_{\var}(a), 
\end{align*}
and
$$ \left|h_5(a)   \right| \prec   p^{-1/2}   L_{\var}(a).$$
 Combining the above estimates on $h_i(a)$, $i=1,2,3,4,5$, and using that 
\begin{align*}
& \sigma^2 a^2 \bigtr{X^{(1)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1}(X^{(1)})^\top }  + \sigma^2   \bigtr{X^{(2)} \hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1}  (X^{(2)})^\top} \\
& =  \sigma^2 \bigtr{ \Sigma^{(2)} \hat \Sigma(a)^{-1}  }=L_{\var}(a),
\end{align*}
 we conclude that on event $\Xi_1$,
 \begin{align}\label{large_devh1}
L(\hat{\beta}_2^{\MTL}(a))  = \left[ 1+ \OO_\prec(p^{-1/2})\right]\cdot \left[L_{\bias} (a) + L_{\var}(a)\right] ,
 \end{align}
 for any fixed $a\in \R$. With a similar argument, we can obtain \eqref{L_AVE_simple} and \eqref{L_STL_simple}.
 
 
 Finally, it remains to extend the result \eqref{large_devh1} to all $a\in \R$ in a uniform way. Fix a large enough constant $C_0>0$, we consider $a$ belonging to a discrete set 
$$ S:=\left\{a_k = \left\lceil kp^{-C_0}\right\rceil: - p^{2C_0}  \le k \le p^{2C_0}\right\}.$$
Then combining \eqref{large_devh1} with a simple union bound, we obtain that for any small constant $c>0$, the event 
$$\Xi:=\left\{ \left|L(\hat{\beta}_2^{\MTL}(a)- L_{\bias} (a) - L_{\var}(a)\right|\le p^{-1/2+c}   \left[ L_{\bias} (a) + L_{\var}(a)\right] \ \text{for all $a\in S$}\right\} \cap \Xi_1$$
holds with high probability.  Using \eqref{Op_norm} and \eqref{Op_norm2}, it is easy to check that on $\Xi$,
for all $a_k \le a\le a_{k+1}$, $- p^{2C_0}  \le k \le p^{2C_0}-1$, the following deterministic estimates hold:
\begin{align*}
\left|L(\hat{\beta}_2^{\MTL}(a))-L(\hat{\beta}_2^{\MTL}(a_k))\right|   \lesssim p^{-C_0} \left( \|\ve^{(1)} \|^2  +  \|\ve^{(2)}\|^2 +\|\beta^{(1)} \|^2  +  \|\beta^{(2)} \|^2\right)  ,
\end{align*}
and 
\begin{align*}
 \left|L_{\bias}(a)-L_{\bias}(a_k)\right|  + \left|L_{\var}(a)-L_{\var}(a_k)\right|  \lesssim p^{-C_0} \left( \|\ve^{(1)} \|^2  +  \|\ve^{(2)}\|^2 +\|\beta^{(1)} \|^2  +  \|\beta^{(2)} \|^2\right)  .
\end{align*}
Similarly, we can also show that the above two estimates hold on $\Xi$ if $a\ge p^{C_0}$ and $a_k=a_{\left\lceil p^{C_0}\right\rceil}$, or if $a\le -p^{C_0}$ and $a_k=a_{-\left\lceil p^{C_0}\right\rceil}$. Now by triangle inequality, we get that on $\Xi$,
\begin{align}
\left|L(\hat{\beta}_2^{\MTL}(a))- L_{\bias} (a) - L_{\var}(a)\right| &\lesssim p^{-1/2+c}   \left[ L_{\bias} (a) + L_{\var}(a)\right]\nonumber \\
& + p^{-C_0} \left( \|\ve^{(1)} \|^2  +  \|\ve^{(2)}\|^2 +\|\beta^{(1)} \|^2  +  \|\beta^{(2)} \|^2\right) ,\label{large_uniform1}
\end{align}
holds simultaneously for all $a\in \R$. Notice that 
$$\|\ve^{(i)} \|^2   =n\sigma^2 + \OO_\prec(n^{1/2}\sigma^2) ,\quad i=1,2.$$
Plugging it into \eqref{large_uniform1}, we can conclude \eqref{L_HPS_simple} since $C_0$ is arbitrary. 

%Finally, for \eqref{loss_large}, using \eqref{Op_norm} and similar concentration estimates as above we obtain that 
% \begin{align*}
%		 g(0) = & \left\|  X^{(1)} \beta^{(1)} +\epsilon^{(1)}  \right\|^2 +  \left\|  \epsilon^{(2)}   \right\|^2 \le 2\left[n_1\|\beta^{(1)}\|^2 + (n_1+n_2)\sigma^2\right]
%	\end{align*}
%	with high probability, and that for $a\ge C_0$,	
%	 \begin{align*}
%		 g(a) \define & \left\| X^{(1)} \hat\Sigma(a)^{-1} (X^{(2)})^\top X^{(2)} (a\beta^{(2)}-\beta^{(1)}) \right. \nonumber\\
%			& \left. + \left(a^2 X^{(1)}\hat \Sigma(a)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)}+ a X^{(1)}\hat \Sigma(a)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2 \nonumber\\
%		   +& \left\| X^{(2)} \hat \Sigma(a)^{-1} (X^{(1)})^\top X^{(1)} (a\beta^{(1)}-a^2\beta^{(2)}) \right. \nonumber\\
%		  &\left.+ \left(X^{(2)}\hat\Sigma(a)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)\epsilon^{(2)} + a X^{(2)}\hat \Sigma(a)^{-1} (X^{(1)})^\top \epsilon^{(1)} \right\|^2.  
%	\end{align*}

\section{Proof of Theorem \ref{thm_many_tasks}}\label{app_proof_error_same_cov}

\iffalse  
For the optimization objective $f(A,B)$ in \eqref{eq_mtl_same_cov}, using the local optimality condition over $B$, that is, $\frac{\partial f}{\partial B} = 0$, we obtain $\hat{B}$ as a function of $A$:
	\begin{align}
		\hat{B}(A) &\define (X^{\top}X)^{-1} X^{\top} \bigbrace{\sum_{j=1}^t Y^{(j)} A_j^{\top}} (A  A^{\top})^{+} \nonumber \\
		&= (X^{\top} X)^{-1} X^{\top} Y A^{\top} (AA^{\top})^{+}, \label{eq_Bhat}
	\end{align}
	where $Y := [Y^{(1)}, Y^{(2)}, \dots, Y^{(t)}]$ and $(AA^{\top})^{+}$ denotes the pseudoinverse of $AA^{\top}$.
	%Here we have used that $X^{\top}X$ is invertible since $n > \rho \cdot p$ and $\rho > 1$ (cf. Fact \ref{fact_minv}).
	%\FY{Is $\dag$ a standard notation? It is a bad notation at least for me because $\dag$ is more often used as Hermitian conjugate. Wiki page uses $(AA^{\top})^{+}$ for pseudo-inverse.}
	Plugging $\hat{B}(A)$ into equation \eqref{eq_mtl_same_cov}, we obtain the following objective that only depends on $A$ (in matrix notation):
	\begin{align}\label{eq_mtl_output_layer}
		g(A) = \bignormFro{X (X^{\top}X)^{-1}X^{\top} Y A^{\top} (AA^{\top})^{+} A - Y}^2.
	\end{align}
	\fi	
	In this section, we give the proof of Theorem \ref{thm_many_tasks}. Note that $A^{\top} (AA^{\top})^{+} A$ is a projection onto the subspace spanned by the rows of $A$. For simplicity, we write it into the form
	$$A^{\top} (AA^{\top})^{+} A= U_A U_A^\top,$$
	where $U_A \in \R^{t\times r}$ is a $t\times r$ partial orthonormal matrix (i.e. $U_A^\top U_A=\id_{r\times r}$). Hence we also denote the function $g(A)$ by $g(U_A)$. 	
		%Recall that $(\hat{A}, \hat{B})$ is the global minimizer of $f(A, B)$, where $\hat{B}$ is equal to $\hat{B}(\hat{A})$ given by equation \eqref{eq_Bhat}.
	%Furthermore, $\hat{A}$ is a global minimizer of $g(A)$ in equation \eqref{eq_mtl_output_layer}.
	First, we can use the concentration estimate, Corollary \ref{cor_calE}, to simplify the expression of $g(U_A)$.
	 In this section, we always let $Q=n^{2/\varphi}$.
	%The proof of Theorem \ref{thm_many_tasks} is also based on the estimates in Appendix \ref{app_tool}. We divide the proof into several claims.
	
	%		\begin{align}
%			\exarg{\set{\varepsilon^{(j)}}_{j=1}^t, X}{g(A)} = n \bignormFro{\Sigma^{1/2} B^{\star} \bigbrace{A^{\top} (AA^{\top})^{+} A - \id_{t\times t}}}^2 + \sigma^2 (n\cdot t - p \cdot r). \label{eq_gA}
% 


	\begin{lemma}\label{lem_exp_opt}
		In the setting of Theorem \ref{thm_many_tasks}, for any small constant $c>0$ and large constant $C>0$, there exists a high probability event $\Xi$, on which the following estimate holds:
		  \begin{align}
		\bigabs{g(U_A) - h(U_A)} &\le  Q n^{1/2+c} \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id_{t\times t})}_F^2 \nonumber\\
		& +  \sigma^2 n^{ 1/2+c} + p^{-C} \bignorm{\Sigma^{1/2} B^{\star}}_F^2,\label{eq_gA_err}
	\end{align}
	uniformly in all rank-$r$ partial orthonormal matrices $U_A\in \R^{t\times r}$. Here %$h(A)$ is
	\be\label{same_hA}h(U_A):= n \bignormFro{\Sigma^{1/2} B^{\star} \bigbrace{U_{A} U_{A}^{\top} - \id_{t\times t}}}^2 + \sigma^2 (n t - p r).\ee		
	\end{lemma}
	\begin{proof} %[Proof of Claim \ref{lem_exp_opt}]
	With Corollary \ref{fact_minv} and Corollary \ref{cor_largedeviation}, we can choose a high probability event $\Xi_1$ on which \eqref{Zv_cor} holds and 
%that will be commonly used in the proof. First, by Corollary \ref{fact_minv}, we  
\be\label{Op_norm3}
C^{-1} n \le \lambda_p ( Z^\top Z)  \le  \lambda_1( Z^\top  Z) \le Cn
\ee
for a large constant $C>0$. Throughout the following proof, we assume that event $\Xi_1$ holds.

	
	To facilitate the analysis, we introduce the following matrix notations.
	Denote
		%\[ \cE A^{\top} := {\sum_{k=1}^t \varepsilon^{(k)} A_k^{\top}}, \]
\[\cE  :=[\epsilon^{(1)},\epsilon^{(2)},\cdots, \epsilon^{(t)}],  \quad \text{ and } \quad \cW \define X(X^{\top} X)^{-1} X^{\top} \cE U_AU_A^\top. \]
	For any $j = 1,2,\dots, t$, let
	\begin{align}\label{defnHj}
		H_j &\define  B^{\star} \bigbrace{U_{A} U_{A}^{\top} - \id_{t\times t}}e_j , \quad \text{ and } \quad E_j \define (\cW -\cal E)e_j .
	\end{align}
	where $e_j$ is the standard basis unit vector along $j$-th direction. 
	Then plugging $Y=XB^\star + \cal E$ into \eqref{eq_mtl_output_layer}, we can write the function $g(U_A)$   as
	\[ g(A) = \sum_{j=1}^t \bignorm{X H_j + E_j}^2. \]
	We will divide $g(A)$ into three parts.
%	For simplicity, we will use matrix notations in the proof, %since they are more compact.
%	that is, stacking $[H_j]_j$ gives matrix $B^{\star} A^{\top} (AA^{\top}) A - B^{\star}$,
%	and stacking $[E_j]_j$ gives $\cW A - \cE$.

\medskip
	\noindent{\bf Part 1:} The first part is %the square of $XH_j$,
	\begin{align*} %\label{eq_gA_p1}
		\sum_{j=1}^t \bignorm{X H_j}^2
				= \bignormFro{X B^{\star} \bigbrace{U_{A} U_{A}^{\top} - \id_{t\times t}}}^2.
	\end{align*}
	%where $U_{A} U_{A}^{\top} \in\real^{t\times t}$ denotes the subspace projection $ {A}^{\top} ( {A}{A}^{\top})^{+}  {A}$.
%	Taking expectation of equation \eqref{eq_gA_p1} over $X$, we get
%		\[ \sum_{j=1}^t \bignorm{X H_j}^2=n\bignorm{\Sigma^{1/2}(B^{\star} U_A U_A^{\top} - B^{\star})}^2. \]
%For equation \eqref{eq_gA_p1}, % $g_0(\cal W)= \sum_{j=1}^t\left\|Z v_j \right\|^2$.
%	$$g_0(\cal W)= \sum_{j=1}^t\left\|Z v_j \right\|^2,\quad v_j:= \Sigma^{1/2}\left(B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j -  \beta_j\right).$$
%	note that $X H_j = Z \Sigma^{1/2} H_j \in \real^n$ is a random vector with i.i.d. entries of mean zero, variance $\|\Sigma^{1/2}H_j\|^2$, and finite $\varphi$-th moment by Assumption \eqref{assume_rm}.
%	Hence by the law of large numbers, we have that with high probability	
	Applying Corollary \ref{cor_largedeviation} to $X H_j = Z \Sigma^{1/2} H_j$, we obtain that 
	\begin{align*}
	\bignorm{X H_j}^2
 &= { n \|\Sigma^{1/2} H_j\|^2} \cdot \left[1 + \OO_\prec (n^{-1/2}Q)\right] \\
 &= n \bignorm{\Sigma^{1/2} B^{\star} \bigbrace{U_{A} U_{A}^{\top} - \id_{t\times t}}e_j}^2\cdot \left[1 + \OO_\prec (n^{-1/2}Q)\right] .
 \end{align*}
	%on a high probability event, which we denote by $\Xi_1$. 
	This implies that %on $\Xi$,
	\begin{align*}
		\abs{\sum_{j=1}^t \|X H_j\|^2 -  n \bignormFro{\Sigma^{1/2} B^{\star} \bigbrace{U_{A} U_{A}^{\top} - \id_{t\times t}}}^2 } \prec Q   n^{1/2} \bignorm{\Sigma^{1/2} B^{\star} (U_{A}U_A^{\top} - \id_{t\times t})}^2. %\label{eq_gA_err1}
	\end{align*}

\medskip
	\noindent{\bf Part 2:}  The second part is the cross term %, which is given by %the following using the matrix notations:
	\begin{align*}%\label{eq_gA_p2}
		2\sum_{j=1}^t\inner{XH_j}{E_j} = 2\inner{XB^{\star} (U_A U_A^{\top} - \id_{t\times t})}{\cW  - \cE}
		= - 2\inner{X B^{\star} (U_A U_A^{\top} - \id_{t\times t})}{\cE}.
	\end{align*}
	%which is zero in expectation over $\cE$.
	%For equation \eqref{eq_gA_p2}, 
	%using Lemma \ref{largedeviation} in Appendix \ref{app_tool} and the fact that all moments of $\varepsilon_i$ exist by Assumption \eqref{assmAhigh2}, 
	Using \eqref{vcalE}, we obtain that % following with high probability:
	\begin{align*}
		&  |\inner{XB^{\star} (U_AU_A^{\top} - \id_{t\times t})}{\cE}|   \prec \sigma   \bignormFro{XB^{\star} (U_AU_A^{\top} - \id_{t\times t})} \nonumber \\
		%&\le  \sigma \norm{Z} \cdot \bignormFro{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id_{t\times t})} \nonumber  \\
		&\lesssim \sigma n^{ 1/2}  \bignormFro{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id_{t\times t})}   \le \sigma^2 n^{1/2} + n^{ 1/2}  \bignormFro{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id_{t\times t})}^2.% \label{eq_gA_err2}
%		&\le  n^{c_\infty +c} \cdot \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id_{t\times t})}^2 + n^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
	\end{align*}
	Here in the second step we use the fact that $X=Z\Sigma^{1/2}$ and \eqref{Op_norm3}, and in the third step we use the AM-GM inequality. % equation \eqref{eq_gA_err2} is bounded by the right-hand side of \eqref{eq_gA_err}.

\medskip
	\noindent{\bf Part 3:}  The last part is %the square of $E_j$:
	\begin{align}\label{eq_gA_p3}
		\sum_{j=1}^t \norm{E_j}^2 &= \bignormFro{\cW  - \cE}^2
		= \bignormFro{\cE}^2 - \inner{\cW }{\cE},
	\end{align}
	where in the second step we use $\norm{\cW}_F^2 = \inner{\cW A}{\cE}$ by algebraic calculation.
	By \eqref{vcalA2}, we have that
	\be\label{eq_gA_err2} \left| \| \ve^{(i)}\|^2 - \sigma^2 n\right\| \prec \sigma^2 n^{1/2},\ee
	and
	\begin{align}
	 &  \left|(\epsilon^{(i)})^\top X(X^{\top} X)^{-1} X^{\top} \epsilon^{(j)} - \delta_{ij}\cdot p\sigma^2 \right| \nonumber\\
	 & =\left|(\epsilon^{(i)})^\top X(X^{\top} X)^{-1} X^{\top} \epsilon^{(j)} - \delta_{ij}\cdot \sigma^2 \tr[X(X^{\top} X)^{-1} X^{\top} ]\right| \nonumber\\
	 & \prec \sigma^2 \left\|X(X^{\top} X)^{-1} X^{\top}\right\|_F = \sigma^2\left\{ \tr\left[ X(X^{\top} X)^{-1} X^{\top}\right] \right\}^{1/2}= \sigma^2 p^{1/2}, \label{eq_gA_err3.5}
	\end{align}
	where we also use $ \tr[X(X^{\top} X)^{-1} X^{\top} ]= \tr [\id_{p\times p}] =p$ in the above derivation. 
	%For equation \eqref{eq_gA_p3}, using Corollary \ref{cor_calE},
	%Lemma \ref{largedeviation} and the fact that all moments of $\varepsilon_i$ exist, 
	Summing \eqref{eq_gA_err2} over $i$, we obtain that %with high probability,
	\begin{align}
		\abs{\normFro{\cE}^2 - \sigma^2 n t}\le \sum_{i=1}^t \left| \|\ve^{(i)}\|^2 - \sigma^2 n \right|\prec  \sigma^2 n^{1/2}. \label{eq_gA_err3}
	\end{align}
	Using \eqref{eq_gA_err3.5}, we can estimate the inner product between $\cW $ and $\cE$ as %with high probability,
	\begin{align}
		\bigabs{\inner{\cW }{\cE} - \sigma^2 pr} &= \bigabs{\bigtr{\left(\cE^\top U_X U_X^{\top}\cE  - p\sigma^2 \cdot  \id_{t\times t} \right)  U_AU_A^{\top} }} \nonumber  \\
		&\le \bignormFro{U_{A} U_A^{\top}} \cdot \bignorm{\cE^\top U_X U_X^{\top} \cE - p \sigma^2 \cdot  \id_{t\times t}} \prec \sigma^2 n^{1/2} .\label{eq_gA_err4}
		%&\prec \sqrt{r} \cdot n^{\e}\cdot \sigma^2\cdot \|U_X U_X^\top\|_F \nonumber \\
		%&\le \sqrt{r} \cdot n^{1/2+c} \cdot \sigma^2.  %  \le  n^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
	\end{align}
	%Here in the third step, we apply equation \eqref{vcalA2} to $\bignorm{\cE^\top U_X U_X^{\top} \cE - p \sigma^2 \cdot  \id_{t\times t}}$ and use that $ \bignormFro{U_{A} U_A^{\top}}=\sqrt{r}$ because $U_A$ has rank $r$. In the fourth step, we use $\|U_X U_X^\top\|_F=\sqrt{p}$ because $U_X$ has rank $p$.
	Combining \eqref{eq_gA_err3} and \eqref{eq_gA_err4}, we obtain that 
	\begin{align*}%\label{eq_gA_p3}
		\sum_{j=1}^t \norm{E_j}^2 &=\sigma^2 (nt-pr)+  \OO_\prec(\sigma^2 n^{1/2} ).
	\end{align*}
%	Hence, it suffices to show that the expectation of equation \eqref{eq_gA_p3} is equal to $\sigma^2 (n\cdot t - p\cdot r)$.
%	First, we have that $\ex{\|\cE\|_F^2} = \sigma^2 \cdot n \cdot t$.
%%	Conditional on $X$, we have that
%%	\begin{align*}
%%		\exarg{\cE}{\norm{E_j}^2}
%%		= \exarg{\cE}{\sum_{j=1}^t \bigbrace{\bignorm{\cW A_j}^2 - 2\inner{\cW A_j}{\varepsilon^{(j)}} }} + \sigma^2 \cdot n \cdot t.
%%	\end{align*}
%%	We consider both terms in the above equation one by one.
%	Second, we show that
%	%	\begin{align*}
%	%		\inner{\cW A}{\cE} = \sigma^2 \cdot r \cdot \bigtr{U_X U_X^{\top}} = \sigma^2 \cdot r \cdot p,
%	%	\end{align*}
%	%because $U_X$ has rank $p$ by Fact \ref{lem_minv}.
%	\begin{align*}
%		\exarg{\cE}{ \inner{\cW A}{\cE}} =\exarg{\cE}{\bigtr{\cal E^\top U_X U_X^\top \cal E U_AU_A^\top} }= p\sigma^2 \cdot \bigtr{ U_AU_A^\top} =  p \sigma^2 \cdot r,
%	\end{align*}
%	where $U_XU_X^\top = X(X^{\top} X)^{-1} X^{\top}$.
%	The first step follows by applying the definition of $\cW$.
%	The last step is because $U_AU_A^\top$ has rank $r$.
%	Hence, it suffices to show the second step is correct.
%	For any $1\le i, j \le t$, let $\delta_{i, j} = 1$ if $i = j$, and $0$ otherwise.
%	%\begin{align}
%	%	\sum_{j=1}^t \norm{\cW A_j}^2 = \sum_{j=1}^t \bigtr{\cW A_j A_j^{\top} \cW^{\top}} = \bigtr{\cW AA^{\top} \cW^{\top}} = \bigtr{U_X U_X^{\top} \cE A^{\top} (AA^{\top})^{-1} A \cE^{\top}}. \label{eq_proof_same_cov_1}
%	%\end{align}
%	%We observe that
%	Because $\varepsilon^{(i)}$ and $\varepsilon^{(j)}$ are pairwise independent, we have that
%	\begin{align*}
%			 \exarg{\cE}{(\cE^{\top}U_XU_X^\top \cE)_{ij} }
%		&= \exarg{\cE}{{\varepsilon^{(i)}}^\top U_X U_X^\top  \varepsilon^{(j)} } = \sigma^2 \cdot \tr\left[U_XU_X^\top \right] \cdot \delta_{ij} = p\sigma^2 \cdot \delta_{ij}.
%	\end{align*}
%	The last step uses the fact that $\tr[U_XU_X^\top] = p$.
%	Hence, the second step is correct.
%	%Above, the second step used the fact that $\E(\varepsilon^{(i)}_k \varepsilon^{(i)}_l )=\sigma^2\cdot \delta_{ij}\delta_{kl}$ for $1\le k,l \le n$, and the third step used the fact that $\tr(U_XU_X^\top) =\tr(U_X^\top U_X) =p$.
%	%Next, note that
%	%\[ \sum_{j=1}^t \inner{\cW A_j}{\varepsilon^{(j)}} = \bigtr{\cW A\cE^{\top}}, \]
%	%which is equal to equation \eqref{eq_proof_same_cov_1} by the definition of $\cW$.
%
 

	
%	Corresponding to the right-hand side of \eqref{eq_gA}, we define the function
%	\be\label{same_hA}h(A):= n \bignormFro{\Sigma^{1/2} B^{\star} \bigbrace{A^{\top} (AA^{\top})^{+} A - \id_{t\times t}}}^2 + \sigma^2 (n\cdot t - p \cdot r).\ee
%	Let $\e$ be a fixed constant that is sufficiently small.
%	Let $c_{\infty}$ be any fixed value within $(0, 1/2 - \e)$.
%	To show that $U_{\hat{A}}U_{\hat{A}}^\top$ is close to $A^\star {A^\star}^\top$, we first show that $g(A)$ is close to $h(A)$ as follows:
%	\begin{align}\label{eq_gA_err}
%		\bigabs{g(A) - h(A)} \lesssim  n^{-c_{\varphi}} \cdot n \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id_{t\times t})}_F^2 + n^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
%	\end{align}
%%	\begin{align}\label{eq_gA_err}
%%		\bigabs{g(A) - \exarg{\cE, X}{g(A)}} \lesssim p^{-c_{\varphi}} \cdot n \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id)}^2 + p^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
%%	\end{align}
%	We consider the concentration error of each part of $g(A)$.

	

	

		

	Combining the concentration error estimates for all three parts, we obtain that on event $\Xi_1$,
	\begin{align*}
	\bigabs{g(U_A) - h(U_A)} &\prec  Q n^{1/2} \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id_{t\times t})}_F^2  +  \sigma^2 n^{ 1/2} ,
		\end{align*}
	for any fixed $U_A$. Then using a similar $\ve$-net argument as in Appendix \ref{app_firstpf}, we can obtain  \eqref{eq_gA_err}. We omit the details.
	\end{proof}
	
	
	
%	Our main idea is to show that the subspaces spanned by the rows of $\hat{A}$ and $A^{\star}$ are close to each other.
%	We carefully keep track of the concentration error between $\hat{A}$ and $A^{\star}$.
%	The proof can be found in Section \ref{app_proof_error_same_cov}.
%	We fill in missing details in the proof. 
	
	From \eqref{same_hA}, it is easy to see that the global minimizer of $h(U_A)$ is the best rank-$r$ approximation of ${B^{\star}}^{\top}\Sigma B^{\star}$, $A^\star$, defined in \eqref{eq_A_star}. On the other hand, let $U_{\hat A}$ be the global minimizer of $g(U_A)$. We have the following characterization of $U_{\hat A}U_{\hat A}^\top$ based on Lemma \ref{lem_exp_opt}.
	%As a result, the minimum of $\ex{g(A)}$, denoted by $A^{\star}{A^\star}^\top$, is the best rank-$r$ approximation of ${B^{\star}}^{\top}\Sigma B^{\star}$. 
	
		
	%Our first claim shows that the subspace spanned by the rows of $\hat{A}$ is close to that of $A^{\star}$.
	\begin{lemma}\label{claim_opt_dist}
		%Let $U_{\hat{A}} U_{\hat{A}}^{\top} \in\real^{t\times t}$ denote the subspace projection $\hat{A}^{\top} (\hat{A}\hat{A}^{\top})^{+} \hat{A}$.
		Let  In the setting of Theorem \ref{thm_many_tasks}, we have that %for any small constant $c>0$,
		\[ \bignormFro{U_{\hat{A}} U_{\hat{A}}^{\top} - A^{\star} {A^{\star}}^{\top}}^2
				\lesssim  n^{-1/2+c}  \frac{ Q  \|{B^{\star}}^\top\Sigma B^{\star}\|  +  \sigma^2 }{\lambda_r - \lambda_{r+1} } ,\]
				on the high probability event $\Xi$ in Lemma \ref{lem_exp_opt}.
	\end{lemma}
	
	%The proof of the above claim is based on the following characterization.
\begin{proof}%[Proof of Claim \ref{claim_opt_dist}]
%Next, we use equation \eqref{eq_gA_err} to prove the claim.
	Using equation \eqref{eq_gA_err} and triangle inequality, we upper bound the gap between $h(A^{\star})$ and $h(U_{\hat{A}})$ as
	\begin{align}
		h(U_{\hat{A}})- h(A^{\star})   &\le (g(U_{\hat{A}}) - g({A}^{\star})) + \bigabs{g(A^{\star}) - h(A^{\star})} +  \big|g(U_{\hat{A}}) - h(U_{\hat{A}})\big| \nonumber \\
		&\le \bigabs{g(A^{\star}) - h(A^{\star})}  + \big|g(U_{\hat{A}}) - h(U_{\hat{A}})\big|  \lesssim Q n^{1/2+c} \|\Sigma^{1/2} B^{\star}\|_F^2 +  \sigma^2 n^{ 1/2+c}   ,\label{eq_g_gap}
		%&\lesssim n^{-c_\varphi}\cdot n \bignormFro{\Sigma^{1/2} B^{\star}}^2 + n^{-c_\infty}\cdot \sigma^2 \cdot n \cdot t. \label{eq_g_gap}
	\end{align}
	on event $\Xi$.
	Here in the second step, we use the fact that $\hat A$ is the global minimizer of $g(\cdot)$, so that $g(U_{\hat{A}}) \le g({A}^\star)$, and in the third step we use equation \eqref{eq_gA_err},  $\|U_A U_A^{\top} - \id_{t\times t}\|\le 1$ and $p^{-C}\le Qn^{1/2+c}$. 
	%With \eqref{eq_g_gap}, we can derive an upper bound on $\bignormFro{A^{\star} {A^{\star}}^{\top}- U_{\hat{A}} U_{\hat{A}}^{\top}}$ as follows. 
	Using the definition of $h(U_A)$ in \eqref{same_hA}, we can check that
	$$h(U_{\hat{A}})-h(A^\star) = n \bigtr{{B^\star}^\top\Sigma B^\star ( A^\star {A^\star}^\top -U_{\hat A}U^\top_{\hat A})} .$$
	For $1\le i \le t$, let $\lambda_i $ be the $i$-th largest eigenvalue of ${B^\star}^\top\Sigma B^\star$, and $v_i$ be the corresponding eigenvector.
	Then we have $A^\star {A^\star}^\top =\sum_{i=1}^r v_i v_i^\top$, and
	\begin{align}
	 h(U_{\hat{A}})-h(A^\star) & = n \sum_{i=1}^r \lambda_i - n\sum_{i=1}^t \lambda_i \| U^\top_{\hat A} v_i\|^2  \nonumber\\
	&= n\sum_{i=1}^r \lambda_i\left(1 -  \| U^\top_{\hat A} v_i\|^2\right)-n\sum_{i=r+1}^t \lambda_i \| U^\top_{\hat A} v_i\|^2 \nonumber\\
	& \ge  n(\lambda_r-\lambda_{r+1}) \sum_{i=r+1}^t \| U^\top_{\hat A} v_i\|^2 , \label{bdd_A-A}
	\end{align}
	where in the last step we use that
	$$\sum_{i=1}^r \left(1 -  \| U^\top_{\hat A} v_i\|^2\right) = r- \sum_{i=1}^r  \| U^\top_{\hat A} v_i\|^2 =\sum_{i=r+1}^t \| U^\top_{\hat A} v_i\|^2  .$$
	From equations \eqref{eq_g_gap} and  \eqref{bdd_A-A}, we obtain that
	\be\label{bdd_A-A2} \sum_{i=r+1}^t \| U^\top_{\hat A} v_i\|^2 \lesssim \frac{Qn^{-1/2+c} \bignormFro{\Sigma^{1/2} B^{\star}}^2 + n^{-1/2+c} \sigma^2 }{\lambda_r- \lambda_{r+1}}  .\ee
	On the other hand, we have
	\begin{align*}
	\left\| A^\star {A^\star}^\top -U_{\hat A}U^\top_{\hat A}\right\|_F^2
	&= 2r - 2\left\langle A^{\star}{A^{\star}}^{\top}, U_{\hat{A}} {U_{\hat{A}}}^{\top}\right\rangle = 2 \sum_{i=r+1}^t \| U^\top_{\hat A} v_i\|^2.
	\end{align*}
	Combining it with \eqref{bdd_A-A2} and using $\|\Sigma^{1/2} B^{\star}\|_F^2\lesssim  \|{B^{\star}}^\top\Sigma B^{\star}\| $, we conclude the proof.
	%Hence the proof is complete.
	\end{proof}
	
	%Let $\hat{A}$ be a global minimizer of $g(A)$, so that $(\hat{A}, \hat{B}(\hat A))$ is a global minimizer of $f(A, B)$.  
	 %One can see that the expected optimization objective also admits a nice bias-variance decomposition.
	%Furthermore, its minimum only depends on the bias term since the variance term is fixed, and the minimizer of the bias term is precisely $A^{\star} {A^{\star}}^{\top}$.

	The last piece of the proof of Theorem \ref{thm_many_tasks} is the following concentration estimate on the prediction loss of $\hat{\beta}_i^{\MTL}(A)$. Its proof is similar to those of Lemma \ref{lem_HPS_loss} and Lemma \ref{lem_exp_opt}. 
	\begin{lemma}\label{claim_pred_err}
		In the setting of Theorem \ref{thm_many_tasks}, let 
		$$ \hat{\beta}_i^{\MTL}(A)\equiv \hat{\beta}_i^{\MTL}(U_A):= \hat{B}(A)A=(X^{\top} X)^{-1} X^{\top} Y U_A U_A^{\top} ,$$
		and ${a}_i := U_A U_A^{\top} e_i.$
	 Then for any small constant $c>0$ and large constant $C>0$, there exists a high probability event $\Xi$, on which the following estimate holds:
		%We have that the prediction loss of $\hat{\beta}_i^{\MTL} := \hat{B} \hat{A}_i$ satisfies that
		\begin{align}
			&\bigabs{L_i(\hat{\beta}_i^{\MTL}(U_A)) - L_i(B^{\star} {a}_i) - \sigma^2 \norm{{a}_i}^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}}\nonumber \\
			&\le  n^{-1/2+c} \left[L_i(B^{\star} {a}_i) + \sigma^2 \| a_i\|^2\right] + p^{-C}  \bignorm{\Sigma^{1/2} B^{\star}}_F^2 ,\label{eq_Li_err}
		\end{align}
		uniformly in all rank-$r$ partial orthonormal matrices $U_A\in \R^{t\times r}$, where $L_i$ is defined
in \eqref{ith_loss}.
\end{lemma}
	

	\begin{proof}%[Proof of Claim \ref{claim_pred_err}]
	%We apply similar arguments from Claim \ref{lem_exp_opt} and \ref{claim_opt_dist} for this proof.
	We choose a high probability event $\Xi_1$ on which \eqref{Op_norm3} holds.	
	For a fixed $U_A$, the prediction loss of $\hat{\beta}_i^{\MTL}(A)$ for task $i$ is  
	\begin{align*}
		L(\hat{\beta}_i^{\MTL}(U_A)) %&= \bignorm{\Sigma^{1/2} (\hat{B} \hat{A}_i - \beta^{(i)})}^2 \\
		&= \bignorm{\Sigma^{1/2} \left((X^{\top} X)^{-1} X^{\top} Y a_i - \beta^{(i)}\right)}^2  = \bignorm{\Sigma^{1/2} (H_i + R_i)}^2,
	\end{align*}
	where we denote $R_i = (X^{\top} X)^{-1} X^{\top} \cE  {a}_i$ and $H_i$ is defined in \eqref{defnHj}. Then the rest of the proof is similar to that of Lemma \ref{lem_exp_opt}. We divide the prediction loss into three parts. 
	
	\medskip
	\noindent{\bf Part 1:} The first part is the bias term $\norm{\Sigma^{1/2}H_i}^2 = L_i(B^{\star}  {a}_i).$

\medskip
	\noindent{\bf Part 2:} The second part is the cross term
	$2\inner{\Sigma^{1/2}H_i}{\Sigma^{1/2}R_i}$.
	%, whose expectation over $\cE$ is zero.
	%Let $b = B^{\star} \hat{a}_i - \beta^{(i)}$ for simplicity.
	%Using equation \eqref{vcalE}, 
	We can bound it as %the concentration error can be bounded as
	\begin{align*}
		 \bigabs{\inner{\Sigma^{1/2}H_i}{\Sigma^{1/2}R_i}} &= \bigabs{\inner{X (X^{\top}X)^{-1} \Sigma H_i  }{\cE{a}_i}}  \le \sum_{j=1}^t \abs{ {a}_i(j)} \cdot \bigabs{\inner{X (X^{\top} X)^{-1} \Sigma H_i}{\varepsilon^{(j)}}} \\
		&\prec \sum_{j=1}^t \abs{ {a}_i(j)} \cdot  \sigma \bignorm{X (X^{\top}X)^{-1} \Sigma H_i}  \lesssim   \frac{\norm{ {a}_i}   \sigma}{ n^{1/2} }\bignorm{ \Sigma^{1/2}H_i} \\
		&\le  n^{-1/2}\sigma^2  \norm{ {a}_i}^2 + n^{-1/2}L_i(B^\star a_i).
	\end{align*}
%	\begin{align*}
%		\bigabs{\inner{\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)}{\Sigma^{1/2}R_i}} &= \bigabs{\inner{\hat a_i^\top X(X^\top X)^{-1}\Sigma (B^{\star} \hat{a}_i - \beta_i)}{\cal E}} \\
%		&\le n^{c}\sigma \cdot \bignormFro{\hat a_i^\top X(X^\top X)^{-1}\Sigma (B^{\star} \hat{a}_i - \beta_i)}\\
%		&\le n^{c}\sigma \cdot\|\hat a_i\| \cdot  \|\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)\| \cdot \| (Z^\top Z)^{-1} \|^{1/2} \\
%		&\lesssim n^{-1/2+c}\sigma \cdot\|\hat a_i\| \cdot \bignorm{\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)} \\
%		&\le n^{-1/2+c}\sigma^2  \cdot\|\hat a_i\|^2 + n^{-1/2+c} L(B^{\star} \hat{a}_i), 
%	\end{align*}
	%In the first step, we plug in the definition of $R_i$ and re-arrange terms.
	Here in the second step we use $ {a}_i(j)$ to denote the $j$-th coordinate of $ {a}_i$, in the third step we use  \eqref{vcalE}, in the fourth step we use \eqref{Op_norm3}, \eqref{assm3} and $\sum_j |a_i(j)|\le \sqrt{t}\|a_i\|$ by Cauchy-Schwarz inequality, and in the last step we use AM-GM inequality and $\bignorm{\Sigma^{1/2}H_i}^2= L_i(B^\star a_i)$.
%	  Finally, we have
%	\begin{align*}
%		\bignormFro{ X(X^\top X)^{-1}\Sigma b} &=\left[ {b}^\top \Sigma (X^\top X)^{-1}X^\top X(X^\top X)^{-1}\Sigma b \right]^{1/2} \\
%		&\le  \|\Sigma^{1/2} b \|  \cdot \left\| \Sigma^{1/2}(X^\top X)^{-1}\Sigma ^{1/2} \right\|^{1/2}  \\
%		&=   \|\Sigma^{1/2} b \|  \cdot \left\| (Z^\top Z)^{-1}\right\|^{1/2}  \\
%		& \lesssim n^{-1/2} \cdot \norm{\Sigma^{1/2} b}.
%	\end{align*}
%%	\begin{align*}
%%	\bignormFro{\hat a_i^\top X(X^\top X)^{-1}\Sigma (B^{\star} \hat{a}_i - \beta_i)} &=\|\hat a_i\|\cdot \left[ (B^{\star} \hat{a}_i - \beta_i)^\top \Sigma (X^\top X)^{-1}X^\top X(X^\top X)^{-1}\Sigma (B^{\star} \hat{a}_i - \beta_i) \right]^{1/2} \\
%%	&\le \|\hat a_i\|\cdot  \|\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)\|  \cdot \left\| \Sigma^{1/2}(X^\top X)^{-1}\Sigma ^{1/2} \right\|^{1/2} \\
%%	&= \|\hat a_i\|\cdot  \|\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)\|  \cdot \left\| (Z^\top Z)^{-1}\right\|^{1/2}
%%	\end{align*}
%	Above, we use $X=Z\Sigma^{1/2}$. In the last step, we use Fact \ref{fact_minv}(ii) to bound the operator norm of $Z^\top Z$ by $\OO(n^{-1})$. %, and apply the AM-GM inequality.
%	One can see that the concentration error from this part is upper bounded by the result in Claim \ref{claim_pred_err}.
	
	\iffalse
	Next, we use the fact that the spectral norm of $\cE$ is at most $\sigma \cdot p^{\e}$.
	Hence, the spectral norm of $\Sigma^{1/2} E_i$ is at most
		\[ \sigma \cdot p^{\e} \cdot \norm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}} \cdot \norm{\hat{a}_i}. \]
	Therefore, the cross term is bounded by $\sigma \cdot p^{\e}$ times
	\begin{align*}
			& \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i)} \cdot \norm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}} \cdot \norm{\hat{a}_i} \\
		\le & \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i)}^2 + \norm{\hat{a}_i}^2 \cdot \bignorm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}}^2 \tag{by Cauchy-Shwartz inequality} \\
		\le & \bignorm{\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)} + \norm{\hat{a}_i}^2 \cdot \bigtr{\Sigma (X^{\top} X)^{-1}}.
	\end{align*}
	\fi

\medskip
	\noindent{\bf Part 3:}  The final part is %the squared term of $R_i$%. We rewrite it as
	\begin{align}
	\|\Sigma^{1/2}R_i\|^2 &= \left\|\sum_{j=1}^t  {a}_i(j) \Sigma^{1/2}(X^{\top} X)^{-1} X^{\top} \epsilon^{(j)}\right\|^2 \nonumber\\
	& = \sum_{1\le j , k \le t} a_i(j)  a_i(k)  {\epsilon^{(j)}}^\top X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} {\epsilon^{(k)}}.\label{E_i^2}
	\end{align}	
%	If $j\ne k$, using Corollary \ref{cor_calE} we obtain that 
%	\begin{align}
%	\left|{\epsilon^{(j)}}^\top X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} {\epsilon^{(k)}} \right| &\le \sigma^2 \cdot \bignormFro{X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} } \nonumber\\
%	& =\sigma^2 \cdot \bignormFro{\Sigma^{1/2}(X^{\top} X)^{-1}\Sigma^{1/2}} \nonumber\\
%	&  \le \sigma^2 \cdot p^{1/2} \cdot \bignorm{(Z^{\top} Z)^{-1}} \lesssim \sigma^2 \cdot n^{-1/2+c} \label{jnek}
%	\end{align}
%	with high probability for any small constant $c>0$. On the other hand, if $j=k$, 
%	First, for any $1 \le j, k \le t$, the expectation is
%	$$\exarg{\cal E}{{\epsilon^{(j)}}^\top X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} \epsilon^{(k)}}=\delta_{jk}\cdot \sigma^2\tr\left[\Sigma(X^\top X)^{-1}\right] .$$
	Using \eqref{vcalA2} and $\tr[X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top}]=\tr[ \Sigma (X^{\top} X)^{-1}] $, we obtain that %the concentration error is at most
	\begin{align}
	& \left|{\epsilon^{(j)}}^\top X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} {\epsilon^{(k)}} -\delta_{jk}\cdot \sigma^2\tr\left[\Sigma(X^\top X)^{-1}\right]\right|\nonumber \\
	& \prec \sigma^2  \bignormFro{X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} } = \sigma^2  \bignormFro{\Sigma^{1/2}(X^{\top} X)^{-1}\Sigma^{1/2}} \nonumber\\
	 & \lesssim \sigma^2 p^{1/2} n^{-1} \le  \sigma^2  n^{-1/2 }, \label{jeqk}
	\end{align}
	where we use \eqref{Op_norm3} in the third step.
	%Above, we used Fact \ref{fact_minv}(ii) in the last step to bound the operator norm of $(Z^{\top} Z)^{-1}$.
	Plugging \eqref{jeqk} into \eqref{E_i^2}, we obtain that 
	\begin{align*}
		\bigabs{\bignorm{\Sigma^{1/2} R_i}^2 -  \sigma^2 \|\hat a_i\|^2 \cdot\tr\left[\Sigma(X^\top X)^{-1}\right]} &\prec  \sigma^2  n^{-1/2 }  \sum_{1\le j,k\le t}|\hat a_i(j)||\hat a_i(k)| \\
		&\lesssim   n^{-1/2} \sigma^2 \|\hat a_i\|^2.
	\end{align*}
%	One can verify that the expectation of $\norm{\Sigma^{1/2} E_i}^2$ is equal to $\sigma^2 \norm{\hat{a}_i}^2 \cdot \tr[\Sigma (X^{\top} X)^{-1}]$.
%	Next, we bound the concentration error.
%	Let $A = X (X^{\top} X)^{-1} \Sigma (X^{\top} X)^{-1} X^{\top}$.
%	We have that
%	\begin{align*}
%		\bigabs{\bignorm{\Sigma^{1/2} E_i}^2 - \exarg{\cE}{\bignorm{\Sigma^{1/2} E_i}^2}}
%		&= \bigabs{\inner{A}{\cE \hat{a}_i \hat{a}_i^{\top} \cE^{\top} - \exarg{\cE}{\cE \hat{a}_i \hat{a}_i^{\top} \cE}}}.
%	\end{align*}
%	In the above equation, we note that the trace of $A$ is precisely the trace of $\Sigma (X^{\top} X)^{-1}$.
%	On the other hand, the concentration error of the spectral norm of $\cE \hat{a}_i \hat{a}_i^{\top} \cE^{\top}$ is at most $\sigma^2 \e $.

	 
	Combining the concentration error estimates for all three parts, we obtain that on event $\Xi_1$,
	\begin{align*}
			&\bigabs{L_i(\hat{\beta}_i^{\MTL}(U_A)) - L_i(B^{\star} {a}_i) - \sigma^2 \norm{{a}_i}^2 \cdot \bigtr{\Sigma (X^{\top}X)^{-1}}}  \prec  n^{-1/2} \left[L_i(B^{\star} {a}_i) + \sigma^2 \| a_i\|^2\right] ,
		\end{align*}
	for any fixed $U_A$. Then using a similar $\ve$-net argument as in Appendix \ref{app_firstpf}, we can obtain  \eqref{eq_Li_err}. We omit the details.
\end{proof}

	%The proof of Claim \ref{claim_opt_dist}, Claim \ref{lem_exp_opt}, and Claim \ref{claim_pred_err} can be found in Appendix \ref{app_proof_error_same_cov}.
	Provided with Lemmas \ref{lem_exp_opt}, \ref{claim_opt_dist} and \ref{claim_pred_err}, we are ready to prove Theorem \ref{thm_many_tasks}.
	\begin{proof}[Proof of Theorem \ref{thm_many_tasks}]
	The estimate \eqref{minimizer_beta} follows immediately from Lemma \ref{claim_opt_dist}.
		Using Lemma \ref{claim_pred_err} and applying Lemma \ref{fact_tr} to $\bigtr{\Sigma (X^{\top}X)^{-1}}$, we get that for $\hat a_i = U_{\hat{A}} U_{\hat{A}}^{\top}e_i$,
		\begin{align}
			&\bigabs{L_i(\hat{\beta}_i^{\MTL}(\hat A)) - L_i(B^{\star} \hat {a}_i) - \frac{p }{n-p}\cdot \sigma^2 \norm{\hat {a}_i}^2}\nonumber \\
			&\le  n^{-1/2+c} \left[L_i(B^{\star} \hat {a}_i) + \sigma^2 \| \hat a_i\|^2\right] + p^{-C}  \|{\Sigma^{1/2} B^{\star}}\|_F^2 , \label{eq_Li_err_add0}
		\end{align}
		with high probability. From this equation, we obtain that
		\begin{align}
			& \bigabs{L_i(\hat{\beta}_i^{\MTL}(\hat A)) - L_i(B^{\star}  {a}^\star_i) - \frac{p }{n-p}\cdot \sigma^2 \norm{ {a}^\star_i}^2} \nonumber\\
			 &\le  n^{-1/2+c} \left[\|\Sigma^{1/2} B^{\star}\|^2 + \sigma^2 \|  a^\star_i\|^2\right] +\left(\| \Sigma^{1/2} B^{\star}\|^2  +\sigma^2\right)  \bignorm{\hat a_i-a_i^\star }    .\label{eq_Li_err_add}
		\end{align}
		where we also use that $L_i(B^{\star} {a}^\star_i)\le \|\Sigma^{1/2} B^{\star}\|^2$. On the other hand, using Lemma \ref{claim_opt_dist} we can bound that
		$$\bignorm{\hat a_i-a_i^\star }^2
				\lesssim  n^{-1/2+c} \frac{ Q  \|{B^{\star}}^\top\Sigma B^{\star}\| +  \sigma^2 }{\lambda_r - \lambda_{r+1} } .$$
		%to upper bound the difference between $\norm{\hat{a}_i}^2$ and $\norm{a_i^{\star}}^2$.
		Plugging it into \eqref{eq_Li_err_add}, we obtain \eqref{Li_multi1}. %by using $n^{-1/2+c}Q\ll n^{-c_\varphi}$ as long as $c$ is chosen smaller than $\frac{\varphi-4}{2\varphi}-c_\varphi$.
		%the prediction loss of $\hat{\beta}_i^{\MTL}$ is equal to  $L(B^{\star}\hat{a}_i)+\sigma^2\norm{\hat{a}_i}^2\cdot \bigtr{\Sigma(X^{\top}X)^{-1}}$ up to a multiplicative error of order $n^{-1/4}$.
		%Moreover, Claim \ref{claim_opt_dist} gives directly an upper bound on $\|\hat a_i - a_i^\star\|^2$. With this estimate, we can bound the difference
		%$$L(B^{\star}\hat{a}_i)+\sigma^2\norm{\hat{a}_i}^2\cdot \bigtr{\Sigma(X^{\top}X)^{-1}} - L(B^{\star} {a}^\star_i)-\sigma^2\norm{{a}^\star_i}^2\cdot \bigtr{\Sigma(X^{\top}X)^{-1}}.$$
		%Combined together, our proof is complete.
		
		For \eqref{Li_multi2}, summing \eqref{eq_Li_err_add0} over $i$ and using Lemma \ref{fact_tr}, we obtain that 
		\begin{align}
			&\bigabs{\sum_i L_i(\hat{\beta}_i^{\MTL}(\hat A)) -\bignorm{\Sigma^{1/2} B^{\star} (U_{\hat A}U_{\hat A}^{\top} - \id_{t\times t})}_F^2 - \frac{p }{n-p}\cdot r \sigma^2  }\nonumber \\
			&\le  n^{-1/2+c} \left[\bignorm{\Sigma^{1/2} B^{\star} (U_{\hat A}U_{\hat A}^{\top} - \id_{t\times t})}_F^2 + \sigma^2 \right] + p^{-C}  \|{\Sigma^{1/2} B^{\star}}\|_F^2 ,  \label{eq_Li_err_add1}
		\end{align}
		with high probability, where we use that $\sum_{i=1}^t\| \hat a_i\|^2=r$ and 		
		$$\sum_{i=1}^t L_i(B^{\star} \hat {a}_i)=\bignorm{\Sigma^{1/2} B^{\star} (U_{\hat A}U_{\hat A}^{\top} - \id_{t\times t})}_F^2.$$
		With \eqref{same_hA}, we can write 
		\begin{align*}
		  \bignorm{\Sigma^{1/2} B^{\star} (U_{\hat A}U_{\hat A}^{\top} - \id_{t\times t})}_F^2 &= \frac{h(U_{\hat A})-\sigma^2 (n t - p r)}{n} \\
		& \ge  \frac{h( A^\star)-\sigma^2 (n t - p r)}{n} =  \bignorm{\Sigma^{1/2} B^{\star} (A^\star{A^\star}^{\top} - \id_{t\times t})}_F^2,
		\end{align*}
		where in the second step we use that $A^\star$ is the global minimizer of $h$. On the other hand, using \eqref{eq_g_gap}, we bound that
		\begin{align*}
	&\frac{h(U_{\hat A})-\sigma^2 (n t - p r)}{n} 	\le \frac{h( A^\star)-\sigma^2 (n t - p r)}{n} + \OO\left( n^{-1/2+c} \left(Q \|\Sigma^{1/2} B^{\star}\|^2 +  \sigma^2\right)\right) 
		\end{align*}	
		with high probability.
		Combining the above two estimates, we get 
		\begin{align*}
		\bignorm{\Sigma^{1/2} B^{\star} (U_{\hat A}U_{\hat A}^{\top} - \id_{t\times t})}_F^2 & = \bignorm{\Sigma^{1/2} B^{\star} (A^\star{A^\star}^{\top} - \id_{t\times t})}_F^2 \\
		&+ \OO\left( n^{-1/2+c} \left(Q \|\Sigma^{1/2} B^{\star}\|^2 +  \sigma^2\right)\right) ,\end{align*}
		with high probability. Plugging it into  \eqref{eq_Li_err_add1}, we obtain \eqref{Li_multi2}.			
%		For the latter, we use Claim \ref{claim_opt_dist} to upper bound the difference between $\norm{\hat{a}_i}^2$ and $\norm{a_i^{\star}}^2$.
%		For $L(B^{\star}\hat{a}_i)$, we again use Claim \ref{claim_opt_dist} to upper bound the distance between $\hat{a}_i$ and $a_i^{\star}$.
%		Combined together, we obtain the difference if we replace $\hat{a}_i$ with $a^{\star}_i$ in Claim \ref{claim_pred_err}, and the proof is complete.
	\end{proof}

	\iffalse	
Next we present the proof of Claim \ref{claim_opt_dist}, Claim \ref{lem_exp_opt}, and Claim \ref{claim_pred_err}. 
	
	

	Second, notice that
	%\begin{align}
	%	\sum_{j=1}^t \norm{\cW A_j}^2 = \sum_{j=1}^t \bigtr{\cW A_j A_j^{\top} \cW^{\top}} = \bigtr{\cW AA^{\top} \cW^{\top}} = \bigtr{U_X U_X^{\top} \cE A^{\top} (AA^{\top})^{-1} A \cE^{\top}}. \label{eq_proof_same_cov_1}
	%\end{align}
	%We observe that
	\begin{align*}
			\exarg{\cE}{\cE A^{\top} (AA^{\top})^{+} A \cE^{\top}}
		&= \exarg{\cE}{\sum_{j=1}^t \sum_{k=1}^t \varepsilon^{(j)} A_j^{\top} (AA^{\top})^{+} A_k {\varepsilon^{(k)}}^{\top}} \\
		&= \exarg{\cE}{\sum_{j=1}^t \varepsilon^{(j)} A_j^{\top} (AA^{\top})^{+} A_j {\varepsilon^{(j)}}^{\top}} \\
		&= \sigma^2 \cdot r \cdot \id_{n\times n}.
	\end{align*}
	In the above derivation, the second step used the fact that for any $j\neq k$, $\varepsilon^{(j)}$ and $\varepsilon^{(k)}$ are pairwise independent.
	The third step used the fact that $\sum_{j=1}^t A_j^{\top} (AA^{\top})^{+} A_j = \bigtr{\id_{r\times r}} = r$, and $\ex{\varepsilon^{(j)} {\varepsilon^{(j)}}^{\top}} = \sigma^2 \cdot \id_{n\times n}$.
	Therefore, we have that
	\begin{align*}
		\exarg{\cE}{ \inner{\cW A}{\cE}} = \sigma^2 \cdot r \cdot \bigtr{X(X^{\top} X)^{-1} X^{\top}} =  \sigma^2 \cdot r \cdot \bigtr{ X^{\top}X(X^{\top} X)^{-1}}  = \sigma^2 \cdot r \cdot p.
	\end{align*}
   because $X^\top X$ is a $p\times p$ matrix.	
	%Next, note that
	%\[ \sum_{j=1}^t \inner{\cW A_j}{\varepsilon^{(j)}} = \bigtr{\cW A\cE^{\top}}, \]
	%which is equal to equation \eqref{eq_proof_same_cov_1} by the definition of $\cW$.
	Hence the proof is complete.

	We simplify equation \eqref{eq_mtl_output_layer} as
	\begin{align*}
		g(A)  &= \normFro{X (X^{\top}X)^{-1} X^{\top} Y U_A U_A^{\top} - Y}^2 \\
					&= \normFro{Y}^2 - \inner{U_A U_A^{\top}}{Y^{\top} X (X^{\top} X)^{-1} X^{\top} Y}.
	\end{align*}
	We denote $H = Y^{\top} X (X^{\top} X)^{-1} X^{\top} Y$, which appears in the above equation.
	Since $\hat{A}$ is a global minimum of $g(A)$, we have that $g(A^{\star}) \ge g(\hat{A})$.
	Hence, we have
	\begin{align}
		g(A^{\star}) - g(\hat{A}) &= \abs{g(A^{\star}) - g(\hat{A})} = \abs{\inner{H}{A^{\star} {A^{\star}}^{\top} - U_{\hat{A}} U_{\hat{A}}^{\top}}} \nonumber \\
												&\ge \lambda_{\min}(H) \cdot \normFro{A^{\star} {A^{\star}}^{\top} - U_{\hat{A}} U_{\hat{A}}^{\top}}. \label{eq_g_gap1}
	\end{align}
	Recall that $Y = X B^{\star} + \cE$ in matrix notation.
	We bound the minimum singular value of $H$ as follows
	\begin{align}
		\lambda_{\min}(H) &= \lambda_{\min}({B^{\star}}^{\top} X^{\top} X B^{\star} + \cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}X^{\top}X B^{\star}) + \lambda_{\min}(\cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}\Sigma B^{\star}) \cdot \lambda_{\min}(Z^{\top}Z) + \lambda_{\min}(\cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}\Sigma B^{\star}) \cdot ((\sqrt n - \sqrt p)^2 - n \cdot p^{-c}) + \sigma^2 \cdot p \cdot (1 - p^{-1/2 + \e}). \label{eq_g_gap2}
	\end{align}
	The second equation uses the fact that the minimum singular value of the sum of two PSD matrices is greater than the sum of the minimum singular value of each matrix.
	The third equation uses the fact that $X = Z \Sigma^{1/2}$ and Fact \ref{fact_proof_gA} in Appendix \ref{app_tool}.
	The last equation uses Fact \ref{fact_minv} (ii) for the minimum singular value of $Z^{\top}Z$ and Lemma \ref{largedeviation} for $\cE\cE^{\top}$.

	Combining equation \eqref{eq_g_gap}, \eqref{eq_g_gap1}, and \eqref{eq_g_gap2}, we conclude that
	\begin{align*}
		\bignormFro{A^{\star} {A^{\star}}^{\top}- U_{\hat{A}} U_{\hat{A}}^{\top}}
		&\le \frac{p^{-c} \cdot n \bignormFro{\Sigma^{1/2} B^{\star}}^2 + p^{-1/2 + c} \cdot \sigma^2 nt} {\lambda_{\min}(H)} \\
		&\le \frac{p^{-c} \rho \normFro{\Sigma^{1/2} B^{\star}}^2 + p^{-1/2 + \e} \cdot \sigma^2 \rho t}{\lambda_{\min}^2(\Sigma^{1/2} B^{\star}) ((\sqrt{\rho} - 1)^2 - \rho \cdot p^{-c}) + \sigma^2 (1 - p^{-1/2 + \e})}
	\end{align*}
	Hence for large enough $p$, we obtain that Claim \ref{claim_opt_dist} holds.
	\fi
	


\section{Proof of Theorem \ref{cor_MTL_loss}}\label{app_iso_cov}
In the setting of Theorem \ref{cor_MTL_loss}, we have 
\begin{align*}
L_{\bias} (a)&=\bv_a^\top (Z^{(1)})^\top Z^{(1)} \left[a^2(Z^{(1)})^\top Z^{(1)}+ (Z^{(2)})^\top Z^{(2)}\right]^{-2 }  (Z^{(1)})^\top Z^{(1)} \bv_a\\
&=\bv_a^\top \cal Q^{(1)} \left(a^2\cal Q^{(1)}+ \frac{n_2}{n_1}\cal Q^{(2)}\right)^{-2 } \cal Q^{(1)} \bv_a,
\end{align*}
where we abbreviate  
$$\bv(a):=(\Sigma^{(1)})^{1/2}\left(a\beta^{(1)}- a^2\beta^{(2)}\right), \quad \cal Q^{(1)}: = \frac1{n_1}(Z^{(1)})^\top Z^{(1)},\quad \cal Q^{(2)}: = \frac1{n_2}(Z^{(2)})^\top Z^{(2)}.$$
Note that $\cal Q^{(1)}$ and $\cal Q^{(2)}$ are both Wishart matrices. Using the rotational invariance of the distributions of $\cal Q^{(1)}$ and $\cal Q^{(2)}$, we can simplify $L_{\bias}(a)$ as in the following lemma.

\begin{lemma}\label{claim_reduce_rota}
In the setting of Theorem \ref{cor_MTL_loss}, we have 
\be\label{red_bias1}L_{\bias} (a) =\left[ 1+\OO_\prec(p^{-1/2})\right] \frac{\|\bv_a\|^2}{p}\bigtr{\left(a^2\cal Q^{(1)}+ \frac{n_2}{n_1}\cal Q^{(2)}\right)^{-2} (\cal Q^{(1)})^2} .\ee
\end{lemma}
\begin{proof}
It is easy to check that the distribution of the matrix $\cal Q^{(1)} (a^2\cal Q^{(1)}+ \frac{n_2}{n_1}\cal Q^{(2)})^{-2 } \cal Q^{(1)}$ is rotationally invariant. Hence for the eigenvalue decomposition 
$$\cal Q^{(1)} \left(a^2\cal Q^{(1)}+ \frac{n_2}{n_1}\cal Q^{(2)}\right)^{-2 } \cal Q^{(1)}= U^\top \Lambda U,$$
we have that $U$ is a Haar distributed matrix that is independent of $\Lambda$. This shows that $\bu:=U\bv_a/\|\bv_a\|$ is a unit vector independent of $\Lambda$ and uniformly distributed on the unit sphere in $\R^p$. Recall that a uniformly distributed unit vector has the same distribution as a normalized Gaussian vector ${\mathbf g}/{\|\mathbf g\|},$ where $\mathbf g=(g_1,\cdots, g_p)$ is a random vector with i.i.d. Gaussian entries of mean zero and variance one. Thus we have 
\begin{align}\label{inserteq0}
\bv_a^\top \cal Q^{(1)} \left(a^2\cal Q^{(1)}+ \frac{n_2}{n_1}\cal Q^{(2)}\right)^{-2 } \cal Q^{(1)} \bv_a \stackrel{d}{=} \frac{\|\bv_a\|^2}{\|\mathbf g\|^2} \mathbf g^\top \Lambda \mathbf g. 
\end{align}
Now using \eqref{vcalA2}, we get that 
\be\label{inserteq1}\|\mathbf g\| = p + \OO_\prec (p^{1/2}),\quad |\mathbf g^\top \Lambda \mathbf g- \tr \Lambda|\prec \|\Lambda\|_F \prec p^{1/2},\ee
where we use Lemma \ref{SxxSyy} to bound that $\|\Lambda\|_F\le p^{1/2}\|\Lambda\|\lesssim 1$ with overwhelming probability. Plugging \eqref{inserteq1} into \eqref{inserteq0}, we conclude that  
$$L_{\bias}(a) = \left[ 1+\OO_\prec(p^{-1/2})\right] \frac{\|\bv_a\|^2}{p}\tr \Lambda=  \left[ 1+\OO_\prec(p^{-1/2})\right] \frac{\|\bv_a\|^2}{p}\tr [U^\top \Lambda U],$$
which implies \eqref{red_bias1}.  
\end{proof}

Without loss of generality, we assume that $a\ne 0$, since otherwise we trivially have $L_{\bias}(a)=0$. Then it is easy to see that 
\be\label{dft}
L_{\bias} (a) =- \left[ 1+\OO_\prec(p^{-1/2})\right] \frac{\|\bv_a\|^2}{a^4} \cdot  \left. \frac{\dd  f_\al (t)}{\dd t}\right|_{t=0},
\ee
where $\al= n_2/(n_1 a^2)$ and
$$f_{\al}(t):=\frac1p \bigtr{\frac{1}{\cal Q^{(1)} + t (\cal Q^{(1)})^2+ \al \cal Q^{(2)}}}.$$
%To calculate the right-hand side of \eqref{red_bias1}, we use the following identity: for any $\al \ge 0$, 
%\be\label{dft} \frac1p \bigtr{\frac{1}{(\cal Q^{(1)}+ \al \cal Q^{(2)})^{2}} (\cal Q^{(1)})^2}= \left. \frac{\dd  f_\al (t)}{\dd t}\right|_{t=0},\ee
Hence to obtain the asymptotic limit of $L_{\bias}(a)$, we only need to calculate the values of $f_\al(t)$ for $t$ around 0. 



 
Our calculation of $f_\al(t)$ is based on the Stieltjes transform method in random matrix theory and free additive convolution (or free addition) in free probability theory. We briefly describe the basic concepts that are needed for the proof, and refer the interested readers to classical texts such as \cite{bai2009spectral,tao2012topics,erdos2017dynamical,nica2006lectures} for a more thorough introduction.
For any probability measure $\mu$ supported on $\R$, the Stieltjes transform of $\mu$ is a complex function defined as
\be\label{def_stj}m_\mu(z):= \int_0^\infty \frac{\dd\mu(x)}{x-z}, \quad \text{ for any } z\in \C\setminus \supp(\mu).\ee
%The Stieltjes transform method reduces the study of a probability measure $\mu$ to the study of a complex function $m_\mu(z)$.
%To study the trace of $\hat{\Sigma}^{-1}$, we consider the Stieltjes transform of its empirical spectral distribution. 
%Recall that  $U\Lambda V^\top$ is the singular value decomposition of $M$, %$\Lambda$ consists of the singular values of $M$, $V$ is an orthonormal matrix,
%It is not hard to verify that $(n_1 + n_2)\hat{\Sigma}^{-1}= (\Sigma^{(2)})^{-1/2} V W^{-1}V^\top (\Sigma^{(2)})^{-1/2}$ and .
For any $p\times p$ symmetric matrix $M$, let $\mu_M:=p^{-1}\sum_{i} \delta_{\lambda_i(M)}$ denote the empirical spectral distribution (ESD) of $M$, where $\lambda_i(M)$ denotes the $i$-th eigenvalues of $M$ and $\delta_{\lambda_i(M)}$ is the point mass measure at $\lambda_i(M)$. Then it is easy to see that the Stieltjes transform of $\mu$ is equal to
 \[ m_{\mu_M}(z) \define \frac{1}{p}\sum_{i=1}^p \frac{1}{\lambda_i(M) - z}= p^{-1}\tr\left[(M-z\id)^{-1}\right]. \]
  Given two $p\times p$ matrices $A_p$ and $B_p$, suppose that their ESD $\mu_{A_p}$ and $\mu_{B_p}$ converge weakly to two probability distributions, say $\mu_{A}$ and $\mu_{ B}$. Let $U_p$ be a sequence of Haar distributed orthogonal matrices. Then it is known in free probability that the ESD of $ A_p + U_p B_p U_p^\top$ converges to the \emph{free addition} of $\mu_A$ and $\mu_B$, denoted by $\mu_A \boxplus \mu_B$. 
  

It is well-known that the ESD of $\Qa$ and $\Qb$ converge weakly to the famous Marchenko-Pastur (MP) law \cite{MP}: $\mu_{\Qa}\Rightarrow \mu^{(1)}$ and $\mu_{\Qb}\Rightarrow \mu^{(2)},$ where $\mu^{(1)}$ and $\mu^{(2)}$ have densities
 $$\rho^{(i)}(x)= \frac1{2\pi \xi_i x}{\sqrt{\left(\lambda_+^{(i)}-x\right)\left(x-\lambda_-^{(i)}\right)}}\mathbf 1_{x\in [\lambda_-^{(i)},\lambda_+^{(i)}]},\quad i=1,2,$$
 where recall that $\xi_i={p}/{n_i}$ and $\lambda_{\pm}^{(i)}:=(1\pm \sqrt{\xi_i})^2.$
Moreover, the Stieltjes transforms of $\mu^{(1)}$ and $\mu^{(2)}$ satisfy the self-consistent equations
$$z\xi_i m^2_{\mu^{(i)}} - (1-\xi_i - z)m_{\mu^{(i)}} +1 =0,\quad i=1,2.$$ 
With this equation, we can check that $g_i(m_{\mu^{(i)}}(z))=z$ for the functions
\be\label{g_i} g_i(m)= \frac{1}{1+\xi_i m}-\frac1m,\quad i=1,2.\ee
The sharp convergence rates of $\mu_{\Qa}$ and $\mu_{\Qb}$ has also been obtained in Theorem 3.3 of \cite{PY}, that is, in the setting of Theorem \ref{cor_MTL_loss} we have
\be\label{Kol_dist}
d_K\left( \mu_{\cal Q^{(i)}},\mu^{(i)}\right) \prec  p^{-1}, \quad i=1,2,
\ee
where $d_K$ denote the Kolmogorov distance between two probability measures:
$$d_K\left( \mu_{\cal Q^{(i)}},\mu^{(i)}\right):=\sum_{x\in \R} \left| \mu_{\cal Q^{(i)}}\left((-\infty,x]\right)-\mu^{(i)}\left((-\infty,x]\right)\right|.$$

For any fixed $\al>0$ and a sufficiently small $t\ge 0$, the ESD of $\al\Qb$ and $\Qa+t(\Qa)^2$ converges weakly two measures $\mub_\al$ and $\mua_t$ defined as follows:
$$\mub_\al((-\infty,x])= \int \mathbf 1_{\al y \in (-\infty, x]} \dd \mub(y),\quad \mua_t((-\infty,x])= \int \mathbf 1_{y+ ty^2 \in (-\infty, x]} \dd \mua(y).$$
Hence their Stieltjes transforms are given by
\be\label{def_mt}
m_{\mub_\al}(z)= \frac1{\al}m_{\mu^{(2)}}\left(\frac{z}{\al}\right),\quad m_{\mua_t}(z)= \int \frac{\dd \mua(x)}{x+tx^2 - z}.
\ee
Note that the eigenmatrices of $\Qa+t(\Qa)^2$ and $\al \Qb$ are independent Haar distributed orthogonal matrices. Hence the ESD of $\Qa+t(\Qa)^2 + \al \Qb$ converges weakly to the free addition $\mua_t\boxplus \mub_\al$. In particular, we will use the following almost sharp estimate on the difference between the Stieltjes transforms of $\mu_{\Qa+t(\Qa)^2 + \al \Qb}$ and  $\mua_t\boxplus \mub_\al$.

\begin{lemma}\label{lem_distance_ab}
In the setting of Theorem \ref{cor_MTL_loss}, suppose $\al, t \in [0,C]$ for a constant $C>0$. Then we have
\begin{align}
& \left|\frac{1}{p}\bigtr{\frac{1}{\Qa+t(\Qa)^2 + \al \Qb-z}} - m_{\mua_t\boxplus \mub_\al}(z)\right| \nonumber\\
& \prec \frac1p+ d_K\left( \mu_{\Qa+t(\Qa)^2},\mu^{(1)}_t\right) + d_K\left( \mu_{\al\cal Q^{(2)}},\mu_\al^{(2)}\right) ,\label{distance_ab}
 \end{align}
for any fixed $z\in \C$ that is away from the support of $\mua_t\boxplus \mub_\al$ by a distance of order 1. 
\end{lemma}
\begin{proof}
This lemma is essentially a consequence of Theorem 2.5 of \cite{BES_free1} and Theorem 2.4 of \cite{BES_free2}. 
In fact, \eqref{distance_ab} is proved for $z=E+\ii\eta$ with $E\in \supp(\mua_t\boxplus \mub_\al)$ and $\eta>0$ in \cite{BES_free2}, but the proofs there can be repeated almost verbatim in our setting where $z$ is away from the support of $\mua_t\boxplus \mub_\al$ by a distance of order 1. We omit the details. 
\end{proof}
  
 With the above lemma, to calculate the right-hand side of \eqref{dft}, it suffices to calculate $\partial_t m_{\mua_t\boxplus \mub_\al}(z=0)$ at $t=0$. 
  
  \begin{lemma}\label{lem_m'}
We have 
\be\label{derv_freeadd}\left. \frac{\dd }{\dd t}m_{\mua_t\boxplus \mub_\al}(0)\right|_{t=0} =- \frac{1 - 2f_1(\al) f_3(\al)  +f_2(\al) f_3(\al) ^2  }{ 1 - \xi_2f_2(\al) f_3(\al)^2 } ,\ee
where the functions $f_1$, $f_2$ and $f_3$ are defined as
\begin{align}
 f_1(\al)&:= m_{\mua_0\boxplus \mub_\al}(0) \nonumber  \\
 &=\frac2{ \al(1-\xi_2) + (1-\xi_1)+ \sqrt{[\al(1-\xi_2) +(1-\xi_1)]^2 + 4\al (\xi_1+\xi_2 -\xi_1\xi_2)}},\label{defnf1}\\
 f_2(\al)&:= \left[ \frac1{f_1(\al,\xi_1,\xi_2)^2} - \frac{\xi_1 }{[1+\xi_1f_1(\al,\xi_1,\xi_2) ]^2}\right]^{-1},\label{defnf2}\\ 
 f_3(\al)&:= \frac{\al }{1+ \al \xi_2f_1(\al,\xi_1,\xi_2)}.\label{defnf3}
 \end{align}
\end{lemma}
\begin{proof}
We calculate the Stieltjes transform of the free addition of $\mua_t$ and $\mub_\al$ using the following lemma.
\begin{lemma}[Theorem 4.1 of \cite{BB_free} and Theorem 2.1 of \cite{CG_free}]
 Given two probability measures, $\mu_1$ and $\mu_2$ on $\R$, there exist analytic functions $\omega_1,\omega_2:\C^+\to \C^+$, where $C^+:=\{z\in \C: \im z>0\}$ is the upper half complex plane, such that the following equations hold: for all $z\in \C^+$,
 \be\label{free_eq0}
 m_{\mu_1}(\omega_2(z))= m_{\mu_2}(\omega_1(z)),\quad \omega_1(z)+\omega_2(z) - z= -\frac{1}{m_{\mu_1}(\omega_2(z))}.
 \ee
Moreover, $m_{\mu_1}(\omega_2(z))$ is the Stieltjes transform of $\mu_1\boxplus \mu_2$, that is,
$$ m_{\mu_1\boxplus \mu_2}(z)=m_{\mu_1}(\omega_2(z)).$$
\end{lemma}

We now solve the equation \eqref{free_eq0} for $\mu_1=\mua_t$ and $\mu_2=\mub_\al$ for $z\to 0$: 
 \be\label{free_eq}
 m_{\mua_\al}(\omega_{2}(\al,t))= m_{\mub_\al}(\omega_1(\al, t)),\quad \omega_1(\al,t)+\omega_2(\al,t) = -\frac{1}{m_{\mu_1}(\omega_2(\al,t))},
 \ee
 where, for simplicity, we omit the argument $z=0$ from $\omega_1(z=0,\al,t)$ and $\omega_2(z=0,\al,t)$. From the definition of $m_{\mub_\al}$ in \eqref{def_mt}, we can verify that 
\be\label{g2al}
\al g_2\left( \al m_{\mub_\al}(z)\right)=z,
\ee
where $g_2$ is defined in \eqref{g_i}. Then applying \eqref{g2al} to the first equation of  \eqref{free_eq}, we get
\be\nonumber
\omega_1 = \frac{\al}{1+\xi_2  \al m_{\mua_t}(\omega_2)} - \frac1{m_{\mua_t}(\omega_2)}.
\ee
Plugging this equation into the second equation of \eqref{free_eq}, we get
\be\label{eq_m12}
 \frac{\al}{1+\xi_2  \al m_{\mua_t}(\omega_2)} + \omega_2=0 \ \ \Leftrightarrow \ \ 
 \al + \omega_2 \left[1+  \al \xi_2   m_{\mua_t}(\omega_2)\right]=0.
\ee
This gives a self-consistent equation of $m_{\mua_t\boxplus \mub_\al}(0)= m_{\mua_t}(\omega_2)$.

Now we define the following quantities at $t=0$: 
$$f_1(\al):=m_{\mua_0}(\omega_2(\al,0)),\quad f_3(\al):=-\omega_2(\al, 0),$$ 
and
$$ f_2(\al) :=\left. \frac{\dd m_{\mua_0}(z)}{\dd z}\right|_{z=\omega_2(\al, 0)}= \int \frac{ \dd \mua(x)}{[x - \omega_2(\al,0)]^2} .$$ 
First, from \eqref{eq_m12} we obtain \eqref{defnf3}. Using the fact that $g_1$ in \eqref{g_i} is the inverse function of $m_{\mua_0}$, we can write equation \eqref{eq_m12} into an equation of $f_1$ only when $t=0$:
\be\nonumber
 \al + \left( \frac{1}{1+\xi_1 f_1} -\frac1{f_1}\right) \left(1+  \al \xi_2  f_1\right)=0 .
 \ee
This equation can be reduced to a quadratic equation: 
\be\label{eq_m1234}
 \al \left( \xi_1+\xi_2-\xi_1\xi_2\right)f_1^2 + \left[ \al(1-\xi_2)+(1-\xi_1)\right]f_1 -1=0.
 \ee
 By definition, $f_1$ is the Stieltjes transform of $\mua_0\boxplus \mub_\al$ at $z=0$. Since  $\mua_0\boxplus \mub_\al$ is supported on $(0,\infty)$, $f_1$ must be positive by \eqref{def_stj}. Then it is not hard to see that the only positive solution of \eqref{eq_m1234} is given by \eqref{defnf1}. Finally, calculating the derivative of $m_{\mua_0}$ using its inverse function, we obtain that $ f_2(\al)= [g_1'(f_1)]^{-1}$, which gives \eqref{defnf2}.

To conclude the proof, we still need to calculate $\partial_t m_{\mua_t}(\omega_2)|_{t=0}$. Taking derivative of equation \eqref{eq_m12} with respect to $t$ at $t=0$, we get
\be\label{calc_partial}\partial_t \omega_2(\al, 0) \cdot \left[1+  \al \xi_2  f_1(\al)\right] - \al \xi_2 f_3(\al) \cdot \left.\partial_t  m_{\mua_t}(\omega_2(\al, t))\right|_{t=0}=0.\ee
Using \eqref{def_mt}, we can calculate that 
\be\nonumber
 \partial_t  m_{\mua_t}(\omega_2(\al, t))= \partial_t \int \frac{\dd \mua(x)}{x+tx^2 - \omega_2(\al,t)}= -  \int \frac{[x^2 -\partial_t \omega(\al,t)]\dd \mua(x)}{[x+tx^2 - \omega_2(\al,t)]^2}.
\ee
Taking $t=0$ in the above equation, we get
\begin{align*}
&\left.\partial_t  m_{\mua_t}(\omega_2(\al, t))\right|_{t=0}=\partial_t \omega(\al,0) f_2(\al)\\
 &-  \int \frac{[(x-\omega_2(\al,0))^2 +2 \omega_2(\al,0)(x-\omega_2(\al,0))+\omega_2(\al,0)^2] \dd \mua(x)}{[x - \omega_2(\al,0)]^2}\\
 %&= \partial_t \omega(\al,0)\cdot f_2(\al)-1- 2\omega_2(\al,0) m_{\mua_0}(\omega_2(\al,0)) - \omega_2(\al,0)^2f_2(\al) \\
 &= \partial_t \omega(\al,0)\cdot f_2(\al)-1+ 2 f_1(\al) f_3(\al)- f_2(\al) f_3(\al)^2.
\end{align*}
Then we can solve that 
\begin{align*}
 \partial_t \omega(\al,0)= \frac{1}{f_2(\al)}\left[\left.\partial_t  m_{\mua_t}(\omega_2(\al, t))\right|_{t=0} +1- 2 f_1(\al)f_3(\al)+ f_3(\al)^2f_2(\al) \right].
\end{align*}
Inserting it into \eqref{calc_partial}, we can solve that 
$$ \left.\partial_t  m_{\mua_t}(\omega_2(\al, t))\right|_{t=0} =- \frac{1- 2f_1(\al) f_3(\al) + f_2(\al)f_3(\al)^2}{1- \frac{\al \xi_2 f_2(\al)f_3(\al)}{1+  \al \xi_2  f_1(\al) }}. $$
Using $(1+  \al \xi_2  f_1(\al))^{-1}=\al^{-1}f_3(\al)$ by equation \eqref{eq_m12}, we conclude Lemma \ref{lem_m'}.
\end{proof}

Now we are ready complete the proof of Theorem \ref{cor_MTL_loss}
\begin{proof}[Proof of Theorem \ref{cor_MTL_loss}]
We first consider the case $|a|> 1$. We can write the variance term \eqref{Lvar} as
\begin{align}\label{Lvar_pf1}
L_{\var}(a)= \sigma^2 \bigtr{ \frac1{n_1 a^2\Qa+n_2\Qb}}= \frac{p\sigma^2}{n_1 a^2}\cdot \frac{1}{p}\bigtr{ \frac1{ \Qa+\al \Qb}}
\end{align}
for $\al= n_2/(n_1 a^2)$. Using Lemma \ref{lem_distance_ab} and \eqref{Kol_dist}, we get
 $$\left|\frac{1}{p}\bigtr{\frac{1}{\Qa+  \al \Qb}} - m_{\mua_0 \boxplus \mub_\al}(0)\right|  \prec \frac1p.$$
 Recall that $m_{\mua_0\boxplus \mub_\al}(0) $ is given by $f_1(\al)$ in \eqref{defnf1}. Inserting it into \eqref{Lvar_pf1}, we can conclude \eqref{Lvar_samplesize} for any fixed $a\in [-1,1]^c$. 
 
 For the bias limit, recall that it is given by \eqref{dft}. Taking $t=p^{-1/2}$, we can use Lemma \ref{SxxSyy} to check that  
$$ \left| \left. \frac{\dd  f_\al (t)}{\dd t}\right|_{t=0} - \frac{f_\al(t)-f_\al(0)}{t}\right| \lesssim t \quad \text{w.o.p.}$$
Similarly, we have 
 $$ \left| \left. \frac{\dd  m_{\mua_t\boxplus \mub_\al}(0)}{\dd t}\right|_{t=0} - \frac{m_{\mua_t\boxplus \mub_\al}(0)-m_{\mua_0\boxplus \mub_\al}(0)}{t}\right| \lesssim t \quad \text{w.o.p.}$$
On the other hand, using Lemma \ref{lem_distance_ab} and \eqref{Kol_dist}, we get we have  
  $$\left|f_\al(0) - m_{\mua_0 \boxplus \mub_\al}(0)\right| + \left|f_\al(t) - m_{\mua_t \boxplus \mub_\al}(0)\right|  \prec \frac1p.$$
 Combining the above three estimates, we obtain that 
 \begin{align*}
 \left| \left. \frac{\dd  f_\al (t)}{\dd t}\right|_{t=0} -\left. \frac{\dd  m_{\mua_t\boxplus \mub_\al}(0)}{\dd t}\right|_{t=0} \right|&\prec t + \frac{\left|f_\al(0) - m_{\mua_0 \boxplus \mub_\al}(0)\right| + \left|f_\al(t) - m_{\mua_t \boxplus \mub_\al}(0)\right| }{t} \\
 &\prec p^{-1/2}.
 \end{align*}
 Now plugging this estimate and \eqref{derv_freeadd} into \eqref{dft}, after a straightforward calculation, we can obtain \eqref{Lbias_samplesize} for any fixed $a\in [-1,1]^c$
   
Finally, using a similar $\ve$-net argument as in Appendix \ref{app_firstpf}, we can show that \eqref{Lvar_samplesize} and \eqref{Lbias_samplesize} hold uniformly for all $a\in [-1,1]^c$ on a high probability event. We omit the details.

It remains to consider the case $|a|\le 1$. In this case, Lemma \ref{lem_distance_ab} cannot be applied when $a\to 0$, in which case $\al\to\infty$. However, we can apply Lemma \ref{lem_distance_ab} to 
$$\frac{1}{p}\bigtr{\frac{1}{a^2 \Qa+t(\Qa)^2 + \Qb-z}},$$
 whose limit is given by $ m_{\mua_{a^2,t}\boxplus \mub}(z)$, where $\mua_{a^2,t}$ is the limiting ESD of $a^2 \Qa+t(\Qa)^2$. Then we can calculate $ m_{\mua_{a^2,t}\boxplus \mub}(0)$ and $\partial_t m_{\mua_{a^2,t}\boxplus \mub}(0)$ at $t=0$ using the same argument as in the proof of Lemma \ref{lem_m'}. Finally, repeating the above proof of the $|a|>1$ case, we can conclude that \eqref{Lvar_samplesize} and \eqref{Lbias_samplesize} hold uniformly for all $a\in [-1,1]$ on a high probability event. We omit the details.
\end{proof}

\section{Proof of  Proposition \ref{lem_hat_v}}\label{app_iso_cov_prop}
\iffalse
 We follow a similar logic to the proof of Theorem \ref{thm_many_tasks}.
We first characterize the global minimizer of $f(A, B)$ in the random-effect model.
Based on the characterization, we reduce the prediction loss of hard parameter sharing to the bias-variance asymptotic limits.
Finally, we prove Corollary \ref{cor_MTL_loss} based on these limiting estimates.
We set up several notations.
In the two-task case, the optimization objective $f(A, B)$ is equal to
	\begin{align}
		f(A, B) =   \bignorm{X^{(1)} B A_1 - Y^{(1)}}^2+ \bignorm{X^{(2)} B A_2 - Y^{(2)}}^2, \label{eq_mtl_2task_cov}
	\end{align}
	where $B \in \real^{p}$ and $A = [A_1, A_2]\in \real^{2}$ because the width of $B$ is one.
Without loss of generality, we assume that $A_1$ and $A_2$ are both nonzero.
Otherwise, the problem reduces to STL. % we add a tiny amount of perturbation $\delta$ to them and the result remains the same.
Using the local optimality condition $\frac{\partial f}{\partial B} = 0$, we obtain that $\hat{B}$ satisfies the following
	\begin{align}
		\hat{B} \define  \left[A_1^2 (X^{(1)})^{\top}X^{(1)} + A_2^2 (X^{(2)})^{\top}X^{(2)}\right]^{-1} \left[A_1 (X^{(1))})^{\top}Y^{(1)} + A_2 (X^{(2)})^{\top}Y^{(2)}\right]. \label{eq_Bhat_2task} %\\
		%&= (B^\star A ^{\top}) (A A^{\top})^{-1} + (X^{\top}X)^{-1}X^{\top}   \bigbrace{\sum_{j=1}^t \varepsilon_i A_i^{\top}} (A  A^{\top})^{-1}.
	\end{align}
We denote $\hat \Sigma(x)= x^2 (X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}$.
Applying $\hat B$ to equation \eqref{eq_mtl_2task_cov}, we obtain an objective that only depends on $x:=A_1/A_2$ as follows %\HZ{$A$ has been used to denote the output layers. Could you replace $A$ with another symbol (say $x$)?}
\fi
In this section, we give the proof of Proposition \ref{lem_hat_v}. First, for $g(a)$ in equation \eqref{eq_mtl_A12}, we can calculate its partial expectation over $\ve^{(1)}$ and $\ve^{(2)}$ as
\begin{align}
\exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(a)}
&= \left\| X^{(1)} \hat\Sigma(a)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-a\beta^{(2)})\right\|^2 \nonumber\\
&+a^2 \left\| X^{(2)} \hat\Sigma(a)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-a\beta^{(2)})\right\|^2  \nonumber\\
& + \sigma^2 \bigtr{  \left(a^2 X^{(1)}\hat \Sigma(a)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2} \nonumber\\
&+\sigma^2 \bigtr { \left(X^{(2)}\hat \Sigma(a)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2 }   \nonumber\\
&+  a^2 \sigma^2\bigtr{ X^{(2)}\hat \Sigma(a)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(a)^{-1} (X^{(2)})^\top } \nonumber\\
& +a^2 \sigma^2\bigtr { X^{(1)}\hat \Sigma(a)^{-1}  (X^{(2)})^\top X^{(2)}\hat\Sigma(a)^{-1} (X^{(1)})^\top }. \label{Eg(x)}
\end{align}
Using the following identity
\begin{align*}
(X^{(1)})^\top X^{(1)} \hat \Sigma(a)^{-1}(X^{(2)})^\top X^{(2)} & =\left( a^2 [(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1} \right)^{-1} \\
&= (X^{(2)})^\top X^{(2)} \hat \Sigma(a)^{-1}(X^{(1)})^\top X^{(1)},
\end{align*}
we can simplify \eqref{Eg(x)} to
 %We have that the conditional expectation of $g(x)$ over $\epsilon^{(1)}$ and $\epsilon^{(2)}$ is
%\begin{align*}
%		 \exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(a)} & = (\beta^{(1)}-a\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(a)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-a\beta^{(2)})  \\
%		 &+\sigma^2(n_1+n_2-p).
%		%		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  \\
%%		&+ \sigma^2\tr\left[ \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2 \right]\\
%%&+ 2 x^2 \sigma^2 \tr\left[ \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right]  \nonumber\\
%\end{align*}
\begin{align*}
		 \exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(a)} & = (\beta^{(1)}-a\beta^{(2)})^\top \cal M(a) (\beta^{(1)}-a\beta^{(2)})  +\sigma^2(n_1+n_2-p),\end{align*}
where we abbreviate 
$$\cal M(a):=(X^{(1)})^\top X^{(1)} \hat \Sigma(a)^{-1}(X^{(2)})^\top X^{(2)} .$$
Furthermore, taking partial expectation over the task-specific components $ \wt \beta^{(1)}$ and $\wt \beta^{(2)}$
of $\beta^{(1)}$ and $\beta^{(2)}$, we obtain that 
\begin{align*}
		 \exarg{\epsilon^{(1)}, \epsilon^{(2)}, \wt \beta^{(1)}, \wt \beta^{(2)}}{g(a)} = h(a),
		 \end{align*}
		 where
		 \begin{align*}
		h(a) & := (a-1)^2\beta_0^\top \cal M(a) \beta_0  +  (a^2+1)\frac{d^2}{p}\bigtr{\cal M(a)}  +\sigma^2(n_1+n_2-p).
		%		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  \\
%		&+ \sigma^2\tr\left[ \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2 \right]\\
%&+ 2 x^2 \sigma^2 \tr\left[ \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right]  \nonumber\\
\end{align*}
%The calculation is tedious but rather straightforward, so we leave the details to the reader.
%In the random-effect model, recall that the entries of $(\beta^{(1)}- x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $p^{-1}[(x-1)^2\kappa^2 +(1+x^2) d^2]$.
%Hence, by further taking expectation over $\beta^{(1)}$ and $\beta^{(2)}$, we obtain
%\begin{align}
%	&\ex{g(x) \mid X_1, X_2} \nonumber\\
%%&=[(x-1)^2\kappa^2 + (x^2+1)d^2/2] \cdot p^{-1}\tr\left[  (X^{(2)})^\top X^{(2)} \hat\Sigma(x)^{-1}(X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)}\right] \nonumber\\
%%&+ A^2[(x-1)^2\kappa^2 + (x^2+1)d^2/2] \cdot p^{-1}\tr\left[  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)}\right] +\sigma^2(n_1+n_2-p) \nonumber\\
%	=& [(x-1)^2\kappa^2 + (x^2+1)d^2]\cdot {p^{-1}} \tr\left[ (X^{(1)})^\top X^{(1)} \hat \Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right]+\sigma^2(n_1+n_2-p), \label{EgA}
%\end{align}
%
%	
% {\cor
%we can simplify that
%\begin{align*}
% & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\right\|^2  +x^2 \left\| X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\right\|^2 \\
%=&(\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1} (X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)}  (\beta^{(1)}-x\beta^{(2)})\\
%&+(\beta^{(1)}-x\beta^{(2)})^\top x^2 (X^{(1)})^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)} )^\top X^{(2)}  \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\\
%&=(\beta^{(1)}-x\beta^{(2)})^\top\left[ x^2 (X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}  \right]\hat\Sigma(x)^{-1} (X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\\
%=&(\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)}).
%\end{align*}
%Using this identity, we can simplify equation \eqref{Eg(x)}  to
%}
%\paragraph{Part $1$: characterizing the global minimum of $f(A, B)$.}
Again, using the concentration bounds in Corollary \ref{cor_calE}, we can show that $g(a)$ concentrates around $h(a)$ for all $a\in \R.$
%First, we show that $g(x)$ and its expectation are close using standard concentration bounds.
\begin{claim}\label{claim_largedev1}
 In the setting of Proposition \ref{lem_hat_v}, for any small constant $\e>0$ and large constant $C>0$, there exists a high probability event $\Xi$, on which the following estimates hold uniformly in all $a\in \R$: 
 $$\left| g(a) - h(a)\right| \le p^{-1/2+c}h(a) + p^{-C}\kappa^2 \|\beta_0\|^2 .$$ % \left(\sigma^2 + \kappa^2+d^2 \right).$$
\end{claim}
\begin{proof}
With Corollary \ref{cor_calE}, the proof of this claim is very similar to that for Lemma \ref{lem_HPS_loss} in Appendix \ref{app_firstpf}. We do not repeat all the arguments here. 
\iffalse
\begin{align*}
		 g(a) \define & \left\| X^{(1)} \hat\Sigma(a)^{-1} (X^{(2)})^\top X^{(2)} (a\beta^{(2)}-\beta^{(1)}) \right. \nonumber\\
			& \left. + \left(a^2 X^{(1)}\hat \Sigma(a)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)}+ a X^{(1)}\hat \Sigma(a)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2 \nonumber\\
		   +& \left\| X^{(2)} \hat \Sigma(a)^{-1} (X^{(1)})^\top X^{(1)} (a\beta^{(1)}-a^2\beta^{(2)}) \right. \nonumber\\
		  &\left.+ \left(X^{(2)}\hat\Sigma(a)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)\epsilon^{(2)} + a X^{(2)}\hat \Sigma(a)^{-1} (X^{(1)})^\top \epsilon^{(1)} \right\|^2. 
	\end{align*}

 There are two terms in $g(A)$ from equation \eqref{eq_mtl_A12}.
 We will focus on dealing with the concentration error of the first term. The second term is similar to the first and we omit the details.
 For the first term, we expand into several equations under various situations involving the random noise and the random-effect model.
 \begin{align}
 & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
		& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2=h_1(x) + h_2(x) +h_3(x) + 2h_4(x) + 2h_5(x) + 2h_6(x), \label{expand_6}
 \end{align}
 where
 \begin{align*}
&h_1(x):=  (\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  ,\\
& h_2(x) := {(\epsilon^{(1)})^\top \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2\epsilon^{(1)}}  ,\\
& h_3(x):=  x^2 { (\epsilon^{(2)} )^\top X^{(2)} \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)}} , \\
& h_4(x):=  (\epsilon^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}), \\
&h_5(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}),\\
&h_6(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)}.
\end{align*}
Next, we estimate each term using Lemma \ref{largedeviation} for random variables with bounded moment up to any order.
We first state several facts that will be commonly used in the proof.
By Fact \ref{fact_minv} (ii), we have that w.h.p. the operator norm of $X^{(1)}$ and $X^{(2)}$ are both bounded by $O(\sqrt{n})$.
Furthermore, the operator norm of $\hat\Sigma(x)^{-1}$ is bounded by $(x^2 + 1)^{-1} O(n_1 + n_2) = (x^2 + 1)^{-1} O(p)$.
 % \be\label{op_X12}
%\|X^{(1)}\|\le \sqrt{(\sqrt{n_1} + \sqrt{p})^2 + n_1 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p}, \quad \|X^{(2)}\|\le \sqrt{(\sqrt{n_2} + \sqrt{p})^2 + n_2 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p},
% \ee
% and
% \be\label{op_Sig1}
%\| \hat\Sigma(x)^{-1}\|\le \frac1{x^2[(\sqrt{n_1} - \sqrt{p})^2 - n_1 \cdot p^{-c_{\varphi}}]+[(\sqrt{n_2} - \sqrt{p})^2 - n_2 \cdot p^{-c_{\varphi}}] }\lesssim \frac{1}{(x^2+1)p},
% \ee
%where we used that $3\le n_1/p\le \tau^{-1}$ and $3\le n_2/p\le \tau^{-1}$ for a small constant $\tau>0$.

For $h_1(x)$, using Lemma \ref{largedeviation} and the fact that the entries of $(\beta^{(1)}-x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $b = p^{-1}((x-1)^2\kappa^2 + (x^2+1)d^2)$, we obtain the following estimate w.h.p.
\begin{align}
	&\left|h_1(x) - \exarg{\beta^{(1)},\beta^{(2)}}{h_1(x) \mid X_1, X_2}\right|\nonumber \\
\le& p^{c}\cdot p^{-1} b \cdot \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F \nonumber\\
	\le& p^{c}\cdot p^{-1} b \cdot p^{1/2} \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| \nonumber \\
	\lesssim& p^{1/2+c}\cdot \frac{b}{(x^2+1)^2}\lesssim p^{1/2+c}(\kappa^2 + d^2).\label{eq_hA1}
\end{align}
In the third step we use the operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)^{-1}$. % as follows
%\begin{align*}
%	\left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| & \le \left\| (X^{(2)})^\top X^{(2)}\right\|^2 \cdot \left\| (X^{(1)})^\top X^{(1)}\right\|\cdot \left\| \hat\Sigma(x)^{-1} \right\|^2 \\
%  & \lesssim \frac{p}{(x^2+1)^2}.
%\end{align*}

For $h_2(x)$ and $h_3(x)$, since the entries of $\epsilon^{(1)},\epsilon^{(2)}\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variances $\sigma^2$, using Lemma \ref{largedeviation}, we obtain w.h.p.
\be\label{eq_hA23}\left|h_2(x) - \exarg{\epsilon^{(1)}}{h_2(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2, \quad \left|h_3(x) - \exarg{\epsilon^{(2)}}{h_3(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2.\ee
For $h_4(x)$, using Lemma \ref{largedeviation}, we obtain w.h.p.:
\begin{align}
			\left|h_4(x)\right|
	\le& p^{c}\cdot \sigma \cdot \sqrt{b / p}\left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F\nonumber \\
	\le& p^{c}\cdot \sigma \sqrt{b / p} \cdot p^{1/2} \left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
	\le&  p^{c}\cdot \sigma \sqrt{b} \cdot \left\|  x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top - \id_{n_1\times n_1} \right\| \cdot \left\|X^{(1)}\right\| \cdot \left\|\hat\Sigma(x)^{-1}\right\| \cdot \left\| (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
	\lesssim& p^{1/2+c}\frac{\sigma \sqrt{b}}{x^2+1} \lesssim p^{1/2+c}(\sigma^2 + \kappa^2 + d^2).\label{eq_hA4}
\end{align}
Above, in the fourth step we use the operator norm of $x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top - \id$ being at most one and the operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)^{-1}$.
In the last step we use AM-GM inequality. Using the same argument, we can show that
$\left|h_5(x)\right| \le p^{1/2+c}(\sigma^2 + \kappa^2 + d^2)$ and $\left|h_6(x)\right| \le p^{1/2+c}\sigma^2$.
Combining the concentration error bound for $h_1(x), h_2(x), \dots, h_6(x)$, we complete the proof.
The second term of $g(A)$ can be dealt in similar ways and we omit the details. \fi
\end{proof}

%Let $\hat{x}$ denote the global minimizer of $g(x)$.
%We show that in the setting of Corollary \ref{cor_MTL_loss}, $\hat x$ is close to 1.
%This gives us the global minimum of $f(A, B)$, since $\hat{B}$ is given by $\hat{x}$ using local optimality conditions.

Now we give the proof of Proposition \ref{lem_hat_v} based on Claim \ref{claim_largedev1}.
%\begin{claim}\label{lem_hat_v}
%%Suppose the assumptions of Lemma \ref{prop_model_shift_tight} hold. Assume that $ \kappa^2 \sim pd^2 \sim \sigma^2$ are of the same order.
%	Let $c$ be a sufficiently small fixed constant.
%	In the setting of Corollary \ref{cor_MTL_loss}, we have that with high probability,
%%There exists a constant $C>0$ such that
%	\be\label{hatw_add1}|\hat x -1|\le  \frac{2d^2}{\kappa^2} + p^{-1/4+c}.
%	\ee
%\end{claim}

\begin{proof}[Proof of Proposition \ref{lem_hat_v}]
%Corresponding to equation \eqref{EgA}, we define the function
%\begin{align*}
%	h(x)  =& [(x-1)^2\kappa^2 + (x^2+1)d^2] \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right] \\
%				=& [(1-x^{-1})^2\kappa^2 + (1+x^{-2})d^2] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].
%\end{align*}
By \eqref{assm3}, \eqref{assm2} and Corollary \ref{fact_minv}, there exists a high probability event $\Xi_1$, on which
%that will be commonly used in the proof. First, by Corollary \ref{fact_minv}, we  
\be\label{Op_norm444}
\begin{split}
&\lambda_1\left( (X^{(1)})^\top X^{(1)} \right)\sim \lambda_p\left( (X^{(1)})^\top X^{(1)} \right) \sim n_1\sim n, \\
&\lambda_1\left( (X^{(2)})^\top X^{(2)} \right)\sim \lambda_p\left( (X^{(2)})^\top X^{(2)} \right) \sim n_2 \sim n,
\end{split}\ee
where we denote $n:=n_1+n_2$. Throughout the following proof, we assume that event $\Xi_1$ holds. Notice that using \eqref{Op_norm444}, we can bound
\be\label{Op_norm455}
h(a)\lesssim n\left[\frac{(a-1)^2}{a^2+1}\|\beta_0\|^2 + \sigma^2 + d^2\right].
\ee

Let $a^{\star}$ denote the global minimizer of $h(a)$. Our proof involves two steps: we first show that $\abs{x^{\star} - 1} \le d^2 / \kappa^2$; then we use Claim \ref{claim_largedev1} to show that the global minimizers of $g(a)$ and $h(a)$ are close to each other.
%Using Lemma \ref{largedeviation} again, we can simplify equation \eqref{revise_eq_val_mtl} as $\val(v)= N_2h(v) \cdot  \left( 1+\OO(p^{-1/2+\e})\right)$, where the function $ h$ is defined as
%	%We define the function
%	\begin{align}
%		h(v) =& \frac{\rho_1}{\rho_2}\left[d^2 +\left( v-1\right)^2\kappa^2\right]\cdot \tr\left[( v^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%		& +  v^2\left[d^2 +\left( v-1\right)^2\kappa^2\right]\cdot \tr\left[( v^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%		& + \left(\frac{\rho_1}{\rho_2} v^2 + 1\right)\sigma^2 \cdot \bigtr{(v^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} }. \nonumber
%	\end{align}
%	%\cor Under the setting of Lemma \ref{prop_model_shift_tight},
%	Furthermore, the validation loss in equation \eqref{approxvalid} reduces to
%	\be\label{boundv-w}
%		g(v)=\left[N_2 h(v) + (N_1+N_2)\sigma^2\right]\cdot \left( 1+\OO(p^{-(1-\e_0)/2+\e})\right)\quad \text{w.h.p.}
%	\ee
%	for any constant $\e>0$. %\nc Thus for the following discussions, it suffices to focus on the behavior of $h (v)$.
%Let $\hat w$ be the minimizer of $h(v)$. Our proof consists of the following two steps.
%\squishlist
%	\item First, we show that $\hat{w}$ is close to $1$.
%	\item Second, with equation \eqref{boundv-w} we show that $\hat v$ is close to $\hat w$. Then we plug $\hat{v}$ into $\te(\hat{\beta}_2^{\MTL})$ to get equation \eqref{simple1}.
%\squishend
%%For the minimizer $\hat w$ of $\val(w)$, we have a similar result as in Proposition \ref{thm_cov_shift}.
%For the first step, we will prove the following result.
%To be consistent with the notation $\hat w$, we change the name of the argument to $w$ in the proof.



For the first step, it is easy to observe that $h(a)< h(-a)$ for any positive $x$.
Hence the minimum of $h(a)$ is achieved when $a$ is positive.
%$h(1)=\OO(pd^2 + \sigma^2)$ and $ h(w)\gtrsim p\kappa^2 \gg h(1)$ for $w\ge 2$ or $w\le 1/2$.  Hence it suffices to consider the case $1/2\le w\le 2$.
We first  consider the case where $a\ge 1$. We define the matrix
$$ \cal M(a):= (X^{(1)} )^\top X^{(1)}  \hat\Sigma(a)^{-1} (X^{(2)})^\top X^{(2)} $$
Notice that for any vector $\bv\in \R^p$, the following function is an increasing function in $a^2$:
\begin{align*}
&a^2 \mathbf v^\top \cal M(a) \bv  = \bv^\top\left([(X^{(2)})^\top X^{(2)}]^{-1} + a^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\mathbf v .
\end{align*}
Hence taking the derivative of $h(a)$, we obtain that %for any $x > 1+d^2/\kappa^2$,
\begin{align}
h'(a)& \ge  \frac{2(a-1)}{a^3} \beta_0^\top \left[a^2 \cal M(a) \right] \beta_0   - 2\frac{d^2}{a^3} \cdot p^{-1} \tr\left[a^2 \cal M(a) \right]  \label{h'(A)1}
\end{align}
By \eqref{Op_norm444}, we have 
\be\nonumber %\label{sim_norm}  
\beta_0^\top \left[a^2 \cal M(a) \right] \beta_0 \sim \|\beta_0\|^2 \cdot p^{-1} \tr\left[a^2 \cal M(a) \right] \ee
Hence if $a-1\gg d^2 /\|\beta_0\|^2$, then $h'(a)>0$. Then we consider the case where $a \le 1$. Notice that for any vector $\bv\in \R^p$, $\bv^\top \cal M(a) \bv $ is a decreasing function in $a^2$.
%$$\tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} +  [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].$$
Hence taking derivative of $h(a)$, we obtain that %for any $x\le 1-d^2/\kappa^2$,
\begin{align}\label{h'(A)2}
	h'(a)\le -2(1-a)  \beta_0^\top \cal M(a)  \beta_0 + 2 ad^2  \cdot p^{-1} \tr\left[\cal M(a)\right] < 0,
\end{align}
if $1-a\gg d^2 /\|\beta_0\|^2$. In sum, we see that there exists a constant $C_0>0$, such that 
\be\label{C0_eq}
h'(a) \begin{cases} >0, \ &\text{if }\ a-1\ge C_0 {d^2}/{\|\beta_0\|^2}\\ <0, \ &\text{if }\ 1-a\ge C_0 {d^2}/{\|\beta_0\|^2} \end{cases}.
\ee
This gives that $|a^\star-1|=\OO({d^2}/{\|\beta_0\|^2})$.

%the global minimizer of $h(a)$ satisfies 
%\be .\ee
%lies within $1-d^2/\kappa^2$ and $1+d^2/\kappa^2$.

%Taking derivative of $h(w)$, we obtain that
%\begin{align}
%	h'(w) \le& \frac{\rho_1}{\rho_2} \left[2\left( w-1\right) \kappa^2\right]\cdot \tr\left[( w^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%	& +  \left[2wd^2 +2w\left( w-1\right)(2w-1)\kappa^2\right]\cdot \tr\left[( w^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%	& + \frac{\rho_1}{\rho_2}(2 w\sigma^2) \cdot \bigtr{(w^2X_1^{\top}X_1 + X_2^{\top}X_2)^{-2} X_2^\top X_2  }= \frac{\rho_1}{\rho_2} \bigtr{(w^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} \cal B} , \nonumber
%\end{align}
%where the matrix $\cal B$ is
%$$\cal B= 2\left( w-1\right) \kappa^2  (X_2^{\top}X_2)^2+\frac{\rho_2}{\rho_1}\left[2wd^2 +2w\left( w-1\right)(2w-1)\kappa^2\right](X_1^{\top}X_1)^2 + 2 w\sigma^2 X_2^\top X_2 .$$
%Using Fact \ref{lem_minv}, we get that $\cal B$ is upper bounded as
%\begin{align*}
%\cal B \preceq - 2(1-w)\kappa^2 n_2^2 (\al_-(\rho_2) -\oo(1))^2 +2w d^2 n_1n_2 (\al_+(\rho_1) +\oo(1))^2 + 2w\sigma^2 n_2 (\al_+(\rho_2)+\oo(1)) \prec 0,
%\end{align*}
%as long as
%$$w< w_2:=1 -   \frac{d^2}{\kappa^2}\frac{\rho_1(\al_+(\rho_1) +\oo(1))^2}{\rho_2 \al_-^2(\rho_2) } -  \frac{\sigma^2}{n_2\kappa^2}\frac{\al_{+}(\rho_2)+\oo(1)}{\al_{-}^2(\rho_2)} .$$
%Hence $h'(w)<0$ on $[0,w_2)$, %i.e. $h(w)$ is strictly decreasing for $w<w_2$. This
%which gives $\hat w\ge w_2$.

%In sum, we have shown that $w_2\le w\le w_1$. Together with
%$$\max\{|w_1 -1|, |w_2 -1|\} =\OO\left(\frac{d^2}{\kappa^2} + \frac{\sigma^2}{p\kappa^2}\right),$$
%we conclude equation \eqref{hatw_add1}.


%For the rest of this section, we choose parameters that satisfy the following relations: \be\label{choiceofpara}
%pd^2 \sim \sigma^2 \sim 1,\quad p^{-1+c_0} \sigma^2 \le \kappa^2  \le p^{-\e_0-c_0}\sigma^2 ,
%\ee
%for some small constant $c_0>0$.

For the second step, we show that if $\hat{a}$ deviates too much from $1\pm C_0 {d^2}/{\|\beta_0\|^2}$, then it is no longer a global minimum of $g(a)$. We first argue that $|\hat a -1|\le 1$.  
%Hence we can apply Claim \ref{claim_largedev1} to bound the concentration error of $g(x)$.
%Using Fact \ref{fact_minv} (ii), we can obtain that with high probability,
%\begin{align}
%p(x^2+1)^{-1}&\lesssim \left( x^2 \left[(\sqrt{n_2}-\sqrt{p})^{2}-n_2p^{-c_\varphi}\right]^{-1} +\left[(\sqrt{n_1}-\sqrt{p})^{2}-n_1p^{-c_\varphi}\right]^{-1} \right)^{-1} \nonumber\\
%&\preceq (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} =\left( x^2 [(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1} \right)^{-1} \nonumber\\
%&\preceq \left( x^2 \left[(\sqrt{n_2}+\sqrt{p})^{2}+n_2p^{-c_\varphi}\right]^{-1} +\left[(\sqrt{n_1}+\sqrt{p})^{2}+n_1p^{-c_\varphi}\right]^{-1} \right)^{-1}\lesssim p(x^2+1)^{-1}, \label{op_uplow}
%\end{align}
%where we also used that $3\le n_1/p \le \tau^{-1}$ and $3\le  n_2/p \le \tau^{-1}$ for a small constant $\tau>0$. Then we get that
%\be\label{eq_h1}h(1)= 2d^2 \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(1)^{-1}(X^{(2)})^\top X^{(2)}\right]\lesssim pd^2 .\ee
In fact, if $|\hat a-1| > 1$, then using \eqref{Op_norm444} we can get that 
$$h(\hat a) -\sigma^2(n-p)\gtrsim n\|\beta_0\|^2 .$$
%\begin{align}\label{eq_hA}
%h(\hat a)\gtrsim \|\beta_0\|^2 .
%\end{align}
%where in the first step we used that $|x-1|^2\gtrsim 1+x^2$ for $|x-1|\ge \delta$ and equation \eqref{op_uplow}.
%and in the second step we used equation \eqref{choiceofpara0} \HZ{I'm not sure which condition we use? put it here}.
On the other hand, we have $h(1)= \sigma^2(n-p)+ \OO( n d^2)$. Hence under \eqref{para_rel}, we have
\be\label{contra_eq1}
h(\hat a)- h(1)\gtrsim n \|\beta_0\|^2.
\ee
Then using Claim \ref{claim_largedev1} and the fact that $g(a^\star)\ge g(\hat a)$, we get
\begin{align*}
	h(\hat{a}) - h(1) &= [g(\hat{a}) - g(1)] + [h(\hat{a}) - g(\hat{a})] + [g(1) - h(1)] \\
	&\le p^{-1/2+c}\left[ h(\hat a) +h(1)\right]+ p^{-C}\|\beta_0\|^2 \\
	& \lesssim p^{-1/2+c}n \left(\|\beta_0\|^2 + d^2 + \sigma^2 \right) \ll n \|\beta_0\|^2,
\end{align*}
where we use \eqref{Op_norm455} in the third step, and condition \eqref{para_rel} in the last step as long as $c<c_0$. This contradicts \eqref{contra_eq1}. Hence we conclude that $|\hat a -1|\le 1$. 

%\begin{align}
%g(x)&\ge  h(x) +\sigma^2(n_1+n_2-p) -p^{-1/2+c}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right) \nonumber\\
%&\ge h(1) +\sigma^2(n_1+n_2-p) + p^{-1/2+c}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right) \ge g(1), \label{gA>g1}
%\end{align}
%under conditions $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$. %\eqref{choiceofpara0} \HZ{again, I don't see which condition we use here?}.
%This gives that
%Hence it remains to consider the region $|A-1|\le \delta$ for a small enough constant $\delta>0$.


Now suppose that 
\be\label{hypo_x}1 + p^{-C_1}+  C_1\frac{d^2}{\|\beta_0\|^2} + p^{-1/4+c_1} \frac{d+\sigma}{\|\beta_0\|}\le  \hat{a} \le 2,\ee
for a small constant $c_1>0$ and a large constant $C_1> 0$. 
Using \eqref{Op_norm444} and equation \eqref{h'(A)1}, it is not hard to check that the following estimate holds for all $a \ge 1+C_1 d^2/(2{\|\beta_0\|^2})$ as long as $C_1$ is large enough: % can lower bound the derivative of $h(a)$ as
	\[ h'(a) \gtrsim \frac{a-1}{a} \beta_0^\top  \cal M(a) \beta_0 \gtrsim n_1\|\beta_0\|^2\frac{a-1}{a (a^2  + 1)}.\]
	%for some constants $c_1,c_2>0$ that are independent of $C_1$.
	%For any $x \ge 1+C_2 d^2/(2\kappa^2)$, 
Therefore, under \eqref{hypo_x}, we have %the difference between $h(x)$ and $h(1)$ is at least the following
\begin{align}
		  h(\hat a)-h(a^\star) & \ge h(\hat a) - h\left(1+\frac{C_1d^2}{2\|\beta_0\|^2 }\right)
	\ge \int_{1+\frac{C_1 d^2}{2\|\beta_0\|^2}}^{\hat a} h'(a)\dd a \nonumber\\
	& \gtrsim n \|\beta_0\|^2 \int_{1+\frac{C_1 d^2}{2\|\beta_0\|^2}}^{\hat a} \frac{a-1}{a(1 + a^2)} \dd a  \gtrsim  n |\hat a-1|^2\|\beta_0\|^2\nonumber \\
	& \gtrsim n \left(|\hat a-1|^2\|\beta_0\|^2 + p^{-1/2+2c_1}d^2+ p^{-1/2+2c_1}\sigma^2\right), \label{ha-ha}
\end{align}
where we use the condition \eqref{para_rel} in the last step. 
%for a constant $c_3>0$ that are independent of $C_1$. 
On the other hand, using Claim \ref{claim_largedev1} and the fact that $g(a^\star)\ge g(\hat a)$, we get
%When $\hat{x}$ is sufficiently far from $1$ (e.g. $2d^2 / \kappa^2 + p^{-1/4+\e}$), one can verify that $h(x) - h(1)$ is at least $\OO(p\frac{d^4}{\kappa^2} + p^{1/2+2c} \kappa^2) > \OO(p^{1/2 + c} (\sigma^2 + \kappa^2 + d^2))$, under the condition that $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$.
%On the other hand, by triangle inequality and Claim \ref{claim_largedev1} we have that
\begin{align*}
	h(\hat{a}) - h(a^\star) &= [g(\hat{a}) - g(a^\star)] + [h(\hat{a}) - g(\hat{a})] + [g(a^\star) - h(a^\star)] \\
	&\le p^{-1/2+c}\left[ h(\hat a) +h(a^\star)\right]+ p^{-C}\|\beta_0\|^2 \lesssim p^{-1/2+c}n \left(|\hat a-1|^2\|\beta_0\|^2 + d^2 + \sigma^2 \right),
\end{align*}
where in the last step we use \eqref{Op_norm455} to bound  $ h(\hat a)$ and $h(a^\star)$. This contradicts \eqref{ha-ha} as long as $c<2c_1$. On the other hand, suppose that
 $$0\le \hat a\le 1 - p^{-C_1} - C_1\frac{d^2}{\|\beta_0\|^2} - p^{-1/4+c_1} \frac{d+\sigma}{\|\beta_0\|} .$$
%by Claim \ref{claim_largedev1}, we have that $g(a)$ and $h(a)$ differ by at most $p^{-1/2+c}h(a) + p^{-C}\kappa^2 \|\beta_0\|^2$. Then 
%$$ C_1^2 d^2 + p^{-1/2+2c_1} \sigma^2 +$$
%Then if we choose the constant $c$ in Claim \ref{claim_largedev1} such that $e_1<2e_2$, we have
%$$ h(x)-h(1)\gtrsim  p^{-1/2+2e_2}\cdot p\kappa^2 \gg p^{-1/2+e_1}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right),$$ where we use the assumption that $\sigma^2$ and $d^2$ are both at most $\OO(\kappa^2)$, which implies the inequality \eqref{gA>g1}.
%Second, suppose that $\hat{x} \le 1 - 2d^2/ \kappa^2 - p^{-1/2 + \e}$.
Then using equation \eqref{h'(A)2} and a similar argument as above, we can also arrive at a contradiction. 
%obtain that for any $x \le 1 - 3d^2/(2\kappa^2)$,
%	\[ -h'(x)\ge  [2(1-x)\kappa^2 - 2xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]  \gtrsim p\kappa^2 \cdot \frac{(1 - x)x^2}{1 + x^2}. \]
%Using a similar argument to the first case, we get that the difference between $h(x)$ and $h(1)$ is at least the integral of the abvoe derivative.
%under equation \eqref{choiceofpara0} \HZ{I don't see which condition we want to use here?},
%This implies that $\hat{x}$ cannot be too far from one.
Hence we have shown that 
$$|\hat a -1| \le  p^{-C_1} + C_1\frac{d^2}{\|\beta_0\|^2} + p^{-1/4+c_1} \frac{d+\sigma}{\|\beta_0\|} ,$$
which completes the proof of \eqref{hatw_add1}.
%Next we prove the following estimate on the optimizer $\hat x$: with high probability,
%%\begin{lemma}
%%For the isotropic model, we have
%\be\label{hatv_add1}
%|\hat v - 1|= \OO\left(\cal E\right), \quad \cal E:=\frac{d^2}{\kappa^2} + \frac{\sigma^2}{p\kappa^2} + p^{-1/2 + \e_0 /2+ 2\e}.
%\ee
%%\end{lemma}
%%\begin{proof}
% In fact, from the proof of Claim \ref{lem_hat_v} above, one can check that if $C\cal E \le |w-1| \le 2C\cal E$ for a large enough constant $C>1$, then $|h'(w)|\gtrsim \sigma^2$. Moreover, under equation \eqref{choiceofpara} we have
%$$h(w) =\OO(\sigma^2),\quad \text{for}\quad   |w-1|\le 2C\cal E.$$
%Thus we obtain that for $|w-1|\ge 2C\cal E$,
%$$\left|h(w) - h(\hat w)\right|\ge |h(w)-\min\{h(w_1),h(w_2)\}|\gtrsim \sigma^2 \cal E \gtrsim \cal E \cdot h(\hat w),$$
%which leads to $g(w) > g(\hat w)$ with high probability by equation \eqref{boundv-w}. Thus $w$ cannot be a minimizer of $g(v)$, and we must have $|\hat v - 1|\le 2C\cal E$. %Together with equation \eqref{hatw_add1}, we conclude equation \eqref{hatv_add1}.
%%\end{proof}
%
%{\cor require $\kappa^2\gg d^2 + p^{-1/2+c}\sigma^2$}
\end{proof}

\iffalse
\paragraph{Part $2$: a reduction to the bias and variance limits.}
Recall that the hard parameter sharing estimator $\hat{\beta}_2^{\MTL}$ is equal to $\hat{B} \hat{A}_2$.
Using the local optimality condition for $\hat{B}$, we obtain the predication loss of HPS:
\begin{align}
L(\hat{\beta}_2^{\MTL}) &=\left\|(\Sigma^{(2)})^{1/2} \left( \hat B \hat A_2 - \beta^{(2)}\right)\right\| \nonumber\\
&=  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x\beta^{(1)}-\hat x^2\beta^{(2)})+ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2. %\label{Lbeta_HPS}
\end{align}
Using Lemma \ref{lem_hat_v} and the concentration estimates in Lemma \ref{largedeviation}, we simplify $L(\hat{\beta}_2^{\MTL})$ as follows.

\begin{claim}\label{claim_reduction}
Recall that $\hat \Sigma(1)$ is equal to $\hat \Sigma$ (cf. equation \eqref{def hatsig}).
In the setting of Claim \ref{cor_MTL_loss}, we have the following estimate w.h.p.
\begin{align*}
&\left|L(\hat{\beta}_2^{\MTL}) - \frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \bigtr{{\hat \Sigma^{-1}  }}\right| \nonumber\\
\lesssim&  \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2).
\end{align*}
\end{claim}
\begin{proof}
Our proof is divided into two steps. First, using Lemma \ref{largedeviation}, we show that
\be\label{claim_largedev2} \left| L(\hat{\beta}_2^{\MTL})- \cal L(\hat x)\right| \le p^{-1/2+c} \left(\sigma^2 +\kappa^2 +d^2 \right),
\ee
where $\cal L(\hat x)$ is defined as
\begin{align*}
	\resizebox{0.97\hsize}{!}{%
	$\cal L(\hat x)	:=  \hat x^2\left[(\hat x-1)^2 \kappa^2 +  (\hat x^2+1)d^2 \right] \cdot p^{-1}\bigtr{ (X^{(1)})^{\top}X^{(1)} \hat\Sigma(\hat x)^{-1} \Sigma^{(2)} \hat\Sigma(\hat x)^{-1} (X^{(1)})^{\top}X^{(1)} }
	 +\sigma^2\cdot \bigtr{\Sigma^{(2)}\hat \Sigma(\hat x)^{-1}}.$
	}%
\end{align*}
% \end{claim}
%We are now ready to finish the proof of Claim \ref{claim_reduction}.
Next, we further simplify $\cal L(\hat x)$ since $\hat{x}$ is close to one and $\Sigma^{(1)},\Sigma^{(2)}$ are both isotropic
\begin{align}
	&\left|\cal L(\hat x)- \frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \bigtr{\hat \Sigma^{-1}}\right| \nonumber\\
\lesssim & \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2). \label{simple12}
\end{align}
Combining equation \eqref{claim_largedev2} and \eqref{simple12}, we obtain the desired claim.
We prove these two equations one by one as follows.

First, we prove equation \eqref{simple12}.
We can bound the left hand side of equation \eqref{simple12} as
\begin{align*}
	&\left|\cal L(\hat x)-\frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left({\hat\Sigma^{-1}  }\right)\right| \\
	\lesssim &\left(|\hat x-1|^2 \kappa^2 + |\hat x-1|d^2\right)\cdot p^{-1}\bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2 } \\
	&+ \frac{d^2}{p}\left|\tr\left[\left(\hat\Sigma(\hat x)^{-2}-\hat\Sigma^{-2}\right) \left((X^{(1)})^\top X^{(1)} \right)^2\right]\right|+ \sigma^2  \left|\bigtr{\hat\Sigma(\hat x)^{-1}-\hat\Sigma^{-1}  }\right|.
\end{align*}
We deal with the trace terms in the above equation one by one.
Using Claim \ref{lem_hat_v} and operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)$, we have that w.h.p.
\begin{align}
	\|\hat\Sigma^{-1}-\hat\Sigma(\hat x)^{-1}\| &\le |\hat x^2-1| \cdot \|\hat\Sigma^{-1}\| \cdot \| (X^{(1)})^\top X^{(1)}\| \cdot \|\hat\Sigma(\hat x)^{-1}\|  \lesssim p^{-1}\left(\frac{d^2}{\kappa^2} + p^{-1/4+c}\right).\label{est111}
\end{align}
Using similar arguments, we get that w.h.p.
\begin{align}\label{est222}
&\left\|\left(\hat\Sigma^{-2}-\hat\Sigma(\hat x)^{-2}\right)\left((X^{(1)})^\top X^{(1)} \right)^2\right\| \lesssim  \frac{d^2}{\kappa^2} + p^{-1/4+c},
\end{align}
and
\begin{align}
& \bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}  \le p \bignorm{\hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}\lesssim p.\label{est333}
\end{align}
Applying the above results \eqref{est111}, \eqref{est222}, and \eqref{est333} to the bound of $\cL(\hat{x})$ above, we have shown that equation \eqref{simple12} holds.

Second, we prove equation \eqref{claim_largedev2}.
The proof is very similar to Claim \ref{claim_largedev1}.
The key difference is that $\hat x$ correlates with $\epsilon^{(1)}$, $\epsilon^{(2)}$, $\beta^{(1)}$, and $\beta^{(2)}$.
Nevertheless, Lemma \ref{largedeviation} still applies for any arbitrary $\hat x$.
We describe a proof sketch and omit the details.
Recall that $\beta_0$ is the shared component of $\beta^{(1)}$ and $\beta^{(2)}$ with i.i.d. Gaussian entries of mean zero and variance $p^{-1}\kappa^2$.
The task-specific components, denoted by $\wt\beta^{(1)}$ and $\wt\beta^{(2)}$, consist of i.i.d. Gaussian random variables with mean zero and variance $p^{-1} d^2$.
We write $L(\hat{\beta}_2^{\MTL}) $ from equation \eqref{Lbeta_HPS} as:
\begin{align}
L(\hat{\beta}_2^{\MTL})  =&  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x -\hat x^2)\beta_0+(X^{(1)})^\top X^{(1)} \hat x\wt \beta^{(1)} - (X^{(1)})^\top X^{(1)}  \hat x^2\wt \beta^{(2)} \right] \right. \nonumber\\
&\left. + (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1}\left[ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2. \label{expand_15}
\end{align}
Similar to the analysis of $g(x)$, we expand $L(\hat{\beta}_2^{\MTL})$ into the sum of 15 terms, and bound the concentration error of each term similar to $h_1(x), \dots, h_6(x)$.
For example, for the leading term 
\[\hat x^2 (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)},\] 
using Lemma \ref{largedeviation} and the operator norm bounds, we obtain the following estimate w.h.p.
\begin{align*}
&  \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)} - \frac{d^2}{p}\bigtr{(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}}\right| \\
&\le p^{-1+c}d^2 \cdot \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|_F \\
&\le p^{-1/2+c}d^2 \cdot  \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|  \lesssim p^{-1/2+c}d^2.
\end{align*} 
For the cross term $\hat x (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}$, using Lemma \ref{largedeviation} and the operator norm bounds, we obtain the following estimate w.h.p.
\begin{align*}
& \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}\right|  \\
  \le & p^c \cdot \sigma\sqrt{p^{-1}d^2} \cdot \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \|_F \\
 \lesssim & p^{c}\cdot \sigma d \cdot  \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \| \\
 \lesssim & p^{-1/2+c}\sigma d\le p^{-1/2+c}(\sigma^2+ d^2).
\end{align*}
The rest of the terms in the expansion of $L(\hat{\beta}_2^{\MTL})$ can be dealt with similarly, and we omit the details.
 \end{proof}

  
\paragraph{Part $3$: applying the bias-variance limits.}
Finally, we are ready to complete the proof of Corollary \ref{cor_MTL_loss}.
We derive the variance term $\sigma^2  \tr[\hat\Sigma^{-1}]$ and the bias term $\frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right]$ using our random matrix theory results.

\begin{proof}[Proof of Corollary \ref{cor_MTL_loss}]
For the variance term, using equation \eqref{lem_cov_shift_eq}, we obtain that
\be\label{eq_var111}\tr[\hat\Sigma^{-1}] = \tr\left[ \left((X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}\right)^{-1}\right]=\bigtr{\frac{(a_1 +a_2)^{-1}\id_{p\times p}}{n_{1}+n_2} }+\OO(p^{-c_\varphi})\ee
with high probability. Solving equation \eqref{eq_a12extra} with $\lambda_i\equiv 1$, $1\le i\le p$, we get that  
	\begin{align}
		 a_1 = \frac{n_1(n_1 + n_2 - p)}{(n_1 + n_2)^2} ,\quad
		& a_2 = \frac{n_2(n_1 + n_2 - p)}{(n_1 +n_2)^2} . \label{simplesovlea12}
			\end{align}
Applying the above to equation \eqref{eq_var111}, we obtain that with high probability
\be\label{eq_var112}\tr[\hat\Sigma^{-1}]  = \frac{p}{n_1+n_2} \cdot \frac{n_1+n_2}{n_1+n_2-p}+\OO(p^{-c_\varphi})=  \frac{p}{n_1+n_2-p}+\OO(p^{-c_\varphi}).\ee
For the bias term, since the spectrum of $(X^{(1)})^{\top} X^{(1)}$ is tightly concentrated by Fact \ref{fact_minv}, we have that
\begin{align}
 \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi})}{p} \bigtr{\hat \Sigma^{-2}}  &\le {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] \label{eq_bias111}\\
 &\le \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi})}{p} \bigtr{\hat\Sigma^{-2}}.\nonumber
\end{align}
%To obtain this inequality, we used
%$$ (\sqrt{n_1}-\sqrt{p})^4 \cdot (1-p^{-c_\varphi}) \preceq \left((X^{(1)})^\top X^{(1)} \right)^2 \preceq (\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi}) \quad \text{with high probability},$$
%by Fact \ref{fact_minv} (ii), and the fact that for the product of two PSD matrices, its trace is always nonnegative.
Using the bias limit \eqref{lem_cov_derv_eq} with $\Sigma^{(1)}=\Sigma^{(2)}=\Lambda=V=\id_{p\times p}$, and $w = e_i$ (the $i$-th coordinate vector), we have w.h.p. (via a union bound)
\begin{align*}%\label{eq_bias112}
 e_i^{\top}\hat\Sigma^{-2} e_i = \frac{1}{(n_1+n_2)^2} \left[\frac{a_3+a_4+1}{(a_1+a_2)^2} +\OO(p^{-c_\varphi})\right], \text{ for all } i = 1, 2, \dots, p.
\end{align*}
We solve the self-consistent equations \eqref{eq_a34extra} given $a_1, a_2$, and obtain
\begin{align*}
		a_3 = \frac{p\cdot n_1}{(n_1 +n_2)(n_1 +n_2 - p)}, \quad
		&  a_4 = \frac{p\cdot n_2}{(n_1 + n_2)(n_1 + n_2 - p)}. %\label{simplesovlea34}
\end{align*}
Applying $a_3, a_4$ to the equation above, we obtain
\begin{align*}%\label{eq_bias113}
 e_i^{\top}\hat\Sigma^{-2}e_i =  \frac{1}{(n_1+n_2)^2} \left[  \frac{ (n_1+n_2)^3 }{(n_1+n_2-p)^3} +\OO(p^{-c_\varphi})\right], \text{ for all } i = 1, 2, \dots, p.
\end{align*}
%In fact, we need this estimate to hold simultaneously for all $1\le i \le p$ with high probability. For this purpose, we shall use equation \eqref{apply derivlocal} to get that
%\begin{align*}%\label{eq_bias113}
%\left| \left(\hat\Sigma^{-2}\right)_{ii} - \frac{1}{(n_1+n_2)^2}\cdot \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\prec \frac{p^{-\frac{\varphi-4}{2\varphi}}}{(n_1+n_2)^2}
%\end{align*}
%on a high probability event that does not depend on $i$. Then using Fact \ref{lem_stodomin} (i), we obtain that
%\begin{align*}
%\left| \sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} - \frac{p}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\prec \frac{p^{1-\frac{\varphi-4}{2\varphi}}}{(n_1+n_2)^2}
%\end{align*}
%with high probability, which by Definition \ref{stoch_domination} gives that
%\begin{align}\label{eq_bias113}
%\left| p^{-1}\sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} - \frac{1}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\le \frac{p^{-c_\varphi}}{(n_1+n_2)^2}
%\end{align}
%with high probability.
Applying the above result to equation \eqref{eq_bias111}, we get the desired result for the bias term.
%\begin{align*}
%		& \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi}) - n_1^2 \cdot (1+\OO(p^{-c_\varphi}))}{(n_1+n_2)^2}\cdot   \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\nonumber \\
%		\le &  {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] - \frac{n_1^2}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3} \nonumber\\
%		\le &  \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi}) - n_1^2 \cdot (1+\OO(p^{-c_\varphi}))}{(n_1+n_2)^2}\cdot   \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}.
%\end{align*}
Combining the bias and variance estimates, we get that
\begin{align*}
	 \left|L(\hat{\beta}_2^{\MTL})  -  \frac{2d^2 n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3} - \frac{\sigma^2 p}{n_1+n_2-p}   \right|&
	  \le   \left[\left( 1+\sqrt{\frac{p}{n_1}}\right)^4-1\right] \cdot \frac{2d^2n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3} \\
	&\hspace{-2cm}+\OO\left( p^{-c_\varphi} (\sigma^2 + d^2)+ \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2) \right).
\end{align*}
Since $\sigma^2 \lesssim  \kappa^2$ and $d^2 \le p^{-\e}{\kappa^2}$ by our assumption, we obtain the desired result.
The proof is complete.
\end{proof}

 \fi



\section{Proof of Theorem \ref{thm_main_RMT} and Proposition \ref{prop_main_RMT}}\label{appendix RMT}
%We first state the asymptotic limit for the bias equation \eqref{eq_bias_2task}.

%\begin{theorem}\label{thm_main_RMT_app}
%
%	In the setting of Theorem \ref{thm_main_RMT},
%\end{theorem}
%For the rest of this section, we present the proof of Theorem \ref{thm_main_RMT} and Theorem \ref{thm_main_RMT_app}

%\paragraph{Proof Overview (cont'd).}
%We continue the proof overview of Theorem \ref{thm_main_RMT} and \ref{thm_main_RMT_app} from Section \ref{sec_diff}.
%Since the full proof of Theorem \ref{thm_main_RMT} and Theorem \ref{thm_main_RMT_app} is rather technical, in this subsection we give a more detailed proof overview, which contains the main ideas of the proof.
%Recall that $(W-z\id)^{-1}$ is the resolvent of matrix $W$.


%For the rest of this section, we present an overview of the proof of Theorem \ref{thm_main_RMT}.

In this section, we give the proofs of Theorem \ref{thm_main_RMT} and Proposition \ref{prop_main_RMT}. The central quantity of interest is the inverse of the sum of two sample covariance matrices $\hat \Sigma(a)^{-1}$. Assume that $M\equiv M(a)=a(\Sigma^{(1)})^{1/2}(\Sigma^{(2)})^{-1/2}$ has singular value decomposition
\be\label{eigen2}
M = U\Lambda V^\top, \quad \text{where} \ \ \Lambda\equiv \Lambda (a):=\text{diag}( \lambda_1(a), \ldots, \lambda_p(a)),
\ee 
Then we can write the variance term in \eqref{Lvar} as
%equation $\tr[\Sigma^{(2)} \hat{\Sigma}^{-1}]$ is equal to $(n_1 + n_2)^{-1} \bigtr{W^{-1}}$, where $W$ is
\begin{align}\label{eigen2extra}
\tr[\Sigma^{(2)} \hat{\Sigma}(a)^{-1}]= \frac1n \tr[W(a)^{-1}],
\end{align}
where we denote $n:=n_1+n_2$ and
$$ W(a):=n^{-1}\left( \Lambda(a) U^\top (Z^{(1)})^\top Z^{(1)} U\Lambda(a)  + V^\top (Z^{(2)})^\top Z^{(2)}V\right).$$
%We recall that $U\Lambda V^\top$ is the SVD of $M$ as defined in \eqref{eigen2}.
%This formulation is helpful because we know that $(Z^{(1)})^{\top} Z^{(1)}$ and $(Z^{(2)})^{\top} Z^{(2)}$ are both sample covariance matrices with isotropic population covariance, and $U, V$ are both orthonormal matrices.
%For example, if $Z^{(1)},Z^{(2)}$ are both Gaussian random matrices, by rotational invariance, $Z^{(1)} U, Z^{(2)}V$ are still Gaussian random matrices.
%Our proof is based on the classical Stieltjes transform or the resolvent method in random matrix theory. We 
%briefly describe the basic concepts and 
%refer the interested readers to classical texts such as  \cite{bai2009spectral,tao2012topics,erdos2017dynamical} for a thorough introduction of this method. 
For $W(a)$, its \emph{resolvent} or \emph{Green's function} is defined as $(W(a) - z\id_{p\times p})^{-1}$ for $z\in \C$.
%When $p$ goes to infinity, it is well-known that $m_{\mu}(z)$ converges to a fixed distribution governed by a set of self-consistent equations.
%These self-consistent equations give the asymptotic limit of the trace of $\hat{\Sigma}^{-1}$.
%The above approach applies when $\Sigma^{(2)}$ is isotropic in Theorem \ref{thm_main_RMT}.
%Since our goal is to show the limit of $\tr\left[ (Y - z\id)^{-1}V^{\top}{\Sigma^{(2)}}^{-1}V \right]$ as shown in equation \eqref{eigen2extra}, we study the  $(Y-z\id)^{-1}$.
%Compared to the Stieltjes transform, the resolvent also applies to random matrices.
In this section, we will prove a local convergence of this resolvent with a sharp convergence rate, which is conventionally referred to as a ``local law of the resolvent''  \cite{isotropic,erdos2017dynamical,Anisotropic}.
%Recent developments in the random matrix literature have shown the convergence of the resolvent matrix using the so-called ``local laws'' or ``deterministic equivalents'' (cf. \cite{Hachem2007deterministic,DS18}).

\subsection{Resolvent and local law}\label{sec pf RMTlemma}


%For our purpose, we use a convenient linearization trick in linear algebra, that is, the SVD of a rectangular matrix $A$ is equivalent to the study of the eigendecomposition of the symmetric block matrix
%$$H(A):=\begin{pmatrix}0 & A \\ A^\top & 0\end{pmatrix},$$
%which is linear in $A$. This trick has been used in many random matrix literatures, such as \cite{Anisotropic, AEK_Gram, XYY_circular,DY20201}.
%We say that $(W-z\id)^{-1}$ converges to a deterministic $p\times p$ matrix limit $R(z)$ if for any sequence of deterministic unit vectors $v\in \R^p$,
%$$v^\top \left[(W-z\id)^{-1}-R(z)\right]v\to 0\ \ \ \text{when $p$ goes to infinity.
%}$$
For $W\equiv W(a)$, we can write that $W=\AF\AF^{\top}$ for a $p\times n$ matrix
%to study $W$'s resolvent, we observe that $W$ is equal to $\AF\AF^{\top}$ for a $p$ by $n_1 + n_2$ matrix
	\be\label{defn AF} \AF := n^{-1/2} [\Lambda U^\top (Z^{(1)})^\top,V^\top (Z^{(2)})^\top]. \ee
%}\HZ{what does this trick mean? use less technical words},
%This idea dates back at least to Girko, see e.g., the works \cite{girko1975random,girko1985spectral} and references therein.
We introduce a convenient self-adjoint linearization trick to study such a matrix. It has been proved to be useful in studying the local laws of random matrices of Gram type \cite{Anisotropic, AEK_Gram, XYY_circular,yang2019spiked}. 

\begin{definition}[Self-adjoint linearization and resolvent]\label{defn_resolventH}
We define the following $(p+n)\times (p+n)$ symmetric block matrix %whose dimension is $p + n_1 + n_2$
%which is a linear function of $(Z^{(1)})$ and $(Z^{(2)})$:
%\begin{definition}[Linearizing block matrix]\label{def_linearHG}%definiton of the Green function
%We define the $(n+N)\times (n+N)$ block matrix
 \begin{equation}\label{linearize_block}
    H \define \left( {\begin{array}{*{20}c}
   0 & \AF  \\
   \AF^{\top} & 0
%   {Z^{(2)}V} & 0 & 0
   \end{array}} \right).
 \end{equation}
We define its resolvent as
$$G(z) \equiv G(Z^{(1)}, Z^{(2)},z)\define \left[H - \begin{pmatrix}z\id_{p\times p}&0\\ 0 & \id_{(n_1+n_2)\times (n_1+n_2)} \end{pmatrix}\right]^{-1},\quad z\in \mathbb C , $$
as long as the inverse exists. Furthermore, we define the following (weighted) partial traces %of $G(z)$:
\be\label{defm}
\begin{split}
m(z) :=\frac1p\sum_{i\in \cal I_0} G_{ii}(z) ,\quad & m_0(z):=\frac1p\sum_{i\in \cal I_0} \lambda_i^2 G_{ii}(z),\\
 m_1(z):= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu\mu}(z) ,\quad & m_2(z):= \frac{1}{n_2}\sum_{\nu\in \cal I_2}G_{\nu\nu}(z),
\end{split}
\ee
where $\cal I_i$, $i=0,1,2$, are index sets defined as 
$$\cal I_0:=\llbracket 1,p\rrbracket, \quad  \cal I_1:=\llbracket p+1,p+n_1\rrbracket, \quad \cal I_2:=\llbracket p+n_1+1,p+n_1+n_2\rrbracket.$$
\end{definition}
\begin{remark}
We will consistently use the latin letters $i,j\in\sI_{0}$ and greek letters $\mu,\nu\in\sI_{1}\cup \sI_{2}$.
Correspondingly, the indices of the matrices $Z^{(1)}$ and $Z^{(2)}$ are labelled as
	\be\label{labelZ}
 Z^{(1)}= \left[Z^{(1)}_{\mu i}:i\in \mathcal I_0, \mu \in \mathcal I_1\right], \quad Z^{(2)}= \left[Z^{(2)}_{\nu i}:i\in \mathcal I_0, \nu \in \mathcal I_2\right].\ee
Moreover, we define the set of all indices $\cal I:=\cal I_0\cup \cal I_1\cup \cal I_2$, and we label the indices in $\cal I$ as $\fa, \ \fb,\ \mathfrak c$ and so on. 
%\cor We first introduce several useful notations. We define $n:=n_1+n_2$ and the following index sets
%$$\cal I_0:=\llbracket 1,p\rrbracket, \quad  \cal I_1:=\llbracket p+1,p+n_1\rrbracket, \quad \cal I_2:=\llbracket p+n_1+1,p+n_1+n_2\rrbracket ,\quad   .$$
\end{remark}
%as the resolvent of $H$,
%for any complex value $z\in \mathbb C\setminus \R$.
Using Schur complement formula for the inverse of a block matrix, it is easy to verify that
	\begin{equation} \label{green2}
	  G(z) =  \left( {\begin{array}{*{20}c}
			(W- z\id)^{-1} & (W - z\id)^{-1} \AF  \\
      \AF^\top (W - z\id)^{-1} & z(\AF^\top \AF - z\id)^{-1}
		\end{array}} \right).%\quad \cal G_R:=(W^\top W - z)^{-1} ,
  \end{equation}
In particular, the upper left block of $G$ is exactly the resolvent of $W$ which we are interested in. Compared with $(W- z\id)^{-1}$, it turns out that $G(z)$ is more convenient to deal with because $H$ is a linear function in $Z^{(1)}$ and $Z^{(2)}$. 
This is why we have chosen to work with $G(z)$.

%\paragraph{Variance asymptotic limit.}
%In Theorem \ref{main_cor}, we will show that for $z$ in a small neighborhood around $0$, when $p$ goes to infinity, $G(z)$ converges to the following limit
We define the matrix limit of $G(z)$ as 
\be \label{defn_piw}
	\Gi(z) \define \begin{pmatrix} [a_{1}(z)\Lambda^2  +  (a_{2}(z)- z)]^{-1} & 0 & 0 \\ 0 & - \frac{n}{n_1} a_{1}(z)\id_{n_1\times n_1} & 0 \\ 0 & 0 & -\frac{n}{n_2}a_{2}(z)\id_{n_2\times n_2}  \end{pmatrix},\ee
where $(a_1(z),a_2(z))$ is the unique solution to the following system of self-consistent equations
\be\label{selfomega_a}
\begin{split}
	&a_1(z) + a_2(z) = 1 - \frac{1}{n_1 + n_2} \bigbrace{\sum_{i=1}^p \frac{\lambda_i^2 a_1(z) + a_2(z)}{\lambda_i^2 a_1(z) + a_2(z) - z}}, \\ %\label{selfomega_a000} \\
	&a_1(z) + \frac{1}{n_1 + n_2}\bigbrace{\sum_{i=1}^p \frac{\lambda_i^2 a_1(z)}{\lambda_i^2 a_1(z) + a_2(z) - z}} = \frac{n_1}{n_1 + n_2},
% \frac{\rho_1}{a_{1}(z)} = \frac{1}{p}\sum_{i=1}^p \frac{\lambda_i^2}{ - z+\lambda_i^2 a_{1}(z) +a_{2} (z) } + (\rho_1+\rho_2),\  \frac{\rho_2}{a_{2}(z)} = \frac{1}{p}\sum_{i=1}^p \frac{1 }{  -z+\lambda_i^2 a_{1}(z) +  a_{2}(z)  }+ (\rho_1+\rho_2) .
\end{split}
\ee
such that $\im a_1(z)\le 0$ and $\im a_2(z)\le 0$ whenever $\im z > 0$. The existence and uniqueness of solutions to the above system will be shown in Lemma \ref{lem_mbehaviorw}.
%First, we define the deterministic limits of $(m_1(z), m_{2}(z))$ by $\left(-\frac{\rho_1+\rho_2}{\rho_1}a_{1}(z),-\frac{\rho_1+\rho_2}{\rho_2}a_{2}(z)\right)$, where
%satisfying that $\im a_{1}(z)< 0$ and $\im a_{2}(z)<0$ for $z\in \C_+$ with $\im z$.
%\be\label{ratios}
% \gamma_n :=\frac{p}{n}=\frac{1}{\rho_1+\rho_2},\quad r_1 :=\frac{n_1}{n}=\frac{\rho_1}{\rho_1+\rho_2},\quad r_2 :=\frac{n_2}{n}=\frac{\rho_2}{\rho_1+\rho_2}.
%\ee


%\paragraph{Schur complement and self-consistent equations.}


%\subsection{Limit of the Resolvent}\label{sec pf RMTlemma}

We now state the main random matrix result---Theorem \ref{LEM_SMALL}---which shows that for $z$ in a small neighborhood around $0$, $G(z)$ converges to the limit $\Gi(z)$ when $p$ goes to infinity. Moreover, it also gives an almost convergence rate on $G(z)$. Such an estimate is conventionally called the {\it anisotropic local law} \cite{Anisotropic}. We define a domain of the spectral parameter $z$ as
\begin{equation}
\mathbf D:= \left\{z=E+ \ii \eta \in \C_+: |z|\le (\log n)^{-1} \right\}. \label{SSET1}
\end{equation}


\begin{theorem} \label{LEM_SMALL} %[Results on covariance matrices with small support]
Suppose $Z^{(1)}$, $Z^{(2)}$, $\rho_1$ and $\rho_2$ satisfy Assumption \ref{assm_big1}. Suppose that $Z^{(1)}$ and $Z^{(2)}$ satisfy the bounded support condition \eqref{eq_support} with $Q= n^{{2}/{\varphi}}$. Suppose that the singular values of $M$ satisfy that
\begin{equation}\label{assm3_app}
 \lambda_p \le\cdots\le\lambda_2 \le \lambda_1 \le \tau^{-1} .
\ee
Then the following local laws hold on $\Xi$.
%for any deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb R^{p+n_1+n_2}$.
\begin{itemize}
\item[(1)] {\bf Averaged local law}: We have
\begin{equation}
 \left\vert {p}^{-1}\sum_{i\in \cal I_0} [G_{ii}(z)- \Gi_{ii}(z)]\right\vert \prec (np)^{-1/2} Q, \label{aver_in} %+ q^2 
\end{equation}
and
\begin{equation}
 \left\vert {p}^{-1}\sum_{i\in \cal I_0} \lambda_i^2 [G_{ii}(z)- \Gi_{ii}(z)]\right\vert \prec (np)^{-1/2} Q, \label{aver_in1} %+ q^2 
\end{equation}
 uniformly in $z \in \mathbf D$.
\item[(2)] {\bf Anisotropic local law}: For any deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb R^{p+n}$, we have
\begin{equation}\label{aniso_law}
	\max_{z\in \mathbf D}\left| \mathbf u^\top [G(z)-\Gi(z)] \mathbf v \right|  \prec  n^{-1/2}Q,
\end{equation}
uniformly in $z \in \mathbf D$.
%where $m$ is defined in equation \eqref{defn_m}. Moreover, outside of the spectrum we have the following stronger estimate
%\begin{equation}\label{aver_out1}
% | m(z)-M(z)|\prec q^2  + \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%uniformly in $z\in \wt  S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$, where $\kappa$ is defined in equation \eqref{KAPPA}. 
\end{itemize}
\end{theorem}
\begin{remark}\label{rem_ale1}
%We will apply this theorem to the setting with $M=a (\Sigma^{(1)})^{1/2}(\Sigma^{(2)})^{-1/2}$. 
%Since $a$ in $M(a)$ can be close to zero, we do not assume a lower bound on $\lambda_p$ in \eqref{assm3_app}.
Here we state a result that works for the case $|a|\le 1$, where the singular values of $M$ are bounded from above but not from below as in \eqref{assm3_app}. To extend the result to the case with $|a|\ge 1$, we only need to apply our result to the inverse of 
$$\Lambda(a)^{-1}\hat{\Sigma}(a) \Lambda(a)^{-1}=   U^\top (Z^{(1)})^\top Z^{(1)} U  +\Lambda(a) ^{-1} V^\top (Z^{(2)})^\top Z^{(2)}V\Lambda(a) ^{-1}  ,$$
so that the eigenvalues of $\Lambda(a)^{-1}$ are bounded from above. 
\end{remark}

With Theorem \ref{LEM_SMALL}, we can complete the proof of Theorem \ref{thm_main_RMT} and Proposition \ref{prop_main_RMT} with a standard cutoff argument.

\iffalse
\begin{corollary}\label{main_cor}
In the setting of Theorem \ref{thm_main_RMT}, let $q$ be equal to $n^{-\frac{\varphi - 4}{2\varphi}}$.
We have that the resolvent $G(z)$ converges to the matrix limit $\Gi(z)$:
for any deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb R^{p+n_1+n_2}$, the following estimate \begin{equation}\label{aniso_law}
	\max_{z\in \mathbf D}\left| \mathbf u^\top (G(z)-\Gi(z)) \mathbf v \right|  \prec  q
\end{equation}
holds on the high probability event
\be\label{one_event}\Xi:=\left\{ \max_{1\le i\le n_1, 1\le j \le p}|Z^{(1)}_{i j}| \le (\log n) n^{\frac{2}{\varphi}}, \ \max_{1\le i\le n_2, 1\le j \le p}|Z^{(2)}_{i j}|\le (\log n) n^{\frac{2}{\varphi}} \right\}.\ee

\end{corollary}
%[{\color{red}FY: I changed the statement a little bit. The subtlety is that for our purpose, we will apply it to a set of $p$ unit vectors simultaneously. Hence we need to take a union bound, which requires the statement to hold with overwhelming probability on a {\bf fixed} high probability event. Our union bound can be taken on this fixed event. This union bound argument is hidden in Fact \ref{lem_stodomin}, which we need to use in the proof of Theorem \ref{thm_main_RMT}. In the original form, if we only know that the estimates holds with high probability $1-\oo(1)$, then we cannot take union bound of $p$ possibly different high probability events.}]
%\begin{remark}
%%[{\cor FY: I added this remark to explain why the bounded support condition is more general in case the reader wonders. You can comment it if you think it is not necessary.}]
%The reason why we say the bounded support assumption is more general is because it provides greater flexibility in dealing with bounded moments. For example, we can also replace equation \eqref{Ptrunc} with
%\begin{align*}
%	\P\left(\max_{1\le i\le n, 1\le j \le p}|Z_{i j}|\ge n^{\frac{2}{\varphi}+\delta}\right) = \OO(  n^{-\varphi \delta}) 
%	\end{align*}
%	for a small constant $\delta>0$. Hence we can replace event \eqref{one_event} with 
%	\be\nonumber \left\{ \max_{1\le i\le n, 1\le j \le p}|Z^{(1)}_{i j}| \le n^{\frac{2}{\varphi}+\delta}, \ \max_{1\le i\le n, 1\le j \le p}|Z^{(2)}_{i j}|\le   n^{\frac{2}{\varphi}+\delta} \right\},\ee
%	which holds with higher probability. But on this event we need to take a larger $q=n^{-\frac{\varphi - 4}{2\varphi}+\delta}$, which means a worse convergence rate. In general, with Theorem \ref{LEM_SMALL}  one can determine the most suitable trade-off between probability and convergence rate depending on one's need.
%\end{remark}
\fi

 
\begin{proof}[Proof of Theorem \ref{thm_main_RMT}]
%Using the above result, we prove Theorem \ref{thm_main_RMT} using a simple cutoff argument.
%\begin{proof}[Proof of Theorem \ref{main_cor}] 
%Fix any sufficiently small constant $\e>0$. We choose $q= n^{-c_a +\e}$ with $c_a:=1/2-2/a $. Then 
%As in \eqref{truncateZ}, 
We introduce the truncated matrices $\wt Z^{(1)}$ and $\wt Z^{(2)}$ with entries
\be\label{truncate1} 
\wt Z^{(1)}_{\mu i}:= \mathbf 1\left(  |Z^{(1)}_{\mu i}|\le Q \log n \right)\cdot Z^{(1)}_{\mu i}, \quad \wt Z^{(2)}_{\nu i}:= \mathbf 1\left(  |Z^{(2)}_{\nu i}|\le Q\log n \right)\cdot Z^{(2)}_{\nu i}, 
\ee
%where, for convenience, we again use $j$ to denote the column index of $X$. 
for $Q= n^{{2}/{\varphi}}$. From equation \eqref{Ptrunc},  we get
\begin{equation}\label{XneX}
\mathbb P(\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}) =1-\OO ( (\log n)^{-\varphi}).
\end{equation}
%By definition, we have 
%\be\nonumber %\label{EwtZ}
%\begin{split}
%\E  \wt  Z^{(1)}_{\mu i} &= - \mathbb E \left[ \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)Z^{(1)}_{\mu i}\right] ,\\ 
%\E  |\wt  Z^{(1)}_{\mu i}|^2 &= 1 - \mathbb E \left[ \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)|Z^{(1)}_{\mu i}|^2\right] .
%\end{split}
%\ee
%Using the formula for expectation in terms of the tail probabilities, we can check that
%\begin{align*}
%&  \mathbb E \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| \\
%= &\int_0^\infty \P\left( \left| \mathbf 1\left(  |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| > s\right)\dd s \\
% = &\int_0^{qn^{1/2}\log n}\P\left( |Z^{(1)}_{\mu i}|> q  n^{1/2}\log n \right)\dd s +\int_{qn^{1/2}\log n}^\infty \P\left(|Z^{(1)}_{\mu i}| > s\right)\dd s  \\
%\lesssim &\int_0^{qn^{1/2}\log n}\left(q  n^{1/2}\log n \right)^{-\varphi}\dd s +\int_{qn^{1/2}\log n}^\infty s^{-\varphi}\dd s \le n^{-2(\varphi-1)/\varphi},
%\end{align*}
%where in the third step we used the finite $\varphi$-th moment condition of $Z_{\mu i}^{(1)}$ %\eqref{assmAhigh} \HZ{check; I don't see which one} 
%and Markov's inequality. Similarly, we can obtain that
%\begin{align*}
%&  \mathbb E \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> qn^{1/2} \log n \right)Z^{(1)}_{\mu i}\right|^2 \\
%= & 2\int_0^\infty s \P\left( \left| \mathbf 1\left( |Z^{(1)}_{\mu i}|> q n^{1/2}\log n \right)Z^{(1)}_{\mu i}\right| > s\right)\dd s \\
%= & 2\int_0^{qn^{1/2}\log n} s \P\left( |Z^{(1)}_{\mu i}|> q  n^{1/2}\log n \right)\dd s +2\int_{qn^{1/2}\log n}^\infty s\P\left(|Z^{(1)}_{\mu i}| > s\right)\dd s  \\
%\lesssim & \int_0^{qn^{1/2}\log n}s\left(q  n^{1/2}\log n \right)^{-\varphi}\dd s +\int_{qn^{1/2}\log n}^\infty s^{-\varphi+1}\dd s \le n^{-2(\varphi-2)/\varphi}.
%\end{align*}
%Plugging the above two estimates into equation \eqref{EwtZ} and using $\varphi>4$, we get that
%Using (\ref{condition_4e}) and integration by parts, it is easy to verify that %we can get that
%\begin{align*}
%\mathbb E  |z^{(\al)}_{ij}|1_{|z^{(\al)}_{ij}|> q} =\OO(n^{-2-\e}), \quad \mathbb E |z^{(\al)}_{ij}|^2 1_{|z^{(\al)}_{ij}|> q} =\OO(n^{-2-\e}), \quad \al=1,2,
%\end{align*}
%which imply that
As in \eqref{meanshif} and \eqref{EZ norm}, we have that
\be \label{meanshif2}
\begin{split}
  |\mathbb E  \wt  Z^{(1)}_{\mu i}| =\OO(n^{-3/2}), \quad &|\mathbb E  \wt  Z^{(2)}_{\nu i}| =\OO(n^{-3/2}), \\ 
 \mathbb E |\wt  Z^{(1)}_{\mu i}|^2 =1+ \OO(n^{-1}),\quad &\mathbb E |\wt  Z^{(2)}_{\mu i}|^2 =1+ \OO(n^{-1}),
\end{split}
\ee
and %From the first estimate in equation \eqref{meanshif}, we can also get a bound on the operator norm:
\be  \label{EZ norm2}
\|\E \wt Z^{(1)}\|=\OO(n^{-1/2}),\quad \|\E \wt Z^{(2)}\|=\OO(n^{-1/2}) .
\ee
%Similar estimates also hold for $\wt Z^{(2)}$. 
%and
%$$\left| \mathbb E\wt  x_{ij}^2\right| =O( n^{-2-\omega/2}), \ \ \text{if $x_{ij}$ is complex.} $$
%Moreover, we trivially have
%$$\mathbb E  |\wt  z^{(\al)}_{ij}|^4 \le \mathbb E  |z^{(\al)}_{ij}|^4 =\OO(n^{-2}), \quad \al=1,2.$$
Then we centralize and rescale $\wt Z^{(1)}$ and $\wt Z^{(2)}$ as
$$ \wh Z^{(1)} := (\E|\wt Z^{(1)}_{\mu i}|^2 )^{-1/2}(\wt Z^{(1)} - \E \wt Z^{(1)} ),\quad \wh Z^{(2)} := (\E|\wt Z^{(2)}_{\mu i}|^2)^{-1/2} (\wt Z^{(2)} - \E \wt Z^{(2)} ).$$ 
Now $\wh Z^{(1)}$ and $\wh Z^{(2)}$ satisfy the assumptions of Theorem \ref{LEM_SMALL}. Moreover, notice that \eqref{assm3_app} holds when $|a|\le 1$. Hence \eqref{aver_in} hold for $G(\wh Z^{(1)},\wh Z^{(2)},z)$ for any fixed $a\in [-1,1]$, where $G(\wh Z^{(1)},\wh Z^{(2)},z)$ is defined in the same way as $G(z)$ but with $(Z^{(1)}, Z^{(2)})$ replaced by $(\wh Z^{(1)},\wh Z^{(2)})$. 
%we have %and equation \eqref{aniso_law} gives that
%\be\label{GwhZZ}\left| \mathbf u^\top (G(\wh Z^{(1)},\wh Z^{(2)},z)-\Gi(z)) \mathbf v \right|  \prec  q,\ee
%where $G(\wh Z^{(1)},\wh Z^{(2)},z)$ is defined in the same way as $G(z)$, but with $(Z^{(1)}, Z^{(2)})$ replaced by $(\wh Z^{(1)},\wh Z^{(2)})$.
Note that by equations \eqref{meanshif2} and \eqref{EZ norm2}, we can bound that for $\al=1,2,$
$$ \|\wh Z^{(\al)} - \wt Z^{(\al)}\|\lesssim n^{-1}\|\wt Z^{(\al)}\| + \|\E \wt Z^{(\al)}\|\lesssim n^{-1/2} \quad \text{w.o.p.},$$
where we also used Lemma \ref{SxxSyy} to bound the operator norm of $\wt Z^{(\al)}$. Together with estimate \eqref{priorim} below, this bound implies that w.o.p.,% check that
$$\left|  G_{ii}(\wh Z^{(1)},\wh Z^{(2)},z)-G_{ii}(\wt Z^{(1)},\wt Z^{(2)},z)  \right|  \lesssim  n^{-1/2} \sum_{\al=1}^2 \|\wh Z^{(\al)} - \wt Z^{(\al)}\| \lesssim n^{-1},\quad i\in \cal I_0.$$
% on the event $\{\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}\}$. 
Combining this estimate with the local law \eqref{aver_in} for $G(\wh Z^{(1)},\wh Z^{(2)},z)$, we obtain that estimate \eqref{aver_in} also holds for $G(z)$ on the event $\Xi_1:=\{\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}\}$. 
%which concludes the proof by equation \eqref{XneX}.
%\end{proof}

%Now we are ready to complete the proof of Theorem \ref{thm_main_RMT} using Theorem \ref{main_cor}.

%\begin{proof}[Proof of Theorem \ref{thm_main_RMT}, Part i)]
%Recall that in the setting of Theorem \ref{thm_main_RMT} we have equation \eqref{rewrite X as R},
%using equation \eqref{eigen2extra} and equation \eqref{mainG} we can write 
%$$\cal R:= ( (X^{(1)})^\top X^{(1)} + (X^{(2)})^\top X^{(2)})^{-1}= n^{-1}\Sigma_2^{-1/2}V \cal G(0)V^\top\Sigma_2^{-1/2}.$$
%where $$\cal G(0)=\left(   \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^{-1}.$$
%%where $\Sigma_1$, $\Sigma_2$, $Z^{(1)}$ and $Z^{(2)}$ satisfy Assumption \ref{assm_big1}. H
%(Here the extra $n^{-1}$ is due to the choice of the scaling---in the setting of Lemma \ref{lem_cov_shift} the variances of the $Z_{1}$ and $Z^{(2)}$ entries are equal to 1, while they are taken to be $n^{-1}$ in the above expression.) 
%%As in equation \eqref{eigen2}, we assume that $M:=\wt\Sig_1^{1/2} \Sig_2^{-1/2}$ has singular value decomposition
%%\be\label{tildeM}
%%M= U\Lambda V^\top, \quad \Lambda=\text{diag}( \sigma, \ldots, \sigma_p).
%%\ee
%Then as in equation \eqref{eigen2extra}, we can rewrite $\cal R$ as
%$$\cal R= n^{-1}\Sigma_2^{-1/2}V \cal G(0)V^\top\Sigma_2^{-1/2},\quad \cal G(0)=\left(   \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^{-1}.$$
%and that the entries of $Z^{(1)}$ and $Z^{(2)}$ have finite $\varphi$-th moments as in equation \eqref{assmAhigh}.
%Suppose $\Sigma=\sum_i \lambda_i(\Sigma)\mathbf u_i \mathbf v_i^\top$ is a SVD of $\Sigma$ with $\{\mathbf u_i\}$ and $\{\bv_i\}$ being the left and right singular vectors. 


Now we are ready to prove \eqref{lem_cov_shift_eq} for the case $|a|\le 1$. Then with \eqref{eigen2extra}, we have 
$$L_{\var}(a)=\frac{ \sigma^2 }{n}\sum_{i\in \cal I_0} G_{ii}(0).$$
We also notice that the equations in \eqref{selfomega_a} reduce to the equations in \eqref{eq_a12extra} when $z=0$, which shows that $a_1 = a_1(0)$ and $a_2 = a_2(0)$. Hence for $\Gi$ in \eqref{defn_piw}, we have
$$\sum_{i\in \cal I_0}\Gi_{ii}(0)= \bigtr{\frac1{a_1 \Lambda(a)^2 + a_2}}=\bigtr{  \frac{1}{a_1 M(a)^\top M(a) + a_2  }  }.$$
Now applying \eqref{aver_in} to $G(z)$ on $\Xi_1$, we conclude that on $\Xi_1$,
$$ \bigabs{L_{\var}(a)- \frac{\sigma^2}{n_1+n_2}\bigtr{  \frac{1}{a_1 M(a)^\top M(a) + a_2  }  }}
				\prec \frac{n^{2/\varphi}}{p^{1/2} n^{1/2}}\cdot\frac{p \sigma^2}{n_1+ n_2},  $$
for any fixed $a\in [-1,1]$. Then using a similar $\ve$-net argument as in Appendix \ref{app_firstpf}, we can show that this estimate holds uniformly for all $a\in [-1,1]$ on a high probability event. We omit the details.

It remains to consider the case $|a|\ge 1$. In this case, we can write
$$L_{\var}(a)= \frac{\sigma^2}{n} \bigtr{\Lambda(a)^{-2} \wt W(a)^{-1} },$$
where
$$ \wt W(a):= U^\top (Z^{(1)})^\top Z^{(1)} U  +\Lambda(a) ^{-1} V^\top (Z^{(2)})^\top Z^{(2)}V\Lambda(a) ^{-1}.$$
Then we can apply the averaged local law \eqref{aver_in1} to $ \wt W(a)^{-1}$ on $\Xi_1$, with the role of $\lambda_i$ replaced by $\lambda_i^{-1}$, and the roles of $X_1$ and $X_2$ exchanged. This concludes \eqref{lem_cov_shift_eq} for the case $|a|\ge 1$. We omit the details.
\iffalse
With the definition of matrix $W$ in equation \eqref{eigen2extra}, we can express $\Sigma^{(2)}\hat \Sigma^{-1}$ as
$$\Sigma^{(2)}\hat \Sigma^{-1} = n^{-1} ({\Sigma^{(2)}})^{1/2} V \cal G(0) V^{\top} ({\Sigma^{(2)}})^{-1/2},$$
where we recall that $\cal G(z)=(W-z\id )^{-1}$ is the resolvent of $W$.
%Following our discussion in Section \ref{sec_diff}, 
Then by Theorem \ref{main_cor}, for any $1\le i \le p$ we have that
%$Z^{(1)}$ and $Z^{(2)}$ have bounded support $q=n^{-1/2}$. Using Theorem \ref{LEM_SMALL}, %Now by Corollary \ref{main_cor}, we obtain that for any small constant $\e>0$, %with probability $1-\oo(1)$,
\begin{align}
& \left| \left[\Sigma^{(2)}\hat \Sigma^{-1} - n^{-1} (\Sigma^{(2)})^{1/2}V \Gi(0)V^\top(\Sigma^{(2)})^{-1/2} \right]_{ii}\right|  \nonumber\\
 = & n^{-1} \left|\mathbf e_i^\top (\Sigma^{(2)})^{1/2}V \left(\cal G(0)-\Gi(0)\right)V^\top (\Sigma^{(2)})^{-1/2} \mathbf e_i\right| \nonumber\\
\prec & n^{-1}q\|V^\top (\Sigma^{(2)})^{-1/2} \mathbf e_i\|\cdot \|V^\top (\Sigma^{(2)})^{1/2} \mathbf e_i\|   \lesssim n^{-1}q ,\label{G0Pi0}
\end{align}
on the event \eqref{one_event}, %$\Omega$ with $\P(\Omega)=1-\oo(1)$, 
where $ q= n^{-\frac{\varphi - 4}{2\varphi}}$ and $\mathbf e_i$ denotes the standard basis vector along the $i$-th direction.
Next, we can verify that %recall from Section \ref{sec_diff} that
$$ n^{-1}  (\Sigma^{(2)})^{1/2}V \Gi(0)V^\top (\Sigma^{(2)})^{-1/2} = n^{-1}  \Sigma^{(2)}(a_1 \Sigma^{(1)}+  a_2\Sigma^{(2)})^{-1} .$$
%$$\Gi(0)= -(a_{1}(0)\Lambda^2  +  a_{2}(0))^{-1}= (a_1 V^\top M^\top M V +  a_2)^{-1},$$
%with $(a_1,a_2)$ satisfying equation \eqref{selfomega0}. 
Together with equation \eqref{G0Pi0}, this identity implies that % we get that
\begin{align*}
 \tr \left[\Sigma^{(2)}\hat \Sigma^{-1} \right]& = \sum_{i=1}^p\left(\Sigma^{(2)}\hat \Sigma^{-1} \right)_{ii} = n^{-1}\tr \left[ \Sigma^{(2)}(a_1 \Sigma^{(1)}+  a_2\Sigma^{(2)})^{-1} \right] +\OO_\prec(q  ) 
 \end{align*}
on the event \eqref{one_event}, where we used Fact \ref{lem_stodomin} (i) in the second step. 
%with probability $1-\oo(1)$. 
This concludes equation \eqref{lem_cov_shift_eq} using Definition \ref{stoch_domination} and the fact that $c_{\varphi}$ is any fixed value within $(0, \frac{\varphi - 4}{2\varphi})$.  %if we rename $(r_1X^{(2)},r_2x_3)$ to $(a_1,a_2)$. 
%For equation \eqref{lem_cov_shift_eq}, it is a well-known result for inverse Whishart matrices {\color{red}(add some references)}. 
%Note that if we set $n_1=0$ and $n_2=n$, then $a_1 = 0$ and $a_2 = (n_2-p) / n_2$ is the solution to equation \eqref{eq_a12extra}. This gives equation \eqref{XXA} using equation \eqref{lem_cov_shift_eq}. 
%\end{proof}

Given this result, we now show that when $z = 0$, the matrix limit $\Gi(0)$ implies the variance limit shown in equation \eqref{lem_cov_shift_eq}.
First, we have that $a_1 = a_1(0)$ and $a_2 = a_2(0)$ since the equations in \eqref{selfomega_a} reduce to equation \eqref{eq_a12extra} when $z=0$.
Second, since $W^{-1}$ is the upper-left block matrix of $G(0)$, we have that $W^{-1}$ converges to $ (a_1\Lambda^2 + a_2\id)^{-1} $.
Using the fact that $\tr[\Sigma^{(2)} \hat{\Sigma}^{-1}] = (n_1 + n_2)^{-1}\bigtr{W^{-1}} $, we get that when $p$ goes to infinity, % the trace of $$ converges to
\begin{align*}
  \bigtr{\Sigma^{(2)} \hat{\Sigma}} \rightarrow \frac{1}{n_1+n_2}\bigtr{(a_1 \Lambda^2 + a_2\id)^{-1}} &= \frac1{n_1+n_2}\bigtr{(a_1 M^{\top}M + a_2 \id)^{-1}} \\
  &=\frac{1}{n_1+n_2} \bigtr{\Sigma^{(2)} (a_1 \Sigma^{(1)} + a_2 \Sigma^{(2)})^{-1}},
  \end{align*}
%\noindent{\bf Variance asymptotics.} Using definition \eqref{mainG}, we can write equation \eqref{eigen2extra} as
%\be\label{rewrite X as R} [(X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}]^{-1}=n^{-1}\Sigma_2^{-1/2}V\cal G(0)V^\top\Sigma_2^{-1/2}.\ee
%When $z=0$, it is easy to check that , which means that we actually have $a_1(0)=a_1$ and $a_2(0)=a_2$. Hence the matrix limit of $\cal G(0)$ is given by $(a_{1}\Lambda^2 + a_{2}\id_p)^{-1}$. Then inserting this limit into equation \eqref{rewrite X as R}, we can write the left-hand side of equation \eqref{lem_cov_shift_eq} as
%\begin{align}
%&\bigtr{\left( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}\right)^{-1} \Sigma}\approx n^{-1}\bigtr{\Sigma_2^{-1/2}V\cal (a_{1}\Lambda^2 + a_{2}\id_p)^{-1}V^\top\Sigma_2^{-1/2}\Sigma}  \nonumber\\
%&=n^{-1}\bigtr{\Sigma_2^{-1/2}\cal (a_{1}\Sigma_2^{-1/2}\Sigma_1\Sigma_2^{-1/2} + a_{2}\id_p)^{-1}\Sigma_2^{-1/2}\Sigma}  = n^{-1}\bigtr{\cal (a_{1} \Sigma_1  + a_{2}\Sigma_2)^{-1}\Sigma}  ,\label{Gi00}
%\end{align}
where we note that $M^\top M = (\Sigma^{(2)})^{-1/2} \Sigma^{(1)} (\Sigma^{(2)})^{-1/2}$ and its SVD is equal to $V^{\top}\Lambda^2 V$.
%For the asymptotic limit, its concentration error is shown in Appendix \ref{appendix RMT}.
\fi
 \end{proof}

\begin{proof}[Proof of Proposition \ref{prop_main_RMT}]
We first prove an estimate on 
 $$\wt L_{\bias} (a) :=n_1^2 \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(a)^{-1} \Sigma^{(1)}\left(a\beta^{(1)}- a^2\beta^{(2)}\right) \right\|^2.$$
%using Theorem \ref{LEM_SMALL}. 
We claim that for any small constant $c>0$ and large constant $C>0$, there exists a high probability event $\Xi$, on which
\begin{align}
				& \bigabs{ \wt L_{\bias}(a) -   (\beta^{(1)}- a\beta^{(2)} )^\top (\Sigma^{(1)})^{1/2} \Pi(a)(\Sigma^{(1)})^{1/2} (\beta^{(1)}- a\beta^{(2)})   }  \nonumber\\
				& \prec  n^{-1/2}Q  \left\|(\Sigma^{(1)})^{1/2} \left(\beta^{(1)}- a\beta^{(2)}\right) \right\|^2 + p^{-C} \left[\| \beta^{(1)}\|^2 + \| \beta^{(2)}\|^2 \right],  \label{lem_cov_derv_app1}
			\end{align}
holds uniformly for $a\in \R$. 


%Recall that in the setting of Theorem \ref{thm_main_RMT}, we have equation \eqref{calculate G'}.
%\begin{align*}
% n^{2}\bignorm{\Sigma_2^{1/2} ( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)})^{-1}\Sigma_1^{1/2}w}^2 =\beta^\top \Sigma_2^{-1/2}  \left(M^\top (Z^{(1)})^{\top} Z^{(1)} M +  (Z^{(2)})^\top Z^{(2)} \right)^{-2}   \Sigma_2^{-1/2}\beta,
%\end{align*}
%%where $\wt\Sigma_1:= w^2 \Sigma_1$, $\Sigma_2$, $Z^{(1)}$ and $Z^{(2)}$ satisfy Assumption \ref{assm_big1} and $M:=\wt\Sig_1^{1/2} \Sig_2^{-1/2}$. 
%where in the second step the $n^{2}$ factor disappeared due to the choice of scaling in equation \eqref{assm1}. 
%Again we assume that $M$ has the singular value decomposition equation \eqref{tildeM}. 
%With equation \eqref{eigen2}, we can write the above expression as
%$$n^2\bignorm{\Sigma_2^{1/2} ( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)})^{-1}\beta}^2= \bv^\top \cal G^2(0)  \bv,\quad \bv:=V^\top  \Sigma_2^{-1/2} \beta .$$ 
%For simplicity, we denote 
First, we fix any $a\in [ -1,1]$. Define the vector $\bv:=V^\top  (\Sigma^{(2)})^{-1/2} \Sigma^{(1)}\left(a\beta^{(1)}- a^2\beta^{(2)}\right)\in \R^{p}$, and its embedding in $\R^{p+n}$, $\mathbf w =(\bv^\top, \mathbf 0_n)^\top$, where $\mathbf 0_n$ is an $n$-dimensional zero row vector. Then we have$$ \wt L_{\bias} (a)= \bw^\top \frac{n_1^2}{\left( \Lambda(a) U^\top (Z^{(1)})^\top Z^{(1)} U\Lambda(a)  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^2} \bw= \frac{n^2_1}{n^2}\bw^\top G'(0) \bw,$$
where $\cal G'(0)$ denotes the derivative of $\cal G(z)$ with respect to $z$ at $z=0$. Now we introduce the truncated matrices $\wt Z^{(1)}$ and $\wt Z^{(2)}$ as in \eqref{truncate1}. Then with a similar argument as in the above proof of Theorem \ref{thm_main_RMT}, we can show that \eqref{aniso_law} holds  
 for $G(z)$ on the event $\Xi_1:=\{\wt Z^{(1)} = Z^{(1)},  \wt Z^{(2)} = Z^{(2)}\}$. 
%Note that $\cal G^2(0)=\partial_z \cal G|_{z=0}$. 
%Under the constant $\varphi$-th moment condition, 
%By Corollary \ref{main_cor}, we have that
%%$Z^{(1)}$ and $Z^{(2)}$ have bounded support $q=n^{-1/2}$. Using Theorem \ref{LEM_SMALL}, %Now by Corollary \ref{main_cor}, we obtain that for any small constant $\e>0$, %with probability $1-\oo(1)$,
%\be\nonumber 
%\max_{z\in \C:|z|=(\log n)^{-1}}|\mathbf v^\top (G(z)-\Gi(z))\mathbf v| \prec q \|\mathbf v\|^2,%\quad q:= n^{-\frac{\varphi - 4}{2\varphi}},
%\ee
%on the event \eqref{one_event} with $ q:= n^{-\frac{\varphi - 4}{2\varphi}}$. 
Now combining \eqref{aniso_law} with Cauchy's integral formula, we get that on $\Xi_1$, %on the event \eqref{one_event}, %with probability $1-\oo(1)$, 
\be\label{apply derivlocal}
\begin{split}
  \bw^\top \cal G'(0)\bw  = \frac{1}{2\pi \ii}\oint_{\cal C} \frac{ \bw^\top \cal G(z)\bw }{z^2}\dd z &=  \frac{1}{2\pi \ii}\oint_{\cal C} \frac{ \bw^\top\Gi(z)\bw}{z^2}\dd z +\OO_\prec(n^{-\frac12}Q\|\mathbf w\|^2) \\
  &=  \bw^\top \Gi'(0)\bw + \OO_\prec(n^{-\frac12}Q\|\mathbf w\|^2),
\end{split}
\ee
where $\cal C$ is the contour $\{z\in \C: |z| = (\log n)^{-1} \}$. With \eqref{defn_piw}, we can calculate the derivative $\bw^\top \Gi'(0)\bw$ as
\be\label{dervPi}
\bw^\top \Gi'(0)\bw = \bv^\top  \frac{a_3\Lambda^2+(1+a_4)\id_p}{(a_{1}\Lambda^2 + a_{2}\id_p)^2}\bv, 
\ee
where
$$ a_3 := - \left. \frac{\dd a_1(z)}{\dd z}\right|_{z=0}, \quad a_4: = -\left. \frac{\dd a_2(z)}{\dd z}\right|_{z=0}.$$
%where we recall equation \eqref{cal G'0} and that $a_3 = - \frac{\dd a_1(0)}{\dd z}$ and $a_4 = - \frac{\dd a_2(0)}{\dd z}$. 
Taking derivatives of the system of equations \eqref{selfomega_a} with respect to $z$ at $z=0$, we can derive equation \eqref{eq_a34extra} for $(a_3,a_4)$. Together with equation \eqref{apply derivlocal}, this concludes the proof of \eqref{lem_cov_derv_app1} for any fixed $a\in [-1,1]$. Then using a similar $\ve$-net argument as in Appendix \ref{app_firstpf}, we can show that \eqref{lem_cov_derv_app1} holds uniformly for all $a\in [-1,1]$ on a high probability event. We omit the details.
 
Next for the case $|a| > 1$, we use a similar argument as above, except that we apply \eqref{aniso_law} to the resolvent of $\Lambda(a)^{-1}\hat{\Sigma}(a) \Lambda(a)^{-1}$ instead as discussed in Remark \ref{rem_ale1}. This will conclude that \eqref{lem_cov_derv_app1} holds uniformly for all $a\in [-1,1]^c$ on a high probability event. We omit the details. 

Now with \eqref{lem_cov_derv_app1}, to conclude \eqref{lem_cov_derv_eq} it remains to bound $ |L_{\bias}(a)-\wt L_{\bias}(a)|$: 
\begin{align*}
&L_{\bias}(a)-\wt L_{\bias}(a) \\
&= {2 n_1}(a\beta^{(1)} - a^2\beta^{(2)})^{\top}(\Sigma^{(1)})^{1/2}\Delta \left[(\Sigma^{(1)})^{1/2}\hat \Sigma(a)^{-1} \Sigma^{(2)} \hat \Sigma(a)^{-1} (\Sigma^{(1)})^{1/2}\right](\Sigma^{(1)})^{1/2} (a\beta^{(1)} - a^2\beta^{(2)}) \nonumber
		\\
		&+ \bignorm{(\Sigma^{(2)})^{1/2} \hat \Sigma(a)^{-1} (\Sigma^{(1)})^{1/2}\Delta  (\Sigma^{(1)})^{1/2}(\beta_1 - a\beta_2)}^2,
		\end{align*}
		where we abbreviate $\Delta = (Z^{(1)})^{\top}Z^{(1)} - {n_1}\id_{p\times p}$. From this equation, we get that
\begin{align*}
		&\left|L_{\bias}(a)-\wt L_{\bias}(a)\right|\\
		&\le a^2 \left[\left( n_1 + \|\Delta\|\right)^2 -n_1^2 \right] \bignorm{(\Sigma^{(1)})^{1/2} \hat \Sigma(a)^{-1}  \Sigma^{(2)}  \hat \Sigma(a)^{-1} (\Sigma^{(1)})^{1/2}} \bignorm{(\Sigma^{(1)})^{1/2} (a\beta^{(1)} - a^2\beta^{(2)})}^2,
	\end{align*}
 Using Corollary \ref{fact_minv}, we can bound that for any constant $c>0$,
	$$\left( n_1 + \|\Delta\|\right)^2 -n_1^2\le n_1^2\left[\left( 1+\sqrt{\frac{p}{n_1}}\right)^4 - 1 + n_1^{-1/2+2/\varphi + c}\right], $$
	and
	$$\left\|(\Sigma^{(2)})^{1/2}\hat \Sigma(a)^{-1}  (\Sigma^{(2)})^{1/2}\right\| \le \frac{1+ n_1^{-1/2+2/\varphi + c}}{ (\sqrt{n_1}-\sqrt{p})^2 \lambda_p^2+ (\sqrt{n_2}-\sqrt{p})^2} ,$$
	with high probability. Combining the above three estimates and using $\| a(\Sigma^{(1)})^{1/2}(\Sigma^{(2)})^{-1/2}\| \le \lambda_1$, we can obtain that with high probability,
	\begin{align*}
				&\left|L_{\bias}(a)-\wt L_{\bias}(a)\right| \nonumber\\
				& \le \left[\left( 1+\sqrt{\frac{p}{n_1}}\right)^4 - 1 + n_1^{-1/2+2/\varphi + c}\right] \frac{n_1^2 \lambda_1^2 \left\|(\Sigma^{(1)})^{1/2} \left(\beta^{(1)}- a\beta^{(2)}\right) \right\|^2}{  [(\sqrt{n_1}-\sqrt{p})^2 \lambda_p^2+ (\sqrt{n_2}-\sqrt{p})^2]^2} .
			\end{align*}
	Together with \eqref{lem_cov_derv_app1}, this estimate concludes Proposition \ref{prop_main_RMT}. 
	\iffalse
	Thus we can estimate that 
	\begin{align*}
	| \delta_{\bias}(v)-\wt\delta_{\bias}(v)|&\le v^2 \left( 2n_1  \|\cal E\| +  \|\cal E\|^2 \right) \bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \bignorm{\Sigma_1^{1/2} (\beta_1 - v\beta_2)}^2 \\
	&=  v^2 \left[\left( n_1 + \|\cal E\|\right)^2 -n_1^2 \right] \bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \bignorm{\Sigma_1^{1/2} (\beta_1 - v\beta_2)}^2 \\
	& \le v^2 n_1^2 \left[ \al_+^2(\rho_1) + \OO(p^{-1/2+\e}) -1\right]\bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \bignorm{\Sigma_1^{1/2} (\beta_1 - v\beta_2)}^2,
	\end{align*}
	which concludes the proof by the definition of $\delta_\e$.	

			Later we will control the difference between $ L_{\bias} (a) $ and $\wt L_{\bias} (a) $ using Lemma \ref{SxxSyy}.  
			{\cor
			Next we use \eqref{eq_isometric} to approximate $\delta_{\bias}(v)$ with $\wt\delta_{\bias}(v)$. %in \eqref{deltabetapf}.  
%relate the first term in equation \eqref{eq_te_model_shift} to $\Delta_{\bias}$.
\begin{claim}\label{prop_model_shift}
	In the setting of Theorem \ref{thm_model_shift},
	we denote by $K = (v^2X_1^{\top}X_1 + X_2^{\top}X_1)^{-1}$, and
	\begin{align*}
		%\delta_1 &= v^2 \bignorm{\Sigma_2^{1/2} K X_1^{\top}X_1(\beta_1 - v\beta_2)}^2, \\
		%\delta_2 &= n_1^2\cdot v^2 \bignorm{\Sigma_2^{1/2}K\Sigma_1(\beta_1 - v\beta_2)}, \\
		\delta_{err}(v) := n_1^2 v^2 \bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \cdot \bignorm{\Sigma_1^{1/2} (\beta_1 - v\beta_2)}^2.
	\end{align*}
	Then we have w.h.p.
	\begin{align*}
		 \left| \delta_{\bias}(v)-\wt\delta_{\bias}(v)\right| 
		\le  \left( \al_+^2(\rho_1)-1 + \OO(p^{-1/2+\e})\right)\delta_{err}.
	\end{align*}
%	We have that
%	\begin{align*}
%		-2n_1^2\bigbrace{{2\sqrt{\frac{p}{n_1}}} + {\frac{p}{n_1}}} \delta_3
%		\le  \delta_1 - \delta_2
%		\le n_1^2\bigbrace{2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}}\bigbrace{2 + 2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}}\delta_3.
%	\end{align*}
%	For the special case when $\Sigma_1 = \id$ and $\beta_1 - \beta_2$ is i.i.d. with mean $0$ and variance $d^2$, we further have
%	\begin{align*}
%		\bigbrace{1 - \sqrt{\frac{p}{n_1}}}^4 \Delta_{\bias}
%		\le \bignorm{\Sigma_2^{1/2} (X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}X_1^{\top}X_1(\beta_1 - \beta_2)}^2.
%	\end{align*}
\end{claim}

\begin{proof}
	%The proof follows by applying equation \eqref{eq_isometric}.
	%Recall that $X_1^{\top}X_1 = \Sigma_1^{1/2}Z_1^{\top}Z_1\Sigma_1^{1/2}$.
	Denote by $\cE = Z_1^{\top}Z_1 - {n_1}\id$. Then we can write
%	Let $\alpha = \bignorm{\Sigma_2^{1/2} K \Sigma_1 (\beta_1 - \hat{w}\beta_2)}^2$.
	%We have
	\begin{align}
%		& \bignorm{\Sigma_2^{1/2}(X_1^{\top}X_1 + X_2^{\top}X_2)^{-1}X_1^{\top}X_1(\beta_1 - \hat{w}\beta_2)}^2 \nonumber \\
		 \delta_{\bias}(v)-\wt\delta_{\bias}(v)&= {2v^2n_1}(\beta_1 - v\beta_2)^{\top}\Sigma_1^{1/2} \cE\left(\Sigma_1^{1/2}K \Sigma_2 K \Sigma_1^{1/2}\right) \Sigma_1^{1/2} (\beta_1 - v\beta_2) \nonumber
		\\
		&+ v^2\bignorm{\Sigma_2^{1/2} K \Sigma_1^{1/2}\cE \Sigma_1^{1/2}(\beta_1 - v\beta_2)}^2. \label{eq_lem_model_shift_1}
%		\le& n_1\bigbrace{{n_1^2}{} + \frac{2n_1}p(p + 2\sqrt{{n_1}p}) + (p + 2\sqrt{{n_1}p})^2} \alpha = n_1^2\bigbrace{1 + \sqrt{\frac{p}{n_1}}}^4 \alpha. \nonumber
	\end{align}
	Using \eqref{eq_isometric}, we can bound  
	$$\|\cal E\|\le \left( \al_+(\rho_1)-1 + \OO(p^{-1/2+\e})\right)n_1, \quad \text{w.h.p.}$$
	Thus we can estimate that 
	\begin{align*}
	| \delta_{\bias}(v)-\wt\delta_{\bias}(v)|&\le v^2 \left( 2n_1  \|\cal E\| +  \|\cal E\|^2 \right) \bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \bignorm{\Sigma_1^{1/2} (\beta_1 - v\beta_2)}^2 \\
	&=  v^2 \left[\left( n_1 + \|\cal E\|\right)^2 -n_1^2 \right] \bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \bignorm{\Sigma_1^{1/2} (\beta_1 - v\beta_2)}^2 \\
	& \le v^2 n_1^2 \left[ \al_+^2(\rho_1) + \OO(p^{-1/2+\e}) -1\right]\bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \bignorm{\Sigma_1^{1/2} (\beta_1 - v\beta_2)}^2,
	\end{align*}
	which concludes the proof by the definition of $\delta_\e$.	
%	we can bound the second term on the RHS of equation \eqref{eq_lem_model_shift_1} as
%	\begin{align*}
%		& \bigabs{(\beta_1 -  \beta_2)^{\top} \Sigma_1^{1/2} \cE \Sigma_1^{1/2} K \Sigma_2 K \Sigma_1 (\beta_1 - v\beta_2)}\le n_1  \|\cal E\| \cdot \bignorm{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2}} \bignorm{\Sigma_1^{1/2} (\beta_1 - v\beta_2)}^2 \\
%		= & \bigabs{\bigtr{\cE \Sigma_1^{1/2}K\Sigma_2 K \Sigma_1(\beta_1 - \hat{w}\beta_2)(\beta_1 - \hat{w}\beta_2)^{\top} \Sigma_1^{1/2}}} \\
%		\le & \norm{\cE} \cdot \bignormNuclear{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1 (\beta_1 - \hat{w}\beta_2) (\beta_1 - \hat{w}\beta_2)^{\top} \Sigma_1^{1/2}} \\
%		\le & n_1 \bigbrace{2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}} \cdot \bignormNuclear{\Sigma_1^{1/2} K \Sigma_2 K \Sigma_1 (\beta_1 - \hat{w}\beta_2)(\beta_1 - \hat{w}\beta_2)^{\top} \Sigma_1^{1/2}} \tag{by equation \eqref{eq_isometric}} \\
%		\le   & n_1 \bigbrace{2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}} \bignorm{\Sigma_1^{1/2}K \Sigma_2 K \Sigma_1^{1/2}} \cdot \bignorm{\Sigma_1^{1/2}(\beta_1 - \hat{w}\beta_2)}^2 \tag{since the matrix inside is rank 1}
%	\end{align*}
%	The third term in equation \eqref{eq_lem_model_shift_1} can be bounded with
%	\begin{align*}
%		\bignorm{\Sigma_2^{1/2}K\Sigma_1^{1/2}\cE\Sigma_1^{1/2}(\beta_1 - v\beta_2)}^2
%		\le n_1^2 \bigbrace{2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}}^2 \bignorm{\Sigma_1^{1/2}K\Sigma_{2}K\Sigma_1^{1/2}} \cdot \bignorm{\Sigma_1^{1/2}(\beta_1 -  \beta_2)}^2.
%	\end{align*}
%	Combined together we have shown the right direction for $\delta_1 - \delta_2$.
%	For the left direction, we simply note that the third term in equation \eqref{eq_lem_model_shift_1} is positive.
%	And the second term is bigger than $-2n_1^2(2\sqrt{\frac{p}{n_1}} + \frac{p}{n_1}) \alpha$ using equation \eqref{eq_isometric}.
\end{proof}
Note by \eqref{eq_isometric}, we have with high probability,
\begin{align*}
&v^2 n_1^2 \Sigma_1^{1/2} K \Sigma_2 K \Sigma_1^{1/2} =n_1^2 \hat M (\hat M^\top Z_1^\top Z_1 \hat M + Z_2^\top Z_2)^{-2}\hat M^\top \\
&\preceq  n_1^2 \hat M \left[n_1 \al_-(\rho_1)\hat M^\top \hat M + n_2 \al_-(\rho_2) + \OO(p^{1/2+\e})\right]^{-2}\hat M^\top \\
&\preceq  \left[ \al_-^2(\rho_1) \hat M\hat M^\top + 2\frac{\rho_2}{\rho_1} \al_-(\rho_1)\al_-(\rho_2) + 2\left(\frac{\rho_2}{\rho_1}\right)^2 \al_-^2(\rho_2) (\hat M \hat M^\top )^{-1}\right]^{-1}+  \OO(p^{-1/2+\e}) \\
&\preceq [\al_-^2(\rho_1) \lambda_{\min}^2(\hat M)]^{-1}\cdot (1 - c)
\end{align*}
for some small enough constant $c>0$. Together with Claim \ref{prop_model_shift}, we get with high probability,
\be\label{bounddelta-}
\left| \delta_{\bias}(v)-\wt\delta_{\bias}(v)\right| 
		\le (1-c) \delta(v)
\ee
for some small constant $c>0$, where we recall $\delta(v)$ defined in \eqref{eq_deltaextra}.
			}

 \paragraph{Bias asymptotic limit.}
 For the bias limit in equation \eqref{lem_cov_derv_eq}, we show that it is governed by the derivative of $(W - z\id)^2$ with respect to $z$ at $z = 0$.
First, we can express the empirical bias term in equation \eqref{lem_cov_derv_eq} as %$W$
\begin{align}\label{calculate G'}
	(n_1 + n_2)^2 \hat{\Sigma}^{-1}\Sigma^{(2)}\hat{\Sigma}^{-1} = {\Sigma^{(2)}}^{-1/2} V W^{-2} V^{\top} {\Sigma^{(2)}}^{-1/2}.
\end{align}
Let $\cal G(z):=(W-z\id )^{-1}$ denote the resolvent of $W$.
Our key observation is that $\frac{\dd{\cal G(z)}}{\dd z} =  \cal G^2(z)$.
Hence, provided that the limit of $(W - z\id)^{-1}$ is $(a_1(z) \Lambda^2 + (a_2(z) - z) \id)^{-1}$ near $z = 0$, the limit of $\frac{\dd{\cal G(0)}}{\dd z}$ satisfies
\begin{align}\label{cal G'0}
	\frac{\dd \cal G(0)}{\dd z} \to \frac{-\frac{\dd a_1(0)}{\dd z}\Lambda^2 - (\frac{\dd a_2(0)}{\dd z} - 1)\id}{(a_{1}(0)\Lambda^2 + a_{2}(0)\id_p)^2}.
\end{align}
To find the derivatives of $a_1(z)$ and $a_2(z)$, we take the derivatives on both sides of the system of equations \eqref{selfomega_a}.
%\begin{align*}
%	\frac{\dd a_1(z)}{\dd z} + \frac{\dd a_2(z)}{\dd z} = -\frac{1}{n_1 + n_2} \sum_{i=1}^p \frac{1}{\lambda_i^2 a_1 + a_2},
%	\frac{\dd a_1(z)}{\dd z} + \frac{1}{n_1 + n_2}\sum_{i=1}^p \frac{\lambda_i^2 (a_1'(z) a_2 - a_2'(z) a_1)}{(\lambda_i^2 a_1 + a_2)^2} = -\frac{1}{n_1 + n_2} \sum_{i=1}^p \frac{\lambda_i^2 a_1}{(\lambda_i^2 a_1 + a_2)^2}
%\end{align*}
Let $a_3 = - \frac{\dd a_1(0)}{\dd z}$ and $a_4 = - \frac{\dd a_2(0)}{\dd z}$.
%then taking implicit differentiation of equation \eqref{selfomega_a}
One can verify that $a_3$ and $a_4$ satisfy the self-consistent equations in \eqref{eq_a34extra} (details omitted).
Applying equation \eqref{cal G'0} to equation \eqref{calculate G'}, we obtain the bias limit.
%\begin{align}
%& n^2\bignorm{\Sigma_2^{1/2} \bigbrace{ (X^{(1)})^{\top}(X^{(1)}) + (X^{(2)})^{\top}(X^{(2)}) }^{-1} \Sigma_1^{1/2} w}^2 \nonumber\\
%&\approx  w^\top \Sigma_1^{1/2}\Sigma_2^{-1/2}V\frac{a_3\Lambda^2 +(1+ a_4)\id_p}{(a_{1}\Lambda^2 + a_{2}\id)^2}V^\top \Sigma_2^{-1/2}\Sigma_1^{1/2}w= w^{\top} \Pi_\bias w, \label{calculatePibias}
%\end{align}
%where in the last step we used $M = \Sigma_1^{1/2}\Sigma_2^{-1/2}$ and $V \Lambda^2 V^\top=M^\top M$. This concludes equation \eqref{lem_cov_derv_eq}.

As a remark, in order for $\frac{\dd \cal G(z)}{\dd z}$ to stay close to its limit at $z = 0$, we not only need to find the limit of $\cal G(0)$, but also the limit of $\cal G(z)$ within a small neighborhood of $0$.
This is why we consider $W$'s resolvent for a general $z$ (as opposed to the Stieljes transform of its empirical spectral distribution discussed earlier).
%Then we have
%$$  \left\|(X^{(1)})^{\top}X^{(1)} (\beta_s - \beta_t)  - \frac{n_1}{n}(\beta_s - \beta_t)\right\|_2 \le C \sqrt{\frac{p}{n}} \left\| (\beta_s - \beta_t)\right\|_2. $$
%\todo{(revise the following proof)} It remain to study the following expression
%\begin{align*}
%\frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}}  \Sigma_2 \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}}  = \Sigma_2^{-1/2}\left(\frac{1}{A^T (Z^{(1)})^{\top}Z^{(1)} A + (Z^{(2)})^{\top}Z^{(2)}} \right)^2 \Sigma_2^{-1/2} \\
%\stackrel{d}{=} \Sigma_2^{-1/2}V \left(\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)}} \right)^2 V^T \Sigma_2^{-1/2},
%\end{align*}
%where
%\be  \label{eigen2000}
%A:=\Sigma_1^{1/2}\Sigma_2^{-1/2} = U\Lambda V^T ,\quad \Lambda=\text{diag}(\lambda_1, \cdots, \lambda_p).
%\ee
%Using
%$$\left(\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)}} \right)^2 =\left. \frac{\dd }{\dd z}\right|_{z=0}\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)} - z} ,$$
%we  need to study the resolvent of
%$$G(z) = \left( \Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)} - z \right)^{-1}.$$
%Its local law can be studied as in previous subsection (be careful we need to switch the roles of $Z^{(1)}$ and $Z^{(2)}$). More precisely,  we have that
%$$ G(z) \approx \diag\left( \frac{1}{-z\left( 1+ m_3(z) + \lambda_i^2 m_4(z)\right)}\right)_{1\le i \le p}= \frac{1}{-z\left( 1+ m_3(z) + \Lambda^2 m_4(z)\right)} .$$
%Here $m_{3,4}(z)$ satisfy the following self-consistent equations
%%$$\frac{1}{G_{ii}} \approx -z \left( 1+m_3 + d_i^2m_4 \right), \quad \frac{1}{G_{\mu\mu}} = -z(1+m_1), \ \ \mu\in \cal I_1,\quad \frac{1}{G_{\nu\nu}} = -z(1+m_2), \ \ \nu\in \cal I_2,$$
%%$$m_1= \frac1n\sum_{i}G_{ii}, \quad m_2= \frac1n\sum_{i}d_i^2 G_{ii}, \quad m_3 = \frac1n\sum_{\mu\in \cal I_1} G_{\mu\mu},\quad m_4 = \frac1n\sum_{\mu\in \cal I_2} G_{\mu\mu}.$$
%\begin{align}\label{m34shift}
%\frac{n_2}{n}\frac1{m_3} = - z +\frac1n\sum_{i=1}^p \frac1{  1+m_3 + \lambda_i^2m_4  } ,\quad \frac{n_1}{n}\frac1{m_4} = - z +\frac1n\sum_{i=1}^p \frac{\lambda_i^2 }{  1+m_3 + \lambda_i^2m_4  } .
%\end{align}
%
%It remains to derive equation \eqref{eq_a34extra} for $(a_3,a_4)$. Taking implicit differentiation of equation \eqref{selfomega_a}, we obtain that
%\be \nonumber%\label{dotm34}
%\begin{split}
% \rho_1\frac{-a'_{1}(0) }{a_{1}^2(0)}=   \frac1p\sum_{i=1}^p \frac{\lambda_i^2\left(1-\lambda_i^2 a'_{1}(0) - a'_{2}(0)\right) }{  ( \lambda_i^2 a_{1}(0) +a_{2}(0))^2 } ,\quad \rho_2\frac{-a'_{2}(0)}{a_2^2(0)} &=  \frac1p\sum_{i=1}^p \frac{1-\lambda_i^2 a'_{1}(0) - a'_{2}(0)}{ ( \lambda_i^2a_{1}(0) +a_{2}(0))^2  } .
%\end{split}
%\ee
%Using $(a_1,a_2)=(a_1(0),a_2(0))$ and $(a_3,a_4)=(-a_1'(0),-a_2'(0))$,
%Recalling that we have denoted  $(a_1, a_2)=(-r_1M_{1}(0),-r_2M_{2}(0))$, we can rewrite these two equations as
%%\be%\label{dotm34}
%%\begin{split}
%% r_1 \frac1{a_1^2}a_4 &=   \frac1n\sum_{i=1}^p \frac{\lambda_i^2\left(1+ \lambda_i^2 a_4 + a_3\right) }{  ( \lambda_i^2 a_1 +a_2)^2 } ,\\
%%r_2\frac1{a_2^2}a_3 &=  \frac1n\sum_{i=1}^p \frac{1+\lambda_i^2 a_4 + a_3}{ ( \lambda_i^2 a_1 +a_2)^2  } .
%%\end{split}
%%\ee
%%We can solve the above equations to get $u'_3$ and $u'_4$. Then we have
%%$$ G^2(z) \approx   \frac{1 +  u_3'(z) +\Lambda^2 u'_4(z)}{ \left( z+ u_3(z) + \Lambda^2  u_4(z)\right)^2}  $$
%%{\cob in certain sense}.
%%\cob $a_3\to a_2$, $a_4\to a_1$ \nc
%%We now simplify the expressions for $z\to 0$ case. When $z\to 0$, we shall have
%%$$u_3(z)= -  {a_3} + \OO(z), \quad u_4(z)= -  a_4 + \OO(z), \quad a_3,a_4 >0.$$
%%For $z\to0$, the equations in equation \eqref{m34shift} are reduced to
%%\begin{align}\label{m35shift}
%%\frac{n_2}{n}\frac{1}{\todo{a_2}} = 1 +\frac1n\sum_{i=1}^p \frac{1}{\todo{a_2} + \lambda_i^2\todo{a_1}  } ,\quad \frac{n_1}{n}\frac1{\todo{a_1}} = 1 +\frac1n\sum_{i=1}^p \frac{\lambda_i^2 }{  \todo{a_2} + \lambda_i^2 \todo{a_1} }.
%%\end{align}
%%It is easy to see that these equations are equivalent to
%%\begin{align} a_1 + a_2 = 1- \gamma_n, \quad a_1 +\frac1n\sum_{i=1}^p \frac{a_1}{a_1 + a_2/\lambda_i^2}=\frac{n_1}{n}  .\end{align}
%%The equations in equation \eqref{dotm34} reduce to
%\be \label{dotm34red}
%\begin{split}
%\left(\frac{r_2}{a_2^2}- \frac1n\sum_{i=1}^p \frac{1 }{ (   \lambda_i^2a_1+a_2   )^2  }\right) a_4 -  \left(\frac1n\sum_{i=1}^p \frac{  \lambda_i^2 }{ (   \lambda_i^2a_1+a_2   )^2  }\right)a_3 =  \frac1n\sum_{i=1}^p \frac{1 }{ (   \lambda_i^2a_1+a_2   )^2  } ,\\
% %\frac{n_1}{n}\frac1{a_4^2}b_4 =   \frac1n\sum_{i=1}^p \frac{\lambda_i^2\left(1+x_3 + \lambda_i^2 b_4\right) }{  (a_3 + \lambda_i^2a_4)^2  } , \\
% \left(  \frac{r_1}{a_1^2} -  \frac1n\sum_{i=1}^p \frac{\lambda_i^4   }{  (\lambda_i^2a_1+a_2)^2  }\right)a_3 -\left( \frac1n\sum_{i=1}^p \frac{\lambda_i^2  }{  (\lambda_i^2a_1+a_2  )^2  }\right)a_4 =   \frac1n\sum_{i=1}^p \frac{\lambda_i^2 }{  (\lambda_i^2a_1+a_2 )^2  } ,
%\end{split}
%\ee
%where we denoted  $(a_3 ,a_4):=(r_1M_{1}'(0),r_2M'_{2}(0))$. 
%It is not hard to see that this equation is equivalent to equation \eqref{eq_a34extra}. 
%Finally, we can calculate $\bv^\top \Gi'(0)\bv$ as in equation \eqref{cal G'0}, which concludes equation \eqref{lem_cov_derv_eq} by equation \eqref{apply derivlocal}. 
%Then by equation \eqref{apply derivlocal} and equation \eqref{dervPi}, we get
%\begin{align*}
%n^2\bignorm{\Sigma_2^{1/2} ( (X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)})^{-1}\beta}^2&=\beta^\top \Sigma_2^{-1/2} V  \frac{1 + a_4+ a_3\Lambda^2 }{(a_1\Lambda^2 + a_2)^2}  V^\top  \Sigma_2^{-1/2} \beta \\
%&=\beta^\top \Sigma_2^{-1/2} \frac{1 + a_4+ a_3M^\top M }{(a_1M^\top M + a_2)^2} \Sigma_2^{-1/2} \beta,
%\end{align*}
%where we used $M^\top M= V\Lambda^2 V^\top$ in the second step. 
%%where we denote $x_3:=u_3'(0)$ and $b_4:=u_4'(0)$. Thus we have
%%$$\left(\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)}} \right)^2  = G^2(0) \approx   \frac{ 1 + x_3 + \Lambda^2  b_4}{\left( a_3 + \Lambda^2  a_4\right)^2} .$$
%%This gives that
%%\be\label{derivative}
%%\begin{split}
%%& \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}}  \Sigma_2 \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}} =\Sigma_2^{-1/2}V \left(\frac{1}{\Lambda (Z^{(1)})^{\top}Z^{(1)} \Lambda + (Z^{(2)})^{\top}Z^{(2)}} \right)^2 V^T \Sigma_2^{-1/2}\\
%%& \approx  \Sigma_2^{-1/2}V \frac{ 1 + x_3 + \Lambda^2  b_4}{\left( a_3 + \Lambda^2  a_4\right)^2}V^T \Sigma_2^{-1/2}= \Sigma_2^{-1/2}  \frac{ 1 + x_3 +  b_4 A^\top A }{\left( a_3 +   a_4 A^\top A\right)^2}\Sigma_2^{-1/2} .
%%\end{split}
%%\ee
%%Hence we have
%%\begin{align*}
%%& (\beta_s - \beta_t)^{\top}(X^{(1)})^{\top}X^{(1)} \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}}  \Sigma_2 \frac{1}{(X^{(1)})^{\top}X^{(1)} + (X^{(2)})^{\top}X^{(2)}} (X^{(1)})^{\top}X^{(1)} (\beta_s - \beta_t) \\
%% & \approx (\beta_s - \beta_t)^{\top}\Sigma_1 \Sigma_2^{-1/2}  \frac{ 1 + a_3 +  a_4 A^\top A }{\left( a_2 +   a_1 A^\top A\right)^2}\Sigma_2^{-1/2} \Sigma_1(\beta_s - \beta_t).
%%\end{align*}
%This concludes Lemma \ref{lem_cov_derivative}.
\fi
\end{proof}
 
 
 


 


%\todo{revise} In Section \ref{sec locallaw1}, we introduce the concept of resolvents, and give an almost optimal convergent estimate on it---Theorem \ref{LEM_SMALL}---in Section \ref{sec aymp_limit_G}. This estimate is conventionally called {\it local law} in random matrix literature. Based on Theorem \ref{LEM_SMALL}, we then complete the proof of Lemmas \ref{lem_cov_shift} and \ref{lem_cov_derivative} in Section \ref{sec pf RMTlemma}. The proof of Theorem \ref{LEM_SMALL} will be presented in Section \ref{sec_Gauss}.

\subsection{Self-consistent equations}\label{sec contract}

The rest of this section is devoted to the proof of Theorem \ref{LEM_SMALL}. In this section, we show that the limiting equation \eqref{selfomega_a} has a unique solution $(a_1(z), a_2(z))$ for any $z\in \mathbf D$ in equation \eqref{SSET1}. Otherwise, Theorem \ref{LEM_SMALL} will be a vacuous statement. For simplicity of notations, we define the following ratios 
\be\label{ratios}
 \gamma_n :=\frac{p}{n_1+n_2} ,\quad r_1 :=\frac{n_1}{n_1+n_2} ,\quad r_2 :=\frac{n_2}{n_1+n_2} .
\ee

When $z=0$, the system of equations in \eqref{selfomega_a} reduces to the system of equations in \eqref{eq_a12extra}. With \eqref{eq_a12extra}, we can derive an equation of $a_1\equiv a_1(0)$ only:
%Note that the function 
\be\label{fa1}f(a_1)=r_1,\quad \text{with}\quad f(a_1):=a_1 +\frac{1}{n_1+n_2} \sum_{i=1}^p \frac{\lambda_i^2 a_1}{ \lambda_i^2 a_1+ (1- \gamma_n - a_1) } .\ee
We can calculate that
$$f'(a_1)=1+\frac{1}{n_1+n_2} \sum_{i=1}^p \frac{\lambda_i^2 (1-\gamma_n)}{ [\lambda_i^2 a_1+ (1- \gamma_n - a_1)]^2 }>0.$$
Hence $f$ is strictly increasing on $[0,1-\gamma_n]$. Moreover, we have $f(0)=0<r_1$, $f(1-\gamma_n )=1<r_1$, and $f(r_1)>r_1$ if $r_1\le 1-\gamma_n$. Hence by mean value theorem, there exists a unique solution $a_1$ to \eqref{fa1} satisfying
$0< a_1 <\min\left\{1-\gamma_n, r_1\right\}.$ Furthermore, using that $f'(x)=\OO(1)$ for any fixed $x\in (0, 1-\gamma_n)$, it is not hard to check that 
\be\label{a230}
r_1 \tau  \le   a_1 \le \min\left\{ 1-\gamma_n,r_1\right\}  
\ee 
for a small constant $\tau>0$. With \eqref{eq_a12extra}, we can also derive a equation of $a_2\equiv a_2(0)$ only. With a similar argument as above, we can get that for a small constant $\tau>0$,
\be\label{a231}
r_1 \tau   \le a_2\le  \min\left\{ 1-\gamma_n,r_2\right\} .
\ee 

%\begin{equation}\label{selfomega0}
%\begin{split}
%&a_1 + a_2= 1-\gamma_n,\quad    a_1 +\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2 a_1}{ \lambda_i^2 a_1+ (1-\gamma_n - a_1) }=r_1 .
%\end{split}
%\ee
%where $a_1:=-r_1M_{1}(0)$ and $a_2:=-r_2M_{2}(0)$. 

Next, we prove the existence and uniqueness of the solution to the self-consistent equation \eqref{selfomega_a} for a general $z\in \mathbf D$. For the proof of Theorem \ref{LEM_SMALL}, it is better to use the following rescaled functions of $a_1(z)$ and $a_2(z)$:
\be\label{M1M2a1a2}
m_{1c}(z):= - r_1^{-1}a_1(z),\quad m_{2c}(z):= - r_2^{-1}a_2(z),
\ee
which, as we will see in \eqref{Xizz} below, denote the classical values (i.e. asymptotic limits) of $m_1(z)$ and $m_2(z)$.
%which are the asymptotic limits of $m_1(z)$ and $m_2(z)$ in equation \eqref{approximate m1m2}.
Moreover, it is more convenient to work with the following system of self-consistent equations of $(m_{1c}(z),m_{2c}(z))$, which can be shown to be equivalent to the system of equations \eqref{selfomega_a}:
% Introduce $(M_1(z),M_2(z))$. First we define the deterministic limits of $(m_1(z), m_{2}(z))$, denoted by $(M_{1}(z),M_{2}(z))$ 
%\HZ{This $M_{1}, M_{2}$ notation is awkward; consider changing it to ${M}_1(z), {M}_2(z)$ (or  better, say $a_1(z), a_2(z)$? if they correspond to $a_1, a_2$ when $z=0$)}, 
%the unique solution to 
% the following system of equations:
\begin{equation}\label{selfomega}
\begin{split}
& \frac1{m_{1c}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2 r_1 m_{1c} +r_2 m_{2c}  } - 1 ,\\
& \frac1{m_{2c}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1 m_{1c} +  r_2 m_{2c}  }- 1 .
\end{split}
\ee
When $z=0$, usings \eqref{eq_a12extra}, \eqref{a230} and \eqref{a231}, we get that
%such that $\im M_{1}(z)>0$ and $\im M_{2}(z)>0$ for $z\in \C_+$, 
\be\label{a23}
\begin{split}
\tau  \le  - m_{1c}(0) \le 1,\quad \tau  \le  - m_{2c}(0) \le 1,\quad -r_1 m_{1c}(0)-r_2 m_{2c}(0)=1-\gamma_n.
\end{split}
\ee 

%For general $z$ around the origin, the existence and uniqueness of the solution $(M_{1}(z),M_{2}(z))$ is given by the following lemma. 
%One can compare equation \eqref{selfomega} for $(M_1(z),M_2(z))$ to equation \eqref{approximate m1m2} for $(m_1(z),m_2(z))$.
%In the following proof, we shall focus on the system of equations \eqref{selfomega} because it is more suitable than equation \eqref{selfomega_a}  for our purpose of showing that $(m_1(z),m_2(z))$ converges to the asymptotic limit $(M_1(z),M_2(z))$.

Now we claim the following lemma, which gives the existence and uniqueness of the solution $(m_{1c}(z),m_{2c}(z))$ to the system of equations \eqref{selfomega}.

\begin{lemma} \label{lem_mbehaviorw}
There exist constants $c_0, C_0>0$ depending only on $\tau$ in Assumption \ref{assm_big1} and equation \eqref{a23} such that the following statements hold.
%Fix any constants $c, C>0$. If equation \eqref{assm20} holds, then we have the following estimates.
%If $|z|\le c_0$, then 
There exists a unique solution $(m_{1c}(z),m_{2c}(z))$ to equation \eqref{selfomega} under the conditions
\be\label{prior1}
|z|\le c_0, \quad  |m_{1c}(z) - m_{1c}(0)| + |m_{2c}(z) - m_{2c}(0)|\le c_0.
\ee
Moreover, the solution satisfies
\be\label{Lipomega}
 |m_{1c}(z) - m_{1c}(0)| + |m_{2c}(z) - m_{2c}(0)| \le C_0|z|.
\ee
%and
%\be\label{Immcw}
%0\le \im m_{\al c}(z) \le C_0\eta ,\quad z= E+ \ii\eta \in \C_+, \ \ \al=2,3.
%\ee
%For $z\in \C_+ \cap \{z: c\le |z| \le C\}$, we have
%\begin{equation}\label{absmcw}
% \vert M_{2}(z) \vert \sim 1,  \quad \left|z^{-1} - (M_{1}(z)+M_{2}(z)) + (z-1)M_{1}(z)M_{2}(z) \right|\sim 1,
% \ee 
% and
% \be\label{Immcw} 0\le \im M_{2}(z) \sim \begin{cases}
%    {\eta}/{\sqrt{\kappa+\eta}}, & \text{ if } E \notin [\lambda_-,\lambda_+] \\
%    \sqrt{\kappa+\eta}, & \text{ if } E \in [\lambda_-,\lambda_+]\\
%  \end{cases}.
%\end{equation}
%\end{itemize}
%The above estimates also hold for $M_{1}$, $M_{2}(z)$, $m_{4c}(z)$ and $m_c(z)$. Finally, the estimates equation \eqref{absmcw}, equation \eqref{eq_mcomplexw} and equation \eqref{eq_mdiffw} hold for $h(z)$. 
\end{lemma}
%The proof is a standard application of the contraction principle. For reader's convenience, we will include its proof in Section \ref{sec contract}. 
% In this section we give the proof of Lemmas \ref{lem_mbehaviorw} and \ref{lem_stabw} using the contraction principle. 
 %We first prove the existence and continuity of the solutions to equation \eqref{falvww}. 


 \begin{proof} %[Proof of Lemma \ref{lem_mbehaviorw}]
 The proof is a standard application of the contraction principle. %For reader's convenience, we give more details. 
 First, it is easy to check that the system of equations in \eqref{selfomega} are equivalent to  
\begin{equation}\label{selfalter}
r_1m_{1c}=-(1-\gamma_n) - r_2m_{2c} - z\left( m_{2c}^{-1}+1\right),\quad g_z(m_{2c}(z))=1, 
\ee
where
$$g_z(m_{2c}):= - m_{2c} +\frac{\gamma_n}p\sum_{i=1}^p \frac{m_{2c} }{  z -\lambda_i^2(1-\gamma_n)+ (1 - \lambda_i^2) r_2m_{2c} - \lambda_i^2 z\left(  m_{2c}^{-1}+1\right) }.$$
 We first show that there exists a unique solution $m_{2c}(z)$ to the equation $g_z(m_{2c}(z))=1$ under the conditions in equation \eqref{prior1}.
%, and (ii) $M_2(z)$ satisfies equation \eqref{Lipomega}. 
We abbreviate $\delta(z):= m_{2c}(z) - m_{2c}(0)$. From equation \eqref{selfalter}, we obtain that 
\begin{equation} \nonumber
0=\left[g_z(m_{2c}(z)) -  g_0(m_{2c}(0)) -g_z'(m_{2c}(0))\delta(z)\right] + g_z'(m_{2c}(0))\delta(z),
\ee
which gives that
\be\nonumber%\label{selfomega1}
 \delta(z) =- \frac{ g_z(m_{2c}(0)) - g_0(m_{2c}(0)) }{g_z'(m_{2c}(0))}- \frac{ g_z(m_{2c}(0)+\delta(z)) -  g_z(m_{2c}(0))-g_z'(m_{2c}(0))\delta(z)}{g_z'(m_{2c}(0))}.
 \ee
%By equation \eqref{selfomega}, we obtain the self-consistent equations for $(m_{2c}(0),m_{2c}(0))$ and $(m_{2c}(z),m_{2c}(z))$:
% \begin{equation}\label{selfomega1}
%\begin{split}
%& \frac{r_1}{m_{2c}(0) }=- 1 + \frac1n\sum_{i=1}^p \frac{\lambda_i^2}{\lambda_i^2m_{2c}(0) + m_{2c} (0) },\quad \frac{r_2}{m_{2c} (0)}=  - 1 +\frac1n\sum_{i=1}^p \frac{1 }{ \lambda_i^2 m_{2c} (0)+  m_{2c}(0)  }.
%\end{split}
%\ee
%Subtract equation \eqref{selfomega1} from equation \eqref{selfomega}, we get that
% \begin{equation}\label{selfomega3}
%\begin{split}
%& \e_{2} \frac{r_1}{(m_{2c} +\e_2) m_{2c}}=  \frac1n\sum_{i=1}^p \frac{\lambda_i^2 (z+\lambda_i^2 \e_2 +\e_3)}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2(m_{2c}+\e_2)  + (m_{2c}+\e_3) ) },\\
%&  \e_3 \frac{r_2}{(m_{2c}+\e_3) m_{2c}}=  \frac1n\sum_{i=1}^p \frac{z+\lambda_i^2 \e_2 +\e_3}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2(m_{2c}+\e_2)  + (m_{2c}+\e_3) ) }.
%\end{split}
%\ee
Inspired by this equation, we define iteratively a sequence ${\delta}^{(k)}(z) \in \C$ such that ${\delta}^{(0)}=0$, and 
\be\label{selfomega2}
\begin{split}
 \delta^{(k+1)} = & - \frac{g_z(m_{2c}(0)) - g_0(m_{2c}(0))}{g_z'(m_{2c}(0))} \\
 &-\frac{g_z(m_{2c}(0)+ \delta^{(k)}) -  g_z(m_{2c}(0))-g_z'(m_{2c}(0))\delta^{(k)} }{g_z'(m_{2c}(0))} .
 \end{split}
 \ee
% \begin{equation}\nonumber%\label{selfomega4}
%\begin{split}
%& \left\{\frac{r_1}{m_{2c}^2  } - \frac1n\sum_{i=1}^p \frac{\sigma_i^4   }{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2 m_{2c}  + m_{2c} ) } \right\} \e^{(k+1)}_{2}  -  \frac1n\sum_{i=1}^p \frac{\lambda_i^2   }{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2 m_{2c}  + m_{2c} ) } \e_3^{(k+1)} \\
%& = \frac1n\sum_{i=1}^p \frac{\lambda_i^2 z}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2m_{2c}  + m_{2c} ) } + \frac{r_1 [\e_2^{(k)}]^2}{m_{2c}^2 (m_{2c}+\e_2^{(k)})} \\
%&+   \frac1n\sum_{i=1}^p \left( \frac{\lambda_i^2 (z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)})}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2(m_{2c}+\e_2^{(k)})  + (m_{2c}+\e_3^{(k)}) ) } - \frac{\lambda_i^2 (z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)})}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2m_{2c}  + m_{2c}) }\right),\\
%& \left\{\frac{r_2}{m_{2c}^2  } - \frac1n\sum_{i=1}^p \frac{1 }{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2 m_{2c}  + m_{2c} ) } \right\} \e^{(k+1)}_{3}  -  \frac1n\sum_{i=1}^p \frac{1 }{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2 m_{2c}  + m_{2c} ) } \e_2^{(k+1)} \\
%& = \frac1n\sum_{i=1}^p \frac{ z}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2m_{2c}  + m_{2c} ) } + \frac{r_2 [\e_3^{(k)}]^2}{m_{2c}^2 (m_{2c}+\e_3^{(k)})} \\
%&+   \frac1n\sum_{i=1}^p \left( \frac{ z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)}}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2(m_{2c}+\e_2^{(k)})  + (m_{2c}+\e_3^{(k)}) ) } - \frac{ z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)}}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2m_{2c}  + m_{2c}) }\right).
%\end{split}
%\ee
Then equation \eqref{selfomega2} defines a mapping $h_z:\C\to \C$, which maps $\delta^{(k)}$ to $\delta^{(k+1)}=h_z(\delta^{(k)})$.
 
%\be\label{iteration} 
%{\e}^{(k+1)}= \mathbf f({ \e}^{(k)}), \quad \mathbf f(\mathbf x):=S^{-1}\mathbf x_0+ S^{-1} \mathbf e(\mathbf x),
%\ee
%where $S$ is a $2\times 2$ matrix with
%\begin{align*}
%S_{11}=\frac{r_1}{m_{2c}^2  } - \frac1n\sum_{i=1}^p \frac{\sigma_i^4   }{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2 m_{2c}  + m_{2c} ) }, \quad S_{12}=-  \frac1n\sum_{i=1}^p \frac{\lambda_i^2   }{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2 m_{2c}  + m_{2c} ) },\\
%S_{21}=-  \frac1n\sum_{i=1}^p \frac{1 }{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2 m_{2c}  + m_{2c} ) } , \quad S_{22}=\frac{r_2}{m_{2c}^2  } - \frac1n\sum_{i=1}^p \frac{1 }{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2 m_{2c}  + m_{2c} ) } ,
%\end{align*}
%%$$
%%S:=\begin{pmatrix} \frac{c_1}{m_{1c}^2  } -  \frac{ \theta_l^2 (1-\theta_l )^2 }{ (1-t_l)^2}g(m_{2c})^2  & -  \frac{ (1-\theta_l )^2\theta_l }{ (1-t_l)^2} \\ -   \frac{ (1-\theta_l )^2\theta_l }{ (1-t_l)^2}  &\frac{c_2}{m_{2c}^2  } -  \frac{ \theta_l^2(1-\theta_l )^2 }{ (1-t_l)^2}g(m_{1c})^2  \end{pmatrix},  
%%$$
%$\mathbf x_0$ is a vector with
%$$\mathbf x_0=\begin{pmatrix} \frac1n\sum_{i=1}^p \frac{\lambda_i^2 z}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2m_{2c}  + m_{2c} ) } \\ \frac1n\sum_{i=1}^p \frac{ z}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2m_{2c}  + m_{2c} ) }  \end{pmatrix},$$
%and $\mathbf e(\mathbf x)$ is
%$$\mathbf e(\mathbf x):= \begin{pmatrix}\frac{r_1 [\e_2^{(k)}]^2}{m_{2c}^2 (m_{2c}+\e_2^{(k)})} +   \frac1n\sum_{i=1}^p \left( \frac{\lambda_i^2 (z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)})}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2(m_{2c}+\e_2^{(k)})  + (m_{2c}+\e_3^{(k)}) ) } - \frac{\lambda_i^2 (z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)})}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2m_{2c}  + m_{2c}) }\right) \\ 
% \frac{r_2 [\e_3^{(k)}]^2}{m_{2c}^2 (m_{2c}+\e_3^{(k)})} +   \frac1n\sum_{i=1}^p \left( \frac{ z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)}}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2(m_{2c}+\e_2^{(k)})  + (m_{2c}+\e_3^{(k)}) ) } - \frac{ z+\lambda_i^2 \e_2^{(k)} +\e_3^{(k)}}{(\lambda_i^2m_{2c}  + m_{2c})(z+\lambda_i^2m_{2c}  + m_{2c}) }\right) \end{pmatrix}.$$
%%Here we have used $\theta_lg(m_{1c})g( m_{2c} ) = f_c(\theta_l) = t_l$ (recall equation \eqref{fcz}) to simplify the expressions a little bit.

With direct calculation, we obtain that
$$g_z'(m_{2c}(0)) = -1 - \frac{\gamma_n}p\sum_{i=1}^p \frac{ \lambda_i^2(1-\gamma_n) - z\left[1- \lambda_i^2 \left(  2m_{2c}^{-1}(0)+1\right)\right]  }{  \left[z -\lambda_i^2(1-\gamma_n)+ (1 - \lambda_i^2) r_2m_{2c}(0) - \lambda_i^2 z\left( m_{2c}^{-1}(0)+1\right)\right]^2 }.$$
%Using equation \eqref{a23}, 
Using \eqref{assm3_app} and \eqref{a23}, it is easy to check that
\begin{align*}
&\left|z -\lambda_i^2(1-\gamma_n)+ (1 - \lambda_i^2) r_2m_{2c}(0) - \lambda_i^2 z\left( m_{2c}^{-1}(0)+1\right)\right| \ge c_\tau - c_\tau^{-1} |z| ,
\end{align*}
for a constant $c_\tau>0$ depending only on $\tau$. Then as long as we choose $c_0\le c_\tau^2/2$, we have 
\begin{align}\label{deno_lower}
&\left|z -\lambda_i^2(1-\gamma_n)+ (1 - \lambda_i^2) r_2m_{2c}(0) - \lambda_i^2 z\left( m_{2c}^{-1}(0)+1\right)\right| \ge c_\tau /2 .
\end{align}
With this estimate, it is not hard to check that there exist constants $\wt c_\tau, \wt C_\tau>0$ depending only on $\tau$ such that the following estimates hold: for all $z$, $\delta_1$ and $\delta_2$ such that $|z|\le \wt c_\tau$, $|\delta_1|  \le \wt c_\tau$ and $|\delta_2|  \le \wt c_\tau$,
\be\label{dust}
\left|\frac{1}{g_z'(m_{2c}(0))} \right|\le \wt C_\tau, \quad   \left|\frac{g_z(m_{2c}(0)) - g_0(m_{2c}(0))}{g_z'(m_{2c}(0))}\right|  \le \wt C_\tau |z| ,
\ee
and 
\be\label{dust222}
\left|\frac{g_z(m_{2c}(0)+ \delta_1) -  g_z(m_{2c}(0)+\delta_2)-g_z'(m_{2c}(0))(\delta_1-\delta_2) }{g_z'(m_{2c}(0))}\right|  \le \wt C_\tau |\delta_1-\delta_2|^2 .
\ee
Using equations \eqref{dust} and \eqref{dust222}, we find that there exists a sufficiently small constant $c_1>0$ depending only on $\wt C_\tau$, such that 
%$$  h_z: B_{d}  \to B_{d} , \quad B_d:=\{\delta \in \C: |\delta| \le d \},$$
$h_z: B_{d}  \to B_{d}$ is a self-mapping on the ball $B_d:=\{\delta \in \C: |\delta| \le d \}$, as long as $d\le c_1$ and $|z| \le c_1$. 
%for some constant $c_\delta>0$ depending only on $\wt C$ and $\delta$. 
%\be\label{priori_cond}
%\zeta+\| \b g\|_\infty+ |z-\wt z|\le c_r
%\ee
%for some constant $ c_r>0$ depending on $r$. 
Now it suffices to prove that $h_z$ restricted to $B_d $ is a contraction, which then implies that ${\delta}:=\lim_{k\to\infty} { \delta}^{(k)}$ exists and $m_{2c}(0)+\delta(z)$ is a unique solution to equation $g_z(m_{2c}(z))=1$ subject to the condition $\|{\delta}\|_\infty \le d$. 


From the iteration relation \eqref{selfomega2}, using \eqref{dust222} one can readily check that
\be\label{k1k}
{ \delta}^{(k+1)} - { \delta}^{(k)}= h_z({\delta}^{(k)}) - h_z({\delta}^{(k-1)}) \le \wt C_\tau | { \delta}^{(k)}-{ \delta}^{(k-1)}|^2.
\ee
Hence as long as $d$ is chosen to be sufficiently small such that $2d\wt C_\tau \le 1/2$,  
%compared to $\theta_l^{-1}-g(m_{1c})g( m_{2c} )= (1-t_l)\theta_l^{-1}$, we have
%%where $q(\bx)$ denotes a vector with components $q(x_i)$. 
%%Using $|q'(0)| = 0$ and equation \eqref{dust}, we get from equation \eqref{k1k} that
%$$
% \left\|\mathbf e({{} \e}^{(k)}) -\mathbf e({{} \e}^{(k-1)})\right\|_\infty \le C (\|\bx^{k }\|_\infty +\|\bx^{k-1 }\|_\infty)\|{{} \e}^{(k)} - {{} \e}^{(k-1)}\|_\infty
% %\|\bx^{k+1}-\bx^k\|_\infty\le  C_\kappa \left(\zeta+\|\bx^{k }\|_\infty +\|\bx^{k-1 }\|_\infty  \right)\cdot \|\bx^{k }-\bx^{k-1}\|_\infty
%$$
%for some constant $C >0$ depending only on $c_1, c_2$ and $\delta_l$. Thus we can choose a sufficiently small constant $0<r \le \min\{\tau, (2C)^{-1}\}$ such that $Cr \le 1/2$, i.e. 
then $h$ is indeed a contraction mapping on $ B_d$. This proves both the existence and uniqueness of the solution $m_{2c}(z)=m_{2c}(0)+\delta(z)$, if we choose $c_0$ in equation \eqref{prior1} as $c_0=\min\{c_\tau^2/2, c_1, (2\wt C_\tau)^{-1}\}$. After obtaining $m_{2c}(z)$, we can then solve $m_{1c}(z)$ directly using the first equation in \eqref{selfalter}. 

Note that with equation \eqref{dust} and ${\delta}^{(0)}= 0$, we can obtain from equation \eqref{selfomega2} that $ |{ \delta}^{(1)}(z)| \le \wt C|z| .$
With the contraction mapping, we have the bound 
\be\label{endalter}|{ \delta}| \le \sum_{k=0}^\infty |{  \delta}^{(k+1)}-{ \delta}^{(k)}| \le 2\wt C_\tau |z| \ \Rightarrow \ |m_{2c}(z)-m_{2c}(0)|\le 2\wt C_\tau |z| .\ee
%This gives the bound equation \eqref{Lipomega} for $m_{2c}(z)$. 
Then using the first equation in equation \eqref{selfalter}, we immediately obtain the bound  
$$|m_{1c}(z)-m_{1c}(0)| \le C|z|$$ for a large constant $C>0$. 
%which concludes equation \eqref{Lipomega} as long as if $r_1\gtrsim 1$. To deal with the $r_1=\oo(1)$ case, we go back to the first equation in \eqref{selfomega} and treat $m_{1c}(z)$ as the solution to the following equation:
%$$\wt g_z(m_{1c}(z))=1,\quad \wt g_z(M_1):=- M_1 + \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2 M_1}{  z+\lambda_i^2 r_1 M_1 +r_2 m_{2c}(z) }. $$
%(Note that we have found the solution $M_2(z)$, so this is an equation of $M_1$ only.) 
%We can calculate that 
%$$g_z'(m_{2c}(0))= -1 +  \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2 (z + r_2m_{2c}(z))}{  (z+\lambda_i^2 r_1 m_{1c}(0) +r_2 m_{2c}(z))^2 }.$$
%At $z=0$, we have 
%$$ |g_0'(m_{2c}(0))|= \left|1+\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2  r_2 x_3}{  (\lambda_i^2 r_1 X^{(2)} + r_2 x_3)^2 }\right|\ge 1,$$
%where $X^{(2)}$ and $x_3$ satisfy equation \eqref{a23}. Thus under equation \eqref{prior1} we have $|g_z'(m_{2c}(0))|\sim 1$ as long as $c_0$ is taken sufficiently small. 
%Then with a similar argument as above (i.e. the proof between equation \eqref{selfalter} and equation \eqref{endalter}), we can conclude $|m_{2c}(z)-m_{2c}(0)|=\OO(|z|)$, which further concludes equation \eqref{Lipomega} together with equation \eqref{endalter}. 
%$c_0$ is taken sufficiently small. 
We omit the details. %This concludes the proof of Lemma \ref{lem_mbehaviorw}.
\end{proof}

As a byproduct of the above contraction mapping argument, we also obtain the following stability result that will be used in the proof of Theorem \ref{LEM_SMALL}. Roughly speaking, it states that if two analytic functions $m_1(z)$ and $m_2(z)$ satisfy the self-consistent equation \eqref{selfomega} approximately up to some small errors, then $m_1(z)$ and $m_2(z)$ will be close to the solutions $m_{1c}(z)$ and $m_{2c}(z)$. 
%Later this result will be applied to equation \eqref{approximate m1m2} to show that the averaged resolvents $m_1(z)$ and $m_2(z)$ indeed converge to $M_1(z)$ and $M_2(z)$, respectively.


\begin{lemma} \label{lem_stabw}
There exist constants $c_0, C_0>0$ depending only on $\tau$ in Assumption \ref{assm_big1} and equation \eqref{a23} such that the system of self-consistent equations in \eqref{selfomega} is stable in the following sense. Suppose $|z|\le c_0$, and $m_{1c} $ and $m_{2c} $ are analytic functions of $z$ such that
\be  \label{prior12}
|m_{1}(z) - m_{1c}(0)| + |m_{2}(z) - m_{2c}(0)|\le c_0.
\ee
Moreover, assume that $(m_1,m_2)$ satisfies the system of equations
\begin{equation}\label{selfomegaerror}
\begin{split}
&\frac{1}{m_{1}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2r_1m_{1} +r_2 m_{2}  } =\cal E_1,\\ &\frac{1}{m_{2}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1m_{1} +  r_2m_{2}  }=\cal E_2,
\end{split}
\ee
for some (deterministic or random) errors such that $  |\mathcal E_1| +  |\mathcal E_2| \le \theta(z),$ where $\theta(z)$ is a deterministic function of $z$ satisfying that $\theta(z) \le (\log n)^{-1}.$ Then we have 
 \begin{equation}
  \left|m_1(z)-m_{1c}(z)\right| +  \left|m_2(z)-m_{2c}(z)\right|\le C_0\theta(z).\label{Stability1}
\end{equation}
\end{lemma}



\begin{proof}%[Proof of Lemma \ref{lem_stabw}]
Under condition \eqref{prior12}, we can obtain equation \eqref{selfalter} approximately up to some small errors:
\be\label{selfalter2}r_1 m_{1}=-(1-\gamma_n) - r_2m_{2} - z\left(  {m_{2}^{-1}}+1\right) + \wt{\cal E}_1(z),\quad g_z(m_{2}(z))=1+ \wt{\cal E}_2(z),\ee
where the errors satisfy that $|\wt{\cal E}_1(z)|+ |\wt{\cal E}_2(z)|=\OO(\theta(z))$. Then we subtract equation \eqref{selfalter} from equation \eqref{selfalter2}, and consider the contraction principle for the function $\delta (z):= m_{2}(z) - m_{2c}(z)$.  The rest of the proof is exactly the same as the one for Lemma \ref{lem_mbehaviorw}, so we omit the details.
\end{proof}



%It was shown in \cite{Separable} that if $d_N \to d \in (0,\infty)$ and $\pi_A^{(n)}$, $\pi_B^{(n)}$ converge to certain probability distributions, then almost surely $\rho^{(n)}$ converges to a deterministic distributions $ \rho_{\infty}$. We now describe it through the Stieltjes transform
%$$m_{\infty}(z):=\int_{\mathbb R} \frac{\rho_{\infty}(\dd x)}{x-z}, \quad z \in \mathbb C_+.$$
%For any finite $N$ and $z\in \mathbb C_+$, we define $(m^{(n)}_{1c}(z),m^{(n)}_{2c}(z))\in \mathbb C_+^2$ as the unique solution to the system of self-consistent equations
%\begin{equation}\label{separa_m12}
%{m^{(n)}_{1c}(z)} = d_N \int\frac{x}{-z\left[1+xm^{(n)}_{2c}(z) \right]} \pi_A^{(n)}(\dd x), \quad  {m^{(n)}_{2c}(z)} =  \int\frac{x}{-z\left[1+xm^{(n)}_{1c}(z) \right]} \pi_B^{(n)}(\dd x).
%\end{equation}
%Then we define
%\begin{equation}\label{def_mc}
%m_c(z)\equiv m_c^{(n)}(z):= \int\frac{1}{-z\left[1+xm^{(n)}_{2c}(z) \right]} \pi_A^{(n)}(\dd x).
%\end{equation}
%It is easy to verify that $m_c^{(n)}(z)\in \mathbb C_+$ for $z\in \mathbb C_+$. Letting $\eta \downarrow 0$, we can obtain a probability measure $\rho_{c}^{(n)}$ with the inverse formula
%\begin{equation}\label{ST_inverse}
%\rho_{c}^{(n)}(E) = \lim_{\eta\downarrow 0} \frac{1}{\pi}\Im\, m^{(n)}_{c}(E+\ii \eta).
%\end{equation}
%If $d_N \to d \in (0,\infty)$ and $\pi_A^{(n)}$, $\pi_B^{(n)}$ converge to certain probability distributions, then $m_c^{(n)}$ also converges and we define
%$$m_{\infty}(z):=\lim_{N\to \infty} m_c^{(n)}(z), \ \ z \in \mathbb C_+.$$
%Letting $\eta \downarrow 0$, we can recover the asymptotic eigenvalue density $ \rho_{\infty}$ with
%\begin{equation}\label{ST_inverse}
%\rho_{\infty}(E) = \lim_{\eta\downarrow 0} \frac{1}{\pi}\Im\, m_{\infty}(E+\ii \eta).
%\end{equation}
%It is also easy to see that $\rho_\infty$ is the weak limit of $\rho_{c}^{(n)}$. 
%%The measure $ \rho_{\infty}$ is sometimes called the {\it{multiplicative free convolution}} of $\pi_A$, $\pi_B$ with the Marchenko-Pastur (MP) law (see e.g. \cite{AGZ,VDN}), i.e. $\pi_A \boxtimes \rho_{MP} \boxtimes \pi_B$, where $\rho_{MP}$ denotes the MP distribution. 
%
%The above definitions of $m_c^{(n)}$, $\rho_c^{(n)}$, $m_\infty$ and $\rho_\infty$ make sense due to the following theorem. Throughout the rest of this paper, we often omit the super-indices $(n)$ and $(N)$ from our notations. 
%
%%Throughout the rest of this paper, we will often omit the super-index $n$ or $N$ from our notations. 
%
%\begin{theorem} [Existence, uniqueness, and continuous density]
%For any $z\in \mathbb C_+$, there exists a unique solution $(m_{1c},m_{2c})\in \mathbb C_+^2$ to the systems of equations in (\ref{separa_m12}). The function $m_c$ in (\ref{def_mc}) is the Stieltjes transform of a probability measure $\mu_c$ supported on $\mathbb R_+$. Moreover, $\mu_c$ has a continuous derivative $\rho_c(x)$ on $(0,\infty)$, which is defined by equation \eqref{ST_inverse}.
%\end{theorem}
%\begin{proof}
%See {\cite[Theorem 1.2.1]{Zhang_thesis}}, {\cite[Theorem 2.4]{Hachem2007}} and {\cite[Theorem 3.1]{Separable_solution}}.
%\end{proof}
%
%
%
% 
%Now we go back to study the equations in (\ref{separa_m12}). If we define the function
%% Corresponding to the equation in (\ref{separa_m12}), we define the function 
%\begin{equation}\label{separable_MP}
%f(z,\al):=- \al + \int\frac{x}{-z+xd_N \int\frac{t}{1+t\al} \pi_A(\dd t)} \pi_B(\dd x) ,
%\end{equation}
%then $m_{2c}(z)$ can be characterized as the unique solution to the equation $f(z,\al)=0$ of $\al$ with $\Im \, \al> 0$, and $m_{1c}(z)$ is defined using the first equation in equation \eqref{separa_m12}.
%%as $$m_{1c}(z) = d_N \int\frac{x}{-z\left[1+xm_{2c}(z) \right]} \pi_A(\dd x).$$
%Moreover, $m_{1,2c}(z)$ are the Stieltjes transforms of densities $\rho_{1,2c}$:
%$$\rho_{1,2c}(E) = \lim_{\eta\downarrow 0} \frac{1}{\pi}\Im\, m_{1,2c}(E+\ii \eta).$$
%Then we have the following result.
%
%\begin{lemma}\label{lambdar}%[Support of the deformed MP law]
%The densities $\rho_{c}$ and $\rho_{1,2c}$ all have the same support on $(0,\infty)$, which is a union of intervals: %connected components:
%\begin{equation}\label{support_rho1c}
%{\rm{supp}} \, \rho_{c} \cap (0,\infty) ={\rm{supp}} \, \rho_{1,2c} \cap (0,\infty) = \bigcup_{k=1}^p [a_{2k}, a_{2k-1}] \cap (0,\infty),
%\end{equation}
%where $p\in \mathbb N$ depends only on $\pi_{A,B}$. Moreover, $(x,\al)=(a_k, m_{2c}(a_k))$ are the real solutions to the equations
%\begin{equation}
%f(x,\al)=0, \ \ \text{and} \ \ \frac{\partial f}{\partial \al}(x,\al) = 0. \label{equationEm2}
%\end{equation}
%Moreover, we have $m_{1c}(a_1) \in (-\wt \sigma_1^{-1}, 0)$ and $m_{2c}(a_1) \in (-\sigma_1^{-1}, 0)$. %Finally, under (\ref{assm2}) and (\ref{assm3}), we have $a_1 \le C$ for some constant $C>0$. 
%\end{lemma}
%\begin{proof}
%See Section 3 of \cite{Separable_solution}.
%\end{proof}
%
% %It is easy to observe that $b_k=m_{2c}(a_k)$ according to the definition of $f$. 
% We shall call $a_k$ the spectral edges. In particular, we will focus on the rightmost edge $\lambda_+ := a_1$. 
%%\begin{equation}\label{right_edge}
%%\lambda_+ := a_1 
%%\end{equation}
%%throughout the following.
%Now we make the following assumption: there exists a constant $\tau>0$ such that %{\color{red}(can we remove one of the conditions?)}
%\begin{equation}\label{assm_gap}
%1 + m_{1c}(\lambda_+) \wt \sigma_1 \ge \tau, \quad 1 + m_{2c}(\lambda_+) \sigma_1\ge \tau. %\quad \frac{\partial^2 f}{\partial m^2}\left(\lambda_+,m_{2c}(\lambda_+)\right) \ge \tau.
%\end{equation}
%This assumption guarantees a regular square-root behavior of the spectral densities $\rho_{1,2c}$ near $\lambda_+$ as shown by the following lemma.
%%(see Lemma \ref{lem_mbehavior} below), which is used in proving the local deformed MP law at the soft edge.
%
%\begin{lemma} \label{lambdar_sqrt}
%Under the assumptions equation \eqref{assm2}, equation \eqref{assm3} and equation \eqref{assm_gap}, there exist constants $a_{1,2}>0$ such that
%\be\label{sqroot3}
%\rho_{1,2c}(\lambda_+ - x) = a_{1,2} x^{1/2} + \OO(x), \quad x\downarrow 0,
%\ee
%and
%\be\label{sqroot4}
%\quad m_{1,2c}(z) = m_{1,2c}(\lambda_+) + \pi a_{1,2}(z-\lambda_+)^{1/2} + \OO(|z-\lambda_+|), \quad z\to \lambda_+ , \ \ \im z\ge 0.
%\ee
%The estimates equation \eqref{sqroot3} and equation \eqref{sqroot4} also hold for $\rho_c$ and $m_c$ with a different constant. 
%\end{lemma}
% 




%In particular, we shall denote
%\begin{equation}
%S(c_0,C_0,-\infty):= \left\{z=E+ \ii \eta: \lambda_r - c_0 \leq E \leq C_0 \lambda_r, 0 \leq \eta \leq 1 \right\}.
%\end{equation}
%We define the distance to the rightmost edge as
%\begin{equation}
%\kappa \equiv \kappa_E := \vert E -\lambda_r\vert , \ \ \text{for } z= E+\ii \eta.\label{KAPPA}
%\end{equation}

%Then we have the following lemma, which summarizes some basic properties of $m_{2c}$ and $\rho_{2c}$.
%%{\color{red}Discuss about the case equation \eqref{assm3extra}. }
%
%\begin{lemma}\label{lem_mbehavior}
%Suppose the assumptions equation \eqref{assm2}, equation \eqref{assm3} and equation \eqref{assm_gap} hold. Then
%there exists sufficiently small constant $\wt  c>0$ such that the following estimates hold:
%\begin{itemize}
%\item[(1)]
%\begin{equation}
%\rho_{1,2c}(x) \sim \sqrt{\lambda_r-x}, \quad \ \ \text{ for } x \in \left[\lambda_r - 2\wt  c,\lambda_r \right];\label{SQUAREROOT}
%\end{equation}
%\item[(2)] for $z =E+\ii \eta\in S(\wt  c,C_0,-\infty)$, 
%\begin{equation}\label{Immc}
%\vert m_{1,2c}(z) \vert \sim 1,  \quad  \im m_{1,2c}(z) \sim \begin{cases}
%    {\eta}/{\sqrt{\kappa+\eta}}, & \text{ if } E\geq \lambda_r \\
%    \sqrt{\kappa+\eta}, & \text{ if } E \le \lambda_r\\
%  \end{cases};
%\end{equation}
%%for $z = E+\ii \eta\in S(\wt  c,C_0,\omega)$;
%\item[(3)] there exists constant $\tau'>0$ such that
%\begin{equation}\label{Piii}
%\min_{\mu\in \mathcal I_2} \vert 1 + m_{1c}(z)\wt  \sigma_\mu \vert \ge \tau', \quad \min_{i\in \mathcal I_1} \vert 1 + m_{2c}(z)\sigma_i  \vert \ge \tau',
%\end{equation}
%for any $z \in S(\wt  c,C_0,-\infty)$.
%\end{itemize}
%The estimates equation \eqref{SQUAREROOT} and equation \eqref{Immc} also hold for $\rho_c$ and $m_c$. 
%\end{lemma}
%%and
%%\begin{equation}
%%  \operatorname{Im} m_{2c}(z) \sim \begin{cases}
%%    {\eta}/{\sqrt{\kappa+\eta}}, & E\geq \lambda_r \\
%%    \sqrt{\kappa+\eta}, & E \le \lambda_r\\
%%  \end{cases},  \label{SQUAREROOTBEHAVIOR}
%%\end{equation}
%\begin{proof}
%The estimate equation \eqref{SQUAREROOT} is already given by Lemma \ref{lambdar_sqrt}. The estimate equation \eqref{Immc} can be proved easily with equation \eqref{sqroot4}. 
%%The estimate equation \eqref{SQUAREROOT} for $\rho_c$ is already given by Lemma \ref{lambdar_sqrt}. The estimate equation \eqref{Immc} for $m_c$ follows from  equation \eqref{def_mc}, equation \eqref{Piii}, and equation \eqref{Immc} for $m_{2c}$.
%It remains to prove equation \eqref{Piii}. By assumption equation \eqref{assm_gap} and the fact $m_{2c}(\lambda_r) \in (-\sigma_1^{-1}, 0)$, we have
%$$\left| 1+ m_{2c}(\lambda_r) \sigma_i \right| \ge \tau,  \quad i\in \mathcal I_1.$$
%With equation \eqref{sqroot4}, we see that if $\kappa+\eta \le 2c_0$ for some sufficiently small constant $c_0>0$, then
%$$\left| 1+ m_{2c}(z)\sigma_k \right| \ge \tau/2.$$
%Then we consider the case with $E \ge \lambda_r + c_0$ and $\eta \le c_1$ for some constant $c_1>0$. In fact, for $\eta=0$ and $E> \lambda_r$, $m_{2c}(E)$ is real and it is easy to verify that $m_{2c}'(E)\ge 0$ using 
%the Stieltjes transform formula 
%%(\ref{Stj_app}). Applying (\ref{SQUAREROOT}) to the Stieltjes transform
%\begin{equation}\label{Stj_app}
%m_{2c}(z):=\int_{\mathbb R} \frac{\rho_{2c}(dx)}{x-z},
%\end{equation}
%Hence we have
%$$ 1+ \sigma_i m_{2c}(E)  \ge 1+ \sigma_i m_{2c}(\lambda_r ) \ge \tau, \ \ \text{ for }E\ge \lambda_r + c_0.$$
%Using (\ref{Stj_app}) again, we can get that 
%$$\left|\frac{\dd m_{2c}(z)}{ \dd z }\right| \le c_0^{-2}, \ \ \text{for } E\ge \lambda_r + c_0.$$ 
%Thus if $c_1$ is sufficiently small, we have
%$$\left| 1+ \sigma_k m_{2c}(E+\ii\eta) \right| \ge  \tau/2$$
%for $E\ge \lambda_r + c_0$ and $\eta \le c_1$. Finally, it remains to consider the case with $\eta \ge c_1$. Note that we have $|m_{2c}(z)| \sim \Im \, m_{2c}(z) \sim 1$ by (\ref{Immc}). If $\sigma_k \le \left|2m_{2c}(z)\right|^{-1}$, then $\left| 1+ \sigma_k m_{2c}(z) \right| \ge 1/2$. Otherwise, we have %Together with (\ref{Immc}), we get that
%$$\left| 1+ \sigma_k m_{2c}(z) \right| \ge \sigma_k \Im\, m_{2c}(z) \ge \frac{\Im\, m_{2c}(z)}{2 |m_{2c}(z)|}\gtrsim 1 .$$
%%for some constant $\tau'>0$. 
%In sum, we have proved the second estimate in equation \eqref{Piii}. The first estimate can be proved in a similar way. 
%\end{proof}

%Then we have the following estimates for $m_{2c}$:
%%\begin{lemma}[Lemma ]\label{lem_mbehavior}
%%and $\delta_N \le (\log N)^{-1}$, 
%%we have
%\begin{equation}\label{Immc}
%\vert m_{2c}(z) \vert \sim 1, \ \  \Im \, m_{2c}(z) \sim \begin{cases}
%    {\eta}/{\sqrt{\kappa+\eta}}, & \text{ if } E \notin \text{supp}\, \rho_{2c}\\
%    \sqrt{\kappa+\eta}, & \text{ if } E \in \text{supp}\, \rho_{2c}\\
%  \end{cases},
%\end{equation}
%%and 
%\begin{equation}\label{Piii}
%\max_{i\in \mathcal I_1} \vert (1 + m_{2c}(z)\sigma_i)^{-1} \vert = \OO(1).
%\end{equation}
%\end{lemma}

%\begin{remark}
%Recall that $a_k$ are the edges of the spectral density $\rho_{2c}$; see (\ref{support_rho1c}). Hence $\rho_{2c}(a_k)=0$, and we must have $a_k < \lambda_r - 2\wt  c$ for $2\le k \le 2p$. In particular, $S(c_0,C_0,\e)$ is away from all the other edges if we choose $c_0 \le \wt  c$. 
%\end{remark}

%\begin{definition} [Classical locations of eigenvalues]
%The classical location $\gamma_j$ of the $j$-th eigenvalue of $\mathcal Q_1$ is defined as
%\begin{equation}\label{gammaj}
%\gamma_j:=\sup_{x}\left\{\int_{x}^{+\infty} \rho_{c}(x)dx > \frac{j-1}{n}\right\}.
%\end{equation}
%In particular, we have $\gamma_1 = \lambda_r$.
%\end{definition}
%%\begin{remark}
%%If $\gamma_j$ lies in the bulk of $\rho_{2c}$, then by the positivity of $\rho_{2c}$ we can define $\gamma_j$ through the equation
%%\begin{equation*}
%%\int_{\gamma_j}^{+\infty} \rho_{2c}(x)dx = \frac{j-1}{N}.
%%\end{equation*}
%%We can also define the classical location of the $j$-th eigenvalue of $\mathcal Q_1$ by changing $\rho_{2c}$ to $\rho_{1c}$ and $(j-1)/{N}$ to $(j-1)/{M}$ in (\ref{gammaj}). By (\ref{def21}), this gives the same location as $\gamma_j$ for $j\le n\wedge N$.
%%\end{remark}
%
%In the rest of this section, we present some results that will be used in the proof of Theorem \ref{main_thm}. Their proofs will be given in subsequent sections. For any matrix $X$ satisfying Assumption \ref{assm_big1} and the tail condition (\ref{tail_cond}), we can construct a matrix $X^s$ that approximates $X$ with probability $1-\oo(1)$, and satisfies Assumption \ref{assm_big1}, the bounded support condition (\ref{eq_support}) with $q\le N^{-\phi}$ for some small constant $\phi>0$, and
%%${\bf x_3 }$:
%\begin{equation}\label{conditionA2}
%\mathbb{E}\vert  x^s_{ij} \vert^3 =\OO(N^{-{3}/{2}}), \quad   \mathbb{E} \vert  x^s_{ij} \vert^4  =\OO_\prec (N^{-2});
%\end{equation}
%see Section \ref{sec_cutoff} for the details. We will need the following local laws, eigenvalues rigidity, eigenvector delocalization, and edge universality results for separable covariance matrices with $X^s$.
%%with support $q\le N^{-\phi}$ and satisfying the condition (\ref{conditionA2}).
%
%%\cor ---------------------------------- (revise starting from here) ------------------ \nc
%
%We define the deterministic limit $\Pi$ of the resolvent $G$ in (\ref{eqn_defG}) as
%\begin{equation}\label{defn_pi}
%\Pi (z): = \left( {\begin{array}{*{20}c}
%   { -\left(1+m_{2c}(z)\Sigma \right)^{-1} } & 0  \\
%   0 & { - z^{-1} (1+m_{1c}(z)\wt  \Sigma )^{-1} }  \\
%\end{array}} \right) .
%\end{equation}
%Note that we have
%\be\label{mcPi}
%\frac1{nz}\sum_{i\in \mathcal I_1} \Pi_{ii} =m_c. 
%\ee
%Define the control parameters
%\begin{equation}\label{eq_defpsi}
%\Psi (z):= \sqrt {\frac{\Im \, m_{2c}(z)}{{N\eta }} } + \frac{1}{N\eta}.
%\end{equation}
%Note that by (\ref{Immc}) and (\ref{Piii}), we have
%\begin{equation}\label{psi12}
%\|\Pi\|=\OO(1), \quad \Psi \gtrsim N^{-1/2} , \quad \Psi^2 \lesssim (N\eta)^{-1}, \quad \Psi(z) \sim  \sqrt {\frac{\Im \, m_{1c}(z)}{{N\eta }} } + \frac{1}{N\eta},
%\end{equation}
%%and 
%%\begin{equation}\labelpsi12
%%\Psi(z) \sim  \sqrt {\frac{\Im \, m_{1c}(z)}{{N\eta }} } + \frac{1}{N\eta},
%%\end{equation}
%for $z\in S(\wt  c, C_0,-\infty)$. Now we are ready to state the local laws for $G(X,z)$. For the purpose of proving Theorem \ref{main_thm}, we shall relax the condition equation \eqref{assm_3rdmoment} a little bit. 

%\begin{definition}[Deterministic limit of $G$]
%We define the deterministic limit $\Pi$ of the Green function $G$ in (\ref{green2}) as
%\begin{equation}
%\Pi (z): = \left( {\begin{array}{*{20}c}
%   { -\left(1+m_{2c}(z)\Sigma \right)^{-1} } & 0  \\
%   0 & { - z^{-1} (1+m_{1c}(z)\wt  \Sigma )^{-1} }  \\
%\end{array}} \right) .
%\end{equation}
%%where $\Sigma$ is defined in (\ref{def_Sigma}).
%\end{definition}



%\begin{theorem} [Local laws]\label{LEM_SMALL} %[Results on covariance matrices with small support]
%
%Suppose Assumption \ref{assm_big1} and equation \eqref{assm_gap} hold. Suppose $X$ satisfies the bounded support condition (\ref{eq_support}) with $q\le N^{-\phi}$ for some constant $\phi>0$. Furthermore, suppose $X$ satisfies equation \eqref{conditionA2} and
%\be\label{assm_3moment}
%%\mathbb E x_{ij}^3=0,  
%\left|\mathbb E x_{ij}^3\right|\le b_N N^{-2}, \quad 1\le i \le n,\ \  1\le j \le N,
%\ee
%%and
%%\begin{equation}\label{conditionA4}
%%\mathbb{E}\vert x_{ij} \vert^3 \leq C N^{-{3}/{2}}, \quad \mathbb{E} \vert x_{ij} \vert^4  \prec N^{-2},  \quad 1\le i \le n, 1\le j \le N. %\mathbb{E}\vert x_{ij} \vert^3 \prec N^{-{3}/{2}}, \quad  
%%\end{equation}
%where $b_N$ is an $N$-dependent deterministic parameter satisfying $1 \leq b_N \le N^{1/2}$. Fix $C_0>1$ and let $c_0>0$ be a sufficiently small constant. Given any $\epsilon>0$, we define the domain
%\be \label{tildeS}
%\wt  S(c_0,C_0,\e):= S(c_0,C_0,\epsilon) \cap \left\{z = E+ \ii \eta: b_N \left(\Psi^2(z) + \frac{q}{N\eta}\right)\le N^{-\e}\right\}.
%\ee
%Then for any fixed $\e>0$, the following estimates hold. 
%\begin{itemize}
%\item[(1)] {\bf Anisotropic local law}: For any $z\in \wt  S(c_0,C_0,\epsilon)$ and deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb C^{\mathcal I}$,
%\begin{equation}\label{aniso_law}
%\left| \langle \mathbf u, G(X,z) \mathbf v\rangle - \langle \mathbf u, \Pi (z)\mathbf v\rangle \right| \prec q+ \Psi(z).
%\end{equation}
%
%\item[(2)] {\bf Averaged local law}: For any $z \in \wt  S(c_0, C_0, \epsilon)$,  we have
%\begin{equation}
% \vert m(z)-M(z) \vert \prec q^2 + (N \eta)^{-1}. \label{aver_in1} %+ q^2 
%\end{equation}
%where $m$ is defined in equation \eqref{defn_m}. Moreover, outside of the spectrum we have the following stronger estimate
%\begin{equation}\label{aver_out1}
% | m(z)-M(z)|\prec q^2  + \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%uniformly in $z\in \wt  S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$, where $\kappa$ is defined in equation \eqref{KAPPA}. 
%\end{itemize}
%The above estimates are uniform in the spectral parameter $z$ and any set of deterministic vectors of cardinality $N^{\OO(1)}$. If $A$ or $B$ is diagonal, then equation \eqref{aniso_law}-equation \eqref{aver_out1} hold for $z\in S(c_0,C_0,\epsilon) $.
%\end{theorem}
%
%The following theorem gives that anisotropic local law for $\cal R(z,0)$.





\iffalse
This section is organized as follows. In Section \ref{sec tools}, we collect some basic estimates and resolvent identities that will be used in the proof of Theorem \ref{LEM_SMALL}  and Proposition \ref{prop_diagonal}. Then in Section \ref{sec entry} we give the proof of Proposition \ref{prop_diagonal}, which concludes Theorem \ref{LEM_SMALL}  when $Z^{(1)}$ and $Z^{(2)}$ have i.i.d. Gaussian entries. In Section \ref{sec_comparison}, we describe how to extend the result in Theorem \ref{LEM_SMALL}  from the Gaussian case to the case where the entries of $Z^{(1)}$ and $Z^{(2)}$ are generally distributed. Finally, in Section \ref{sec contract}, we give the proof of Lemma \ref{lem_mbehaviorw} and Lemma \ref{lem_stabw}. In the proof, we always denote the spectral parameter by $z=E+\ii\eta$. 
\fi




\subsection{Multivariate Gaussian matrices: Entrywise local law}\label{sec entry}

 The main difficulty in the proof is due to the fact that the entries of $Z^{(1)} U\Lambda$ and ${Z^{(2)}V}$ are not independent. However, notice that if the entries of $Z^{(1)}$ and $Z^{(2)}$ are i.i.d. Gaussian, then by the rotational invariance of the multivariate Gaussian distribution, %$Z^{(1)} U$ and $Z^{(2)}V$ still obey the Gaussian distribution.
we have
\be\label{eq in Gauss}  Z^{(1)}  U\Lambda \stackrel{d}{=}  Z^{(1)} \Lambda, \quad  Z^{(2)}  V \stackrel{d}{=} (Z^{(2)})  ,\ee
where ``$\stackrel{d}{=}$" means ``equal in distribution". In this case, the problem is reduced to proving the anisotropic local law for $G(z)$ with $U=\id$ and $V=\id$, such that the entries of $ Z^{(1)} \Lambda  $ and ${Z^{(2)}}$ are independent.
%$Y_1$ and $Y_2$ are independent. 
In this case, we use the standard resolvent methods in \cite{isotropic,yang2019spiked,PY} to prove the following result. Note that if the entries of $ Z^{(1)}$ and $ Z^{(2)}$ are Gaussian, then we have $\varphi=\infty$ in \eqref{conditionA2}, which gives $Q=n^{2/\varphi}=1$.
%To go from the Gaussian case to the general $X$ case, we will adopt a continuous self-consistent comparison argument developed in \cite{Anisotropic}. 

\iffalse
First, we consider the special case where $Z^{(1)}$ and $Z^{(2)}$ are both multivariate Gaussian random matrices.
By rotational invariance, we have that $Z^{(1)} U$ and $Z^{(2)} V$ are still multivariate Gaussian random matrices.
Next, we use the Schur complement formula to deal with the resolvent $G(z)$.
We show that $G(z)$'s diagonal entries satisfy a set of self-consistent equations in the limit, leading to equations in \eqref{selfomega_a}.
On the other hand, $G(z)$'s off-diagonal entries are approximately zero using standard concentration bounds.
Finally, we extend our result to general random matrices under the finite $\varphi$-th moment condition.
We prove an anisotropic local law using recent developments in random matrix theory \cite{erdos2017dynamical,Anisotropic}.
The proof of Theorem \ref{thm_main_RMT} is shown in Appendix \ref{appendix RMT}. %Appendix \ref{appendix RMT}.
\fi

%As discussed above, we first prove Theorem \ref{LEM_SMALL} for the case $U=\id$ and $V=\id$, which will imply the local laws in the Gaussian $Z^{(1)}$ and $Z^{(2)}$ case. Thus 

%Now consider the case $U=\id$ and $V=\id$, where we need to deal with the following resolvent:
% \begin{equation}\label{eqn_comparison1}
% G_0 (z):= \left( {\begin{array}{*{20}c}
%   { -z \id_{p}} & \Lambda (Z^{(1)})^{\top} & (Z^{(2)})^\top  \\
%   {Z^{(1)} \Lambda  } & {-\id_{n_1}} & 0 \\
%   {Z^{(2)}} & 0 & -\id_{n_2}
%   \end{array}} \right) ^{-1} , \quad z\in \mathbb C_+ .
% \end{equation}
%separable covariance matrices of the form $\Sig^{1/2} X \wt  \Sig X^\top \Sig^{1/2}$, which will imply the local laws in the Gaussian $X$ case. Thus in this section, we deal with the following resolvent:
%\begin{equation}\label{eqn_comparison1}
%G(X,z) {=}  \left[\left( {\begin{array}{*{20}c}
%   { 0 } & \Sig^{1/2} X \wt  \Sig^{1/2}   \\
%   {\wt \Sig^{1/2}X^\top\Sig^{1/2} } & {0}  \\
%   \end{array}} \right)-\left( {\begin{array}{*{20}c}
%   { I_{n\times n}} & 0  \\
%   0 & { zI_{N\times N}}  \\
%\end{array}} \right)\right]^{-1}
%\end{equation}
%with $X$ satisfying equation \eqref{eq_support} with $q=N^{-1/2}$.
%we choose the entries of $X$ to be $i.i.d.$ Gaussian due to the following reason. If $X=X^{\text{Gauss}}$ is Gaussian, then $U^\top X^{\text{Gauss}} V \stackrel{d}{=} X^{\text{Gauss}}$. Thus for the resolvent $G$ defined in in (\ref{eqn_defG}), we have
%\begin{equation}\label{eqn_comparison1}
%G(X,z) \stackrel{d}{=}  \left[\left( {\begin{array}{*{20}c}
%   { 0 } & \Sig^{1/2} X \wt  \Sig^{1/2}   \\
%   {\wt \Sig^{1/2}X^\top\Sig^{1/2} } & {0}  \\
%   \end{array}} \right)-\left( {\begin{array}{*{20}c}
%   { I_{n\times n}} & 0  \\
%   0 & { zI_{N\times N}}  \\
%\end{array}} \right)\right]^{-1}
%\end{equation}
%provided that $X$ is Gaussian. In particular, the entries of $\Sig^{1/2} X^{\text{Gauss}}\wt  \Sig^{1/2}$ are independent and satisfies the bounded support condition equation \eqref{eq_support} with $q=N^{-1/2}$, which make the direct proof of Theorem \ref{LEM_SMALL} possible using the methods in \cite{isotropic}. 
%Then we claim the following proposition for $G_0(z)$.
\begin{proposition}\label{prop_diagonal}
 In the setting of Theorem \ref{LEM_SMALL} , assume further that the entries of $ Z^{(1)}$ and $ Z^{(2)}$ are i.i.d. Gaussian random variables. %which satisfy the bounded support condition \eqref{eq_support} with $q= n^{-1/2}$. 
 Suppose $U$ and $V$ are identity. Then the estimates \eqref{aver_in}, \eqref{aver_in1} and \eqref{aniso_law} hold uniformly in $z\in \mathbf D$ for $Q= 1$.
%Suppose $X$ satisfies the bounded support condition (\ref{eq_support}) with $q= N^{-1/2}$. Suppose $A$ and $B$ are diagonal, i.e. $U=I_{n\times n}$ and $V=I_{N\times N}$. Fix $C_0>1$ and let $c_0>0$ be a sufficiently small constant. Then for any fixed $\epsilon>0$, the following estimates hold. %there exist constants $C_1>0$ and $\xi_1 \ge 3$ such that the following events hold with $\xi_1$-overwhelming probability:
%\begin{itemize}
%\item[(1)] {\bf Anisotropic local law}:  For any $z\in S(c_0,C_0,\epsilon)$ and deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb C^{\mathcal I}$,
%\begin{equation}\label{aniso_diagonal}
%\left| \langle \mathbf u, G(X,z) \mathbf v\rangle - \langle \mathbf u, \Pi (z)\mathbf v\rangle \right| \prec \Psi(z).
%\end{equation}
%
%\item[(2)] {\bf Averaged local law}: We have %{\bf Local deformed MP law}:
%\begin{equation}\label{aver_diagonal}
% | m(z)-M(z)|\prec ({N\eta})^{-1}
%\end{equation}
%for any $z\in S(c_0,C_0,\epsilon)$, and 
%%Moreover, outside of the spectrum we have the following stronger averaged local law (recall equation \eqref{KAPPA})
%\begin{equation}\label{aver_out}
% | m(z)-M(z)|\prec \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%for any $z\in S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$. 
%\end{itemize}
%Both of the above estimates are uniform in the spectral parameter $z$ and the deterministic vectors $\mathbf u, \mathbf v$.
\end{proposition}

%If the entries of $Z^{(1)}$ and $Z^{(2)}$ are i.i.d. Gaussian, then by Gaussian concentration $Z^{(1)}$ and $Z^{(2)}$ have support $q=n^{-1/2}$. Hence combing equation \eqref{eq in Gauss} with Proposition \ref{prop_diagonal}, we obtain that Theorem \ref{LEM_SMALL} holds when $Z^{(1)}$ and $Z^{(2)}$ are Gaussian. 
%The proof Proposition \ref{prop_diagonal} is similar to the previous proof of the local laws, such as \cite{isotropic, DY, Anisotropic, yang2019spiked}. Thus instead of giving all the details, we only describe briefly the proof. In particular, we shall focus on the key self-consistent equation argument, which is (almost) the only part that departs significantly from the previous proof in e.g. \cite{isotropic}. 
%Next we briefly describe how to extend Theorem \ref{LEM_SMALL} from the Gaussian case to the case with generally distributed $Z^{(1)}$ and $Z^{(2)}$. We will adopt a continuous comparison argument developed in \cite{Anisotropic}. Since the proof is almost the same as the ones in Sections 7 and 8 of \cite{Anisotropic} and Section 6 of \cite{yang2019spiked}, we will not write down all the details.



%Now it remains to prove Proposition \ref{prop_diagonal}, 
The proof of Proposition \ref{prop_diagonal} is based on the following entrywise local law. %, Lemma \ref{prop_entry}. 
\begin{lemma}\label{prop_entry}
In the setting of Proposition \ref{prop_diagonal}, %Fix $C_0>0$ and let $c_0>0$ be a sufficiently small constant. 
the averaged local laws \eqref{aver_in} and \eqref{aver_in1}, and the following entrywise local law hold uniformly in $z\in \mathbf D$ for $Q= 1$: %there exist constants 
\begin{equation}\label{entry_diagonal}
\max_{\fa,\fb\in \cal I}\left| G_{\fa\fb}(z)  - \Gi_{\fa\fb} (z) \right| \prec n^{-1/2}.
\end{equation} 
\end{lemma}


With Lemma \ref{prop_entry}, we can complete the proof of %the anisotropic local law  \eqref{aniso_law} in 
Proposition \ref{prop_diagonal}. %using a standard polynomialization method. %as we will explain later. 

\begin{proof}[Proof of Proposition \ref{prop_diagonal}]
With estimate (\ref{entry_diagonal}), one can use the polynomialization method in Section 5 of \cite{isotropic} to get the anisotropic local law (\ref{aniso_law}) with $Q=1$. The proof is exactly the same, except for some minor differences in notations. Hence we omit the details.
\end{proof}

 
The rest of this subsection is devoted to the proof of Lemma \ref{prop_entry}. In the setting of Lemma \ref{prop_entry}, the resolvent $G$ in Definition \ref{defn_resolventH} becomes
 \begin{equation} \label{resolv Gauss1}
   G(z)= \left( {\begin{array}{*{20}c}
   { -z\id_{p\times p} } & n^{-1/2}\Lambda (Z^{(1)})^\top & n^{-1/2} (Z^{(2)})^\top  \\
   {n^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1\times n_1}} & 0 \\
   {n^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2\times n_2}}
   \end{array}} \right)^{-1}.
 \end{equation}
To deal with the matrix inverse, we introduce the following resolvent minors of $G(z)$.
\begin{definition}[Resolvent minors]\label{defn_Minor}
	For any $(p+n)\times (p+n)$ matrix $\cal A$ and $\mathfrak c \in \cal I$, the minor of $\cal A$ after removing the $\mathfrak c$-th row and column of $\cal A$ is denoted by $\cal A^{(\mathfrak c)} := [\cal A_{\fa \mathfrak b }:  \fa , \mathfrak b\in \mathcal I\setminus \{\mathfrak c\}]$ as a square matrix of dimension $p + n - 1$. 
	%For the indices of $X^{(i)}$, we use $X^{(i)}_{a_1 a_2}$ to denote $ \cal A_{a_1 a_2}$ when $a_1$ and $a_2$ are both not equal to $i$, and $X^{(i)}_{a_1 a_2}=0$ when $a_1 = i$ or $a_2 = i$.
	Note that we keep the names of indices when defining $\cal A^{(\mathfrak c)}$, i.e. $\cal A^{(\mathfrak c)}_{\fa\fb }= \cal A_{\fa\fb }$ for $ {\fa, \fb } \ne \mathfrak c$. Correspondingly, we define the resolvent minor of $G(z)$ as
	%The resolvent minor of $G(z)$ after removing the $i$-th row and column is defined as
	\begin{align*}
		G^{(\mathfrak c)}(z) := \left[ \left( {\begin{array}{*{20}c}
		  { -z\id_{p\times p} } & n^{-1/2}\Lambda (Z^{(1)})^\top & n^{-1/2} (Z^{(2)})^\top  \\
      {n^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1\times n_1}} & 0 \\
			{n^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2\times n_2}}
    \end{array}} \right)^{(\mathfrak c)}\right]^{-1}.
	\end{align*}
	We define the partial traces $m^{(\mathfrak c)}(z)$, $m_0^{(\mathfrak c)}(z)$, $m_1^{(\mathfrak c)}(z)$ and $m_2^{(\mathfrak c)}(z)$ by replacing $G(z)$ with $G^{(\mathfrak c)}(z)$ in equation \eqref{defm}. For convenience, we will adopt the convention that $G^{(\mathfrak c)}_{\fa\fb} = 0$ if $\fa = \mathfrak c$ or $\mathfrak b = \mathfrak c$. %Moreover, we will abbreviate $(\{a\})\equiv (a)$ and $(\{a, b\})\equiv (ab)$.
\end{definition}


\iffalse

 
  We begin with a warm up analysis when the entries of $Z^{(1)}$ and $Z^{(2)}$ are drawn i.i.d. from an isotropic Gaussian distribution.
By the rotational invariance of the multivariate Gaussian distribution, we have that the entries of $Z^{(1)} U$ and $Z^{(2)}V$ also follow an isotropic Gaussian distribution.
Hence it suffices to consider the following resolvent

We show how to derive the matrix limit $\Gi(z)$ and the self-consistent equation system \eqref{selfomega_a}.


\paragraph{Self-consistent equations.}
We briefly describe the ideas for deriving the system of self-consistent equations \eqref{selfomega_a}.
A complete proof can be found in Lemma \ref{lemm_selfcons_weak}.
We show that with high probability, the following equations hold approximately:
\be\label{approximate m1m2}
\begin{split}
& m_1^{-1}(z) = -1+ \frac{1}{n} \sum_{i=1}^p \frac{\lambda_i^2 }{ z +\lambda_i^2 \frac{n_1}{n_1+ n_2} m_1(z) +  \frac{n_2}{n_1+n_2}m_2(z)+\oo(1)}+ \oo(1),\\
& m_2^{-1}(z) = -1+\frac{1}{n} \sum_{i=1}^p \frac{1}{ z +\lambda_i^2 \frac{n_1}{n_1 + n_2} m_1(z) +  \frac{n_2}{n_1+n_2}m_2(z)+\oo(1)}  + \oo(1).
\end{split}
\ee
With algebraic calculations, it is not hard to verify that these equations reduce to the self-consistent equations that we stated in equation \eqref{selfomega_a} up to a small error $\oo(1)$.
More precisely, we have that $m_1(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_1} a_1(z) $ and $m_2(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_2} a_2(z)$.

The core idea is to study $G(z)$ using the Schur complement formula.
First, we consider the diagonal entries of $G(z)$ for each block in $\cal I_0$, $\cal I_1$, and $\cal I_2$.
For any $i$ in $\cal I_0$, any $\mu$ in $\cal I_1$, and any $\nu$ in $\cal I_2$, we have that
\begin{align*}
	&G_{ii}^{-1}(z) = -z - \frac{\lambda_i^2}{n} \sum_{\mu,\nu\in \mathcal I_1} Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}G^{\left( i \right)}_{\mu\nu}(z) - \frac{1}{n} \sum_{\mu,\nu\in \mathcal I_2} Z^{(2)}_{\mu i}Z^{(2)}_{\nu i} G^{\left( i \right)}_{\mu\nu}(z) -\frac{2\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu\nu}(z) \\
	&G_{\mu\mu}^{-1}(z) =  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}\lambda_i \lambda_j Z^{(1)}_{\mu i}Z^{(1)}_{\mu j} G^{\left(\mu\right)}_{ij}(z) \\
	&G_{\nu\nu}^{-1}(z) =  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}  Z^{(2)}_{\nu i}Z^{(2)}_{\nu j}  G^{\left(\nu\right)}_{ij}(z).
\end{align*}
For the first equation, we expand the Schur complement formula $G_{ii}^{-1}(z) = -z - H_i G^{(i)}(z) H_{i}^\top$, where $H_i$ is the $i$-th row of $H$ with the $(i,i)$-th entry removed.
The second and third equations follow by similar calculations.

%\be\label{approx m12 add}
%(m_1,m_2) =\left(-\frac{\rho_1+\rho_2}{\rho_1}a_{1}(z),-\frac{\rho_1+\rho_2}{\rho_2}a_{2}(z)\right)+\oo(1) \quad \text{with overwhelming probability. }
%\ee

Next, we apply standard concentration bounds to simplify the above results.
For $G^{-1}_{i i}(z)$, recall that the resolvent minor $G^{(i)}$ is defined such that it is independent of the $i$-th row and column of $Z^{(1)}$ and $Z^{(2)}$.
Hence by standard concentration inequalities, we have that the cross terms are approximately zero.
%is approximately equal to the expectation over $\{Z^{(1)}_{\mu i}: \mu \in \cal I_1 \}\cup \{Z^{(2)}_{\nu i}: \nu \in \cal I_2\}$.
As shown in Lemma \ref{lemm_selfcons_weak}, we have that with high probability the following holds
\begin{align*}
	G_{ii}^{-1}(z) &= -z - \frac{\lambda_i^2}{n} \sum_{\mu \in \mathcal I_1}  G^{\left( i \right)}_{\mu\mu} - \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} +\oo(1) \\
	&= - z - \frac{\lambda_i^2 \cdot n_1}{n_1 + n_2} m_1^{(i)}(z)-  \frac{n_2}{n_1 + n_2} m_2^{(i)}(z)+\oo(1),
\end{align*}
by our definition of the partial traces $m_1^{(i)}(z)$ and $m_2^{(i)}(z)$. %with respect to the resolvent minor $G^{(i)}(z)$.
Since we have removed only one column and one row from $H(z)$, $m_1^{(i)}(z)$ and $m_2^{(i)}(z)$ should be approximately equal to $m_1(z)$ and $m_2(z)$.
Hence we obtain that
\begin{align}\label{1self_Gii}
	G_{ii}(z)  = -\left(z + \frac{\lambda_i^2 \cdot n_1}{n_1 + n_2} m_1(z) +  \frac{n_2}{n_1 + n_2} m_2(z) + \oo(1)\right)^{-1}.
\end{align}
For the other two blocks $\cal I_1$ and $\cal I_2$, using similar ideas we obtain the following equations with high probability:
\begin{align*}
	G_{\mu\mu}(z) &= -\left(1+\frac{p}{n_1 + n_2} m_0(z) + \oo(1)\right)^{-1},\\ 
	G_{\nu\nu}(z) &= -\left(1+\frac{p}{n_1 + n_2} m(z) + \oo(1)\right)^{-1}.
\end{align*}
By averaging the above results over $\mu \in \cal I_1$ and $\nu \in \cal I_2$, we obtain that with high probability
\begin{align*}
	m_1(z) &= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu\mu}(z) = -\left(1+\frac{p}{n_1 + n_2} m_0(z) + \oo(1)\right)^{-1} ,\\
	m_2(z) &= \frac{1}{n_2}\sum_{\nu \in \cal I_2}G_{\nu\nu}(z) = -\left(1+\frac{p}{n_1 + n_2} m(z) + \oo(1)\right)^{-1}.
\end{align*}
Furthermore, we obtain that for $\mu \in \cal I_1$ and $\nu\in \cal I_2$, with high probability
$G_{\mu\mu}(z) = m_1(z) +\oo(1)$ and $G_{\nu\nu}(z) = m_2+\oo(1)$.
In other words, both block matrices within $\cal I_1$ and $\cal I_2$ are approximately a scaling of the identity matrix.
The above results for $m_1(z)$ and $m_2(z)$ imply that
\begin{align*}
	m_1^{-1}(z) &= -1- \frac{1}{n } \sum_{i=1}^p \lambda_i^2 G_{ii}(z)+ \oo(1),\\ 
	 m_2^{-1}(z) &= -1- \frac{1}{n } \sum_{i=1}^p G_{ii}(z)  + \oo(1),
\end{align*}
where we used the definitions of $m(z)$ and $m_0(z)$.
By applying equation \eqref{1self_Gii} for $G_{i i}(z)$ to these two equations, we obtain the system of self-consistent equations \eqref{approximate m1m2}.
In Lemma \ref{lem_stabw}, we show that the self-consistent equations are stable, that is, a small perturbation of the equations leads to a small perturbation of the solution.

\paragraph{Matrix limit.}
Finally, we derive the matrix limit $\Gi(z)$.
We have shown that $m_1(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_1} a_1(z) $ and $m_2(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_2} a_2(z)$ because we know that \eqref{approximate m1m2} holds.
Inserting $m_1(z)$ and $m_2(z)$ into equation \eqref{1self_Gii}, we get that for $i$ in $\cal I_0$,
$G_{ii}(z) = (-z +\lambda_i^2 a_{1}(z) + a_2(z)+\oo(1))^{-1}$ with high probability.
For $\mu$ in $\cal I_1$ and $\nu$ in $\cal I_2$, by $G_{\mu\mu}(z) = m_1(z) +\oo(1)$ and $G_{\nu\nu}(z) = m_2+\oo(1)$, we have that $G_{\mu \mu}(z) = -\frac{n_1 + n_2}{n_1}a_{1}(z)+\oo(1)$ and $G_{\nu\nu}(z) = -\frac{n_1 + n_2}{n_2}a_{2}(z)+\oo(1)$ with high probability.
Hence we have derived the diagonal entries of $\Gi(z)$.
In Lemma \ref{Z_lemma}, we show that the off-diagonal entries are close to zero.
For example, for $i\ne j\in \cal I_0$, by Schur complement, we have that
$$G_{ij}(z) = -G_{ii}(z)\cdot {n^{-1/2}}\Big({\lambda_i}\sum_{\mu \in \cal I_1} Z^{(1)}_{\mu i} G^{(i)}_{\mu j}(z) + \sum_{\mu \in \cal I_2} Z^{(2)}_{\mu i} G^{(i)}_{\mu j}(z) \Big).$$
Using standard concentration inequalities, we can show that $\sum_{\mu \in \cal I_1} Z^{(1)}_{\mu i} G^{(i)}_{\mu j}(z)$ and $\sum_{\mu \in \cal I_2} Z^{(2)}_{\mu i} G^{(i)}_{\mu j}(z)$ are both close to zero.
The other off-diagonal entries are bounded similarly.
%The bound on the off-diagonal entries will be proved rigorously .

\paragraph{Notations.}
We introduce several useful notations for the proof of Theorem \ref{thm_main_RMT}.
\fi




The following resolvent identities are important tools for our proof. All of them can be proved directly using Schur's complement formula, cf. \cite[Lemma 4.4]{Anisotropic}. %Recall that the matrix $\AF$ is defined in \eqref{defn AF}. In the setting of Lemma \ref{prop_entry}, we have $U=V=\id_{p\times p}$. %resolvent minors have been defined in Definition \ref{defn_Minor}, and matrix $\AF$ is given in equation \eqref{defn AF}. %\eqref{diagW}.
\begin{lemma}\label{lemm_resolvent}
We have the following resolvent identities.
\begin{itemize}
	\item[(i)] For $i\in \mathcal I_1$ and $\mu\in \mathcal I_1\cup \cal I_2$, we have
		\begin{equation}
			\frac{1}{G_{ii}} =  - z - \left( {\AF G^{\left( i \right)} \AF^\top} \right)_{ii} ,\quad  \frac{1}{{G_{\mu \mu } }} =  - 1  - \left( {\AF^\top  G^{\left( \mu  \right)} \AF} \right)_{\mu \mu }.\label{resolvent2}
		\end{equation}
	\item[(ii)] For $i\in \mathcal I_1$, $\mu \in \mathcal I_1\cup \cal I_2$, $\fa\in \cal I\setminus \{i\}$ and $\fb\in \cal I\setminus \{ \mu\}$, we have
		\begin{equation}
			G_{i\fa}   = -G_{ii}  \left( \AF G^{\left( {i} \right)} \right)_{i\fa},\quad  G_{\mu \fb }  = - G_{\mu \mu }  \left( \AF^\top  G^{\left( {\mu } \right)}  \right)_{\mu \fb }. \label{resolvent3}
		\end{equation}
%For $i\in \mathcal I_1$ and $\mu\in \mathcal I_2$, we have
%\begin{equation}\label{resolvent6}
%\begin{split}
% G_{i\mu } =  -G_{ii}  \left( WG^{\left( {i} \right)} \right)_{i\mu}, \quad G_{\mu i}= -G_{\mu\mu}\left( W^\top G^{(\mu)}\right)_{\mu i}. %\left( { - Y_{i\mu }  +  {\left( {YG^{\left( {i\mu } \right)} Y} \right)_{i\mu } } } \right) . %, \ \  G_{\mu i}  = G_{\mu \mu } G_{ii}^{\left( \mu  \right)} \left( { - Y_{\mu i}^\top  + \left( {Y^\top  G^{\left( {\mu i} \right)} Y^\top  } \right)_{\mu i} } \right).
%\end{split}
%\end{equation}
 \item[(iii)] For $\mathfrak c \in \mathcal I$ and $\fa,\fb \in \mathcal I \setminus \{\mathfrak c\}$, we have
		\begin{equation}
			G_{\fa\fb}^{\left( \mathfrak c \right)}  = G_{\fa\fb}  - \frac{G_{\fa\mathfrak c} G_{\mathfrak c\fb}}{G_{\mathfrak c\mathfrak c}}.
%, \quad  \frac{1}{{G_{{b}{b}} }} = \frac{1}{{G_{{b}{b}}^{({a})} }} - \frac{{G_{{b}{a}} G_{{a}{b}} }}{{G_{{b}{b}} G_{{b}{b}}^{({a})} G_{{a}{a}} }}.
			\label{resolvent8}
		\end{equation}
%and
%\begin{equation}
%\frac{1}{{G_{ss} }} = \frac{1}{{G_{ss}^{(r)} }} - \frac{{G_{sr} G_{rs} }}{{G_{ss} G_{ss}^{(r)} G_{rr} }}.
%\label{resolvent9}
%\end{equation}
% \item[(iv)]
%All of the above identities hold for $G^{(\mathbb T)}$ instead of $G$ for $\mathbb T \subset \mathcal I$, and in the case where $A$ and $B$ are not diagonal.
\end{itemize}
\end{lemma}
 


%We first collect some preliminary results, Lemmas \ref{lemm apri}-\ref{lemm_resolvent}, that will be used in the proof. We remark that theses results work under the general setting in Theorem \ref{LEM_SMALL} , that is, we do not require $Z^{(1)}$ and $Z^{(2)}$ to be Gaussian as in Lemma \ref{prop_entry}.
We claim the following a priori estimate on the resolvent $G(z)$ for $z\in \mathbf D$.

\begin{lemma}\label{lemm apri}
	In the setting of Theorem \ref{LEM_SMALL}, there exists a constant $C>0$ such that the following estimates hold uniformly in $z,z'\in \mathbf D$ with overwhelming probability:
\be\label{priorim}
\|G(z)\| \le C,%|\im \langle \bu,\cal R  (z,0)\bv\rangle| \le C\eta, 
\ee
and %for any deterministic unit vectors $\mathbf u, \mathbf v \in \mathbb R^{p+n}$,
\be\label{priordiff} 
\left\|G  (z) - G(z')\right\| \le C|z-z'|.
\ee
\end{lemma}
\begin{proof}
 Our proof is a simple application of the spectral decomposition of $G$. Recall the matrix $\AF$ defined in equation \eqref{defn AF}. Let
\be\label{SVDW}
\AF= \sum_{k = 1}^{p} {\sqrt {\mu_k} \xi_k } \zeta _{k}^\top ,\quad \mu_1\ge \mu_2 \ge \cdots \ge \mu_{p} \ge 0 =\mu_{p+1} = \ldots = \mu_{n},\ee
be a singular value decomposition of $F$, where
%$\lambda_1\ge \lambda_2 \ge \ldots \ge \lambda_{p} \ge 0 = \lambda_{p+1} = \ldots = \lambda_{n}$ are the eigenvalues, 
$\{\xi_{k}\}_{k=1}^{p}$ are the left-singular vectors and $\{\zeta_{k}\}_{k=1}^{n}$ are the right-singular vectors.
%orthonormal bases of $\mathbb R^{\mathcal I_1}$ and $\mathbb R^{\mathcal I_2}$, respectively. 
%Then using the definition of the resolve $G(z)$, 
Then using equation \eqref{green2}, we get that for $i,j\in \mathcal I_1$ and $\mu,\nu\in \mathcal I_1\cup \cal I_2$,
\be\label{spectral}
G_{ij} = \sum_{k = 1}^{p} \frac{\xi_k(i) \xi_k^\top(j)}{\mu_k-z}, \ \quad \ G_{\mu\nu} =
%z\sum_{k = 1}^{n} \frac{\zeta_k(\mu) \zeta_k^\top(\nu)}{\lambda_k-z}=
z\sum_{k = 1}^{n} \frac{\zeta_k(\mu) \zeta_k^\top(\nu)}{\mu_k-z} , \ee
and
\be \label{spectral2}
G_{i\mu} = G_{\mu i}= \sum_{k = 1}^{p} \frac{\sqrt{\mu_k}\xi_k(i) \zeta_k^\top(\mu)}{\mu_k-z}.
%\\&   \quad G_{\mu i} = \sum_{k = 1}^{p} \frac{\sqrt{\lambda_k}\zeta_k(\mu) \xi_k^\top(i)}{\lambda_k-z}. 
\ee
%As in equation \eqref{SVDW}, $\{\mu_k\}_{1\le k \le p}$ are the eigenvalues of $WW^\top$. 
Using the fact $n^{-1}V^\top (Z^{(2)})^\top Z^{(2)}V \preceq FF^\top$ and Lemma \ref{SxxSyy}, we obtain that 
%\be\label{lambdap} 
$$\mu_p \ge \lambda_p\left(n^{-1}(Z^{(2)})^\top Z^{(2)}\right) \ge c_\tau \quad \text{with overwhelming probability}, $$ 
for a constant $c_\tau>0$ depending only on $\tau$.  This further implies that
$$ \inf_{z\in \mathbf D}\min_{1\le k \le p}|\mu_k-z| \ge c_\tau - (\log n)^{-1}.$$
Combining this bound with \eqref{spectral} and \eqref{spectral2}, we can easily conclude \eqref{priorim} and \eqref{priordiff}.
\end{proof}

%Note that by equation \eqref{priordiff} and local law equation \eqref{aniso_outstrong}, we have the rough bound
%\be\label{roughinitial}
%\max_{z\in \mathbf D} \max_{{a} , {b}\in \cal I}|\cal R_{{a} {b}} (z)- \Pi_{{a}{b}}(\theta_l)|\le C(\log n)^{-1} 
%\ee
%with overwhelming probability. 

%For simplicity, we denote $Y:=\Sig^{1/2} X \wt  \Sig^{1/2}$. 


%Now we introduce the concept of minors, which are defined by removing certain rows and columns of the matrix $H$.
%
%\begin{definition}[Minors]\label{defnMinor}
%For any $ (p+n)\times (p+n)$ matrix $\cal A$ and index subset $\mathbb T \subseteq \mathcal I$, we define the minor $\cal A^{(\mathbb T)}:=(\cal A_{{a}{b}}:{a},{b} \in \mathcal I\setminus \mathbb T)$ as the $ (p+n-|\mathbb T|)\times (p+n-|\mathbb T|)$ matrix obtained by removing all rows and columns indexed by $\mathbb T$. Note that we keep the names of indices when defining $\cal A^{(\mathbb T)}$, i.e. $(\cal A^{(\mathbb{T})})_{{a}{b}}= \cal A_{{a}{b}}$ for ${a},{b} \notin \mathbb{{T}}$. Correspondingly, we define the resolvent minor as (recall equation \eqref{green2})
%\begin{align*}
%G^{(\mathbb T)}:&=\left[\left(H - \left( {\begin{array}{*{20}c}
%   { zI_{p}} & 0  \\
%   0 & { I_{n}}  \\
%\end{array}} \right)\right)^{(\mathbb T)}\right]^{-1} = \left( {\begin{array}{*{20}c}
%   { \mathcal G^{(\mathbb T)}} & \mathcal G^{(\mathbb T)} W^{(\mathbb T)}  \\
%   {\left(W^{(\mathbb T)}\right)^\top\mathcal G^{(\mathbb T)}} & { \mathcal G_R^{(\mathbb T)} }  \\
%\end{array}} \right)  ,
%%= \left( {\begin{array}{*{20}c}
%%   { z\mathcal G_1^{(\mathbb T)}} & Y^{(\mathbb T)}\mathcal G_2^{(\mathbb T)}   \\
%%   {\mathcal G_2^{(\mathbb T)}}\left(Y^{(\mathbb T)}\right)^\top & { \mathcal G_2^{(\mathbb T)} }  \\
%%\end{array}} \right),
%\end{align*}
%and define the partial traces $m^{(\mathbb T)}$ and $m^{(\mathbb T)}_k$, $k=0,1,2,$ by replacing $G$ with $G^{(\mathbb T)}$ in equation \eqref{defm}.
%%$$m_1^{(\mathbb T)}:=\frac{1}{Nz}\sum_{i\notin \mathbb T}\sigma_i G_{ii}^{(\mathbb T)},\ \ m_2^{(\mathbb T)}:= \frac{1}{N}\sum_{\mu \notin \mathbb T}\wt  \sigma_\mu G_{\mu\mu}^{(\mathbb T)}.$$ 
%%\be\label{m123}
%%\begin{split} 
%%m^{(\mathbb T)} := \frac1p \sum_{i \in \cal I_1}G^{(\mathbb T)}_{ii}(z),\quad & m^{(\mathbb T)}_1:=\frac1p\sum^{(\mathbb T)}_{i \in \cal I_1}\lambda_i^2 G^{(\mathbb T)}_{ii}(z),\\
%% m_2^{(\mathbb T)}(z):=  \frac1{n_1} \sum_{\mu\in \cal I_2} G_{\mu\mu}^{(\mathbb T)}(z),\quad  &m_3^{(\mathbb T)}(z):=  \frac1{n_2} \sum_{\mu\in \cal I_3} G_{\mu\mu}^{(\mathbb T)}(z),
%%\end{split}
%%\ee
%%where we abbreviated that $\sum_{a}^{(\mathbb T)} := \sum_{a\notin \mathbb T} $. 
%For convenience, we will adopt the convention that for any minor $\cal A^{(\mathbb T)}$ defined as above, $\cal A^{(\mathbb T)}_{{a}{b}} = 0$ if ${a} \in \mathbb T$ or ${b} \in \mathbb T$. Moreover, we will abbreviate $(\{{a}\})\equiv ({a})$ and $(\{{a}, {b}\})\equiv ({a}{b})$ for ${a},{b}\in \cal I$.
%\end{definition}

%\begin{definition} [Minor of matrix] For a $M \times N$ matrix $X$, $\ \mathbb{T}$ is a subset of  $\ \{1,2,\cdots,N\}$, we define $X^{\{\mathbb{T}\}}$ as the $M \times (N- \vert \mathbb{T} \vert)$ minor of matrix $X$ by deleting the $i$-th($i \in \mathbb{T}$) columns of $X$. We will keep the name of index of $X$ for $X^{\{\mathbb{T} \}}$, namely,
%$(X^{\{\mathbb{T}\}})_{ij}=\mathbf{1}_{ \{j \notin \mathbb{{T}}\}} X_{ij}$. 
%\end{definition}


%\begin{lemma}\label{Ward_id}
%%Fix constants $c_0,C_0>0$. The following estimates hold uniformly for all $z\in S(c_0,C_0,a)$ for any $a\in \mathbb R$:
%%\begin{equation}
%%\left\| G \right\| \le C\eta ^{ - 1} ,\ \ \left\| {\partial _z G} \right\| \le C\eta ^{ - 2}. \label{eq_gbound}
%%\end{equation}
%%Furthermore, 
%We have the following identities:
%\begin{align}
%& \sum_{i \in \mathcal I_1 }  \left| {G_{j i} } \right|^2 = \sum_{i \in \mathcal I_1 }  \left| {G_{ij} } \right|^2  = \frac{|z|^2}{\eta}\Im\left(\frac{G_{jj}}{z}\right) ,  \label{eq_gsq2} \\
%& \sum_{\mu  \in \mathcal I_2 } {\left| {G_{\nu \mu } } \right|^2 } = \sum_{\mu  \in \mathcal I_2 } {\left| {G_{\mu \nu} } \right|^2 }  = \frac{{\Im \, G_{\nu\nu} }}{\eta}, \label{eq_gsq1}\\ 
%& \sum_{i \in \mathcal I_1 } {\left| {G_{\mu i} } \right|^2 } = \sum_{i \in \mathcal I_1 } {\left| {G_{i\mu} } \right|^2 } = {G}_{\mu \mu}  + \frac{\bar z}{\eta} \Im \, G_{\mu\mu} , \label{eq_gsq3} \\ 
%&\sum_{\mu \in \mathcal I_2 } {\left| {G_{i \mu} } \right|^2 } = \sum_{\mu \in \mathcal I_2 } {\left| {G_{\mu i} } \right|^2 } =  \frac{{G}_{ii}}{z}  + \frac{\bar z}{\eta} \Im\left(\frac{{G_{ii} }}{z}\right) . \label{eq_gsq4} 
% \end{align}
%All of the above estimates remain true for $G^{(\mathbb T)}$ instead of $G$ for any $\mathbb T \subseteq \mathcal I$, and in the case where $A$ and $B$ are not diagonal.
%%Finally, suppose $\{\mathbf v_{i}\}_{i=1}^{N}$ and $\{\mathbf w_{\mu}\}_{\mu=1}^{N}$ are orthonormal bases of $\mathbb C^{\mathcal I_1}$ and $\mathbb C^{\mathcal I_2}$, respectively, then the above estimates remain true if we replace $\mathbf e_i$ with $\mathbf v_i$ and $\mathbf e_\mu$ with $\mathbf v_{\mu}$.
%\label{lemma_Im}
%\end{lemma}
%\begin{proof}
%These estimates and identities can be proved through simple calculations with (\ref{green2}), (\ref{spectral1}) and (\ref{spectral2}). We refer the reader to \cite[Lemma 4.6]{Anisotropic} and \cite[Lemma 3.5]{XYY_circular}.
%\end{proof}
%






%Finally, we have the following lemma, which is a consequence of the Assumption \ref{assm_big2}.
%\begin{lemma}\label{lem_assm3}
%There exists constants $c_0, \tau' >0$ such that 
%\begin{equation}\label{Piii}
%|1+M_{2}(z)\sigma_k |\ge \tau',
%\end{equation}
%for all $z \in S(c_0,C_0,C_1)$ and $1\le k \le M$.
%\end{lemma}
%\begin{proof}
%By Assumption \ref{assm_big2} and the fact $M_{2}(\lambda_r) \in (-\sigma_1^{-1}, 0)$, we have
%$$\left| 1+ M_{2}(\lambda_r) \sigma_k \right| \ge \tau,  \ \ 1\le k \le M.$$
%Applying (\ref{SQUAREROOT}) to the Stieltjes transform
%\begin{equation}\label{Stj_app}
%M_{2}(z):=\int_{\mathbb R} \frac{\rho_{2c}(dx)}{x-z},
%\end{equation}
%one can verify that $M_{2}(z) \sim \sqrt{z-\lambda_r}$ for $z$ close to $\lambda_r$. Hence if $\kappa+\eta \le 2c_0$ for some sufficiently small constant $c_0>0$, we have
%$$\left| 1+ M_{2}(z)\sigma_k \right| \ge \tau/2.$$
%Then we consider the case with $E-\lambda_r \ge c_0$ and $\eta \le c_1$ for some constant $c_1>0$. In fact, for $\eta=0$ and $E\ge \lambda_r + c_0$, $M_{2}(E)$ is real and it is easy to verify that $M_{2}'(E)\ge 0$ using the formula (\ref{Stj_app}). Hence we have
%$$\left| 1+ \sigma_k M_{2}(E) \right| \ge \left| 1+ \sigma_k M_{2}(\lambda_r + c_0) \right| \ge \tau/2, \ \ \text{ for }E\ge \lambda_r + c_0.$$
%Using (\ref{Stj_app}) again, we can get that 
%$$\left|\frac{dM_{2}(z)}{ d z }\right| \le c_0^{-2}, \ \ \text{for } E\ge \lambda_r + c_0.$$ 
%So if $c_1$ is sufficiently small, we have
%$$\left| 1+ \sigma_k M_{2}(E+\ii\eta) \right| \ge \frac{1}{2}\left| 1+ \sigma_k M_{2}(E) \right| \ge \tau/4$$
%for $E\ge \lambda_r + c_0$ and $\eta \le c_1$. Finally, it remains to consider the case with $\eta \ge c_1$. If $\sigma_k \le \left|2M_{2}(z)\right|^{-1}$, then we have $\left| 1+ \sigma_k M_{2}(z) \right| \ge 1/2$. Otherwise, we have $\Im \, M_{2}(z) \sim 1$ by (\ref{SQUAREROOTBEHAVIOR}). Together with (\ref{Immc}), we get that
%$$\left| 1+ \sigma_k M_{2}(z) \right| \ge \sigma_k \Im\, M_{2}(z) \ge \frac{\Im\, M_{2}(z)}{2 M_{2}(z)} \ge \tau' $$
%for some constant $\tau'>0$.
%\end{proof}



%For the proof of Proposition \ref{prop_diagonal}, it is convenient to introduce the following random control parameters.
%
%\begin{definition}[Control parameters]
%We define the random errors
%\begin{equation}\label{eqn_randomerror}
%\Lambda : = \mathop {\max }\limits_{a,b \in \mathcal I} \left| {\left( {G - \Pi } \right)_{ab} } \right|,\ \ \Lambda _o : = \mathop {\max }\limits_{a \ne b \in \mathcal I} \left| {G_{ab} } \right|, \ \ \theta:= |m_1-M_{1}| +  |m_2-M_{2}| ,
%\end{equation}
%%Moreover, we define 
%and the random control parameter (recall $\Psi$ defined in equation \eqref{eq_defpsi})
%\begin{equation}\label{eq_defpsitheta}
%\Psi _\theta  : = \sqrt {\frac{{\Im \, M_{2}  + \theta }}{{N\eta }}} + \frac{1}{N\eta}.
%\end{equation}
%%and the deterministic control parameter
%%\begin{equation}\label{eq_defpsi}
%%\Psi := \sqrt {\frac{\Im\, M_{2} }{{N\eta }} } + \frac{1}{N\eta}.
%%\end{equation}
%\end{definition}



%We fix a $\xi_1\ge 3$ throughout this section. 
%Our goal is to prove that $G$ is close to $\Pi$ in the sense of entrywise and averaged local laws. Hence it is convenient to introduce the following random control parameters.

%\begin{definition}[Control parameters]
%We define the entrywise and averaged errors
%\begin{equation}\label{eqn_randomerror}
%\Lambda : = \mathop {\max }\limits_{a,b \in \mathcal I} \left| {\left( {G - \Pi } \right)_{ab} } \right|,\ \ \Lambda _o : = \mathop {\max }\limits_{a \ne b \in \mathcal I} \left| {G_{ab} } \right|, \ \ \theta:= |m_2-M_{2}| .
%\end{equation}
%Moreover, we define the random control parameter
%\begin{equation}\label{eq_defpsitheta}
%\Psi _\theta  : = \sqrt {\frac{{\Im \, M_{2}  + \theta }}{{N\eta }}} + \frac{1}{N\eta},
%\end{equation}
%and the deterministic control parameter
%\begin{equation}\label{eq_defpsi}
%\Psi := \sqrt {\frac{\Im\, M_{2} }{{N\eta }} } + \frac{1}{N\eta}.
%\end{equation}
%\end{definition}

%For the rest of this subsection, we present the proof of Lemma \ref{prop_entry}, which is the most technical part of the whole proof of Theorem \ref{LEM_SMALL} . 

Now we are ready to give the proof of Lemma \ref{prop_entry}.

\begin{proof}[Proof of Lemma \ref{prop_entry}]
Recall that in the setting of Lemma \ref{prop_entry},  we have %have %$q=n^{-1/2}$ and
\be\label{diagW}\AF \stackrel{d}{=} n^{-1/2}[\Lambda (Z^{(1)})^{\top}, (Z^{(2)})^\top],\ee
%Hence in the following proof, 
and it suffices to consider the resolvent in equation \eqref{resolv Gauss1} throughout the whole proof. 
%For simplicity, we will denote $G\equiv G_0$ in the following proof, while keeping in mind that $W$ takes the form in equation \eqref{diagW}. 
The proof is divided into four steps. 

\vspace{5pt}

\noindent{\bf Step 1: Large deviation estimates.} In this step, we prove some sharp large deviation estimates on the off-diagonal entries of $G$, and on the following $\cal Z$ variables. In analogy to Section 3 of \cite{EKYY1} and Section 5 of \cite{Anisotropic}, we introduce the $\cal Z$ variables  
\begin{equation*}
 \cal  Z_{{\fa}} :=(1-\mathbb E_{{\fa}})\left[\big(G_{{\fa}{\fa}}\big)^{-1}\right], \quad \fa\in \cal I, %\quad {a}\notin \mathbb T,
\end{equation*}
where $\mathbb E_{{\fa}}[\cdot]:=\mathbb E[\cdot\mid H^{({\fa})}]$ denotes the partial expectation over the entries in the ${\fa}$-th row and column of $H$. Now using equation (\ref{resolvent2}), we get that for $i \in \cal I_0$, 
\begin{align}
\cal Z_i &=   \frac{\lambda_i^2}{n} \sum_{\mu ,\nu\in \mathcal I_1}  G^{(i)}_{\mu\nu} \left(\delta_{\mu\nu} - Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}\right)+\frac1n \sum_{\mu ,\nu\in \mathcal I_2}  G^{(i)}_{\mu\nu} \left( \delta_{\mu\nu} - Z^{(2)}_{\mu i}Z^{(2)}_{\nu i}\right) \nonumber\\
& - 2 \frac{\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu\nu},  \label{Zi}
\end{align}
and for $\mu\in \cal I_1$ and $\nu\in \cal I_2$, 
\begin{align}
&\cal  Z_\mu= \frac{1}{n} \sum_{i,j \in \mathcal I_0}  {\lambda_i \lambda_j}G^{(\mu)}_{ij} \left(\delta_{ij} - Z^{(1)}_{\mu i}Z^{(1)}_{\mu j}\right), \quad \cal Z_\nu = \frac{1}{n} \sum_{i,j \in \mathcal I_0} G^{(\nu)}_{ij} \left( \delta_{ij} - Z^{(2)}_{\nu i}Z^{(2)}_{\nu j}\right).\label{Zmu} 
\end{align}
Moreover, we introduce the random error
\begin{equation}  \label{eqn_randomerror}
%\begin{split}
 \Lambda _o : = % \max_{{a} \ne {b} } \left|  G_{{a}{b}}   \right| +  
 \max_{{\fa} \ne {\fb} } \left|  G_{{\fa}{\fa}}^{-1}G_{{\fa}{\fb}}   \right| ,
%\end{split}
\end{equation}
which controls the size of the off-diagonal entries. The following lemma gives the desired large deviation estimate on $\Lambda_o$ and $\cal Z$ variables.

\begin{lemma}\label{Z_lemma}
%Suppose the assumptions of Lemma \ref{prop_entry} hold. 
%Let $c_0>0$ be a sufficiently small constant and fix $C_0, \epsilon >0$. Define the $z$-dependent event $\Xi(z):=\{\Lambda(z) \le (\log N)^{-1}\}$. Then there exists constant $C>0$ such that 
Under the assumptions of Proposition \ref{prop_diagonal}, the following estimate holds uniformly in all $z\in \mathbf D$:
\begin{align}
\Lambda_o + \max_{{a}\in \cal I} |\cal Z_{{a}}|  \prec n^{-1/2}. \label{Zestimate1}
\end{align}
%and 
%\begin{align}
%{\mathbf 1}\left(\eta \ge 1 \right)\left(\Lambda_o + |Z_{a}|\right)\prec \Psi_\theta. \label{Zestimate2}
%\end{align}
%Moreover, then for $z\in S(C_0)$,
%\begin{align}
%1(\Xi)\Lambda_0 \le C(\log N)^{2\xi}\left(q+\Psi_\theta\right) \label{offestimate}
%\end{align}
%holds with $\xi$-overwhelming probability.
\end{lemma}
\begin{proof}
%Suppose $\Xi$ holds, then we have $|G_{{a}{a}} -\Pi|\lesssim (\log N)^{-1}$ on $\Xi$, and  
 Note that for any ${\fa}\in \cal I$, $H^{({\fa})}$ and $G^{({\fa})}$ also satisfy the assumptions in Lemma \ref{lemm apri}. Hence the estimates \eqref{priorim} and \eqref{priordiff} also hold for $G^{({\fa})}$ with overwhelming probability. For any $i\in \cal I_0$, since $G^{(i)}$ is independent of the entries in the $i$-th row and column of $H$, we can apply \eqref{eq largedev1}, \eqref{eq largedev2} and \eqref{eq largedev3} to (\ref{Zi}) to obtain that %and using the a priori bound equation \eqref{priorim}, 
%we get that , %on $\Xi$,
\begin{equation}\nonumber%\label{estimate_Zi}
\begin{split}
\left| \cal Z_{i}\right|&\lesssim \frac{1}{n} \sum_{\al=1}^2 \left|\sum_{\mu ,\nu\in \mathcal I_\al}  G^{(i)}_{\mu\nu} \left(\delta_{\mu\nu} - Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}\right)\right|+ \frac{1}{n} \left|\sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu\nu}\right| \\
&\prec  \frac{1}{n} \left( \sum_{\mu,\nu \in \cal I_1\cup \cal I_2 }  {| G_{\mu\nu}^{(i)}|^2 }  \right)^{1/2} \prec n^{-1/2} .
%+ \frac{1}{\sqrt{n}} \left( \frac{1}{n}\sum_{\mu \in \cal I_{\al+2}}  {\left(\cal R^{(i)} J_{\al+2} (\cal R^{(i)})^*\right)_{\mu\mu} }  \right)^{1/2} \prec n^{-1/2},
%\\& \prec  \phi_n + \frac{1}{n} \left[ \sum_{\mu\in \cal I_3} \left( 1+ \frac{ \im \left( U(z) G_{[\mu\mu]}^{(i)} \right)_{11} }{\eta}   \right]  \right)^{1/2} ,%= q+ \frac{1}{N}\left( {\sum_\mu \frac{\wt \sigma_\mu  \im G_{\mu\mu}^{(i)} }{\eta} } \right)^{1/2}= q + \sqrt { \frac{ \Im\, m_2^{(i)}  } {N\eta} },
\end{split}
\end{equation}
Here in the last step we use \eqref{priorim} to get that for any $\mu\in \cal I_1\cup \cal I_2$,
\be\label{GG*}\sum_{\nu \in \cal I_1\cup \cal I_2 }  | G_{\mu\nu}^{(i)} |^2\le \sum_{{\fa} \in \cal I } | G_{\mu{\fa}}^{(i)} |^2 =\left[G^{(i)}(G^{(i)})^* \right]_{\mu\mu} =\OO(1),\ee
 with overwhelming probability. Here $(G^{(i)})^*$ denotes the complex conjugate transpose of $G^{(i)}$. Similarly, applying \eqref{eq largedev1}, \eqref{eq largedev2} and \eqref{eq largedev3} to $\cal Z_{\mu}$ and $\cal Z_\nu$ in equation (\ref{Zmu}) and using \eqref{priorim}, we can obtain the same bound. This gives that $\max_{{a}\in \cal I} |\cal Z_{{a}}|  \prec n^{-1/2}$.
%\begin{equation}\label{estimate_Zmu} \|Z_{[\mu]}\|\prec \frac{1}{n} \left( \sum_{i,j \in \cal I_{1}\cup \cal I_2}  {\left| \cal R_{ij}^{[\mu]}  \right|^2 }  \right)^{1/2} =  \frac{1}{\sqrt{n}} \left(  \frac{1}{n}\sum_{i \in \cal I_{1}\cup \cal I_2}  {\left(\cal R^{[\mu]} (J_{1}+J_2) (\cal R^{[\mu]})^*\right)_{ii} } \right)^{1/2} \prec n^{-1/2}.\ee
%This completes the proof of equation \eqref{Zestimate1}.
%we have
%\begin{equation}
%G_{i{a}}   = -G_{ii}  \left( WG^{\left( {i} \right)} \right)_{i{a}},\quad  G_{\mu {b} }  = - G_{\mu \mu }  \left( W^\top  G^{\left( {\mu } \right)}  \right)_{\mu {b} }.  
%\end{equation}

Next we prove the off-diagonal estimate on $\Lambda_o$. For $i\in \mathcal I_1$ and ${\fa}\in \cal I\setminus \{i\}$, using equations \eqref{resolvent3}, \eqref{eq largedev1} and \eqref{priorim}, we can obtain that 
\begin{align*}
  \left|G_{ii}^{-1}G_{i{\fa}}\right| &\lesssim {n^{-1/2}}\left|  \sum_{\mu \in \cal I_1} Z^{(1)}_{\mu i} G^{(i)}_{\mu \fa} \right| + n^{-1/2}\left|\sum_{\mu \in \cal I_2} Z^{(2)}_{\mu i} G^{(i)}_{\mu \fa}(z)  \right| \\
& \prec  n^{-1/2}\left( \sum_{\mu \in \cal I_1\cup \cal I_2}  {| G_{\mu {\fa}}^{(i)} |^2 }  \right)^{1/2} \prec n^{-1/2}. 
%\\& \left|G_{\mu\mu}^{-1} G_{\mu{b}} \right| \prec n^{-1/2}+  \frac{1}{\sqrt n} \left( \sum_{i \in \cal I_{1}}  {\left| G_{i{b}}^{(\mu)}  \right|^2 }  \right)^{1/2} \prec n^{-1/2}.
 \end{align*}
 We can get the same estimate for $\left|G_{\mu\mu}^{-1} G_{\mu{\fb}} \right|$, $\mu \in \mathcal I_1\cup \cal I_2$ and ${\fb}\in \cal I\setminus \{ \mu\}$, using a similar argument.  
%For $i\in \cal I_1\cup \cal I_2$ and $\mu \in \mathcal I_3$, using equation \eqref{resolvent3}, Lemma \ref{largedeviation} and equation \eqref{priorim}, we obtain that  
%$$ \left| G_{ii}^{-1}G_{i\mu} \right|+ \left| G_{\mu\mu}^{-1}G_{\mu i} \right| \prec n^{-1/2} + \frac{1}{\sqrt n} \left( \sum_{\nu \in \cal I_{2}\cup \cal I_3 }  {\left|G^{(i)}_{\nu\mu}  \right|^2 }  \right)^{1/2} + \frac{1}{\sqrt n} \left( \sum_{j \in \cal I_{1} }  {\left|G^{(\mu)}_{ji}  \right|^2 }  \right)^{1/2} \prec   n^{-1/2}.$$
This gives that $\Lambda_o\prec n^{-1/2}$. %, which concludes equation \eqref{Zestimate1}.
\end{proof}

Note that combining \eqref{Zestimate1} with the estimate $\max_\fa|G_{\fa\fa}|=\OO(1)$ w.o.p. by \eqref{priorim}, we immediately conclude \eqref{entry_diagonal} for the off-diagonal entries with ${a}\ne {b}$.


%{\cor
%\begin{lemma}
%Fix constants $c_0,C_0>0$. For any $\mathbb T \subseteq \mathcal I$ and $a\in \mathbb R$, the following bounds hold uniformly in $z\in S(c_0,C_0,a)$:
%\begin{equation}\label{m_T}
%\big| {m_1  - m_1^{\left( \mathbb T \right)} } \big| + \big| {m_2  - m_2^{\left( \mathbb T \right)} } \big| \le \frac{{C\left| \mathbb T \right|}}{{N\eta }}, %\ \ i= 1,2, 
%\end{equation}
%%and 
%%\begin{equation}\label{m11_T}
%%\left| {\frac{1}{N}\sum_{i=1}^M \sigma_i \left(G_{ii}^{(\mathbb T)} - G_{ii}\right)} \right| \le \frac{{C\left| \mathbb T \right|}}{{N\eta }}, %\ \ i= 1,2, 
%%\end{equation}
%where $C>0$ is a constant depending only on $\tau$.
%%where $C>0$ depends only on $C_0 \lambda_r$. %is an absolute constant. %depending only on the aspect ratio $d$.
%\end{lemma}
%\begin{proof}
%For $\mu\in\mathcal I_2$, we have
%\begin{align*}
%\left|m_2-m_2^{(\mu)}\right|& =\frac{1}{N}\left|\sum_{\nu\in\mathcal I_2}  \wt  \sigma_\nu\frac{G_{\nu\mu}G_{\mu\nu}}{G_{\mu\mu}}\right| \le \frac{C}{N|G_{\mu\mu}|} \sum_{\nu\in\mathcal I_2} |G_{\nu\mu}|^2 = \frac{C\Im\, G_{\mu\mu}}{N\eta |G_{\mu\mu}|} \le \frac{C}{N\eta}, % \label{rough_boundmi}
%\end{align*}
%where in the first step we used (\ref{resolvent8}), and in the second and third steps we used (\ref{eq_gsq1}). Similarly, using (\ref{resolvent8}) and (\ref{eq_gsq3}) we get
%\begin{align*}
%\left|m_2 -m_2^{(i)}\right| & = \frac{1}{N}\left|\sum_{\nu \in\mathcal I_2}\wt  \sigma_\nu\frac{G_{\nu i}G_{i\nu}}{G_{ii}}\right| \le \frac{C}{N|G_{ii}|} \left( \frac{{G}_{ii}}{z}  + \frac{\bar z}{\eta} \Im\left(\frac{{G_{ii} }}{z}\right)\right)   \le \frac{C}{N\eta}.
%\end{align*}
%Similarly, we can prove the same bounds for the $m_1$ case. Then (\ref{m_T}) can be proved by induction on the indices in $\mathbb T$. %The proof for (\ref{m11_T}) is similar except that one needs to use the assumption (\ref{assm3}).
%\end{proof}
%}

\vspace{5pt}

\noindent{\bf Step 2: Self-consistent equations.} 
%This is the key step of the proof for Lemma \ref{prop_entry}, where 
%In this step, we derive the approximate self-consistent equations in \eqref{approximate m1m2} satisfied by $m_1(z)$ and $m_2(z)$ with more precise error rates. %defined in equation \eqref{defm}. 
%More precisely, 
In this step, we show that $(m_1(z),m_2(z))$ satisfies the approximate self-consistent equations in \eqref{selfomegaerror} for some small errors $\cal E_{1}$ and $\cal E_{2}$. Later in Step 3, we will apply Lemma \ref{lem_stabw} to show that $(m_1(z),m_2(z))$ is close to $(m_{1c}(z),m_{2c}(z))$.  


Note that by \eqref{Lipomega}, for $z\in \mathbf D$ the following estimates hold:
$$|m_{1c}(z)-m_{1c}(0)| \lesssim (\log n)^{-1},\quad |m_{2c}(z)-m_{2c}(0)| \lesssim (\log n)^{-1}.$$
%(recall that $a_1=-r_1M_{1}(0)$ and $a_2=-r_2M_{2}(0)$). 
Together with the estimates in equation \eqref{a23}, %and the assumption \eqref{assm3_app}, 
we obtain that % following estimates
\be\label{Gsim1}
 |m_{1c}| \sim |m_{2c}| \sim 1, \quad |z+\lambda_i^2 r_1m_{1c} + r_2 m_{2c}|\sim 1,\quad \text{uniformly in $z\in \mathbf D$}  . \ee
 Moreover, using equation \eqref{selfomega} we get
 \be\label{Gsim0}
\left|1 + \gamma_n m_c (z)\right| = |m_{2c}^{-1}(z)| \sim 1, \quad  |1+\gamma_n m_{0c}(z)| = |m_{1c}^{-1}(z)| \sim 1  ,
\ee
 uniformly in $z\in \mathbf D$, where we abbreviated
 \begin{align}
 m_c(z)&:=-\frac1p\sum_{i=1}^p\frac{1}{z+\lambda_i^2 r_1m_{1c}(z) +r_2m_{2c}(z)},\label{defn mc1c}\\
  m_{0c}(z)&:=-\frac1p\sum_{i=1}^p\frac{\lambda_i^2}{z+\lambda_i^2 r_1m_{1c}(z) +r_2m_{2c}(z)}. \label{defn mc0c}
 \end{align}
In fact, we will see that $m_c(z)$ and $m_{0c}(z)$ are the asymptotic limits of $m(z)$ and $m_0(z)$, respectively. Applying \eqref{Gsim1} to \eqref{defn_piw} (recall \eqref{M1M2a1a2}), we get that
\be
|\Gi_{{\fa}{\fa}}(z)| \sim 1 \ \ \text{uniformly in } z\in \mathbf D \ \text{ and } \ {\fa}\in \cal I.
\ee 

We define the following $z$-dependent event 
%\be\label{Xiz}\Xi(z):=\left\{\max_{{a}\in \cal I}|G_{{a}{a}}(z)-\Pi_{{a}{a}}(z)| \le (\log n)^{-1/2}\right\}.\ee
\be\label{Xiz}\Xi(z):=\left\{ |m_{1}(z)-m_{1c}(0)| + |m_{2}(z)-m_{2c}(0)| \le (\log n)^{-1/2}\right\}.\ee
With equation \eqref{Gsim1}, we immediately get that on $\Xi(z)$,
\be\label{Gsim012} |m_{1}(z)| \sim |m_{2}(z)| \sim 1, \quad |z + \lambda_i^2 r_1m_1(z)+r_2m_2(z)|\sim1.\ee 
%In particular, on $\Xi$ we have
%\be\label{Gsim1}
%\mathbf 1(\Xi)|G_{{a}{a}}(z)| \sim 1 .
%\ee 
Then we prove the following key lemma, which shows that $(m_1(z),m_2(z))$ satisfies equation \eqref{selfomegaerror} approximately on $\Xi(z)$. %with some small errors $\cal E_{1}$ and $\cal E_{2}$.
%that $M_{2}(z)$ is the solution to the equation $z=f(m)$ for $f$ defined in (\ref{deformed_MP2}).

\begin{lemma}\label{lemm_selfcons_weak}
In the setting of Lemma \ref{prop_entry}, the following estimates hold uniformly in $z \in \mathbf D$: 
\begin{equation} \label{selfcons_lemm}
\mathbf 1(\Xi) \left|\frac{1}{m_{1}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2r_1m_{1} + r_2m_{2}  } \right|\prec n^{-1}+n^{-{1}/2}\Theta +|[\cal Z]_0|+  |[\cal Z]_1|,\ee
and
\begin{equation} \label{selfcons_lemm2}
\mathbf 1(\Xi) \left|\frac{1}{m_{2}} + 1 -\frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1m_{1} +  r_2m_{2}  }\right|\prec n^{-1}+n^{-1/2}\Theta +|[\cal Z]|+  |[\cal Z]_2|,
\ee
where we denote 
\be\label{defn_Theta}\Theta:= |m_1(z)-m_{1c}(z)|+|m_2(z)-m_{2c}(z)|,\ee
and
\begin{equation} \label{def_Zaver}
 [\cal Z]:= \frac1p\sum_{i\in \cal I_0} \frac{\cal Z_i}{(z+ \lambda_i^2 r_1m_{1c}+ r_2m_{2c})^2} ,\ \ \ \ [\cal Z]_0:= \frac1p\sum_{i\in \cal I_0} \frac{\lambda_i^2 \cal Z_i}{(z+ \lambda_i^2 r_1m_{1c}+ r_2m_{2c})^2} ,\\
\end{equation}
\begin{equation}\label{def_Zaver2}
  [\cal Z]_1:=\frac{1}{n_1}\sum_{i\in \mathcal I_1}\cal Z_\mu, \ \ \ \  [\cal Z]_2:=\frac{1}{n_2}\sum_{\nu \in \mathcal I_2}  \cal Z_\nu.
\end{equation}
\end{lemma}

\begin{proof}
With equations (\ref{resolvent2}), (\ref{Zi}) and (\ref{Zmu}), we obtain that %for  $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
\begin{align}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu\in \mathcal I_1} G^{\left( i \right)}_{\mu\mu}- \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} + \cal Z_i \nonumber\\
&=  - z - \lambda_i^2 r_1m_1 - r_2m_2 + \cal E_i, \quad \text{for }i \in \cal I_0, \label{self_Gii}\\
\frac{1}{{G_{\mu\mu} }}&=  - 1 - \frac{1}{n} \sum_{i\in \mathcal I_0}\lambda_i^2 G^{\left(\mu\right)}_{ii}+ \cal Z_{\mu} =  - 1  - \gamma_n m_0 + \cal E_\mu, \quad \text{for }\mu \in \cal I_1,  \label{self_Gmu1}\\
\frac{1}{{G_{\nu\nu} }}&=  - 1 - \frac{1}{n} \sum_{i\in \mathcal I_0} G^{\left(\nu\right)}_{ii}+\cal Z_{\nu} =   - 1 - \gamma_n m + \cal E_\nu, \quad \text{for }\nu \in \cal I_2, \label{self_Gmu2}
\end{align}
where we denote (recall \eqref{defm} and Definition \ref{defn_Minor}) %, and define %equation \eqref{m123}, and 
$$\cal E_i :=\cal Z_i + \lambda_i^2 r_1\left(m_1 - m_1^{(i)}\right) + r_2\left(m_2-m_2^{(i)}\right) ,$$
and
$$\cal E_\mu :=\cal Z_{\mu} + \gamma_n(m_0-m_0^{(\mu)}),\quad \cal E_\nu:=\cal Z_{\nu} +\gamma_n(m-m^{(\nu)}) .$$
Using equations (\ref{resolvent8}), \eqref{eqn_randomerror} and (\ref{Zestimate1}), we can bound that  
\begin{equation}\label{m1i}
  |m_1 - m_1^{(i)}| \le   \frac1{n_1}\sum_{\mu\in \mathcal I_1}  \left|\frac{G_{\mu i} G_{i\mu}}{G_{ii}}\right| \le |\Lambda_o|^2|G_{ii}| \prec n^{-1}.
\end{equation}
where we also use bound \eqref{priorim} in the last step. Similarly, we can also obtain that 
\be \label{higherr}  
 |m_2 - m_2^{(i)}| \prec n^{-1} , \quad |m_0 - m_0^{(\mu)}| \prec n^{-1},\quad  |m-m^{(\nu)}| \prec n^{-1},  \ee
for any $i\in \cal I_0$, $\mu \in \cal I_{1}$ and $\nu\in \cal I_2$. Together with (\ref{Zestimate1}), we obtain the bound %for all $i$ and $\mu$,
\begin{equation}\label{erri}
\max_{i\in \cal I_0} |\cal E_i | +\max_{\mu \in \cal I_1\cup \cal I_2} |\cal E_\mu|  \prec n^{-1/2}.
\end{equation}
%with $\xi $-overwhelming probability.
%Similarly, we can also get that
%\begin{equation}\label{self_Gmu}
%\frac{1}{{G_{\mu\mu} }}=  - z - \frac{1}{N} \sum_{i\in \mathcal I_1}\sigma_i G^{\left( \mu\right)}_{ii}+ Z_{\mu} =  - z - \frac{1}{N} \sum_{i\in \mathcal I_1}\sigma_i G_{ii} + \epsilon_\mu,
%\end{equation}
%where 
%$$\epsilon_\mu := Z_{\mu} + \frac{1}{N} \sum_{i\in \mathcal I_1}\sigma_i \left(G^{\left( \mu\right)}_{ii}-G_{ii}\right).$$
%Moreover, we have

% With equation \eqref{Gsim1} and the definition of the event $\Xi$ in \eqref{Xiz}, we get that 
% $$\mathbf 1(\Xi)|z + \lambda_i^2 r_1m_1+r_2m_2|\sim1.$$ 
% Combining it with equations (\ref{self_Gii}) and \eqref{erri}, 
 
From equation \eqref{self_Gii}, we obtain that on $\Xi$,
\begin{align}
 &G_{ii}= -\frac{1}{z + \lambda_i^2 r_1 m_1+r_2 m_2} - \frac{\cal E_i}{(z + \lambda_i^2 r_1 m_1+r_2 m_2)^2} +\OO_\prec\left(n^{-1}\right) \nonumber\\
&= -\frac{1}{z + \lambda_i^2 r_1 m_1+r_2 m_2} - \frac{\cal Z_i}{(z + \lambda_i^2 r_1 m_{1c}+r_2 m_{2c})^2} +\OO_\prec\left(n^{-1} + n^{-1/2}\Theta\right) .\label{Gmumu0}
\end{align}
where in the first step we use \eqref{erri} and \eqref{Gsim012} on $\Xi$, and in the second step we use \eqref{defn_Theta}, \eqref{m1i} and \eqref{higherr}. Plugging \eqref{Gmumu0} into the definitions of $m$ and $m_0$ in \eqref{defm} and using \eqref{def_Zaver}, we get that on $\Xi$,
\begin{align}
 m&= -\frac1p\sum_{i\in \cal I_0}\frac{1}{z + \lambda_i^2 r_1 m_1+r_2 m_2} -[\cal Z] +\OO_\prec\left(n^{-1} + n^{-1/2}\Theta\right) ,\label{Gmumu} \\
 m_0&= -\frac1p\sum_{i\in \cal I_0}\frac{\lambda_i^2}{z + \lambda_i^2 r_1 m_1+r_2 m_2}  -[\cal Z]_0 +\OO_\prec\left(n^{-1} + n^{-1/2}\Theta\right) . \label{Gmumu2}
\end{align}
As a byproduct, comparing these two equations with \eqref{defn mc1c} and \eqref{defn mc0c}, we obtain that  
\be\label{Gsim11}
 |m(z)-m_c(z)| +|m_0(z)-m_{0c}(z)|  \lesssim (\log n)^{-1/2}, \quad \text{w.o.p. on } \Xi. 
\ee
Together with equation \eqref{Gsim0}, we get that %from equation \eqref{self_Gmu1} and equation \eqref{self_Gmu2} that
\be\label{Gsim2}
|1+\gamma_nm (z)|\sim 1, \quad |1+\gamma_nm_0(z)|\sim 1, \quad \text{w.o.p. on } \Xi.
\ee
Now with a similar argument as above, from equations \eqref{self_Gmu1}, \eqref{self_Gmu2}, we obtain that on $\Xi$,%can obtain that with overwhelming probability,
\begin{align}
&G_{\mu\mu}=-\frac{1}{1 + \gamma_nm_0}  - \frac{\cal Z_\mu}{(1 + \gamma_nm_0)^2}+\OO_\prec\left(n^{-1} + n^{-1/2}\Theta\right)   ,\quad \mu \in \cal I_1,\label{Gii0} \\
& G_{\nu\nu}=-\frac{1}{1 + \gamma_nm} - \frac{\cal Z_\nu}{(1 + \gamma_nm)^2}+\OO_\prec\left(n^{-1} + n^{-1/2}\Theta\right) ,\quad \nu \in \cal I_2  ,\label{Gii00} 
%\\ \quad \nu \in \cal I_3.\label{Gii1}
\end{align}
 %for $\mu \in \cal I_1$ and $\nu \in \cal I_2,$ 
 where we use \eqref{erri}, \eqref{Gsim2}, \eqref{defn_Theta}, \eqref{m1i} and \eqref{higherr} in the derivation.
Taking average of \eqref{Gii0} and \eqref{Gii00} over $\mu\in \cal I_1$ and $\nu\in \cal I_2$, we get that  on $\Xi$, %with overwhelming probability,
\begin{align}
& m_1=-\frac{1}{1 + \gamma_n m_0} - \frac{[\cal Z]_1}{(1 + \gamma_nm_0)^2}+\OO_\prec\left(n^{-1} + n^{-1/2}\Theta\right)    ,\label{Gii000}\\
&m_2=-\frac{1}{1 +\gamma_n  m}- \frac{[\cal Z]_2}{(1 + \gamma_nm)^2}+\OO_\prec\left(n^{-1} + n^{-1/2}\Theta\right) ,\label{Gii001}
\end{align}
which further implies that  on $\Xi$,
\begin{align}
 &\frac{1}{m_1} + 1 + \gamma_nm_0  \prec  n^{-1} + n^{-1/2}\Theta + |[\cal Z]_1|,\label{Gii}\\
 & \frac{1}{m_2} + 1 + \gamma_nm \prec   n^{-1} + n^{-1/2}\Theta + |[\cal Z]_2|.\label{Gii111}
\end{align}
Finally, plugging \eqref{Gmumu} and \eqref{Gmumu2} into equations \eqref{Gii} and \eqref{Gii111}, we conclude equations (\ref{selfcons_lemm}) and (\ref{selfcons_lemm2}). 
\end{proof}

%The following lemma gives the stability of the equation $ f(z,m)=0$. Roughly speaking, it states that if $f(z, m_{2}(z))$ is small and $m_2(\wt  z)-M_{2}(\wt  z)$ is small for $\Im\, \wt  z \ge \Im\, z$, then $m_{2}(z)-M_{2}(z)$ is small. For an arbitrary $z\in S(c_0,C_0, \e)$, we define the discrete set
%\begin{align*}%\label{eqn_def_L}
%L(w):=\{z\}\cup \{z'\in S(c_0,C_0, \e): \text{Re}\, z' = \text{Re}\, z, \text{Im}\, z'\in [\text{Im}\, z, 1]\cap (N^{-10}\mathbb N)\} .
%\end{align*}
%Thus, if $\text{Im}\, z \ge 1$, then $L(z)=\{z\}$; if $\text{Im}\, z<1$, then $L(z)$ is a 1-dimensional lattice with spacing $N^{-10}$ plus the point $z$. Obviously, we have $|L(z)|\le N^{10}$. %The following lemma is stated as Definition 5.4 of \cite{KY2} %and Lemma 4.5 of \cite{BEKYY}.
%
%\begin{lemma}\label{stability}
%Let $c_0>0$ be a sufficiently small constant and fix $C_0,\epsilon>0$. The self-consistent equation $f(z,m)=0$ is stable on $S(c_0,C_0, \epsilon)$ in the following sense. Suppose the $z$-dependent function $\delta$ satisfies $N^{-2} \le \delta(z) \le (\log N)^{-1}$ for $z\in S(c_0,C_0, \epsilon)$ and that $\delta$ is Lipschitz continuous with Lipschitz constant $\le N^2$. Suppose moreover that for each fixed $E$, the function $\eta \mapsto \delta(E+\ii\eta)$ is non-increasing for $\eta>0$. Suppose that $u_2: S(c_0,C_0,\epsilon)\to \mathbb C$ is the Stieltjes transform of a probability measure. Let $z\in S(c_0,C_0,\epsilon)$ and suppose that for all $z'\in L(z)$ we have 
%\begin{equation}\label{Stability0}
%\left| f(z, u_2)\right| \le \delta(z).
%\end{equation}
%Then we have
%\begin{equation}
%\left|u_2(z)-M_{2}(z)\right|\le \frac{C\delta}{\sqrt{\kappa+\eta+\delta}},\label{Stability1}
%\end{equation}
%for some constant $C>0$ independent of $z$ and $N$, where $\kappa$ is defined in (\ref{KAPPA}). 
%%Similarly, the self-consistent equation $\mathcal D_2$ in (\ref{def_D12}) is also stable on $S(C_1)$.
%\end{lemma}
%\begin{proof}
%This lemma can proved with the same method as in e.g. \cite[Lemma 4.5]{isotropic} and \cite[Appendix A.2]{Anisotropic}. The only input is Lemma \ref{lambdar_sqrt}. 
%\end{proof}

%\vspace{5pt}

\noindent{\bf Step 3: Entrywise local law.} In this step, we show that the event $\Xi(z)$ in \eqref{Xiz} actually holds with overwhelming probability for all $z\in \mathbf D$. Once we have proved this fact, applying Lemma \ref{lem_stabw} to equations \eqref{selfcons_lemm} and  \eqref{selfcons_lemm2} immediately shows that $(m_1(z),m_2(z))$ is close to $(m_{1c}(z),m_{2c}(z))$ up to an error of order $\OO_\prec(n^{-1/2})$, with which we can conclude the entrywise local law \eqref{entry_diagonal}. 

We claim that it suffices to show that
\be\label{Xiz0}
|m_{1}(0)-m_{1c}(0)| + |m_{2}(0)-m_{2c}(0)| \prec n^{-1/2}.
\ee
 In fact, notice that by \eqref{Lipomega} and \eqref{priordiff} we have
$$ |m_{1c}(z)-m_{1c}(0)|+|m_{2c}(z)-m_{2c}(0)|=\OO((\log n)^{-1}),$$
and
$$  |m_{1}(z)-m_{1}(0)|+ |m_{2}(z)-m_{2}(0)|=\OO((\log n)^{-1}),$$
with overwhelming probability for all $z\in \mathbf D$. Thus if \eqref{Xiz0} holds, using triangle inequality we can obtain from the above two estimates that 
%\be\label{roughh1} 
%\sup_{z\in \mathbf D} \left(|m_{1}(z)-m_{1c}(z)| + |m_{2}(z)-m_{2c}(z)|\right)  \lesssim (\log n)^{-1} \quad \text{w.o.p.},\ee
%and %Moreover, with this estimate and equation \eqref{Lipomega}, we conclude that 
\be\label{roughh2} 
\sup_{z\in \mathbf D} \left( |m_{1}(z)-m_{1c}(0)|+ |m_{2}(z)-m_{2c}(0)|\right) \lesssim (\log n)^{-1} \quad \text{w.o.p.}\ee
The equation \eqref{roughh2} shows that $\Xi(z)$ holds with overwhelming probability, %while the equation \eqref{roughh2} 
and it also verifies the condition \eqref{prior12} of Lemma \ref{lem_stabw}. Now applying Lemma \ref{lem_stabw} to equations \eqref{selfcons_lemm} and \eqref{selfcons_lemm2}, we obtain that
\begin{align*} 
\Theta(z)&=|m_1(z)-m_{1c}(z)|+|m_2(z)-m_{2c}(z)| \\
&\prec n^{-1}+n^{-1/2}\Theta(z) +|[\cal Z]|+  |[\cal Z]_0| +|[\cal Z]_1|+  |[\cal Z]_2| ,
\end{align*}
which implies that 
\be\label{Xizz}
\Theta(z) \prec n^{-1} +|[\cal Z]|+  |[\cal Z]_0| +|[\cal Z]_1|+  |[\cal Z]_2| \prec n^{-1/2},
\ee
uniformly for all $z\in \mathbf D$. Here in the second step, we use \eqref{Zestimate1}. On the other hand, with equations \eqref{Gii0}-\eqref{Gii001}, we obtain that 
$$\max_{\mu\in \cal I_1} |G_{\mu\mu}(z)-m_{1}(z)|+ \max_{\nu\in \cal I_2} |G_{\nu\nu}(z)-m_{2}(z)|\prec n^{-1/2}.
$$
Combining this estimate with \eqref{Xizz}, we get that
\be\label{Xizz2} \max_{\mu\in \cal I_1} |G_{\mu\mu}(z)-m_{1c}(z)|+ \max_{\nu\in \cal I_2} |G_{\nu\nu}(z)-m_{2c}(z)|\prec n^{-1/2}.
\ee
Then plugging \eqref{Xizz} into equation \eqref{Gmumu0} and recalling \eqref{M1M2a1a2}, we obtain that 
$$\max_{{i}\in \cal I_1}|G_{ii}(z)-\Gi_{ii}(z)| \prec n^{-1/2}. 
$$
Together with equation \eqref{Xizz2}, it gives the diagonal estimate
\be\label{diagest}
\max_{{\fa}\in \cal I}|G_{{\fa}{\fa}}(z)-\Pi_{{\fa}{\fa}}(z)| \prec n^{-1/2}. 
\ee
Combining equation \eqref{diagest} with the off-diagonal estimate on $\Lambda_o$ in equation \eqref{Zestimate1}, we conclude the entrywise local law \eqref{entry_diagonal}. % proof of Lemma \ref{prop_entry}. 

Now we give the proof of \eqref{Xiz0}.
%It remains to show that equation \eqref{Xiz0} holds.% In the following lemma, we pick $0=0$.
%\begin{lemma}
%Under the assumptions of Proposition \ref{prop_diagonal}, the estimate equation \eqref{Xiz0} holds.
%\end{lemma}
%\begin{proof}[Proof of equation \eqref{Xiz0}]
Using \eqref{priorim} and \eqref{spectral}, we get that with overwhelming probability,
$$1\gtrsim m(0)=\frac1p\sum_{i\in \cal I_0}G_{ii}(0) = \frac1p\sum_{i\in \cal I_0}\sum_{k = 1}^{p} \frac{|\xi_k(i)|^2 }{\mu_k} \ge \mu_1^{-1} \gtrsim 1,$$
where we use Lemma \ref{SxxSyy} to bound  $\mu_1$. Similarly, we can also get that $m_0(0)$ is positive and has size $m_0(0)\sim 1$. Hence we have the estimates
\be\label{add_1+m}1+\gamma_n m(0)\sim 1,\quad 1+\gamma_n m_0(0)\sim 1.\ee
Combining these estimates with equations \eqref{self_Gmu1}, \eqref{self_Gmu2} and  \eqref{erri}, we obtain that  \eqref{Gii000} and \eqref{Gii001} hold at $z=0$ even without the indicator function $\mathbf 1(\Xi)$, which further give that with overwhelming probability,
$$  \left|\lambda_i^2 r_1m_1(0)+r_2m_2(0)\right|=\left|\frac{\lambda_i^2 r_1}{ 1+\gamma_n m_0(0)} +\frac{r_2}{1+\gamma_n m(0)}+ \OO_\prec (n^{-1/2})\right| \sim 1 .$$
 Then combining this estimate with (\ref{self_Gii}) and \eqref{erri}, we obtain that \eqref{Gmumu} and \eqref{Gmumu2} also hold at $z=0$ even without the indicator function $\mathbf 1(\Xi)$. Finally, plugging  \eqref{Gmumu} and \eqref{Gmumu2} into equations \eqref{Gii} and \eqref{Gii111}, we conclude that \eqref{selfcons_lemm} and  \eqref{selfcons_lemm2}  hold at $z=0$, that is,
\begin{equation} \label{selfcons_lemma222}
\begin{split}
& \left|\frac{1}{m_{1}(0)} + 1 -\frac1n\sum_{i=1}^p \frac{\lambda_i^2}{ \lambda_i^2r_1m_{1}(0) + r_2m_{2}(0)  } \right|\prec n^{-1/2},\\ 
&\left|\frac{1}{m_{2}(0)} + 1 -\frac1n\sum_{i=1}^p \frac{1 }{ \lambda_i^2 r_1m_{1}(0) + r_2 m_{2}(0)  }\right|\prec n^{-1/2}.
\end{split}
\ee

Denoting $y_{1}=-m_{1}(0)$ and $y_{2}=-m_{2}(0)$, by \eqref{Gii000} and \eqref{Gii001} at $z=0$ we have
$$y_1= \frac{1}{1+\gamma_n m_{0}(0)} +\OO_\prec(n^{-1/2}),\quad y_2= \frac{1}{1+\gamma_n m(0)}+\OO_\prec(n^{-1/2}).$$ 
Hence by \eqref{add_1+m}, there exists a constant $c>0$ such that 
\be\label{omega12} c \le y_1 \le 1, \quad  c\le y_2\le 1, \quad \text{with overwhelming probability}.\ee
%This shows that we can choose a sufficiently small constant $c_1$ such that 
%\be \nonumber %\label{omega12}
%c \le  \omega_2 \le 1 - \gamma_n - c,\quad c \le  \omega_3 \le 1 - \gamma_n - c , \quad \text{with overwhelming probability},
%\ee
%where we also used $1-\gamma_n - r_{1,2}\gtrsim 1$ by equation \eqref{assm2}. 
Also one can verify from equation \eqref{selfcons_lemma222} that $(r_1y_1,r_2y_2)$ satisfies approximately the same system of equations as equation \eqref{eq_a12extra}:
\be\label{selfcons_lemm000}
r_1y_1+r_2 y_2 = 1-\gamma_n + \OO_\prec (n^{-1/2}),\quad  f(r_1y_1)=r_1 + \OO_\prec (n^{-1/2}),
\ee
where recall that the function $f$ was defined in equation \eqref{fa1}. The first equation of \eqref{selfcons_lemm000} and equation \eqref{omega12} together imply that $y_1 \in [0,r_1^{-1}(1-\gamma_n)]$ with overwhelming probability. For the second equation of \eqref{selfcons_lemm000}, we know that $y_1=r_1^{-1}a_1$ is a solution. Moreover, it is easy to check that the function $f(r_1y_1)$ is strictly increasing and has bounded derivative on $[0,r_1^{-1}(1-\gamma_n)]$. So by basic calculus, %the second equation in equation \eqref{selfcons_lemm000} gives 
we obtain that 
$$|m_1(0)-m_{1c}(0)|=|y_1-r_1^{-1}a_1|\prec n^{-1/2}.$$ 
Plugging it into the first equation of \eqref{selfcons_lemm000}, we get 
$$|m_2(0)-m_{2c}(0)|=|y_2-r_2^{-1}a_2|\prec n^{-1/2}.$$ 
The above two estimates conclude \eqref{Xiz0}.
%\end{proof}

\vspace{10pt}

\noindent{\bf Step 4: Averaged local law.} Finally, we prove the averaged local laws \eqref{aver_in} and \eqref{aver_in1}. For this purpose, we need to use the following {\it{fluctuation averaging estimate}}. 

\begin{lemma}[Fluctuation averaging] \label{abstractdecoupling}
%Suppose $\Phi$ and $\Phi_o$ are positive, $N$-dependent deterministic functions on $S(c_0,C_0,\epsilon)$ satisfying $N^{-1/2} \le \Phi, \Phi_o \le N^{-c}$ for some constant $c>0$. Suppose moreover that $\Lambda \prec \Phi$ and $\Lambda_o \prec \Phi_o$. Then for all $z \in S(c_0,C_0,\epsilon)$ we have
In the setting of Lemma \ref{prop_diagonal}, suppose the entrywise local law \eqref{entry_diagonal} holds uniformly in $z\in \mathbf D$. Then we have that 
\begin{equation}\label{flucaver_ZZ}
|[\cal Z]|+|[\cal Z]_0|+|[\cal Z]_1|+|[\cal Z]_2| \prec (np)^{-1/2},
\end{equation}
uniformly in $z\in \mathbf D$.
%Fix a constant $\xi>0$. Suppose $q\le \varphi^{-5\xi}$ and that there exists $\wt  S\subseteq S(c_0,C_0,L)$ with $L\ge 18\xi$ such that with $\xi$-overwhelming probability,
%\begin{equation} 
%\Lambda(z) \le \gamma(z) \text{ for } z\in \wt  S,
%\end{equation}
%where $\gamma$ is a deterministic function satisfying $\gamma(z)\le \varphi^{-\xi}$. Then we have that with $(\xi-\tau_N)$-overwhelming probability,
%\begin{equation}
%\left|[Z]_1(z)\right|+ \left|[Z]_2(z)\right| \le \varphi^{18\xi} \left(q^2 + \frac{1}{(N\eta)^2} + \frac{\Im \, M_{2}(z) + \gamma(z)}{N\eta} \right),
%\end{equation}
%for $z\in \wt  S$, where $\tau_N:=2/\log \log N$. 
\end{lemma}
\begin{proof}
The proof is the same as the one for Theorem 4.7 of \cite{Semicircle}. % in \cite[Appendix B]{Semicircle}.
\end{proof}

Plugging \eqref{Xizz} and \eqref{flucaver_ZZ} into equations \eqref{selfcons_lemm} and \eqref{selfcons_lemm2}, and applying Lemma \ref{lem_stabw}, we obtain that 
\be\label{eq_finerm1m2}
|m_1(z)-m_{1c}(z)|+|m_2(z)-m_{2c}(z)|  \prec (np)^{-1/2}.
\ee
Now subtracting \eqref{defn mc1c} from \eqref{Gmumu},  and using \eqref{flucaver_ZZ} and \eqref{eq_finerm1m2}, we obtain that 
$$\left|m(z) - m_c(z)\right| \prec (np)^{-1/2}. $$
This is exactly the averaged local law \eqref{aver_in} with $Q=1$. The proof of \eqref{aver_in1} is similar.
% and $\Gi_{ii}(z)=-(z + \lambda_i^2 r_1m_{1c}(z)+m_{2c}(z))^{-1}$ (recall \eqref{defn_piw} and \eqref{M1M2a1a2})
%to show equation \eqref{aver_in} it suffices to prove that
%\be \label{avergoal} 
%|m_{2}-M_{2}|+|m_{3}-M_{2}|+|[Z]| \prec n^{-1}.
%\ee
%For this purpose, we need stronger bounds on $[Z]_1$ and $[Z]_2$ in (\ref{selfcons_improved}). 
%Now we give the proof of Proposition \ref{prop_entry}.
%
%%Fix $c_0,C_0>0$, $\xi> 3$ and set 
%%$$L:=120\xi, \ \ \wt  \xi:= 2/\log 2 + \xi.$$
%%Hence we have $\wt  \xi \le 2\xi$ and $L\ge 60\wt  \xi$. Then to prove (\ref{DIAGONAL}), it suffices to prove 
%%\begin{equation}\label{goal_law1}
%%\bigcap_{z \in S(c_0,C_0,L)} \left\{ \Lambda(z) \leq C\varphi^{20\wt  \xi}\left(q+ \sqrt{\frac{\operatorname{Im} M_{2}(z) }{N \eta}}+ \frac{1}{N\eta}\right) \right\},
%%\end{equation}
%%with $\xi$-overwhelming probability. %For notational convenience, we shall denote $m_c:=M_{1}+M_{2}$. 
%
%By Lemma \ref{alem_weak}, the event $\Xi$ holds with overwhelming probability. Then by Lemma \ref{alem_weak} and Lemma \ref{Z_lemma}, we can take
%\be\label{initial_phio}
%\Phi_o = \sqrt{\frac{\im M_{2} + (N\eta)^{-1/4}}{N\eta}} + \frac{1}{N\eta},\quad \Phi= \frac{1}{(N\eta)^{1/4}},
%\ee
% in Lemma \ref{abstractdecoupling}. 
%%have that
%%$\Lambda\prec |w|^{-3/8}(N\eta)^{-1/4}$. Therefore  $\theta\prec |w|^{-3/8}(N\eta)^{-1/4}$ and
%%$$\Lambda_o\prec\Psi_\theta\prec\sqrt{\frac{\Im(M_{1}+M_{2})+|w|^{-3/8}(N\eta)^{-1/4}}{N\eta}}, $$
%%where we use $|w|^{-1/2}(N\eta)^{-3/8}\ge (N\eta)^{-1}$ by the definition (\ref{eq_domainD}) of $\bD$.
%%Lemma \ref{fluc_aver} then gives 
%%\[\Phi_o = |w|^{1/2}\sqrt{\frac{\Im(M_{1}+M_{2})+|w|^{-3/8}(N\eta)^{-1/4}}{N\eta}},\ \ \ \Phi=\left(\frac{|w|^{1/2}}{N\eta}\right)^{1/4}\]
%%$\|[Z]\| + \|\langle Z \rangle\|\prec |w|^{-1/2}\Phi_o^2.$ 
%Then (\ref{selfcons_improved}) gives
%$$|f(z,m_2)| \prec\frac{ \im M_{2} + (N\eta)^{-1/4}}{N\eta}.$$
%Using Lemma \ref{stability}, we get
%\be\label{m2}
%|m_2-M_{2}|\prec\frac{\im M_{2}}{N\eta\sqrt{\kappa+\eta}}+\frac{1}{(N\eta)^{5/8}} \prec \frac{1}{(N\eta)^{5/8}} ,
%\ee
%where we used $\im M_{2}=\OO(\sqrt{\kappa+\eta})$ by equation \eqref{Immc} in the second step. With (\ref{selfcons_improved2}) and equation \eqref{m2}, we get the same bound for $m_1$, which gives
%\be\label{m1}
%\theta \prec {(N\eta)^{-5/8}} ,
%\ee
%Then using Lemma \ref{Z_lemma} and (\ref{m1}), we obtain that
%\begin{align}\label{1iteration}
%\Lambda_o \prec  \sqrt{\frac{\im M_{2} + (N\eta)^{-5/8}}{N\eta}} + \frac{1}{N\eta}
%\end{align}
%uniformly in $z\in S(c_0,C_0,\epsilon)$, which is a better bound than the one in (\ref{initial_phio}). Taking the RHS of equation \eqref{1iteration} as the new $\Phi_o$, we can obtain an even better bound for $\Lambda_o$. Iterating the above arguments, we get the bound
%$$\theta \prec \left({N\eta}\right)^{-\sum_{k=1}^l 2^{-k} - 2^{-l-2} }$$
%after $l$ iterations. This implies %the averaged local law
%\be\label{aver_proof}
%\theta\prec(N\eta)^{-1}
%\ee
%since $l$ can be arbitrarily large. Now with equation \eqref{aver_proof}, Lemma \ref{Z_lemma}, equation \eqref{Gii0} and equation \eqref{Gmumu0}, we can obtain equation \eqref{entry_diagonal}. 
%%that
%%$$\Lambda(z)\prec \Psi(z),$$
%%which proves Proposition \ref{prop_entry}.
\end{proof}

%\begin{lemma}[Weak entrywise local law]\label{alem_weak} 
%Let $c_0>0$ be a sufficiently small constant and fix $C_0,\epsilon>0$. Then we have %there exists $C>0$ such that with $\xi$-overwhelming probability,
%\begin{equation} \label{localweakm}
%\Lambda(z) \prec (N\eta)^{-1/4},
%\end{equation}
%uniformly in $z \in S(c_0,C_0,\epsilon)$.
%\end{lemma}
%\begin{proof}
%One can prove this lemma using a continuity argument as in e.g. \cite[Section 4.1]{isotropic}, \cite[Section 5.3]{Semicircle} or \cite[Section 3.6]{EKYY1}. The key inputs are Lemmas \ref{Z_lemma}-\ref{stability}, and the estimates (\ref{average_L})-(\ref{diag_L}) in the $\eta \ge 1$ case. All the other parts of the proof are essentially the same. 
%\end{proof}

%This lemma concludes equation \eqref{Xiz0}, and as explained above, concludes the proof of Lemma \ref{prop_entry}. 
%It remains to show that equation \eqref{aver_in} holds.
%


%\end{proof}
%
%
%\subsection{Proof of Proposition \ref{prop_diagonal}}
%
%Plugging equation \eqref{flucaver_ZZ} into equation \eqref{selfcons_improved}, we get 
%\begin{equation} \nonumber
%\begin{split}
%&  \left|\frac{r_1}{m_{2}} + 1 -\frac1n\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2m_{2} + m_{3}  } \right|\prec   n^{-1},\quad \left|\frac{r_2}{m_{3}} + 1 -\frac1n\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 m_{2} +  m_{3}  }\right|\prec  n^{-1} .
%  %\\ {\mathbf 1}(\Xi)\left|f(z, m_2)\right| \prec  {\mathbf 1}(\Xi)\left(\left|[Z]_1\right| + \left|[Z]_2\right|\right) + \Psi^2_\theta, 
%\end{split}
%\end{equation}
%Then using Lemma \ref{lem_stabw}, we get $|m_{2}-M_{2}|+|m_{3}-M_{2}|\prec n^{-1}$, which concludes equation \eqref{avergoal}. This concludes the proof of equation \eqref{aver_in}, and hence Lemma \ref{prop_entry}.




\subsection{Anisotropic local law}\label{sec_Gauss}

%We divide the proof of Theorem \ref{LEM_SMALL} into two steps. We first prove Theorem \ref{LEM_SMALL} in the special case where $X$ is Gaussian. Then we use a self-consistent comparison arguments developed in \cite{Anisotropic} to prove Theorem \ref{LEM_SMALL} in the general case. 
 
In this section, we prove the anisotropic local law in Theorem \ref{LEM_SMALL}  by extending from the multivariate Gaussian random matrices to generally distributed random matrices.
%\subsubsection{Comparison Argument with Continuous Interpolation}\label{sec_comparison}
%Following the above discussions, we divide the proof of Theorem \ref{LEM_SMALL} into two steps. In Section \ref{sec_Gauss}, we give the proof for separable covariance matrices of the form $\Sig^{1/2} X \wt  \Sig X^\top \Sig^{1/2}$, which implies the local laws in the Gaussian $X$ case. In Section \ref{sec_comparison}, we apply the self-consistent comparison argument in \cite{Anisotropic} to extend the result to the general $X$ case. Compared with \cite{Anisotropic}, there are two differences in our setting: (1) the support of $X$ in Theorem \ref{LEM_SMALL} is $q=\OO(N^{-\phi})$ for some constant $0<\phi \le 1/2$, while \cite{Anisotropic} dealt with $X$ with smaller support $q=\OO(N^{-1/2})$; (2) one has $B=I$ in \cite{Anisotropic}, which simplifies the proof a little bit.
\iffalse
Next we briefly describe how to extend Theorem \ref{LEM_SMALL}  from the Gaussian case to the case with general $Z^{(1)}$ and $Z^{(2)}$ satisfying the bounded support condition (\ref{eq_support}) with $Q=\sqrt{n}q=n^{\frac{2}{\varphi}}$. 
\fi
%As remarked at the beginning of Section \ref{sec_Gauss},
%Proposition \ref{prop_diagonal} implies that equation \eqref{aniso_law} holds for Gaussian $(Z^{(1)})^{\text{Gauss}}$ and $(Z^{(2)})^{\text{Gauss}}$ as discussed above. 
With Proposition \ref{prop_diagonal}, it suffices is to prove that for $Z^{(1)}$ and $Z^{(2)}$ satisfying the assumptions in Theorem \ref{LEM_SMALL}, we have
\begin{equation*}%\label{Gaussian_starting}
 \left|\mathbf u^\top  \left( G(Z,z) -  G(Z^{\text{Gauss}}, z)\right) \mathbf v \right| \prec n^{-1/2}Q 
\end{equation*}
for any deterministic unit vectors $\mathbf u,\mathbf v\in{\mathbb R}^{p+n}$ and $z\in \mathbf D$, where we abbreviated that 
$$Z:=\begin{pmatrix}Z^{(1)} \\ Z^{(2)}\end{pmatrix},\quad \text{and} \quad Z^{\text{Gauss}}:=\begin{pmatrix}(Z^{(1)})^{\text{Gauss}}\\ (Z^{(2)})^{\text{Gauss}}\end{pmatrix}.$$
Here $(Z^{(1)})^{\text{Gauss}}$ and $(Z^{(2)})^{\text{Gauss}}$ are Gaussian random matrices satisfying the assumptions in Proposition \ref{prop_diagonal}.
%Now similar to Lemma \ref{lemma_Im}, we can prove the following estimates for $\mathcal G$.
%
%\begin{lemma}\label{lem_comp_gbound}
%For $i\in \mathcal I_1$ and $\mu\in \mathcal I_2$, we define $\mathbf u_i=U^\top \mathbf e_i  \in \mathbb C^{\mathcal I_1}$ and $\mathbf v_\mu=V^\top \mathbf e_\mu  \in \mathbb C^{\mathcal I_2}$, i.e. $\mathbf u_i$ is the $i$-th row vector of $U$ and $\mathbf v_\mu$ is the $\mu$-th row vector of $V$. Let $\mathbf x \in \mathbb C^{\mathcal I_1}$ and $\mathbf y \in \mathbb C^{\mathcal I_2}$. Then we have %for some constant $C>0$,
%  \begin{align}
% & \sum_{i \in \mathcal I_1 }  \left| {G_{\mathbf x \mathbf u_i} } \right|^2  =\sum_{i \in \mathcal I_1 }  \left| {G_{ \mathbf u_i \mathbf x} } \right|^2  = \frac{|z|^2}{\eta}\im\left(\frac{ G_{\mathbf x\mathbf x}}{z}\right) , \label{eq_sgsq2} \\
%& \sum_{\mu  \in \mathcal I_2 } {\left| {G_{\mathbf y \mathbf v_\mu } } \right|^2 }=\sum_{\mu  \in \mathcal I_2 } {\left| {G_{\mathbf v_\mu \mathbf y } } \right|^2 }  = \frac{{\im G_{\mathbf y\mathbf y} }}{\eta }, \label{eq_sgsq1}\\ 
%& \sum_{i \in \mathcal I_1 } {\left| {G_{\mathbf y \mathbf u_i} } \right|^2 } =\sum_{i \in \mathcal I_1 } {\left| {G_{ \mathbf u_i \mathbf y} } \right|^2 } = {G}_{\mathbf y\mathbf y}  +\frac{\bar z}{\eta} \im G_{\mathbf y\mathbf y}  , \label{eq_sgsq3} \\
%& \sum_{\mu \in \mathcal I_2 } {\left| {G_{\mathbf x \mathbf v_\mu} } \right|^2 }= \sum_{\mu \in \mathcal I_2 } {\left| {G_{\mathbf v_\mu \mathbf x } } \right|^2 }= \frac{G_{\mathbf x\mathbf x}}{z}  + \frac{\bar z}{\eta} \im \left(\frac{G_{\mathbf x\mathbf x}}{z}\right) .\label{eq_sgsq4}
% \end{align}
% All of the above estimates remain true for $G^{(\mathbb T)}$ instead of $G$ for any $\mathbb T \subseteq \mathcal I$. 
%\end{lemma}
%%\begin{proof}
%%The proof is almost the same as the proof of Lemma \ref{lemma_Im}, except that we use
%%$$ \sum_{i\in \mathcal I_1}\mathbf v_i \mathbf v_i^\dag = V_1 V_1^\dag = I_{N\times N}.$$
%%\end{proof}
%\begin{proof}
%We only prove equation \eqref{eq_sgsq1} and equation \eqref{eq_sgsq3}. The proof for equation \eqref{eq_sgsq2} and equation \eqref{eq_sgsq4} is very similar. With  equation \eqref{spectral1}, we get that
%\begin{align}\label{middle}
%\sum_{\mu  \in \mathcal I_2 } {\left| {G_{\mathbf y \mathbf v_\mu } } \right|^2 } =& \sum_{\mu  \in \mathcal I_2 } \left\langle \mathbf y,G {\mathbf v_\mu  } \right\rangle \left\langle {\mathbf v_\mu}, G^\dag \mathbf y \right\rangle  = \sum_{k = 1}^N {\frac{{\left| {\left\langle {\mathbf y,\zeta _k } \right\rangle } \right|^2  }}{{\left( {\lambda _k  - E} \right)^2  + \eta ^2 }} }   =\frac{{\im  G_{\mathbf y\mathbf y} }}{\eta }.
%\end{align}
%For simplicity, we denote $Y:=\Sig^{1/2} U^{*}X V\wt  \Sig^{1/2}$. Then with equation \eqref{green2} and equation \eqref{spectral2}, we get that
%\begin{align*}
% \sum_{i \in \mathcal I_1 } {\left| {G_{\mathbf y\mathbf u_i} } \right|^2 } =  \left( {{\mathcal G_2} Y^\dag Y \mathcal G_2^\dag  } \right)_{\mathbf y\mathbf y}=  \left( {{\mathcal G_2} \left(Y^\dag Y-\bar z\right) \mathcal G_2^\dag  } \right)_{\mathbf y\mathbf y} + \bar z \left( {{\mathcal G_2} \mathcal G_2^\dag  } \right)_{\mathbf y\mathbf y} =  {G}_{\mathbf y\mathbf y}  +\frac{\bar z}{\eta} \im G_{\mathbf y\mathbf y}  ,
% \end{align*}
% where we used $\mathcal G_2^\dag= \left(Y^\dag Y-\bar z\right)^{-1}$ and equation \eqref{middle} in the last step.
%\end{proof}
%
%
%%\subsection{Bootstrapping on the spectral scale}
%%\begin{subsection}{Self-consistent comparison}\label{subsection_selfcomp}
%Our proof basically follows the arguments in \cite[Section 7]{Anisotropic} with some modifications. Thus we will not give all the details. We first focus on proving the anisotropic local law equation \eqref{aniso_law}, and the proof of equation \eqref{aver_in1}-equation \eqref{aver_out1} will be given at the end of this section. By polarization, to prove equation \eqref{aniso_law} it suffices to prove that %the following bound:
% \begin{equation}\label{goal_ani2}
%\left\langle \mathbf v, \left(G(X,z)- \Pi(z)\right) \mathbf v \right\rangle \prec q+\Psi(z)
%\end{equation}
%uniformly in $z\in \wt  S(c_0,C_0,\e)$ and any deterministic unit vector $ \mathbf v\in{\mathbb C}^{\mathcal I}$. In fact, we can obtain the more general bound equation \eqref{aniso_law}
%%\begin{equation*}%\label{goal_ani}
%%\left\langle \mathbf u, \left(G(X,z) - \Pi(z)\right) \mathbf v \right\rangle \prec \Psi(z)
%%\end{equation*}
%by applying (\ref{goal_ani2}) to the vectors $\mathbf u + \mathbf v$ and $\mathbf u + i\mathbf v$, respectively.
%
%%\begin{proposition}\label{comparison_prop}
%%Suppose the assumptions of Theorem \ref{LEM_SMALL} hold.  Fix ${\left| z \right|^2 } \le 1 - \tau$ and suppose that the assumptions of Theorem \ref{law_wideT} hold. If (\ref{assm_3rdmoment}) holds or $\eta \ge N^{-1/2+\zeta}|M_{2}|^{-1}$, then for any regular domain $\mathbf S \subseteq \mathbf D$,
%% \begin{equation}\label{goal_ani2}
%%\left\langle \mathbf v, \left( G(w)-\Pi(w)\right) \mathbf v \right\rangle \prec \Psi(z)
%%\end{equation}
%%uniformly in $w\in \bS$ and any deterministic unit vectors $ \mathbf v\in{\mathbb C}^{\mathcal I}$.
%%\end{proposition}
%
%%We first assume that (\ref{assm_3rdmoment}) holds. Then we will show how to modify the arguments to prove the $\eta \ge N^{-1/2+\zeta}|M_{2}|^{-1}$ case.
%The proof consists of a bootstrap argument from larger scales to smaller scales in multiplicative increments of $N^{-\delta}$, where
%\begin{equation}
% \delta \in\left(0,\frac{\min\{\epsilon,\phi\}}{2C_a}\right). \label{assm_comp_delta}
%\end{equation}
%Here $\e>0$ is the constant in $\wt  S(c_0,C_0,\e)$, $\phi>0$ is a constant such that $q\le N^{-\phi}$, $C_a> 0$ is an absolute constant that will be chosen large enough in the proof. For any $\eta\ge N^{-1+\e}$, we define
%\begin{equation}\label{eq_comp_eta}
%\eta_l:=\eta N^{\delta l} \text{ for } \ l=0,...,L-1,\ \ \ \eta_L:=1.
%\end{equation}
%where
%%\begin{equation}\label{eq_comp_L}
%$L\equiv L(\eta):=\max\left\{l\in\mathbb N|\ \eta N^{\delta(l-1)}<1\right\}.$
%%\end{equation}
%%through
%% \begin{equation}\label{eq_comp_eta}
%%  \eta_l:=\eta N^{\delta l}\ \ l=0,...,L-1,\ \ \ \eta_L:=1.
%% \end{equation}
%Note that $L\le \delta^{-1}$.
%
%By (\ref{eq_gbound}), the function $z\mapsto G(z)- \Pi(z)$ is Lipschitz continuous in $\wt  S(c_0,C_0,\e)$ with Lipschitz constant bounded by $N^2$. Thus to prove (\ref{goal_ani2}) for all $z\in \wt  S(c_0,C_0,\e)$, it suffices to show that (\ref{goal_ani2}) holds for all $z$ in some discrete but sufficiently dense subset ${\mathbf S} \subset \wt  S(c_0,C_0,\e)$. We will use the following discretized domain $\bS$.
%\begin{definition}
%Let $\mathbf S$ be an $N^{-10}$-net of $\wt  S(c_0,C_0,\e)$ such that $ |\mathbf S |\le N^{20}$ and
%\[E+\ii\eta\in\mathbf S\Rightarrow E+\ii\eta_l\in\mathbf S\text{ for }l=1,...,L(\eta).\]
%\end{definition}
%
%The bootstrapping is formulated in terms of two scale-dependent properties ($\bA_m$) and ($\bC_m$) defined on the subsets
%\[\mathbf S_m:=\left\{z\in\mathbf S\mid\text{Im} \, z\ge N^{-\delta m}\right\}.\]
%${(\bA_m)}$ For all $z\in\mathbf S_m$, all deterministic unit vectors $\mathbf x \in \mathbb C^{\mathcal I_1}$ and $\mathbf y \in \mathbb C^{\mathcal I_2}$, and all $X$ satisfying the assumptions in Theorem \ref{LEM_SMALL}, we have
%\begin{equation}\label{eq_comp_Am}
% \im \left(\frac{G_{\mathbf x\mathbf x}(z)}{z}\right) + \im G_{\mathbf y\mathbf y}(z)\prec \im M_{2}(z) +N^{C_a\delta}(q+\Psi(z)).
%\end{equation}
%${(\bC_m)}$ For all $z\in\mathbf S_m$, all deterministic unit vector $\mathbf v\in \mathbb C^{\mathcal I}$, and all $X$ satisfying the assumptions in Theorem \ref{LEM_SMALL}, %(\ref{assm1})-(\ref{assm2}), 
%we have
%\begin{equation}\label{eq_comp_Cm}
% \left|G_{\mathbf v\mathbf v}(z)-\Pi_{\mathbf v\mathbf v}(z)\right|\prec N^{C_a\delta}(q+\Psi(z)).
%\end{equation}
%%The bootstrapping is started by the following result
%%\begin{lemma}\label{lemm_boot0}
%It is trivial to see that ${(\mathbf A_0)}$ holds by equation \eqref{eq_gbound} and equation \eqref{Immc}. Moreover, it is easy to observe the following result.
%%\end{lemma}
%%\begin{proof}
%% By Lemma \ref{lemma_Im} and the assumption (\ref{assm3}), we have for $w\in\widehat\bS_0$,
%% \[\text{Im} G_{\mathbf{vv}}(w)\le C |w|^{1/2}\left\|G(w)\right\| \le \frac{C}{\eta}\le C |w|^{1/2}\Im \left[M_{1}(w)+M_{2}(w)\right],\]
%%where we use (\ref{estimate1_bulk}) for $w\in{\widehat\bS}_0$.
%%\end{proof}
%
%\begin{lemma}\label{lemm_boot2}
%For any $m$, property ${(\mathbf C_m)}$ implies property $(\mathbf A_m)$.
%\end{lemma}
%\begin{proof}
%By equation \eqref{Immc}, equation \eqref{Piii} and the definition of $\Pi$ in equation \eqref{defn_pi}, it is easy to get that 
%$$\im \left(\frac{\Pi_{\mathbf x\mathbf x}(z)}{z}\right) + \im \Pi_{\mathbf y\mathbf y}(z)\lesssim \im M_{2}(z) ,$$
%%$\im \Pi_{\bv\bv}=\OO(\im M_{2})$, 
%which finishes the proof.
%%Suppose property $(\mathbf C_m)$ holds. By (\ref{def_PiPhi}), we have
%%\begin{align*}
%%\widetilde \Pi_{\mathbf v\mathbf v} = |w|^{1/2} \left\langle \mathbf v, \overline T^\dag \Pi \overline T \mathbf v \right\rangle = |w|^{1/2} \left(\Pi_d \right)_{ {\mathbf u} {\mathbf u}} ,
%%\end{align*}
%%where $ \mathbf u = \bar T  \mathbf v.$ Now (\ref{estimate_PiImw}) implies
%%\begin{equation}\label{eqn_ImPi}
%%\Im\, \widetilde \Pi_{\mathbf v\mathbf v} \le C \Im(M_{1}+M_{2}),
%%\end{equation}
%%and further
%%$$\text{Im} G_{\mathbf{vv}}(w)\le\text{Im}\, \Pi_{\mathbf {vv}}+\left| G_{\mathbf{vv}}(w)-\Pi_{\mathbf{vv}}(w)\right|\prec|w|^{1/2}\text{Im}\left[M_{1}(w)+M_{2}(w)\right]+N^{C_a\delta}\Psi(z).$$
%%Thus the property $(\mathbf A_m)$ follows.
%\end{proof}
%
%The key step is the following induction result.
%\begin{lemma}\label{lemm_boot}
%For any $1\le m\le \delta^{-1}$, property $(\mathbf A_{m-1})$ implies property $(\mathbf C_m)$.
%\end{lemma}
%
%Combining Lemmas \ref{lemm_boot2} and \ref{lemm_boot}, we conclude that (\ref{eq_comp_Cm}) holds for all $w\in\mathbf S$. Since $\delta$ can be chosen arbitrarily small under the condition (\ref{assm_comp_delta}), we conclude that (\ref{goal_ani2}) holds for all $w\in\mathbf S$, and equation \eqref{aniso_law} follows for all $z\in \wt  S(c_0,C_0,\e)$. What remains now is the proof of Lemma \ref{lemm_boot}. Denote
%\begin{equation}\label{eq_comp_F(X)}
% F_{\mathbf v}(X,z):=\left|G_{\mathbf{vv}}(X,z)-\Pi_{\mathbf {vv}}(z)\right|.
%\end{equation}
%By Markov's inequality, it suffices to prove the following lemma.
%\begin{lemma}\label{lemm_comp_0}
% Fix $p\in \mathbb N$ and $m\le \delta^{-1}$. Suppose that the assumptions of Theorem \ref{LEM_SMALL} and property $(\mathbf A_{m-1})$ hold. Then we have
% \begin{equation}
%  \mathbb EF_{\mathbf v}^p(X,z)\le\left[ N^{C_a\delta}\left(q+\Psi(z)\right)\right]^p
% \end{equation}
% for all $z\in{\mathbf S}_m$ and any deterministic unit vector $\mathbf v$.
%\end{lemma}
%In the rest of this section, we focus on proving Lemma \ref{lemm_comp_0}. 
%%\begin{subsubsection}{Rough bound}
%First, in order to make use of the assumption $(\mathbf A_{m-1})$, which has spectral parameters in $\mathbf S_{m-1}$, to get some estimates for $G$ with spectral parameters in $\mathbf S_{m}$, we shall use the following rough bounds for $ G_{\mathbf{xy}}$.
%
%\begin{lemma}\label{lemm_comp_1}
%For any $z=E+\ii\eta\in\mathbf S$ and unit vectors $\mathbf x,\mathbf y\in \mathbb C^{\mathcal I}$,  we have %{\cor need to revise}
%\begin{align*}
%\left|G_{\mathbf x\mathbf y}(z)-\Pi_{\mathbf x\mathbf y}(z)\right|\prec & N^{2\delta}\sum_{l=1}^{L(\eta)} \left[\im \left(\frac{G_{\mathbf X^{(1)}\mathbf X^{(1)}}(E+\ii\eta_l)}{E+\ii\eta_l}\right)+\im G_{\mathbf X^{(2)}\mathbf X^{(2)}}(E+\ii\eta_l) \right.\\
%& \left. +\im \left(\frac{G_{\mathbf y_1\mathbf y_1}(E+\ii\eta_l)}{E+\ii\eta_l}\right)+\im G_{\mathbf y_2\mathbf y_2}(E+\ii\eta_l)\right]+1,
%\end{align*}
%where $\mathbf x=\left( {\begin{array}{*{20}c}
%   {\mathbf x}_1   \\
%   {\mathbf x}_2 \\
%   \end{array}} \right)$ and $\mathbf y=\left( {\begin{array}{*{20}c}
%   {\mathbf y}_1   \\
%   {\mathbf y}_2 \\
%   \end{array}} \right)$ for ${\mathbf x}_1,{\mathbf y}_1\in\mathbb C^{\mathcal I_1}$ and ${\mathbf x}_2,{\mathbf y}_2\in\mathbb C^{\mathcal I_2}$, and $\eta_l$ is defined in (\ref{eq_comp_eta}).
%%recall that $L(\eta)$ and $\eta_l$ are defined in $(\ref{eq_comp_L})$ and $(\ref{eq_comp_eta})$.
%\end{lemma}
%\begin{proof} The proof is the same as the one for \cite[Lemma 7.12]{Anisotropic}.\end{proof}
%%\begin{proof}
%%By (\ref{estimate_Piw12}) and the definition of $\widetilde \Pi$ in (\ref{def_PiPhi}), we get that $\left|\Pi_{\mathbf x\mathbf y}\right| \le |\mathbf x||\mathbf y|.$ Thus it suffices to estimate $|\mathcal G_{\mathbf{xy}}|$. By the definition of $\mathcal G$ in (\ref{def_mathcalg}), we see that $\mathcal G_{\mathbf{xy}}=R_{\mathbf{\bar x \bar y}}$ for $R:=|w|^{1/2}G$ and $\bar {\mathbf u}:=\overline T\mathbf u$ for $\mathbf u\in\{\mathbf x, \mathbf y\}$.
%%Using the singular value decomposition (\ref{singular_rep}) we get that
%%\begin{equation}\label{eqn_roughbound1}
%%\left|R_{\bar{\mathbf x}_1\bar{\mathbf y}_1}\right|=\left|\inprod{\bar{\mathbf x}_1,|w|^{1/2} \sum \limits_{ k=1 }^{ N } \frac { \xi_k \xi_k ^{ \dag  } }{ \lambda _{ k }-w } \bar{\mathbf y}_1}\right|\le |w|^{1/2} \sum \limits_{ k=1 }^{ N }\frac{|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}+ |w|^{1/2} \sum \limits_{ k=1 }^{ N }\frac{|\inprod{\bar{\mathbf y}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|},
%%\end{equation}
%%and
%%\begin{equation}\label{eqn_roughbound2}
%%\left|R_{\bar{\mathbf x}_1\bar{\mathbf y}_2}\right| = \left|\inprod{\bar{\mathbf x}_1,w^{-1/2}|w|^{1/2} \sum_{k=1}^{ N } \frac { \sqrt{\lambda_k}\xi_k \zeta_{\bar k}^\dag }{\lambda_k-w } \bar{\mathbf y}_2}\right|\le \sum \limits_{ k=1 }^{ N }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}+\sum \limits_{ k=1 }^{ N }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf y}_2,\zeta_{\bar k}}|^2}{2\left|\lambda_k-w\right|}.
%%\end{equation}
%%
%%%where the second step is by
%%%\begin{align*}
%%% \frac{\sqrt{\lambda_k} }{|\lambda^k-w|}=&\sqrt{\frac{\lambda_k}{\lambda_k^2+E^2+\eta^2-2E\lambda_k}}\\
%%% \le &\sqrt{\frac{\lambda_k}{2\lambda_k\sqrt{E^2+\eta^2}-2E\lambda_k}}\\
%%% =&\sqrt{\frac{\sqrt{E^2+\eta^2}+E}{2\eta^2}}\le \frac{|w|^{1/2}}{\eta}.
%%%\end{align*}
%%%{\color{red} \[R(w)=\begin{pmatrix} |w|^{1/2} \sum \limits_{ k=1 }^{ N }\frac { \xi_k \xi_k ^{ \dag  } }{ \lambda _{ k }-w }  & w^{-1/2}|w|^{1/2}\sum \limits_{ k=1 }^{ N } \frac { \sqrt \lambda_k\xi_k \zeta_k ^{ \dag  } }{ \lambda _{ k }-w }  \\ w^{-1/2}|w|^{1/2}\sum \limits_{ k=1 }^{ N } \frac { \sqrt\lambda_k\zeta_k \xi_k ^{ \dag  } }{ \lambda _{ k }-w }  & |w|^{1/2}\sum \limits_{ k=1 }^{ N } \frac { \zeta_k \zeta_k ^{ \dag  } }{ \lambda _{ k }-w }  \end{pmatrix}.\]}
%%Recall the notation $\eta_l$ in (\ref{eq_comp_eta}), define the subsets of indices
%%\[U_l=\{k\mid\eta_{l-1}\le|\lambda_k-E|<\eta_l\},\ \ l=0,...,L+1,\]
%%where we set $\eta_{-1}=0$ and $\eta_{L+1}=\infty$. Now we split the summation in (\ref{eqn_roughbound2}) according to $U_l$. For $l=1,...,L$ we have
%%\begin{align*}
%% \sum \limits_{ k\in U_l }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}\le& \sum_{k\in U_l}\frac{\sqrt{E+\eta_l}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_l}{2(\lambda_k-E)^2} \le \sum_{k\in U_l}\frac{({2E^2+2\eta_l^2})^{1/4}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_l}{(\lambda_k-E)^2+\eta_{l-1}^2}\\
%% \le& 2^{1/4}N^{2\delta}\sum_{k\in U_l}\frac{|{E+\ii\eta_l}|^{1/2}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_l}{(\lambda_k-E)^2+\eta_{l}^2} = 2^{1/4}N^{2\delta}\text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}\left(E+\ii\eta_l\right);
%%\end{align*}
%%for $l=0$,
%%\begin{align*}
%% \sum \limits_{ k\in U_0 }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}\le& \sum_{k\in U_0}\frac{\sqrt{E+\eta}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2 \sqrt{2}\eta}{2\left[(\lambda_k-E)^2+\eta^2\right]} \le 2^{-1/4}N^{2\delta}\sum_{k\in U_0}\frac{|{E+\ii\eta_1}|^{1/2}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_1}{(\lambda_k-E)^2+\eta_1^2}\\
%% =&2^{-1/4}N^{\delta}\text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}\left(E+\ii\eta_1\right);
%%\end{align*}
%%and for $l={L+1}$,
%%\begin{align*}
%% \sum \limits_{ k\in U_{L+1} }\frac{\sqrt{\lambda_k}\inprod{\bar{\mathbf x}_1,\xi_k}^2}{2\left|\lambda_k-w\right|} & \le \sum_{k\in U_{L+1}}\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2 \sqrt{2}|\lambda_k-E|}{2\left[(\lambda_k-E)^2+\eta_L^2\right]} \prec \sum_{k\in U_{L+1}}\frac{|\inprod{\bar{\mathbf x}_1,\xi_k}|^2\eta_L}{(\lambda_k-E)^2+\eta_{L}^2} \\
%% & \le C \text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}\left(E+\ii\eta_L\right),
%%\end{align*}
%%where in the second step we used that $\lambda_k\prec 1$, which follows from (\ref{norm_upperbound}). Combining the above estimates we get that
%%\[\sum \limits_{ k=1 }^{ N }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf x}_1,\xi_k}|^2}{2\left|\lambda_k-w\right|}\prec N^{2\delta}\sum_{l=1}^{L}\text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}(E+\ii\eta_l).\]
%%Similarly, we can prove that
%%\[\sum \limits_{ k=1 }^{ N }\frac{\sqrt{\lambda_k}|\inprod{\bar{\mathbf y}_2,\zeta_k}|^2}{\left|\lambda_k-w\right|}\prec N^{2\delta}\sum_{l=1}^{L}\text{Im}\, R_{\bar{\mathbf y}_2\bar{\mathbf y}_2}(E+\ii\eta_l).\]
%%Since $|E+\ii\eta|\le|E+\ii\eta_l|$, we immediately get
%%\[|w|^{1/2}\sum \limits_{ k=1 }^{ N }\frac{\inprod{\bar{\mathbf u}_1,\xi_k}^2}{\left|\lambda_k-w\right|}\prec N^{2\delta}\sum_{l=1}^{L}\text{Im}\, R_{\bar{\mathbf u}_1\bar{\mathbf u}_1}(E+\ii\eta_l)\]
%%for $\mathbf u \in \{\mathbf x, \mathbf y\}$.
%%%and
%%%\[|w|^{1/2}\sum \limits_{ k=1 }^{ N }\frac{\inprod{\bar{\mathbf y}_1,\xi_k}^2}{\left|\lambda_k-w\right|}\prec N^{2\delta}\sum_{l=1}^{L}\text{Im}R_{\bar{\mathbf y}_1\bar{\mathbf y}_1}(E+\ii\eta_l)\]
%%Plugging into (\ref{eqn_roughbound1}) and (\ref{eqn_roughbound2}), we get that
%%\[|R_{\bar{\mathbf x}_1\bar{\mathbf y}_1}|+|R_{\bar{\mathbf x}_1\bar{\mathbf y}_2}|\prec N^{2\delta}\sum_{l=1}^{L(\eta)}\left[\text{Im}\, R_{\bar{\mathbf x}_1\bar{\mathbf x}_1}(E+\ii\eta_l)+\text{Im}\, R_{\bar{\mathbf y}_1\bar{\mathbf y}_1}(E+\ii\eta_l)+\text{Im}\, R_{\bar{\mathbf y}_2\bar{\mathbf y}_2}(E+\ii\eta_l)\right].\]
%%We can bound $|R_{\bar{\mathbf x}_2 \bar{\mathbf y}_1}|$ and $|R_{\bar{\mathbf x}_2\bar{\mathbf y}_2}|$ in a similar way. This concludes the proof.
%%\end{proof}
%
%Recall that for a given family of random matrices $A$, we use $A=O_\prec(\zeta)$ to mean $\left|\left\langle\mathbf v, A\mathbf w\right\rangle\right|\prec\zeta \| \mathbf v\|_2 \|\mathbf w\|_2 $ uniformly in any deterministic vectors $\mathbf v$ and $\mathbf w$ (see Definition \ref{stoch_domination} (ii)).
%
%\begin{lemma}\label{lemm_comp_2}
%Suppose $(\mathbf A_{m-1})$ holds, then
% \begin{equation}\label{eq_comp_apbound}
%  G(z)-\Pi(z)=\OO_{\prec}(N^{2\delta}),
% \end{equation}
% and
%%for all $w\in \mathbf S_m$. Moreover for any unit vector $\mathbf v$ we have
%\begin{equation}\label{eq_comp_apbound2}
%\im \left(\frac{G_{\mathbf x\mathbf x}(z)}{z}\right) + \im G_{\mathbf y\mathbf y}(z) \prec N^{2\delta}\left[ \im M_{2}(z)+N^{C_a\delta}(q+\Psi(z))\right],
%\end{equation}
% for all $z \in \mathbf S_m$ and any deterministic unit vectors $\mathbf x \in \mathbb C^{\mathcal I_1}$ and $\mathbf y \in \mathbb C^{\mathcal I_2}$.
%\end{lemma}
%\begin{proof} The proof is the same as the one for \cite[Lemma 7.13]{Anisotropic}.\end{proof}
%%\begin{proof}
%%Let $z=E+\ii\eta \in \mathbf S_m$. Then $E+\ii\eta_l \in \mathbf S_{m-1}$ for $l=1,\ldots, L(\eta)$, and (\ref{eq_comp_Am}) gives $\im G_{\mathbf v\mathbf v}(z)\prec 1.$ The estimate (\ref{eq_comp_apbound}) now follows immediately from Lemma \ref{lemm_comp_1}. To prove (\ref{eq_comp_apbound2}), we remark that if $s(w)$ is the Stieltjes transform of any positive integrable function on $\mathbb R$, the map $\eta \mapsto \eta\Im\, s(E+\ii\eta)$ is nondecreasing and the map $\eta \mapsto \eta^{-1} \im s(E+\ii\eta)$ is nonincreasing. We apply them to $\im G_{\mathbf v\mathbf v}(E+\ii\eta)$ and $\im M_{2}(E+\ii\eta)$ to get for $Z^{(1)}=E+\ii\eta_1\in \mathbf S_{m-1}$,
%%\begin{align*}
%%\im G_{\mathbf v\mathbf v}(w) & \le N^{\delta}\frac{|w|^{1/2}}{|w_1|^{1/2}}\im G_{\mathbf v\mathbf v}(w_1)\prec N^{\delta}\left[|w|^{1/2}\im\left(M_{1}(w_1)+M_{2}(w_1)\right)+N^{C_a\delta}\frac{|w|^{1/2}}{|w_1|^{1/2}}\Phi(w_1)\right] \\
%%& \le N^{2\delta}\left[|w|^{1/2}\im\left(M_{1}(w)+M_{2}(w)\right)+N^{C_a\delta}\Psi(z)\right],
%%\end{align*}
%%where we used $\Psi(z):=|w|^{1/2}\Psi(w)$ and the fact that $\eta \mapsto \Psi(E+\ii\eta)$ is nonincreasing, which is clear from the definition (\ref{eq_defpsi}).
%%\end{proof}
%%\end{subsubsection}
We will prove the above statement using a continuous comparison argument developed in \cite{Anisotropic}. %to prove Lemma \ref{lemm_comp_0}. 
%The proof is similar to the ones in Sections 7 and 8 of \cite{Anisotropic}, so we only describe briefly the basic ideas, without writing down all the details. 
%We divide the proof into three subsections. In Sections \ref{subsec_interp}-\ref{section_words}, we prove Lemma \ref{lemm_comp_0} under the condition 
Since the arguments are similar to the one in Sections 7 and 8 of \cite{Anisotropic} and Section 6 of \cite{yang2019spiked}, we will not write down all the details.

We define the following continuous sequence of interpolating matrices between $Z^{\text{Gauss}}$ and $Z$. 

%\begin{subsection}{Interpolation and expansion} \label{subsec_interp}
%The self-consistent comparison is performed with the following interpolation.
\begin{definition}[Interpolation]\label{defn_interp}
%Introduce the notation $X^0:=X^{\text{Gauss}}$ and $X^1:=X$. We define the interpolation matrix $X^\theta$ by setting
%\begin{equation}
% X^\theta_{i\mu}:=\chi^\theta_{i\mu} X^1+(1-\chi^\theta_{i\mu})X^0, \ \ \theta\in [0,1],
%\end{equation}
%for $i\in \mathcal I_1$ and $\mu\in \mathcal I_2$ (recall the Definition \ref{def_indexsets}). Here $(\chi^\theta_{i\mu})$ is a family of i.i.d Bernoulli random variables, independent of $X^0$ and $X^1$, satisfying $\bbP(\chi_{i\mu}^\theta=1)=\theta$ and $\bbP(\chi_{i\mu}^\theta=0)=1-\theta$.
We denote $Z^0:=Z^{\text{Gauss}}$ and $Z^1:=Z$. Let $\rho_{\mu i}^0$ and $\rho_{\mu i}^1$ be the laws of $Z_{\mu i}^0$ and $Z_{\mu i}^1$, respectively, for $i\in \cal I_0$ and $\mu \in \cal I_1\cup \cal I_2$. For any $\theta\in [0,1]$, we define the interpolated law
$\rho_{\mu i}^\theta := (1-\theta)\rho_{\mu i}^0+\theta\rho_{\mu i}^1.$ We shall work on the probability space consisting of triples $(Z^0,Z^\theta, Z^1)$ of independent $n\times p$ random matrices, where the matrix $Z^\theta=(Z_{\mu i}^\theta)$ has law
\begin{equation}\label{law_interpol}
\prod_{i\in \mathcal I_0}\prod_{\mu\in \mathcal I_1\cup \cal I_2} \rho_{\mu i}^\theta(\dd Z_{\mu i}^\theta).
\end{equation}
For $\lambda \in \mathbb R$, $i\in \mathcal I_0$ and $\mu\in \mathcal I_1\cup \cal I_2$, we define the matrix $Z_{(\mu i)}^{\theta,\lambda}$ through
\[\left(Z_{(\mu i)}^{\theta,\lambda}\right)_{\nu j}:=\begin{cases}Z_{\mu i}^{\theta}, &\text{ if }(j,\nu)\ne (i,\mu)\\ \lambda, &\text{ if }(j,\nu)=(i,\mu)\end{cases},\]
that is, it replaces the $(\mu,i)$-th entry of $Z^\theta$ with $\lambda$.
We also abbreviate 
$$G^{\theta}(z):=G\left(Z^{\theta},z\right),\quad G^{\theta, \lambda}_{(\mu i)}(z):=G\left(Z_{(\mu i)}^{\theta,\lambda},z\right).$$
%according to (\ref{def_mathcalg}) and the Definition \ref{def_linearHG}.
\end{definition}


%\begin{proof}[Proof of Theorem \ref{LEM_SMALL} ]
We shall prove the anisotropic local law \eqref{aniso_law} through interpolating matrices $Z^\theta$ between $Z^0$ and $Z^1$. We have seen that \eqref{aniso_law} holds for $G(Z^0,z)$ by Proposition \ref{prop_diagonal}. %as remarked at the beginning of Section \ref{sec_Gauss}.
%\begin{lemma}\label{Gaussian_case}
%Lemma \ref{lemm_comp_0} holds if $X=X^0$.
%\end{lemma}
%\begin{proof}
%As remarked above (\ref{Gaussian_starting}), the anisotropic law (\ref{goal_ani}) holds for $X^0$, i.e. $F_{\mathbf v}^p(X^0,w)\prec \Phi^p$. Now to apply (iii) of Lemma \ref{lem_stodomin}, we need an upper bound $\mathbb E\left(F_{\mathbf v}^p(X^0,w)\right)^2 \le N^{C_p}$ for some constant $C_p$. This follows easily from (\ref{eq_gbound}) and
%(\ref{estimate_Piw12}).
%\end{proof}
Using (\ref{law_interpol}) and fundamental calculus, we get the following basic interpolation formula:
%\begin{lemma}\label{lemm_comp_3}
for any differentiable $F:\mathbb R^{n \times p}\rightarrow \mathbb C$,
\begin{equation}\label{basic_interp}
\frac{\dd}{\dd\theta}\mathbb E F(Z^\theta)=\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left[\mathbb E F\left(Z^{\theta,Z_{\mu i}^1}_{(\mu i)}\right)-\mathbb E F\left(Z^{\theta,Z_{\mu i}^0}_{(\mu i)}\right)\right],
\end{equation}
 provided all the expectations exist.
%\end{lemma}
We shall apply equation \eqref{basic_interp} to the function $F(Z):=F_{\bu\mathbf v}^s(Z,z)$ for any fixed $s\in 2\N$, where %$F_{\mathbf u\mathbf v}(Z,z)$ defined as
\begin{equation}\label{eq_comp_F(X)}
 F_{\bu\mathbf v}(Z,z):=\left|\mathbf u^\top \left(G (Z,z)-\Gi(z)\right)\mathbf v\right|.
\end{equation}
%Here for simplicity of notations, we introduce the following notation of generalized entries: for $\mathbf u,\mathbf v \in \mathbb R^{\mathcal I}$, we shall denote $
%G_{\mathbf{uv}}:= \mathbf u^\top G\mathbf v   . %\quad G_{a\mathbf{w}}:=\langle \mathbf e_a,G\mathbf w\rangle,
%$
%Moreover, we shall abbreviate $G_{\mathbf{u}{a}}:= G_{\bu\mathbf e_{{a}}}$ for ${a}\in \mathcal I$, where $\mathbf e_{{a}}$ is the standard unit vector along ${a}$-th axis. Given any vector $\mathbf u\in \mathbb \R^{\mathcal I_{1,2,3}}$, we always identify it with its natural embedding in $\mathbb R^{\mathcal I}$. 
%%$\left( {\begin{array}{*{20}c}
%%   {\mathbf x}  \\
%%   0 \\
%%\end{array}} \right)$ and $\left( {\begin{array}{*{20}c}
%%   0  \\
%%   \mathbf y \\
%%\end{array}} \right)$ in $\mathbb C^{\mathcal I}$.
%The exact meanings will be clear from the context. 
The main part of the proof is to show the following self-consistent estimate for the right-hand side of  (\ref{basic_interp}): for any fixed $s\in 2\N$, any constant $\e>0$ and all $\theta\in[0,1]$,
%\begin{lemma}\label{lemm_comp_4}
 %Fix $p\in 2\mathbb N$ and $m\le \delta^{-1}$. Suppose (\ref{3moment}) and $\mathbf{(A_{m-1})}$ hold, then we have
 \begin{equation}\label{lemm_comp_4}
  \sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left[\mathbb EF_{\bu\mathbf v}^s\left(Z^{\theta,Z_{\mu i}^1}_{(\mu i)},z\right)-\mathbb EF_{\bu\mathbf v}^s\left(Z^{\theta,Z_{\mu i}^0}_{(\mu i)},z\right)\right]\le (n^\e q)^{s}+C\E F_{\bu\mathbf v}^s (Z^{\theta},z ) ,
 \end{equation}
 for a constant $C>0$. Here and throughout the following proof, we abbreviate 
 $$q:=n^{-1/2}Q.$$
 If \eqref{lemm_comp_4} holds, then combining equation \eqref{basic_interp} with  Gr\"onwall's inequality we obtain that for any fixed $s\in 2\N$ and constant $\e>0$, 
 \be\label{lemm_comp_4.4}\E\left|\bu^\top \left(G(Z^1,z)-\Pi(z)\right)\bv\right|^s  \lesssim (n^\e q)^{s}.\ee
Finally applying Markov's inequality and noticing that $\e$ can be chosen arbitrarily small, we conclude  \eqref{aniso_law}. 
%we can conclude Lemma \ref{lemm_comp_0} and hence equation \eqref{goal_ani2}. %Theorem \ref{LEM_SMALL}.%Proposition \ref{comparison_prop}.

%\end{proof}
%This follows from an improved self-consistent comparison argument for sample covariance matrices in \cite[Section 8]{Anisotropic}. The argument for our case is almost the same except for some notational differences, so we omit the details. 

%-------------remove-------------
%Throughout the rest of the proof, 
%During the proof, we always assume that $(\mathbf A_{m-1})$ holds. Also the rest of the proof is performed at a fixed $z\in \mathbf S_m$. 

In order to prove equation \eqref{lemm_comp_4}, we compare $Z^{\theta,Z_{\mu i}^0}_{(\mu i)}$ and $Z^{\theta,Z_{\mu i}^1}_{(\mu i)}$ via a common $Z^{\theta,0}_{(\mu i)}$, i.e. we will prove that for any constant $c>0$,
\begin{equation}\label{lemm_comp_5}
\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left[\mathbb EF_{\bu\mathbf v}^s\left(Z^{\theta,Z_{\mu i}^\al}_{(\mu i)},z\right)-\mathbb EF_{\mathbf v}^s\left(Z^{\theta,0}_{(\mu i)},z\right)\right]\lesssim (n^\e q)^{s}+\E F_{\bu\mathbf v}^s\left(Z^{\theta},z\right) ,
 \end{equation}
for all $\al\in \{0,1\}$ and $\theta\in[0,1]$. Underlying the proof of the estimate (\ref{lemm_comp_5}) is an expansion approach which we describe now. 
%which is the same as the ones for Lemma 7.10 of \cite{Anisotropic} and Lemma 6.11 of \cite{yang2019spiked}. So we omit the details.
We define the $\mathcal I \times \mathcal I$ matrix $\Delta_{(\mu i)}^\lambda$ as
\begin{equation}\label{deltaimu}
\Delta_{(\mu i)}^{\lambda} :=\lambda \left( {\begin{array}{*{20}c}
   { 0 } &   \mathbf u_i^{(\mu)} \mathbf e_\mu^\top     \\
   {\mathbf e_\mu (\mathbf u_i^{(\mu)})^\top } & {0}  \\
   \end{array}} \right),%   \lambda \mathbf u_i \delta_{is}\delta_{\mu t}+\lambda\delta_{it}\delta_{\mu s}, \ \  i\in \mathcal I_1, \mu\in \mathcal I_2,
\end{equation}
where we denote $\bu_i^{(\mu)}:=\Lambda U\mathbf e_i$ if $\mu \in \cal I_1$, and $\bu_i^{(\mu)}:=V\mathbf e_i$ if $\mu \in \cal I_2$. Here $\mathbf e_i$ and $\mathbf e_\mu$ denote the standard basis vectors along the $i$-th and $\mu$-th directions. Then by the definition of $H$ in equation \eqref{linearize_block}, we have for any $\lambda,\lambda'\in \mathbb R$ and $K\in \mathbb N$,
\begin{equation}\label{eq_comp_expansion}
G_{(\mu i)}^{\theta,\lambda'} = G_{(\mu i)}^{\theta,\lambda}+n^{-\frac{k}2}\sum_{k=1}^{K}  G_{(\mu i)}^{\theta,\lambda}\left( \Delta_{(\mu i)}^{\lambda-\lambda'} G_{(\mu i)}^{\theta,\lambda}\right)^k+ n^{-\frac{K+1}2}G_{(\mu i)}^{\theta,\lambda'}\left(\Delta_{(\mu i)}^{\lambda-\lambda'} G_{(\mu i)}^{\theta,\lambda}\right)^{K+1}.
\end{equation}
%In the remainder of this section, we prove (\ref{lemm_comp_5}) for $u=1$.
%where
%$\overline V:=\begin{pmatrix}V_1 & 0\\ 0 & I\end{pmatrix}$ and $\alpha:=\frac{w^{1/2}}{|w|^{1/2}}.$
%The following result provides a priori bounds for the entries of $G_{(\mu i)}^{\theta,\lambda}$.
Using this expansion and the a priori bound \eqref{priorim}, it is easy to prove the following estimate: if $y$ is a random variable satisfying $|y|\prec Q$ (specifically the entries of all the interpolating matrices $Z^\theta$ satisfy this bound), then
 \begin{equation}\label{comp_eq_apriori}
   G_{(\mu i)}^{\theta,y}=\OO (1),\quad i\in\sI_1, \ \mu\in\sI_2 \cup \cal I_3,
 \end{equation}
 with overwhelming probability. 
% for all $i\in\sI^M_1$ and $\mu\in\sI_2$.
%\end{lemma}
%\begin{proof} The proof is the same as the one for \cite[Lemma 7.14]{Anisotropic}. \end{proof}
%\begin{proof}
% It suffices to show that $G_{(\mu i)}^{\theta,y}=O_{\prec}(N^{2\delta})$ since $\|\Pi\|=\OO(1)$ by equation \eqref{Piii}. By assumption ($\mathbf A_{m-1}$), the Lemma \ref{lemm_comp_2} holds for the matrix ensemble $X^\theta$ (since it satisfies (\ref{assm1})-(\ref{assm2})). In particular $G^{\theta,X_{i\mu}^u}_{(\mu i)}=O_\prec(N^{2\delta})$. Now we apply the expansion (\ref{eq_comp_expansion}) with $\lambda:=X_{i\mu}^\theta$, $\lambda':=y$ and large enough $K$ such that $K(2\delta-\phi)\le -2$. Using $|\lambda-\lambda'|\prec q$, it is easy to estimate all the terms in (\ref{eq_comp_expansion}) using Lemma \ref{lemm_comp_2} except the rest term.
%To handle the rest term, we use the rough bound $G_{(\mu i)}^{\theta,\lambda'}\prec N$ coming from a simple modification of (\ref{eq_gbound}).
%\end{proof}

In the following proof, for simplicity of notations, we denote
$$f_{(\mu i)}(\lambda):=F_{\mathbf u\mathbf v}^s\left(Z_{(\mu i)}^{\theta, \lambda}\right)=\left|\mathbf u^\top \left(G \left(Z_{(\mu i)}^{\theta, \lambda},z\right)-\Gi(z)\right)\mathbf v\right|^s.$$ We use $f_{(\mu i)}^{(r)}$ to denote the $r$-th derivative of $f_{(\mu i)}$. By equation \eqref{comp_eq_apriori}, it is easy to see that for any fixed $r\in\bbN$, $ f_{(\mu i)}^{(r)}(y) =\OO(1)$ with overwhelming probability for any random variable $y$ satisfying $|y|\prec Q$. 
% With Lemma \ref{lemm_comp_6} and (\ref{eq_comp_expansion}), it is easy to prove the following result.
%\begin{lemma}
%Suppose that $y$ is a random variable satisfying $|y|\prec q$. Then for fixed $r\in\bbN$,
%  \begin{equation}
%  \left|f_{(\mu i)}^{(r)}(y)\right|\prec N^{2\delta(r+p)}.
% \end{equation}
%\end{lemma}
%Then in Section \ref{subsec_3moment}, we show how to relax equation \eqref{3moment} to equation \eqref{assm_3moment} for $z\in \wt  S(c_0,C_0,\e)$.
Then the Taylor expansion of $f_{(\mu i)}$ gives
\begin{equation}\label{eq_comp_taylor}
f_{(\mu i)}(y)=\frac{1}{n^{r/2}}\sum_{r=0}^{s+4}\frac{y^r}{r!}f^{(r)}_{(\mu i)}(0)+\OO_\prec\left( q^{s+4}\right).
\end{equation}
%provided $C_a$ is chosen large enough in (\ref{assm_comp_delta}). 
Therefore we have for $\al\in\{0,1\}$,
\begin{align}
&\mathbb EF_{\mathbf u\mathbf v}^s\left(Z^{\theta,Z_{\mu i}^\al}_{(\mu i)}\right)-\mathbb EF_{\mathbf u\mathbf v}^s\left(Z^{\theta,0}_{(\mu i)}\right)=\bbE\left[f_{(\mu i)}\left(Z_{\mu i}^\al\right)-f_{(\mu i)}(0)\right]\nonumber\\
& =\bbE f_{(\mu i)}(0)+\frac{1}{2n}\bbE f_{(\mu i)}^{(2)}(0)+\sum_{r=3}^{s+4}\frac{n^{-r/2}}{r!}\bbE f^{(r)}_{(\mu i)}(0)\bbE\left(Z_{\mu i}^\al\right)^r+\OO_\prec(q^{s+4}). \label{taylor1}
\end{align}
Here to illustrate the idea in a more concise way, we assume the extra condition 
\be\label{3moment}
\mathbb E (Z^1_{\mu i})^3=0, \quad 1\le \mu \le n,\ \  1\le i \le p.
\ee
Hence the $r=3$ term in the Taylor expansion \eqref{taylor1} vanishes. However, this condition is not necessary as we will explain at the end of the proof.

%where we used that $Z_{\mu i}^u$ has vanishing first and third moments and its variance is $1/N$. (Note that this is the only place where we need the condition equation \eqref{3moment}.) 


Recall that the entries of $Z_{\mu i}^1$ have finite fourth moment as given by \eqref{conditionA2}. Combining it with the bounded support condition, we have
\be\label{moment-4}
\left|\bbE\left(Z_{\mu i}^a\right)^r\right| \prec  Q^{r-4} , \quad r \ge 4.
\ee
Thus to show (\ref{lemm_comp_5}) under \eqref{3moment}, we only need to prove that for $r=4, \cdots,s+4$,
\begin{equation}\label{eq_comp_est}
n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left|\bbE f^{(r)}_{(\mu i)}(0)\right|\lesssim \left(n^\e q\right)^s+\mathbb EF_{\bu\mathbf v}^s(Z^\theta,z) .\end{equation}
%where we used (\ref{assm2}). 
In order to get a self-consistent estimate in terms of the matrix $Z^\theta$ on the right-hand side of (\ref{eq_comp_est}), we want to replace $Z^{\theta,0}_{(\mu i)}$ in $f_{(\mu i)}(0)=F_{\bu\mathbf v}^s(Z_{(\mu i)}^{\theta, 0})$ with $Z^\theta \equiv Z_{(\mu i)}^{\theta, Z_{\mu i}^\theta}$. %We have the following lemma.
\begin{lemma}
Suppose that
\begin{equation}\label{eq_comp_selfest}
n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1 \cup \cal I_2}\left|\bbE f^{(r)}_{(\mu i)}(Z_{\mu i}^\theta)\right|\lesssim \left(n^\e q\right)^s+\mathbb EF_{\mathbf v}^s(Z^\theta) 
\end{equation}
holds for $r=4,\cdots,s+4$. Then (\ref{eq_comp_est}) holds for $r=4,\cdots,s+4$.
\end{lemma}
\begin{proof}
The proof is the same as the one for \cite[Lemma 7.16]{Anisotropic}.
%We abbreviate $ f_{(\mu i)}\equiv f$ and $X_{i\mu}^\theta \equiv \xi$. Then with (\ref{eq_comp_taylor}) we can get
%\begin{equation}\label{eq_comp_taylor2}
%\E f^{(l)}(0)=\E f^{(l)}(\xi)-\sum_{k=1}^{4p+4-l}\E f^{(l+k)}(0)\frac{\E \xi^k}{k!}+\OO_\prec(q^{p+4-l}).
%\end{equation}
%The estimate equation \eqref{eq_comp_est} then follows from a repeated application of (\ref{eq_comp_taylor2}).  Fix $r=4,...,4p+4$. Using (\ref{eq_comp_taylor2}), we get
%\begin{align*}
%\mathbb E f^{(r)}(0)&=\mathbb E f^{(r)}(\xi) - \sum_{k_1\ge 1} \mathbf 1(r+k_1 \le 4p +4)\mathbb E f^{(r+k_1)}(0) \frac{\mathbb E\xi^{k_1}}{k_1!}+\OO_\prec(q^{p+4-r}) \\
%&=\mathbb E f^{(r)}(\xi) - \sum_{k_1\ge 1} \mathbf 1(r+k_1 \le 4p +4)\mathbb E f^{(r+k_1)}(\xi) \frac{\mathbb E\xi^{k_1}}{k_1!} \\
%&+\sum_{k_1,k_2\ge 1} \mathbf 1(r+k_1+k_2 \le 4p +4)\mathbb E f^{(r+k_1+k_2)}(0) \frac{\mathbb E\xi^{k_1}}{k_1!} \frac{\mathbb E\xi^{k_2}}{k_2!} + \OO_\prec(q^{p+4-r}) \\
%&=\cdots=\sum_{t=0}^{4p+4-r}(-1)^t \sum_{k_1,\cdots, k_t \ge 1}\mathbf 1\left(r+\sum_{j=1}^t k_j \le 4p +4\right)\mathbb E f^{(r+\sum_{j=1}^t k_j)}(\xi)\prod_{j=1}^t \frac{\mathbb E\xi^{k_j}}{k_j!} + \OO_\prec(q^{p+4-r}).
%\end{align*}
%The lemma now follows easily by using equation \eqref{moment-4}.
\end{proof}
%\end{subsection}

%\begin{subsection}{Conclusion of the proof with words}\label{section_words}



  What remains now is to prove (\ref{eq_comp_selfest}). For simplicity of notations, we shall abbreviate $Z^\theta \equiv Z$ in the following proof. 
For any $k\in \N$, we denote
\be\label{Amui}
A_{\mu i}(k):= \left(\frac{\partial}{\partial Z_{\mu i}}\right)^k \mathbf u^\top\left( G-\Gi\right)\mathbf v.\ee
The derivative on the right-hand side can be calculated using the expansion equation \eqref{eq_comp_expansion}. In particular, it is easy to verify that it satisfies the following bound 
 \begin{equation}\label{eq_comp_A2}
 |A_{\mu i}(k)|\prec \begin{cases}(\mathcal R_i^{(\mu)})^2+\mathcal R_\mu^2 , \ & \text{if } k \ge 2 \\ 
 \mathcal R_i^{(\mu)}\mathcal R_\mu , \ & \text{if } k = 1 \end{cases},
  \end{equation}
where for $i\in \cal I_1$ and $\mu \in \cal I_1\cup \cal I_2$, we denote
\begin{equation}\label{eq_comp_Rs}
\mathcal R_i^{(\mu)}:=|\mathbf u^\top G{ \bu_i^{(\mu)}}|+| \mathbf v^\top G{ \bu_i^{(\mu)}}|,\quad \mathcal R_\mu:=|\mathbf u^\top G\mathbf e_{ \mu}|+|\mathbf v^\top G \mathbf e_{ \mu}|.
\end{equation}
%for $a\in \sI$, where $\mathbf w_i:= \Sigma^{1/2} \bu_i$ for $i\in\sI_1$ and $\mathbf w_\mu:=\wt  \Sigma^{1/2} \bv_\mu$ for $\mu\in\sI_2$.
%\begin{definition}[Words]\label{def_comp_words}
%Given $i\in \mathcal I_1$ and $\mu\in \mathcal I_1\cup \cal I_2$. Let $\sW$ be the set of words of even length in two letters $\{\mathbf i, {\mu}\}$. We denote the length of a word $w\in\sW$ by $2m(w)$ with $m(w)\in \mathbb N$. We use bold symbols to denote the letters of words. For instance, $w=\mathbf t_1\mathbf s_2\mathbf t_2\mathbf s_3\cdots\mathbf t_r\mathbf s_{r+1}$ denotes a word of length $2r$.
%Define $\sW_r:=\{w\in \mathcal W: m(w)=r\}$ to be the set of words of length $2r$, and such that
%%We require that 
%each word $w\in \sW_r$ satisfies that $\mathbf t_l\mathbf s_{l+1}\in\{\mathbf i{}{\mu},{}{\mu}\mathbf i\}$ for all $1\le l\le r$.
%
%Next we assign to each letter $*$ a value $[*]$ through $[\mathbf i]:=\Sigma \bu_i$, $[{} {\mu}]:=\wt  \Sigma \mathbf v_\mu,$ where $\mathbf u_i$ and $\bv_\mu$ are defined in Lemma \ref{lem_comp_gbound} and are regarded as summation indices. Note that it is important to distinguish the abstract letter from its value, which is a summation index. Finally, to each word $w$ we assign a random variable $A_{\mathbf v, i, \mu}(w)$ as follows. If $m(w)=0$ we define
% $$A_{\mathbf v, i, \mu}(w):=G_{\mathbf v\mathbf v}-\Pi_{\mathbf v\mathbf v}.$$
% If $m(w)\ge 1$, say $w=\mathbf t_1\mathbf s_2\mathbf t_2\mathbf s_3\cdots\mathbf t_r\mathbf s_{r+1}$, we define
% \begin{equation}\label{eq_comp_A(W)}
% A_{\mathbf v, i, \mu}(w):=G_{\bv[\mathbf t_1]} G_{[\mathbf s_2][\mathbf t_2]}\cdots G_{[\mathbf s_r][\mathbf t_r]} G_{[\mathbf s_{r+1}]\bv}.
% \end{equation}
%\end{definition}
%Notice the words are constructed such that, by equation \eqref{deltaimu} and (\ref{eq_comp_expansion}) ,
%\[\left(\frac{\partial}{\partial X_{i\mu}}\right)^r \left( G_{\mathbf v\mathbf v}-\Pi_{\mathbf v\mathbf v}\right)=(-1)^r r!\sum_{w\in \mathcal W_r} A_{\mathbf v, i, \mu}(w),\quad r\in \mathbb N,\]
%with which we get that
Then we can calculate the derivative
\begin{align*}
f^{(r)}_{(\mu i)}(Z_{\mu i})=\left(\frac{\partial}{\partial Z_{\mu i}}\right)^r F_{\bu\bv}^s(Z)= \sum_{k_1+\cdots+k_s=r}\prod_{t=1}^{s/2} \left(A_{\mu i}(k_t)\overline{A_{\mu i}(k_{t+s/2})}\right).
\end{align*}
Then to prove (\ref{eq_comp_selfest}), it suffices to show that
\begin{equation}
n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left|\bbE\prod_{t=1}^{s/2}A_{\mu i}(k_t)\overline{A_{\mu i}(k_{t+s/2})}\right|\lesssim \left(n^\e q\right)^s+\mathbb EF_{\bu\mathbf v}^s(Z,z),\label{eq_comp_goal1}
\end{equation}
for  $4\le r\le s+4$ and $(k_1,\cdots,k_s)\in \N^s$ satisfying $k_1 +\cdots+k_s=r$. 
%To avoid the unimportant notational complications associated with the complex conjugates, we will actually prove that
%\begin{equation}\label{eq_comp_goal2}
%N^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE\prod_{t=1}^{p}A_{\mathbf v, i, \mu}(w_t)\right|=\OO\left(\left(n^\e q\right)^p+\mathbb EF_{\bu\mathbf v}^p(Z,z)\right).
%\end{equation}
%The proof of $(\ref{eq_comp_goal1})$ is essentially the same but with slightly heavier notations. 
Treating zero $k_t$'s separately (note $A_{\mu i}(0)=(G_{\mathbf u\mathbf v}-\Pi_{\mathbf u\mathbf v})$ by definition), we find that it suffices to prove
\begin{equation}
\label{eq_comp_goal3}n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\bbE|A_{\mu i}(0)|^{s-l}\prod_{t=1}^{l}\left|A_{\mu i}(k_t)\right|\lesssim \left(n^\e q\right)^s+\mathbb EF_{\bu\mathbf v}^s(Z,z)
\end{equation}
for  $4\le r\le s+4$ and $1\le l \le s$. Here without loss of generality, we assume that $k_t=0$ for $l+1\le t \le s$, $k_t \ge 1$ for $1\le t\le l$, and $\sum_{t=1}^l k_t=r$.

%To estimate (\ref{eq_comp_goal3}) we introduce the quantity
%\begin{equation}\label{eq_comp_Rs}
%\mathcal R_a:=|G_{\mathbf v \mathbf w_a}|+|G_{\mathbf w_a \mathbf v}|
%\end{equation}
%for $a\in \sI$, where $\mathbf w_i:= \Sigma^{1/2} \bu_i$ for $i\in\sI_1$ and $\mathbf w_\mu:=\wt  \Sigma^{1/2} \bv_\mu$ for $\mu\in\sI_2$.
%
%\begin{lemma}\label{lem_comp_A}
%  For $w\in\sW$, we have the rough bound
%  \begin{equation}
%  |A_{\mathbf v, i, \mu}(w)|\prec N^{2\delta(m(w)+1)}.\label{eq_comp_A1}
%  \end{equation}
%  Furthermore, for $m(w)\ge 1$ we have
%  \begin{equation}
%  |A_{\mathbf v, i, \mu}(w)|\prec(\mathcal R_i^2+\mathcal R_\mu^2)N^{2\delta(m(w)-1)}.\label{eq_comp_A2}
%  \end{equation}
%  For $m(w)=1$, we have the better bound
%  \begin{equation}
%  |A_{\mathbf v, i, \mu}(w)|\prec \mathcal R_i\mathcal R_\mu.\label{eq_comp_A3}
%  \end{equation}
%\end{lemma}
%\begin{proof}
%The estimates (\ref{eq_comp_A1}) and (\ref{eq_comp_A2}) follow immediately from the rough bound (\ref{eq_comp_apbound}) and definition (\ref{eq_comp_A(W)}).  
%%For (\ref{eq_comp_A2}), we break $A_{\mathbf v, i, \mu}(w)$ into $G_{\bv[\mathbf t_1]}(G_{[\mathbf s_2][\mathbf t_2]}\cdots G_{[\mathbf s_n][\mathbf t_n]})^{1/2}$ times $(G_{[\mathbf s_2][\mathbf t_2]}\cdots G_{[\mathbf s_n][\mathbf t_n]})^{1/2}G_{[\mathbf s_{n+1}]\bv}$ and use Cauchy-Schwarz inequality. 
%The estimate (\ref{eq_comp_A3}) follows from the constraint $\mathbf t_1\ne\mathbf s_2$ in the definition (\ref{eq_comp_A(W)}).
%\end{proof}
For \eqref{eq_comp_goal3}, notice that there are at least one non-zero $k_t$ in all cases, while in the case $r\le 2l-2$, by pigeonhole principle, there exist at least two $k_t$'s with $k_t=1$. Therefore with \eqref{eq_comp_A2}, we have that
\begin{equation}\label{eq_comp_r1}
\prod_{t=1}^{l}\left|A_{\mu i}(k_t)\right| \prec  \one(r\ge 2l-1)\left[(\mathcal R_i^{(\mu)})^2+\mathcal R_\mu^2\right]+\one(r\le 2l-2)(\mathcal R_i^{(\mu)})^2\mathcal R_\mu^2 .
\end{equation}
%Let $\mathbf v=\left( {\begin{array}{*{20}c}
%   {\mathbf v}_1   \\
%   {\mathbf v}_2 \\
%   \end{array}} \right)$ for ${\mathbf v}_1 \in\mathbb C^{\mathcal I_1}$ and ${\mathbf v}_2\in\mathbb C^{\mathcal I_2}$. 
Using \eqref{priorim} and a similar argument as in \eqref{GG*}, we can obtain that
\begin{align}
\sum_{i\in\sI_0}(\mathcal R_i^{(\mu)})^2 =\OO(1),\quad  \sum_{\mu\in\sI_1 \cup \cal I_2}\mathcal R_{\mu}^2 =\OO(1), \quad \text{w.o.p.}\label{eq_comp_r2}
\end{align}
%where in the second step we used the two bounds in Lemma \ref{lemm_comp_2} and $\eta =\OO(\im M_{2})$ by equation \eqref{Immc}, and in the last step the definition of $\Psi$ in equation \eqref{eq_defpsi}. Using the same method we can get
%\begin{equation}\label{eq_comp_r3}
%\frac{1}{N^2}\sum_{i\in\sI_1}\sum_{\mu\in\sI_2}\mathcal R_i^2\mathcal R_\mu^2\prec \left[N^{(C_a+2)\delta}\left(\Psi^2(z) + \frac{q}{N\eta}\right)\right]^2.
%\end{equation}
Using (\ref{eq_comp_r2}) and $n^{-1/2}\le n^{-1/2}Q=q$, we get that %the left-hand side of (\ref{eq_comp_goal3}) is bounded by
\begin{align}
&n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}|A_{\mu i}(0)|^{s-l}\prod_{t=1}^{l}\left|A_{\mu i}(k_t)\right| \nonumber\\
&\prec  q^{r-4} F_{\bu\bv}^{s-l}(Z)\left[\one(r\ge 2l-1) n^{-1} +\one(r\le 2l-2)n^{-2}\right] \nonumber\\
&\le  F_{\bu\bv}^{s-l}(Z)\left[\one(r\ge 2l-1)q^{r-2}+\one(r\le 2l-2)q^r\right]. \label{add_add}
%\\ &\le \bbE F_{\bv}^{p-l}(X)\left[\one(r\ge 2l-1)\left(N^{C_a\delta/2+12\delta}(q+\Psi)\right)^{r-2}+\one(r\le 2l-2)\left(N^{C_a\delta/2+12\delta}(q+\Psi)\right)^r\right],
\end{align}
%\[q^{m-4}N^{2\delta(n+l+2)}\bbE F_{\bv}^{p-l}(X)\left[\one(m\ge 2l-1)\left(N^{C_a\delta/2}(q+\Psi(z))\right)^2+\one(m\le 2l-2)\left(N^{C_a\delta/2}(q+\Psi(z))\right)^4\right].\]
%Using $\Phi \gtrsim N^{-1/2}$, we find that the left hand side of (\ref{eq_comp_goal3}) is bounded by
%\begin{align*}
% & N^{2\delta(n+q+2)} \bbE F_{\bv}^{p-q}(X)\left(\one(m\ge 2l-1)\left(N^{C_0\delta/2}\Phi\right)^{n-2}+\one(m\le 2l-2)\left(N^{C_0\delta/2}\Phi\right)^n\right)\\
% &\le \bbE F_{\bv}^{p-q}(X)\left(\one(m\ge 2l-1)\left(N^{C_0\delta/2+12\delta}\Phi\right)^{n-2}+\one(m\le 2l-2)\left(N^{C_0\delta/2+12\delta}\Phi\right)^n\right)
%\end{align*}
If $r\le 2l-2$, then we have $q^r\le q^l$ by the trivial inequality $r\ge l$. On the other hand, if $r\ge 4$ and $r\ge 2l-1$, then $r\ge l+2$ and we get $q^r\le q^{l+2}$. Thus with \eqref{add_add}, we conclude that %the left-hand side of $(\ref{eq_comp_goal3})$ is bounded by
\begin{align*} 
&n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2} \E |A_{\mu i}(0)|^{s-l}\prod_{t=1}^{l}\left|A_{\mu i}(k_t)\right|  \\
&\prec \E F_{\bu\bv}^{s-l}(Z) q^l\le \left[\E F_{\bu\bv}^{s}(Z)\right]^{\frac{s-l}{s}} q^l \lesssim  \E F_{\bu\bv}^{s}(Z) + q^s,
\end{align*}
where we use H\"older's inequality in the second step, and Young's inequality in the last step. This gives (\ref{eq_comp_goal3}), which concludes the proof of (\ref{eq_comp_selfest}), and hence of (\ref{lemm_comp_5}), and hence of \eqref{lemm_comp_4}, which concludes \eqref{lemm_comp_4.4} and completes the proof of the anisotropic local law \eqref{aniso_law} under the condition \eqref{3moment}.

Finally, if the condition equation \eqref{3moment} does not hold, then there is also an $r=3$ term in the Taylor expansion  \eqref{taylor1}:
$$\frac{1}{6n^{3/2}}\bbE f^{(3)}_{(\mu i)}(0)\bbE\left(Z_{i\mu}^\al\right)^3.$$
%Note that $\bbE\left(Z_{i\mu}^a\right)^3$ is of order $n^{-3/2}$, while the 
But the sum over $i$ and $\mu$ in equation \eqref{lemm_comp_5} provides a factor $n^2$, which cannot be cancelled by the $n^{-3/2}$ factor in the above equation. In fact, $\bbE f^{(3)}_{(\mu i)}(0)$ will provide an extra $n^{-1/2}$ factor to compensate the remaining $n^{1/2}$ factor. This follows from an improved self-consistent comparison argument for sample covariance matrices in \cite[Section 8]{Anisotropic}. The argument for our setting is almost the same except for some notational differences, so we omit the details. This concludes the proof of \eqref{aniso_law} without the condition \eqref{3moment}.

%Proposition \ref{comparison_prop} under the assumption (\ref{assm_3rdmoment}).
%the anisotropic local law in Theorem \ref{law_wideT}.
%\end{subsection}
%\end{subsection}



%\subsection{Non-vanishing third moment}\label{subsec_3moment}
%
%In this subsection, we prove Lemma \ref{lemm_comp_0} under equation \eqref{assm_3moment} for $z\in \wt  S(c_0, C_0,\e)$. 
%%In this case, we can verify that
%%\begin{equation}\label{eq_comp_boundPhi}
%%\Phi \le N^{-1/4-\zeta/2}.
%%\end{equation}
%Following the arguments in Sections \ref{subsec_interp}-\ref{section_words}, we see that it suffices to prove the estimate ($\ref{eq_comp_selfest}$) in the $r=3$ case. In other words, we need to prove the following lemma. 
%\begin{lemma}\label{lemm_comparison_big}
%Fix $p\in 2\mathbb N$ and $m \le \delta^{-1}$. Let $z\in {\mathbf S}_m $ and suppose $(\mathbf A_{m-1})$ holds. Then %we have
%\begin{equation}\label{eq_comp_selfest_generalX}
%b_N N^{-2}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE f^{(3)}_{(\mu i)}(X_{i\mu}^\theta)\right|=\OO\left(\left[N^{C_a\delta} (q+\Psi)\right]^p+\mathbb EF_{\mathbf v}^p(X^\theta,z)\right).
%\end{equation}
%\end{lemma}
%\begin{proof}
%The main new ingredient of the proof is a further iteration step at a fixed $z$. Suppose
%\begin{equation}\label{comp_geX_iteration}
%G-\wt \Pi=\OO_\prec(\Phi)
%\end{equation}
%for some deterministic parameter $\Phi\equiv \Phi_N$. By the a priori bound (\ref{eq_comp_apbound}), we can take $\Phi\le N^{2\delta}$. Assuming (\ref{comp_geX_iteration}), we shall prove a self-improving bound of the form
%\begin{equation}\label{comp_geX_self-improving-bound}
%b_N N^{-2}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE f^{(3)}_{(\mu i)}(X_{i\mu}^\theta)\right|=\OO\left(\left[N^{C_a\delta} (q+\Psi)\right]^p+(N^{-\epsilon/2}\Phi)^p+\mathbb EF_{\mathbf v}^p(X^\theta,w)\right).
%\end{equation}
%Once (\ref{comp_geX_self-improving-bound}) is proved, we can use it iteratively to get an increasingly accurate bound 
%for $\left|G_{\mathbf{vv}}(X,z)-\Pi_{\mathbf {vv}}(z)\right|$. After each step, we obtain a better bound (\ref{comp_geX_iteration}) with $\Phi$ reduced by $N^{-\e/2}$. Hence after $\OO(\e^{-1})$ many iterations we can get (\ref{eq_comp_selfest_generalX}).
%
%As in Section \ref{section_words}, to prove (\ref{comp_geX_self-improving-bound}) it suffices to show 
%\begin{equation}\label{comp_geX_words}
%b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}A^{p-l}_{\mathbf v, i, \mu}(w_0)\prod_{t=1}^{l}A_{\mathbf v, i, \mu}(w_t)\right|\prec F_{\bv}^{p-l}(X)\left[N^{(C_0-1)\delta}(q+\Psi) + N^{-\e/2}\Phi\right]^l,
%\end{equation}
%which follows from the bound
%\begin{equation}\label{comp_geX_words2}
%b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\prod_{t=1}^{l}A_{\mathbf v, i, \mu}(w_t)\right|\prec \left[N^{(C_0-1)\delta}(q+\Psi) + N^{-\e/2}\Phi\right]^l.
%\end{equation}
%%Each of the three cases $l=1,\, 2,\, 3$ can be proved as in \cite[Lemma 12.7]{Anisotropic}, and we leave the details to the reader. This concludes Lemma \ref{lemm_comparison_big}.
%%The rest of the proof is straightforward. 
%We now list all the three cases with $l=1,\, 2,\, 3$, and discuss each case separately.  
%
%When $l = 1$, the single factor $A_{\mathbf v, i, \mu}(w_1)$ is of the form
%\[ G_{\mathbf v[\mathbf t_1]} G_{[\mathbf s_2][\mathbf t_2]} G_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v}.\]
%%\[\wt G_{[s][t]}= G_{[s][t]}-\wt\Pi_{[s][t]},\] 
%Then we split it as
%\begin{align}
%G_{\mathbf v[\mathbf t_1]} G_{[\mathbf s_2][\mathbf t_2]} G_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v}
%=& G_{\mathbf v[\mathbf t_1]} \Pi_{[\mathbf s_2][\mathbf t_2]} \Pi_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v} + G_{\mathbf v[\mathbf t_1]}\wt G_{[\mathbf s_2][\mathbf t_2]} \Pi_{[\mathbf s_3][\mathbf t_3]}G_{[\mathbf s_4]\mathbf v}\nonumber\\
% + & G_{\mathbf v[\mathbf t_1]} \Pi_{[\mathbf s_2][\mathbf t_2]} \wt G_{[\mathbf s_3][\mathbf t_3]}  G_{[\mathbf s_4]\mathbf v}+ G_{\mathbf v[\mathbf t_1]}\wt G_{[\mathbf s_2][\mathbf t_2]}
%\wt G_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v},\label{comp_geX_expG}
%\end{align}
%where we abbreviate $\wt G: = G - \Pi$. For the second term, we have
%\begin{align}\label{term11}
%b_N N^{-2}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|G_{\mathbf v[\mathbf t_1]}\wt G_{[\mathbf s_2][\mathbf t_2]} \Pi_{[\mathbf s_3][\mathbf t_3]}G_{[\mathbf s_4]\mathbf v}\right|\prec b_N \Phi \cdot N^{(C_a+2)\delta}\left(\Psi^2 + \frac{q}{N\eta}\right)\prec N^{-\e/2}\Phi
%\end{align}
%provided $\delta$ is small enough, where we used (\ref{eq_comp_r2}), (\ref{comp_geX_iteration}) and the definition equation \eqref{tildeS}. The third and fourth term of (\ref{comp_geX_expG}) can be dealt with in a similar way. For the first term, when $[\mathbf t_1]=\mathbf w_i$ and $[\mathbf s_4]=\bw_\mu$, we have
%\begin{align*}
%& \Big|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2} G_{\mathbf v \mathbf w_i} \Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]} G_{\mathbf w_\mu \mathbf v}\Big| \prec N^{1+2\delta}\left(\sum_{\mu\in\mathcal I_2}| G_{\mathbf w_\mu\mathbf v}|^2\right)^{1/2}\prec N^{3/2+(C_a/2+3)\delta}(q+\Psi),
%\end{align*}
%where we used (\ref{eq_comp_r2}) and the fact that $\Pi$ is deterministic, such that the a priori bound (\ref{comp_eq_apriori}) gives
%$$\Big|\sum_{i\in\mathcal I_0} G_{\mathbf v \mathbf w_i} \Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]} \Big| \prec N^{1/2+2\delta} .$$
%%Cauchy-Schwarz inequality, a priori bounds (\ref{comp_eq_apriori}) and (\ref{eq_comp_r2}), and $\|\sum_i\mathbf v_i\|\le \sqrt N$. 
%If $[\mathbf t_1]=\mathbf w_\mu$ and $[\mathbf s_4]=\mathbf v_i$, the proof is similar. If $[\mathbf t_1]=[\mathbf s_4]$, then at least one of the terms $\Pi_{[\mathbf s_2][\mathbf t_2]}$ and $\Pi_{[\mathbf s_3][\mathbf t_3]}$ must be of the form $\Pi_{\mathbf w_i\mathbf w_\mu}$ or $\Pi_{\mathbf w_\mu\mathbf w_i}$, and hence we have
%$$\sum_i|\Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]}|=\OO(N^{1/2}) \quad\text{ or }\quad \sum_\mu | \Pi_{[\mathbf s_2][\mathbf t_2]} \Pi_{[\mathbf s_3][\mathbf t_3]}|=\OO(N^{1/2}).$$
%Therefore using $(\ref{eq_comp_r2})$ and (\ref{tildeS}), we get
%\begin{align*}
%\Big|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}G_{\mathbf v[\mathbf t_1]} \Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]}G_{[\mathbf s_4]\mathbf v}\Big|
%& \prec N^{3/2+(C_a+2)\delta}\left(q^2 + \Psi^2 \right) \le  N^{3/2}(q+\Psi) .
%\end{align*}
%provided $\delta$ is small enough. 
%%where we used $(\ref{eq_comp_r2})$ and (\ref{eq_comp_boundPhi}).
%In sum, we obtain that
%$$b_NN^{-2}\Big|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2} G_{\mathbf v[\mathbf t_1]} \Pi_{[\mathbf s_2][\mathbf t_2]}\Pi_{[\mathbf s_3][\mathbf t_3]} G_{[\mathbf s_4]\mathbf v}\Big|\prec N^{(C_a-1)\delta}(q+\Psi)$$
%provided that $C_a\ge 8$. Together with equation \eqref{term11}, this proves  (\ref{comp_geX_words2}) for $l=1$.
%
%When $l=2$, $\prod_{t=1}^2 A_{\mathbf v, i, \mu}(w_t)$ is of the form
%\begin{align}
% & G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i\mathbf v}, \quad   G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_\mu} G_{\mathbf w_i \mathbf w_i} G_{\mathbf w_\mu\mathbf v}, \label{eqn_q21}  \\
% &  G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_i} G_{\mathbf w_\mu \mathbf w_i} G_{\mathbf w_\mu\mathbf v}, \quad   G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_\mu} G_{\mathbf w_i \mathbf w_\mu} G_{\mathbf w_i\mathbf v},\label{eqn_q22}
%\end{align}
%or an expression obtained from one of these four by exchanging $\mathbf w_i$ and $\mathbf w_\mu$. The first expression in (\ref{eqn_q21}) can be estimated using (\ref{comp_eq_apriori}), (\ref{eq_comp_r2}) and (\ref{comp_geX_iteration}):
%\begin{equation}
%\label{q=2_1}\sum_\mu G_{\mathbf w_\mu \mathbf v} G_{\mathbf w_\mu\mathbf w_\mu}=\sum_\mu G_{\mathbf w_\mu \mathbf v}\wt G_{\mathbf w_\mu\mathbf w_\mu}+\sum_\mu  G_{\mathbf w_\mu \mathbf v}\Pi_{\mathbf w_\mu\mathbf w_\mu}=\OO_\prec\left[N^{1+(C_a/2+1)\delta}\Phi\left(\Psi^2 + \frac{q}{N\eta}\right)^{1/2}+ N^{1/2+2\delta}\right],
%\end{equation}
%and
%\begin{equation}\label{q=2_2}
%\Big|\sum_i G_{\mathbf v\mathbf w_i} G_{\mathbf v \mathbf w_i} G_{\mathbf w_i\mathbf v}\Big|\prec N^{1+(C_a+4)\delta} \left(\Psi^2 + \frac{q}{N\eta}\right).
%\end{equation}
%Combining equation \eqref{tildeS}, (\ref{q=2_1}) and (\ref{q=2_2}), we get that 
%\[b_N N^{-2}\Big|\sum_i\sum_\mu G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i\mathbf v}\Big| \prec \left(N^{(C_a-1)\delta}(q+\Psi) + N^{-\e/2}\Phi\right)^2,\]
%provided $\delta$ is small enough. The second expression in (\ref{eqn_q21}) can be estimated similarly. The first expression of (\ref{eqn_q22}) can be estimated using equation \eqref{tildeS}, (\ref{comp_eq_apriori}) and (\ref{eq_comp_r2}) by
%\begin{equation*}
%\begin{split}
%b_N N^{-2}\left|\sum_i\sum_\mu  G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v} G_{\mathbf v \mathbf w_i} G_{\mathbf w_\mu \mathbf w_i} G_{\mathbf w_\mu\mathbf v}\right|& \prec b_N N^{-2+2\delta}\sum_i\sum_\mu\left| G_{\mathbf v\mathbf w_i}\right|^2\left| G_{\mathbf w_\mu\mathbf v}\right|^2 \\
%& \prec b_N N^{(2C_0+6)\delta}\left( \Psi^2 +\frac{q}{N\eta}\right)^2 \le (q +\Psi)^2
%\end{split}
%\end{equation*}
%for small enough $\delta$. The second expression in (\ref{eqn_q22}) is estimated similarly.  This proves (\ref{comp_geX_words2}) for $l=2$.
%
%When $l = 3$, $\prod_{t=1}^3 A_{\mathbf v, i, \mu}(w_t)$ is of the form 
%$( G_{\mathbf v\mathbf w_i} G_{\mathbf w_\mu \mathbf v})^3$ or an expression obtained by exchanging $\mathbf w_i$ and $\mathbf w_\mu$ in some of the three factors. We use (\ref{eq_comp_r2}) and $\sum_i|\Pi_{\mathbf v\mathbf w_i}|^2 = \OO(1)$ to get that
%\[\left|\sum_i( G_{\mathbf v\mathbf w_i})^3\right|\prec \sum_i|\wt G_{\mathbf v\mathbf w_i}|^3+\sum_i|\Pi_{\mathbf v\mathbf w_i}|^3\prec \Phi\sum_i \left(| G_{\mathbf v\mathbf w_i}|^2+|\Pi_{\mathbf v\mathbf w_i}|^2 \right)+1\prec N^{1+(C_0+2)\delta}\left(\Psi^2 +\frac{q}{N\eta}\right)\Phi+\Phi+1.\]
%Now we conclude (\ref{comp_geX_words2}) for $l=3$ using equation \eqref{tildeS} and $N^{-1/2}=\OO( q+\Psi)$.
%\end{proof}
%----------remove end----------------





\subsection{Averaged local law}\label{section_averageTX}
Finally, in this subsection, we prove the averaged local law \eqref{aver_in} in the setting of Theorem \ref{LEM_SMALL}. The proof of \eqref{aver_in1} is almost the same.
%\begin{lemma} \label{thm_largerigidity2}
%Suppose the assumptions in Theorem \ref{thm_largerigidity} hold. %Fix the constants $c_0$ and $C_0$ as given in Theorem \ref{LEM_SMALL}. 
%Then for any fixed $\epsilon>0$, we have
%%there exists constant $C_1>0$, depending only on $c_0$, $C_0$, $B$ and $\phi$, such that %with overwhelming probability we have
%\begin{equation}
% \vert m(z)-M(z) \vert \prec  q^2 + (N \eta)^{-1}, \label{NEWMPBOUNDS2}
%\end{equation}
%uniformly for all $z \in  S(c_0, C_0, \epsilon)$. Moreover, outside of the spectrum we have the following stronger averaged local law (recall equation \eqref{KAPPA})
%\begin{equation}\label{aver_out2}
% | m(z)-M(z)|\prec q^2   + \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%uniformly in $z\in S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$ for any constant $\epsilon>0$. If $A$ or $B$ is diagonal, then equation \eqref{NEWMPBOUNDS2} and equation \eqref{aver_out2} hold without the condition equation \eqref{assm_3moment}.
%\end{lemma}
%
%In this section, we prove the weak averaged local laws in equation \eqref{aver_in1} and equation \eqref{aver_out1}. %\begin{proof}

The proof of \eqref{aver_in} for $G(Z,z)$ is similar to that for equation \eqref{aniso_law} in previous subsection, and we only explain the differences. 
%Note that the bootstrapping argument is not necessary, since we already have a good a priori bound by equation \eqref{aniso_law}.
%In this section we prove the averaged local law in Theorem \ref{law_wideT}. The anisotropic local law proved in the previous section gives a good a priori bound. 
In analogy to (\ref{eq_comp_F(X)}), we define
%\begin{align*}
%\wt F(X,w) : &=|w|^{1/2} |m_2(w)-M_{2}(w)|=|w|^{1/2}\left|\frac{1}{N}\sum_{\nu\in\sI_2}G_{\nu\nu}(w)-M_{2}(w)\right|\\
%&=\left|\frac{1}{N}\sum_{\nu\in\sI_2} G_{\nu\nu}(w)-|w|^{1/2}M_{2}(w)\right|.
%\end{align*}
\begin{align*}
\wt F(Z,z)  %= |m(z)-m_c(z)| 
:= \left|\frac{1}{p}\sum_{i\in\sI_0} \left[G_{ii}(Z,z)- \Gi_{ii}(z)\right]\right|.
\end{align*}
%where we used equation \eqref{mcPi}. 
%$\Phi^2 =\OO(|w|^{1/2}/{(N\eta)})$, 
%it suffices to prove that $\wt F\prec (q+\Psi(z))^2$. 
%Moreover, by Proposition \ref{prop_diagonal}, equation \eqref{aver_in1} and equation \eqref{aver_out1} hold for Gaussian $X$ (without the $q^2$ term). 
In the notation of Definition \ref{defn_interp}, we have proved that $ \wt F(Z^0,z)\prec (np)^{-1/2}$ in Lemma \ref{prop_entry}. 
To illustrate the idea, we assume the condition \eqref{3moment} holds in the following argument. 
%and prove the following stronger estimates:
%\begin{equation}
% \vert m(z)-M(z) \vert \prec (N \eta)^{-1} \label{aver_ins} %+ q^2 
%\end{equation}
%for $z\in S(c_0,C_0,\epsilon)$, and 
%\begin{equation}\label{aver_outs}
% | m(z)-M(z)|\prec \frac{q}{N\eta}  + \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}},
%\end{equation}
%for $z\in S(c_0,C_0,\epsilon)\cap \{z=E+\ii\eta: E\ge \lambda_r, N\eta\sqrt{\kappa + \eta} \ge N^\epsilon\}$. At the end of this section, we will show how to relax equation \eqref{3moment} to equation \eqref{assm_3moment} for $z\in \wt  S(c_0,C_0,\e)$.
%Note that
%\be\label{psi2}
%\Psi^2(z) \lesssim \frac{1}{N\eta}, \quad \text{and} \quad \Psi^2(z) \lesssim \frac{1}{N(\kappa +\eta)} + \frac{1}{(N\eta)^2\sqrt{\kappa +\eta}} \ \text{ outside of the spectrum}.
%\ee
Following the argument in Section \ref{sec_Gauss}, analogous to (\ref{eq_comp_selfest}), we only need to prove that for $q=n^{-1/2}Q$ and any small constant $c>0$,
\begin{equation}\label{eq_comp_selfestAvg}
n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\left|\bbE \left(\frac{\partial}{\partial Z_{\mu i}}\right)^r\wt F^s(Z)\right|\lesssim \left(p^{-1/2+c}q\right)^s+\mathbb E\wt F^s(Z) ,
\end{equation}
for all $r=4,...,s+4$. Similar to \eqref{Amui}, we denote
$$A_{j, \mu i}(k):= \left(\frac{\partial}{\partial Z_{\mu i}}\right)^k \left( G_{jj}-\Gi_{jj}\right).$$
Analogous to (\ref{eq_comp_goal1}), it suffices to prove that 
\begin{equation}\nonumber
\begin{split}
& n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left|\bbE\prod_{t=1}^{s/2}\left(\frac{1}{p}\sum_{j\in\sI_0}A_{j, \mu i}(k_t)\right)\left(\frac{1}{p}\sum_{j\in\sI_0}\overline{A_{j,\mu i}(k_{t+s/2})}\right)\right| \\
&  \lesssim \left(p^{-1/2+c}q\right)^s +\mathbb E\wt F^s(Z) ,
\end{split}
\end{equation}
for all $4\le r\le s+4$ and $(k_1,\cdots,k_s)\in \N^s$ satisfying $k_1 +\cdots+k_s=r$. Without loss of generality, we assume that $k_t=0$ for $l+1\le t \le s$, $k_t \ge 1$ for $1\le t\le l$, and $\sum_{t=1}^l k_t=r$. Then it suffices to prove
\begin{equation}\label{eq_comp_goalAvg}
n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\bbE \wt F^{s-l}(Z)\prod_{t=1}^{l}\left|\frac{1}{p}\sum_{j\in\sI_0} A_{j,\mu i}(k_{t})\right|\lesssim \left(p^{-1/2+c}q\right)^s+\mathbb E\wt F^s(Z)
\end{equation}
for  $4\le r\le s+4$ and $1\le l \le s$. 

First, using \eqref{priorim} and a similar argument as in \eqref{GG*}, we can obtain that for $1\le t \le l$,
\begin{equation}\label{average_bound}
\left| \sum_{j\in\sI_0} A_{j,\mu i}(k_{t})\right|\lesssim 1 \quad \text{ w.o.p }
\end{equation}
%The only difference in the definition of $A_{\mathbf v, i, \mu}(w)$ is that when $m(w)=0$, we define
%\[A_{\mathbf v, i, \mu}(w):= G_{\mathbf v\mathbf v}-|w|^{1/2}M_{2}.\]
On the other hand, similar to (\ref{eq_comp_Rs}) we define
%\begin{equation}\nonumber%\label{eq_comp_RsAvg}
%\mathcal R_{j, a}:=| G_{j \mathbf w_a}|+| G_{\mathbf w_a j}|.
%\end{equation}
$$\mathcal R_{j,i}^{(\mu)}:=|\mathbf u^\top G{ \bu_i^{(\mu)}}|+| \mathbf v^\top G{ \bu_i^{(\mu)}}|,\quad \mathcal R_{j,\mu}:=|\mathbf u^\top G\mathbf e_{ \mu}|+|\mathbf v^\top G \mathbf e_{ \mu}|.$$
As in \eqref{eq_comp_r2}, we have that
\begin{align}
\sum_{i\in\sI_0}(\mathcal R_{j,i}^{(\mu)})^2 =\OO(1),\quad  \sum_{\mu\in\sI_1 \cup \cal I_2}\mathcal R_{j,\mu}^2 =\OO(1), \quad \text{w.o.p.}\label{eq_comp_r2_add}
\end{align}
Now with \eqref{average_bound} and a similar bound as for \eqref{eq_comp_r1}, we obtain that 
\begin{align*}
\prod_{t=1}^{l}\left|\frac{1}{p}\sum_{j\in\sI_0} A_{j,\mu i}(k_{t})\right| &\prec \one(r\ge 2l-1)p^{-(l-1)}\frac1p\sum_{j\in \cal I_0}\left[(\mathcal R_{j,i}^{(\mu)})^2+\mathcal R_{j,\mu}^2\right] \\
&+\one(r\le 2l-2)p^{-(l-2)}\mathcal R_{j_1,i}^{(\mu)}\mathcal R_{j_2,i}^{(\mu)}\mathcal R_{j_1,\mu}\mathcal R_{j_2,\mu} 
\end{align*}
Summing this equation over $i\cal I_0$ and $\mu\in \cal I_1\cup \cal I_2$ and using \eqref{eq_comp_r2_add}, we get that 
\begin{align*}
& n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\bbE \wt F^{s-l}(Z)\prod_{t=1}^{l}\left|\frac{1}{p}\sum_{j\in\sI_0} A_{j,\mu i}(k_{t})\right|\\
& \prec q^{r-4}\bbE \wt F^{s-l}(Z) \left[ \one(r\ge 2l-1)p^{-(l-1)} n^{-1} + \one(r\le 2l-2)n^{-2}p^{-(l-2)}\right]
\end{align*}
We consider the following cases.
\begin{itemize}
\item If $r\ge 2l-1$ and $l\ge 2$, then we have $r\ge l+2$ and $l-1\ge l/2$, which gives 
$$p^{-(l-1)} q^{r-4}n^{-1}\le p^{-l/2} q^{r-2} \le (p^{-1/2}q)^l.$$
\item If $r\ge 2l-1$ and $l = 1$, then we have  
$$p^{-(l-1)} q^{r-4}n^{-1}\le n^{-1} \le (p^{-1/2}q)^l.$$
\item If $r\le 2l-2$ and $l\ge 4$, then we have $r\ge l$ and $l-2\ge l/2$, which gives 
$$p^{-(l-2)} q^{r-4}n^{-2}\le p^{-l/2} q^{r} \le (p^{-1/2}q)^l.$$
\item If $r\le 2l-2$ and $l < 4$, then we must have $l=3$ (because $r\ge 4$), which gives
$$p^{-(l-2)} q^{r-4}n^{-2}\le p^{-1} n^{-2} \le (p^{-1/2}q)^l.$$
\end{itemize}
%Using equation \eqref{aniso_law} and Lemma \ref{lem_comp_gbound}, similarly to equation \eqref{eq_comp_r2}, we get that
%\begin{equation}\label{eq_comp_r22}
%\begin{split}
% \frac{1}{n}\sum_{j\in\sI_1}\mathcal R_{j,a}^2 & \prec \frac{ \im \left(z^{-1}G_{\mathbf w_i\mathbf w_i}\right) + \im G_{\mathbf w_\mu\mathbf w_\mu} + \eta\left(\left| G_{\mathbf w_i\mathbf w_i} \right|+ \left| G_{\mathbf w_\mu \mathbf w_\mu} \right|\right)}{N\eta} \prec \Psi^2+\frac{q}{N\eta}.
% \end{split}
%\end{equation}
%Since $G=\OO_\prec(1)$ by equation \eqref{aniso_law}, we have 
Combining the above cases, we see that
%With (\ref{average_bound}), for any $r\ge 4$, the left-hand side of (\ref{eq_comp_goalAvg}) is bounded by
\[n^{-2}q^{r-4}\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_1\cup \cal I_2}\bbE \wt F^{s-l}(Z)\prod_{t=1}^{l}\left|\frac{1}{p}\sum_{j\in\sI_0} A_{j,\mu i}(k_{t})\right| \prec \bbE\wt F^{p-l}(X)\left(p^{-1/2}q\right)^{l}.\]
Applying Holder's inequality and Young's inequality, we then obtain \eqref{eq_comp_selfestAvg}, which completes the proof of the averaged local law \eqref{aver_in} under condition \eqref{3moment}. %\cor about removing 3rd moment assumption \nc


%\end{proof}

%
%Then we prove the averaged local law for $z\in \wt  S(c_0,C_0,\e)$ under equation \eqref{assm_3moment}. By equation \eqref{psi2}, it suffices to prove 
%\begin{equation}\label{comp_avg_geX_self-improving-bound}
%b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\bbE \left(\frac{\partial}{\partial X_{i\mu}}\right)^3\wt F^p(X)\right|=\OO\left(\left[N^\delta (q^2 +\Psi^2)\right]^p + \left( \frac{N^{-\e/2}}{N\eta}\right)^p+\mathbb E\wt F^p(X)\right),
%\end{equation}
%for any constant $\delta>0$. Analogous to the arguments in Section \ref{subsec_3moment}, it reduces to showing that
%\begin{equation}\label{eq_comp_goalAvg_genX}
%b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2} \prod_{t=1}^{l}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)\right|=\OO_\prec\left(\left(q^2+\Psi^2\right)^{l} + \left( \frac{N^{-\e/2}}{N\eta}\right)^l\right),
%\end{equation}
%where $l\in \{1,2,3\}$ is the number of words with nonzero length. Then we can discuss these three cases using a similar argument as in Section \ref{subsec_3moment}, with the only difference being that we now can use the anisotropic local law equation \eqref{aniso_law} instead of the a priori bounds equation \eqref{comp_eq_apriori}  and (\ref{comp_geX_iteration}). %As an example, we only give the proof for the case with $l=1$.
%
%%Again we can prove the three cases $q=1,\, 2,\, 3$ as in \cite[Lemma 12.8]{Anisotropic}, and we leave the details to the reader. This concludes the averaged local law. 
%%
%%Again we discuss the three cases $q=1,\, 2,\, 3$ separately. During the proof we tacitly use the anisotropic local law proved above.
%
%In the $l=1$ case, we first consider the expression $A_{ \mathbf e_j, i, \mu}(w_1) =  G_{j\mathbf w_i} G_{\mathbf w_\mu \mathbf w_\mu} G_{\mathbf w_i\mathbf w_i} G_{\mathbf w_\mu j}$. We have 
%\begin{equation}\nonumber
%\left|\sum_i G_{j\mathbf w_i} G_{\mathbf w_i\mathbf w_i}\right| \le \left|\sum_i G_{j\mathbf w_i} \Pi_{\mathbf w_i\mathbf w_i}\right| + \sum_i (q+\Psi)\left| G_{j\mathbf w_i} \right|\prec \sqrt N + N (q+\Psi) \left(\Psi^2 +\frac{q}{N\eta}\right)^{1/2},
%\end{equation}
%where we used equation \eqref{aniso_law} and equation \eqref{eq_comp_r2}.
%%since the leading term is $\sum_i\wt\Pi_{\nu\mathbf v_i}\wt\Pi_{\mathbf v_i\mathbf v_i}$. 
%Similarly, we also have
%\begin{equation}\nonumber
% \left|\sum_\mu G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_\mu j}\right|\prec \left|\sum_\mu {\Pi}_{\mathbf w_\mu \mathbf w_\mu}  G_{\mathbf w_\mu j}\right|  + \left|\sum_\mu \wt { G}_{\mathbf w_\mu \mathbf w_\mu}  G_{\mathbf w_\mu j}\right|  \prec \sqrt N(q+\Psi)+N (q+\Psi) \left(\Psi^2 +\frac{q}{N\eta}\right)^{1/2},
%\end{equation} 
%where we also used $\Pi_{\mathbf w_\mu j}=0$ for any $\mu$ in the second step. Then with equation \eqref{tildeS}, we can see that the LHS of (\ref{eq_comp_goalAvg_genX}) is bounded by $\OO_\prec(q^2 + \Psi^2)$ in this case.
%%$$b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_1)\right)\right| \prec q^2+\Psi^2 .$$
%For the case $A_{ \mathbf e_j, i, \mu}(w_1) =  G_{j\mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i \mathbf w_\mu} G_{\mathbf w_i j}$, we can estimate that
%$$\left|\sum_\mu  G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i \mathbf w_\mu} \right| \le \left|\sum_\mu  \Pi_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i \mathbf w_\mu} \right| + \sum_\mu (q+\Psi)\left|G_{\mathbf w_i \mathbf w_\mu} \right| \prec \sqrt N + N (q+\Psi) \left(\Psi^2 +\frac{q}{N\eta}\right)^{1/2},$$
%and
%$$ \sum_i \left|G_{j\mathbf w_i} G_{\mathbf w_i j}\right|\prec N\left(\Psi^2 +\frac{q}{N\eta}\right).$$
%Thus in this case the LHS of (\ref{eq_comp_goalAvg_genX}) is also bounded by $\OO_\prec(q^2 + \Psi^2)$. The case $A_{ \mathbf e_j, i, \mu}(w_1) =  G_{j\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu \mathbf w_\mu} G_{\mathbf w_i j}$ can be handled similarly. Finally in the case $A_{ \mathbf e_j, i, \mu}(w_1) =  G_{j\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu \mathbf w_i} G_{\mathbf w_\mu j}$, we can estimate that
%$$ \left|\sum_{i,\mu}  G_{j\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu \mathbf w_i} G_{\mathbf w_\mu j} \right| \prec  \sum_{i,\mu} \left(\left| G_{j\mathbf w_i}\right|^2 +\left| G_{\mathbf w_\mu j} \right|^2 \right) | G_{\mathbf w_\mu \mathbf w_i}|^2 \prec N^2 \left(\Psi^2 +\frac{q}{N\eta}\right)^2.$$
%Again in this case the LHS of (\ref{eq_comp_goalAvg_genX}) is bounded by $\OO_\prec(q^2 + \Psi^2)$. All the other expressions are obtained from these four by exchanging $\mathbf w_i$ and $\mathbf w_\mu$.
%
%In the $l=2$ case, $\prod_{t=1}^{2}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)$ is of the forms
%\[\frac{1}{N^2}\sum_{j_1,j_2} G_{j_1\mathbf w_i} G_{\mathbf w_\mu j_1} G_{j_2 \mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu} G_{\mathbf w_i j_2}\quad \text{ or }\quad \frac{1}{N^2}\sum_{j_1,j_2} G_{j_1\mathbf w_i} G_{\mathbf w_\mu j_1} G_{j_2\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu j_2},\]
%or an expression obtained from one of these terms by exchanging $\mathbf w_i$ and $\mathbf w_\mu$. These two expressions can be written as 
%\be\label{2terms}
%N^{-2}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}(G^{\times 2})_{\mathbf w_i\mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu}, \quad N^{-2}( G^{\times 2})^2_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i}, \quad G^{\times 2}:= G \begin{pmatrix}I_{\mathcal I_1 \times \mathcal I_1} & 0\\ 0 & 0\end{pmatrix} G.
%\ee
%For the second term, using equation \eqref{green2}, equation \eqref{spectral1} and recalling that $Y=\Sig^{1/2} U^{*}X V\wt  \Sig^{1/2}$, we can get that
%\begin{align}
%& \left|\frac{1}{N^2}\sum_{i,\mu} ( G^{\times 2})^2_{\mathbf w_\mu\mathbf w_i} G_{\mathbf w_\mu\mathbf w_i}\right| \le \frac{1}{N^2}\sum_{i,\mu} \left|( G^{\times 2})_{\mathbf w_\mu\mathbf w_i} \right|^2 = \frac{|z|^2}{N^2}\text{Tr}\left[(\mathcal G_1^{*})^2 YY^\top (\mathcal G_1)^2\right] \nonumber\\
%& =  \frac{|z|^2}{N^2}\text{Tr}\left[\mathcal G_1^{*} (\mathcal G_1)^2\right]  +  \frac{\bar z |z|^2}{N^2}\text{Tr}\left[(\mathcal G_1^{*})^2 (\mathcal G_1)^2\right]  \lesssim \frac{1}{N^2}\sum_k \frac{1}{\left[(\lambda_k-E)^2 +\eta^2\right]^{3/2}}+ \frac{1}{N^2}\sum_k \frac{1}{\left[(\lambda_k-E)^2 +\eta^2\right]^{2}} \nonumber\\
%& \lesssim \frac{1}{N\eta^3}\left(\frac1n \sum_k \frac{\eta}{(\lambda_k-E)^2 +\eta^2} \right) =\frac{\im m}{N\eta^3}\prec  \frac{\im m_c + q+\Psi}{N\eta^3} \lesssim \eta^{-2}\left(\Psi^2 +\frac{q}{N\eta}\right).\label{3term}
%\end{align}
%Using equation \eqref{aniso_law} and equation \eqref{eq_comp_r2}, it is easy to show that
%\be \label{3.5term}
%\left|\sum_{\mu}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i} \Pi_{\mathbf w_\mu\mathbf w_\mu}\right| \prec N^{3/2}\left( \Psi^2 + \frac{q}{N\eta}\right),\quad \text{ and } \quad \left|(G^{\times 2})_{\mathbf x\mathbf y} \right| \prec N\left( \Psi^2 + \frac{q}{N\eta}\right), \ee
%for any deterministic unit vectors $\mathbf x$, $\mathbf y$. Thus for the first term in equation \eqref{2terms}, we have
%\begin{align}
%\left|\frac{1}{N^2}\sum_{i,\mu}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}(G^{\times 2})_{\mathbf w_i\mathbf w_i} G_{\mathbf w_\mu\mathbf w_\mu}\right| & \le \left|\frac{1}{N^2}\sum_{i,\mu}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}(G^{\times 2})_{\mathbf w_i\mathbf w_i} \wt  G_{\mathbf w_\mu\mathbf w_\mu}\right| + \left|\frac{1}{N^2}\sum_{i,\mu}( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}(G^{\times 2})_{\mathbf w_i\mathbf w_i} \Pi_{\mathbf w_\mu\mathbf w_\mu}\right| \nonumber\\
%& \prec N(q+\Psi)\left( \Psi^2 + \frac{q}{N\eta}\right)\left(\frac{1}{N^2}\sum_{i,\mu}\left|( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}\right|^2\right)^{1/2} +N^{3/2}\left( \Psi^2 + \frac{q}{N\eta}\right)^2 \nonumber\\
%& \prec N\eta^{-1}(q+\Psi)\left( \Psi^2 + \frac{q}{N\eta}\right)^{3/2} +N^{3/2}\left( \Psi^2 + \frac{q}{N\eta}\right)^2,\label{4term}
%\end{align}
%where in the last step we used the bound in equation \eqref{3term}. Now using equation \eqref{3term}, equation \eqref{4term} and equation \eqref{tildeS}, we get
%$$b_N N^{-2}\left|\sum_{i\in\mathcal I_0}\sum_{\mu\in\mathcal I_2} \prod_{t=1}^{2}\left(\frac{1}{n}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)\right| \prec \left(q^2+\Psi^2\right)^{2} + \left( \frac{N^{-\e/2}}{N\eta}\right)^2 .$$%\left( q^2 + \Psi^2 + \frac{N^{-\e/2}}{N\eta}\right)^2.$$
%
%Finally, in the $l=3$ case, $\prod_{t=1}^{3}\left(\frac{1}{N}\sum_{j\in\sI_1}A_{ \mathbf e_j, i, \mu}(w_t)\right)$ is of the form 
%${N^{-3}}( G^{\times 2})^3_{\mathbf w_i\mathbf w_\mu}$, or an expression obtained by exchanging $\mathbf w_i$ and $\mathbf w_\mu$ in some of the three factors. Using equation \eqref{3.5term} and the bound in equation \eqref{3term}, we can estimate that %Using $\|\mathcal G_2\|\prec 1$, we can estimate $\left|\sum_i( G_2)^3_{\mathbf v_i\mu}\right|\prec 1$. Therefore 
%$$\frac{1}{N^3}\left|\sum_{i,\mu}( G^{\times 2})^3_{\mathbf w_i\mathbf w_\mu}\right| \prec \left( \Psi^2 + \frac{q}{N\eta}\right)\frac{1}{N^2}\sum_{i,\mu}\left|( G^{\times 2} )_{\mathbf w_\mu\mathbf w_i}\right|^2 \prec \eta^{-2}\left(\Psi^2 +\frac{q}{N\eta}\right)^2,$$
%Then the LHS of (\ref{eq_comp_goalAvg_genX}) is bounded by 
%$$O_\prec\left(\left(q^2 + \Psi^2\right) \left(\frac{N^{-\e/2}}{N\eta}\right)^2\right).$$
%
%Combining the above three cases, we conclude equation \eqref{comp_avg_geX_self-improving-bound}, which finishes the proof of equation \eqref{aver_in1} and equation \eqref{aver_out1}. %under equation \eqref{assm_3moment}.

%If $A$ or $B$ is diagonal, then by the remark at the end of Section \ref{subsec_3moment}, the anisotropic local law equation \eqref{aniso_law} holds for all $z\in S(c_0,C_0,\e)$ even in the case with $b_N=N^{1/2}$ in equation \eqref{assm_3moment}. Then with equation \eqref{aniso_law} and the self-consistent comparison argument in \cite[Section 9]{Anisotropic}, we can prove equation \eqref{aver_in} and equation \eqref{aver_out1} for $z\in S(c_0,C_0,\e)$. Again most of the arguments are the same as the ones in \cite[Section 9]{Anisotropic}, hence we omit the details. 

Finally, even if the condition \eqref{3moment} does not hold, using the self-consistent comparison argument in \cite[Section 9]{Anisotropic}, we can still prove \eqref{aver_in} for $G(Z,z)$. Again the arguments are almost the same as the ones in \cite[Section 9]{Anisotropic}, hence we omit the details.  

 



\iffalse
%In this section, we will work under the assumptions of Lemma \ref{lem_cov_shift}. However,
In random matrix theory, it is much more convenient to consider matrices $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ with an extra $n^{-1/2}$ factor, where we denote $n:=n_1+n_2$.
%rescale the matrices $Z^{(1)}$ and $Z^{(2)}$ such that their entries have variance $n^{-1}$, where $n:=n_1+n_2$. \HZ{I don't think is the point; I think you want to say that the sample covariance of both tasks $((Z^{(1)})^{\top}Z^{(1)} + (Z^{(2)})^{\top}Z^{(2)})/n$ is what matters}
The advantage of this scaling is that the singular eigenvalues of $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ all lie in a bounded support that does not grow with $n$. For reader's convenience, we first state the exact properties (some of which have already been stated in words in Section \ref{sec_prelim}) that we shall need for the results and proofs in this section. %Moreover, we will also introduce a couple of other notations.
%\HZ{Remove A.1 and A.2 below, which are already in Sec 2.}


%\textbf{Basic setting.}
%We denote the two sample covariance matrices by $\mathcal Q_1:=(X^{(1)})^\top X^{(1)}$ and $\cal Q_2:= (X^{(2)})^\top X^{(2)}$.
We have assumed that $Z^{(1)}$ and $Z^{(2)}$ are $n_1\times p$ and $n_2\times p$ random matrices with i.i.d. entries satisfying
\begin{equation}\label{assm1}
\mathbb{E} (Z^{(1)})_{11} =\mathbb{E} (Z^{(2)})_{11} =0, \ \quad \ \mathbb{E} \vert (Z^{(1)})_{11} \vert^2=\mathbb{E} \vert (Z^{(2)})_{11} \vert^2  =1.
\end{equation}
%\HZ{What is the subscript $11$ above?}
%where we denote $n:=n_1+n_2$. Here we have chosen the scaling that is more standard in the random matrix theory literature---under this $n^{-1/2}$ scaling, the eigenvalues of $\cal Q_1$ and $\cal Q_2$ are all of order 1.
%Moreover, we assume that the fourth moments exist:
%\be \label{conditionA2}
%\mathbb{E} \vert (Z^{(1)})_{11} \vert^4 +\mathbb{E} \vert (Z^{(2)})_{11} \vert^4  \le C,
%\ee
%for some constant $C>0$.
%In this paper, we regard $N$ as the fundamental (large) parameter and $n \equiv n(N)$ as depending on $N$.
Recall that we have defined $\rho_1= n_1/p$ and $\rho_2=n_2/p$ in introduction. We assume that they satisfy
%\HZ{We have defined $\rho_1,\rho_2$ already in sec 2. Consider using ``Recall that $\rho_1 =, \rho_2 =$.''}
\be\label{assm2}
0\le \rho_1 \le \tau^{-1}, \quad 1+\tau \le \rho_{2} \le \tau^{-1},
\ee
%\HZ{Remove $k$ above.}
for a small constant $0<\tau<1$.
\fi


 \iffalse
We assume that $\Sig_1$ and $\Sig_2$ have eigendecompositions
%\HZ{Using $O_1, O_2$ in eigendecomposition seems non-standard; suggestion: change to $Q_1, Q_2$.}
\be\label{eigen}\Sig_1= Q_1\Lambda_1 Q_1^\top, \ \ \Sig_2= Q_2\Lambda_2 Q_2^\top,\ \ \Lambda_1=\text{diag}(\lambda_1^{(1)}, \ldots, \lambda^{(1)}_p), \ \ \Lambda_2=\text{diag}( \lambda^{(2)}_1, \ldots, \lambda^{(2)}_p),
\ee
where the eigenvalues satisfy that
\begin{equation}\label{assm3}
\tau \le \lambda^{(1)}_p \le \ldots \le \lambda^{(1)}_2  \le \lambda^{(1)}_1 \le \tau^{-1}, \quad \tau \le   \lambda^{(2)}_p \le \ldots \le \lambda^{(2)}_2 \le \lambda^{(2)}_1 \le \tau^{-1}.
\ee
%such that $\Sigma_1$ and $\Sigma_2$ are both well-conditioned.


We assume that $M=\Sig_1^{1/2} \Sig_2^{-1/2}$ has a singular value decomposition \eqref{eigen2} .....
\be%\label{eigen2}
M= U\Lambda V^\top, \quad \Lambda=\text{diag}( \lambda_1, \ldots, \lambda_p),
\ee
where by equation \eqref{assm3} we have % the singular values satisfy that
\begin{equation}\label{assm32}
\tau \le \lambda_p \le \lambda_1 \le \tau^{-1} .%, \quad \max\left\{\pi_A^{(n)}([0,\tau]), \pi_B^{(n)}([0,\tau])\right\} \le 1 - \tau .
\end{equation}
\fi


%\begin{remark}
%Note that the Gaussian distribution satisfies the condition (\ref{eq_support}) with $q< N^{-\phi}$ for any $\phi<1/2$. We also remark that if (\ref{eq_support}) holds, then the event $\left\{\vert x_{ij}\vert \le q, \forall 1\le i \le M,1\le j \le N\right\}$ holds with $\xi$-overwhelming probability for any fixed $\xi>0$ according to Definition \ref{high_prob}. For this reason, the bad event $\left\{\vert x_{ij}\vert > q \text{ for some }i,j\right\}$ is negligible, and we will not consider the case it happens throughout the proof.
%\end{remark}

%Since $(Z^{(1)})^{\top} Z^{(1)}$ (resp.\;$(Z^{(2)})^\top Z^{(2)}$) is a standard sample covariance matrix, and it is well-known that its nonzero eigenvalues are all inside the support of the Marchenko-Pastur law $[(1-\sqrt{d_1})^2 , (1+\sqrt{d_1})^2]$ (resp. $[(1-\sqrt{d_2})^2 , (1+\sqrt{d_2})^2]$) with probability $1-\oo(1)$ \cite{No_outside}. In our proof, we shall need a slightly stronger probability bound, which is given by the following lemma.

\iffalse
Before entering into the formal proof, we state two preparation lemmas, both of which have been used crucially in some previous proofs. First, for $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ with bounded support $q$, we have the following estimates on their singular values.


%Then we state the following lemma on the eigenvalues of $(Z^{(1)})^{\top} Z^{(1)}$ and $(Z^{(2)})^\top Z^{(2)}$, which are denoted as $\lambda_1 ((Z^{(1)})^{\top} Z^{(1)}) \ge \cdots \ge \lambda_{p} ((Z^{(1)})^{\top} Z^{(1)})$ and $\lambda_1 ((Z^{(2)})^\top Z^{(2)}) \ge \cdots \ge \lambda_p ((Z^{(2)})^\top Z^{(2)})$. We have used it in our previous proofs; see equation \eqref{eq_isometric}.


\begin{lemma}\label{SxxSyy}
Suppose Assumption \ref{assm_big1} holds. % and $n^{-1/2}Z^{(1)}$ and $n^{-1/2}Z^{(2)}$ satisfy the bounded support condition \eqref{eq_support} for some deterministic parameter $q$ satisfying $ n^{-{1}/{2}} \leq q \leq n^{- \phi} $ for a small constant $\phi>0$.
Then for any constant $\e>0$, we have that with overwhelming probability,
\be\label{op rough1} %(1-\sqrt{d_1})^2 - \e \le  \lambda_p((Z^{(1)})^{\top} Z^{(1)})   \le
\lambda_1\left((Z^{(1)})^{\top} Z^{(1)}\right) \le {(\sqrt{n_1}+\sqrt{p})^2} + n_1^{1+\e} q,
\ee
and
\be\label{op rough2}
 (\sqrt{n_2}-\sqrt{p})^2  -  n_2^{1+\e} q \le  \lambda_p \left((Z^{(2)})^\top Z^{(2)}\right)  \le  \lambda_1\left((Z^{(2)})^\top Z^{(2)}\right) \le  (\sqrt{n_2}+\sqrt{p})^2 +  n_2^{1+\e} q .
\ee
where $\lambda_i(Z_k^\top Z_k)$, $k=1,2$ and $i=1,\cdots,p$, is the $i$-th largest eigenvalue of $Z_k^\top Z_k$.
\end{lemma}
\begin{proof}
%\HZ{This proof does not parse.}
%This lemma essentially follows from \cite[Theorem 2.10]{isotropic}, although the authors considered the case with $q = n^{-1/2}$ only. The results for the general case with $ n^{-{1}/{2}} \leq q \leq n^{- \phi} $ follows from \cite[Lemma 3.12]{DY}.
%, but only the bounds for largest eigenvalues are given there in order to avoid the issue with the smallest eigenvalue when $d_{2}$ is close to 1. However, under the assumption equation \eqref{assm2}, the lower bound for the smallest eigenvalue follows from the same arguments as in \cite{DY}. Hence we omit the details.
This lemma is a corollary of  \cite[Theorem 2.10]{isotropic} and \cite[Lemma 3.12]{DY}.
\end{proof}
\begin{remark}
it is well-known that the eigenvalues of $n_2^{-1}(Z^{(2)})^\top Z^{(2)}$ are all inside the support of the Marchenko-Pastur law $[(1-\sqrt{p/n_2})^2-\oo(1) ,(1+\sqrt{p/n_2})^2+\oo(1)]$ with probability $1-\oo(1)$ \cite{No_outside}. Here the estimate \eqref{op rough2} has improved the error to $n_2^\e q$ and the probability to $1-\OO(n^{-D})$.
\end{remark}
\fi




\iffalse
\subsection{Resolvents and Local Laws}\label{sec locallaw1}


Our main goal is to study the matrix inverse $((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}$ for $X^{(1)}= Z^{(1)}\Sigma_1^{1/2}$ and $X^{(2)}= Z^{(2)}\Sigma_2^{1/2}$.
%\begin{align*}
%(\cal Q_1+\cal Q_2)^{-1}=\left( \Sigma_1^{1/2}(Z^{(1)})^{\top} Z^{(1)}\Sigma_1^{1/2}+\Sigma_2^{1/2}(Z^{(2)})^\top Z^{(2)}\Sigma_2^{1/2}\right)^{-1} .
%\end{align*}
Using equation \eqref{eigen2}, we can rewrite it as
\be\label{eigen2extra}((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}= \Sigma_2^{-1/2}V\left(   \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + V^\top (Z^{(2)})^\top Z^{(2)}V\right)^{-1}V^\top\Sigma_2^{-1/2}.\ee
For our purpose, we introduce a convenient self-adjoint linearization trick,
%This idea dates back at least to Girko, see e.g., the works \cite{girko1975random,girko1985spectral} and references therein.
which has been proved to be useful in studying the local laws of random matrices of the Gram type \cite{Anisotropic, AEK_Gram, XYY_circular}. We define the following $(p+n)\times (p+n)$ self-adjoint block matrix, which is a linear function of $Z^{(1)}$ and $Z^{(2)}$:
%\begin{definition}[Linearizing block matrix]\label{def_linearHG}%definiton of the Green function
%We define the $(n+N)\times (n+N)$ block matrix
 \begin{equation}\label{linearize_block}
   H \equiv H(Z^{(1)},Z^{(2)}): = n^{-1/2}\left( {\begin{array}{*{20}c}
   { 0 } & \Lambda U^{\top}(Z^{(1)})^{\top} & V^\top (Z^{(2)})^\top  \\
   {Z^{(1)} U\Lambda  } & {0} & 0 \\
   {Z^{(2)}V} & 0 & 0
   \end{array}} \right).
 \end{equation}
For simplicity of notations, we define the index sets
$$\cal I_0:=\llbracket 1,p\rrbracket, \quad  \cal I_1:=\llbracket p+1,p+n_1\rrbracket, \quad \cal I_2:=\llbracket p+n_1+1,p+n_1+n_2\rrbracket ,\quad \cal I:=\cal I_0\cup \cal I_1\cup \cal I_2  .$$
 We will consistently use the latin letters $i,j\in\sI_{0}$, greek letters $\mu,\nu\in\sI_{1}\cup \sI_{2}$, and ${a},{b},\mathfrak c\in \cal I$. Correspondingly, the indices of the matrices $Z^{(1)}$ and $Z^{(2)}$ are labelled as
 \be\label{labelZ}
 Z^{(1)}= (z_{\mu i}:i\in \mathcal I_0, \mu \in \mathcal I_1), \quad Z^{(2)}= (z_{\nu i}:i\in \mathcal I_0, \nu \in \mathcal I_2).\ee
Now we define the resolvents as follows.
\begin{definition}[Resolvents]
We define the resolvent (or Green's function) of $H$ as
 \begin{equation}\label{eqn_defG}
 G \equiv G (Z^{(1)},Z^{(2)},z):= \left[H(Z^{(1)},Z^{(2)})-\left( {\begin{array}{*{20}c}
   { z\id_{p}} & 0 & 0 \\
   0 & { \id_{n_1}}  & 0\\
      0 & 0  & { \id_{n_2}}\\
\end{array}} \right)\right]^{-1} , \quad z\in \mathbb C .
 \end{equation}
%It is easy to verify that the eigenvalues $\lambda_1(H)\ge \ldots \ge \lambda_{n+N}(H)$ of $H$ are related to the ones of $\mathcal Q_1$ through
%\begin{equation}\label{Heigen}
%\lambda_i(H)=-\lambda_{n+N-i+1}(H)=\sqrt{\lambda_i\left(\mathcal Q_2\right)}, \ \ 1\le i \le n\wedge N, \quad \text{and}\quad \lambda_i(H)=0, \ \ n\wedge N + 1 \le i \le n\vee N.
%\end{equation}
%and
%$$\lambda_i(H)=0, \ \ n\wedge N + 1 \le i \le n\vee N.$$
%where we used the notations $n\wedge N:=\min\{N,M\}$ and $n\vee N:=\max\{N,M\}$.
%\begin{definition}[Index sets]\label{def_index}
and the resolvent of $  n^{-1}\Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + n^{-1}V^\top (Z^{(2)})^\top Z^{(2)}V$ as
\be\label{mainG}
\cal G(z):=\left( n^{-1}  \Lambda U^\top (Z^{(1)})^{\top} Z^{(1)} U\Lambda  + n^{-1}V^\top (Z^{(2)})^\top Z^{(2)}V -z\right)^{-1},\quad z\in \C.
\ee
Moreover, we also define the following averaged resolvents, which are the (weighted) partial traces of $G$:
\be\label{defm}
\begin{split}
m(z) :=\frac1p\sum_{i=1}^p G_{ii}(z) ,\quad & m_0(z):=\frac1p\sum_{i=1}^p \lambda_i^2 G_{ii}(z),\\
 m_1(z):= \frac{1}{n_1}\sum_{\mu =p+1}^{p+n_1}G_{\mu\mu}(z) ,\quad & m_2(z):= \frac{1}{n_2}\sum_{\nu=p+n_1+1}^{p+n_1+n_2}G_{\nu\nu}(z) .
\end{split}
\ee
\end{definition}

Then we define the resolvent minors, which are defined by removing certain rows and columns of the matrix $H$.

\begin{definition}[Resolvent minors]\label{defn_Minor}
 For any $ (p+n)\times (p+n)$ matrix $\cal A$ and ${a} \in \mathcal I$, we define the minor $\cal A^{(\mathfrak c)}:=(\cal A_{{a}{b}}:{a},{b} \in \mathcal I\setminus \{\mathfrak a\})$ as the $ (p+n-1)\times (p+n-1)$ matrix obtained by removing the $\mathfrak c$-th row and column in $\cal A$. Note that we keep the names of indices when defining $\cal A^{(\mathfrak c)}$, i.e. $(\cal A^{(\mathfrak c)})_{{a}{b}}= \cal A_{{a}{b}}$ for ${a},{b} \ne \mathfrak c$. Correspondingly, we define the resolvent minor as %(recall equation \eqref{green2})
\begin{align*}
G^{(\mathfrak c)}:&=\left[\left(H - \left( {\begin{array}{*{20}c}
   { zI_{p}} & 0  \\
   0 & { I_{n}}  \\
\end{array}} \right)\right)^{(\mathfrak c)}\right]^{-1} ,
%= \left( {\begin{array}{*{20}c}
%   { \mathcal G^{(\mathbb T)}} & \mathcal G^{(\mathbb T)} W^{(\mathbb T)}  \\
%   {\left(W^{(\mathbb T)}\right)^\top\mathcal G^{(\mathbb T)}} & { \mathcal G_R^{(\mathbb T)} }  \\
%\end{array}} \right)  ,
\end{align*}
and define the partial traces $m^{(\mathfrak c)}$ and $m^{(\mathfrak c)}_k$, $k=0,1,2,$ by replacing $G$ with $G^{(\mathfrak c)}$ in equation \eqref{defm}. For convenience, we will adopt the convention that for the resolvent minor $G^{(\mathfrak c)}$ defined as above, $G^{(\mathfrak c)}_{{a}{b}} = 0$ if ${a} =\mathfrak c$ or ${b} =\mathfrak c$.
\end{definition}

Note that the resolvent minor $G^{(\mathfrak c)}$ is defined such that it is independent of the entries in the $\mathfrak c$-th row and column of $H$. One will see a crucial use of this fact in the heuristic proof below.

Notice that using equation \eqref{mainG}, we can write  equation \eqref{eigen2extra} as
\be\label{rewrite X as R} ((X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)})^{-1}=n^{-1}\Sigma_2^{-1/2}V\cal G(0)V^\top\Sigma_2^{-1/2}.\ee
In the above definition, we have taken the argument of $\cal G$ to be a general complex number, because we will need to use $\cal G'(0)$ in the proof of Lemma \ref{lem_cov_derivative}, which requires a good estimate of $\cal G(z)$ for $z$ around the origin. Moreover, by Schur complement formula, we can obtain that
 \begin{equation} \label{green2}
 G (z):=  \left( {\begin{array}{*{20}c}
   { \cal G (z)} & \cal G(z)W  \\
   W^\top \cal G(z) & z \cal G_R
\end{array}} \right)^{-1},\quad \cal G_R:=(W^\top W - z)^{-1} ,
 \end{equation}
 where we abbreviated $W:=n^{-1/2}(\Lambda U^\top (Z^{(1)})^{\top}, V^\top (Z^{(2)})^\top)$, and
%one can find that %(recall equation \eqref{mainG})
%\be \label{green2}
%\begin{split}
%& \cal G_{L}=\left(WW^\top -z \right)^{-1} =\cal G ,\quad \cal G_{LR}=\cal G_{RL}^\top  = \cal G W , \quad \cal G_R= z\left(W^\top W - z\right)^{-1}.
%\end{split}
%\ee
the subindex of $\cal G_R$ means the lower-right block. This shows that a control of $G(z)$ yields directly a control of $\mathcal G(z)$. On the other hand, $G(z)$ is a little more easier to use than $\cal G(z)$ in the proof.
%Hence we choose to work with the


%Moreover, we denote $\overline i:= i+p$ for $i\in \cal I_1$, $\overline j:= j-p$ for $j\in \cal I_2$, $\overline \mu : = \mu +n $ for $\mu \in \cal I_3$, and $\overline \nu : = \nu - n $ for $\nu \in \cal I_4$.
%\end{definition}
%\begin{definition}[Resolvents]\label{resol_not}
%For $z = E+ \ii \eta \in \mathbb C_+,$ we define the resolvents $G(z)$.
%Then we denote the $\cal I_\al \times \cal I_\beta$ block of $ G(z)$ by $ \cal G_{\al\beta}(z)$ for $\al,\beta=1,2,3$. Moreover,
%Then we denote the $\cal I_1 \times \cal I_1$ block of $ G(z)$ by $ \cal G_{L}(z)$, the $\cal I_1 \times (\cal I_2 \cup \cal I_3)$ block by $\cal G_{LR}$, the $ (\cal I_2 \cup \cal I_3)\times \cal I_1$ block by $\cal G_{RL}$, and the $ (\cal I_2 \cup \cal I_3)\times (\cal I_2 \cup \cal I_3)$ block by $\cal G_R$.
%We denote the $(\cal I_1\cup \cal I_2)\times (\cal I_1\cup \cal I_2)$ block of $ G(z)$ by $ \cal G_L(z)$, the $(\cal I_1\cup \cal I_2)\times (\cal I_3\cup \cal I_4)$ block by $ \cal G_{LR}(z)$, the $(\cal I_3\cup \cal I_4)\times (\cal I_1\cup \cal I_2)$ block  by $ \cal G_{RL}(z)$, and the $(\cal I_3\cup \cal I_4)\times (\cal I_3\cup \cal I_4)$ block by $ \cal G_R(z)$.
%Recalling the notations in equation \eqref{def Sxy}, we define $\cal H:=S_{xx}^{-1/2}S_{xy}S_{yy}^{-1/2}$ and
%\be\label{Rxy}
%\begin{split}
% R_1(z):=(\cal C_{XY}-z)^{-1}&=(\cal H\cal H^T-z)^{-1}, \\
% R_2(z):=(\cal C_{YX}-z)^{-1}&=(\cal H^T\cal H-z)^{-1},  \quad m(z):= q^{-1}\tr  R_2(z).
% \end{split}
%\ee
%Note that we have $R_1\cal H = \cal HR_2$, $\cal H^T R_1 = R_2 \cal H^T $, and
%\be\label{R12} \tr  R_1 = \tr  R_2 - \frac{p-q}{z}= q  m(z) - \frac{p-q}{z},\ee
%since $\cal C_{XY}$ has $(p-q)$ more zeros eigenvalues than $\cal C_{YX}$.
%Finally, we can define ${\cal G}^b_L(z)$, ${\cal G}^b_R(z)$, $ m^b_\al(z)$, $\cal H^b$, $R_{1,2}^b$, etc.\;in the obvious way by replacing $Y$ with $\cal Y$.
%for $\wt {\mathcal Q}_{1,2}$ as
%\begin{equation}\label{def_green}
%\mathcal G_1(X,z):=\left(\wt{\mathcal Q}_1(X) -z\right)^{-1} , \ \ \ \mathcal G_2 (X,z):=\left(\wt{\mathcal Q}_2(X)-z\right)^{-1} .
%\end{equation}
% We denote the ESD $\rho^{(n)}$ of $\wt {\mathcal Q}_{1}$ and its Stieltjes transform as
%\be\label{defn_m}
%\rho\equiv \rho^{(n)} := \frac{1}{n} \sum_{i=1}^n \delta_{\lambda_i(\wt{\mathcal Q}_1)},\quad m(z)\equiv m^{(n)}(z):=\int \frac{1}{x-z}\rho_{1}^{(n)}(dx)=\frac{1}{n} \mathrm{Tr} \, \mathcal G_1(z).
%\ee
%We also introduce the following quantities:
%$$m_1(z)\equiv m_1^{(n)}(z):= \frac{1}{N}\sum_{i=1}^n\sigma_i (\mathcal G_1)_{ii}(z) ,\quad m_2(z)\equiv m_2^{(n)}(x):=\frac{1}{N}\sum_{\mu=1}^N \wt\sigma_\mu (\mathcal G_2)_{\mu\mu}(z). $$
%\end{definition}

%Now using Schur complement formula, we can verify that the (recall equation \eqref{def_green})
%\begin{align}
%G = \left( {\begin{array}{*{20}c}
%   { z\mathcal G_1} & \mathcal G_1 \Sig^{1/2} U^{*}X V\wt  \Sig^{1/2}  \\
%   {\wt \Sig^{1/2}V^\topX^\top U\Sig^{1/2} \mathcal G_1} & { \mathcal G_2 }  \\
%\end{array}} \right) = \left( {\begin{array}{*{20}c}
%   { z\mathcal G_1} & \Sig^{1/2} U^{*}X V\wt  \Sig^{1/2} \mathcal G_2   \\
%   {\mathcal G_2}\wt \Sig^{1/2}V^\topX^\top U\Sig^{1/2} & { \mathcal G_2 }  \\
%\end{array}} \right). \label{green2}
%\end{align}
%where $\mathcal G_{1,2}$ are defined in (\ref{def_green}).


\subsubsection{Asymptotic Limit of the Resolvent}\label{sec aymp_limit_G}



%We denote the eigenvalues of $\mathcal Q_1$ and $\mathcal Q_2$ in descending order by $\lambda_1(\mathcal Q_1)\geq \ldots \geq \lambda_{p}(\mathcal Q_1)$ and $\lambda_1(\mathcal Q_2) \geq \ldots \geq \lambda_p(\mathcal Q_2)$. Since $\mathcal Q_1$ and $\mathcal Q_2$ share the same nonzero eigenvalues, we will for simplicity write $\lambda_j$, $1\le j \le N\wedge n$, to denote the $j$-th eigenvalue of both $\mathcal Q_1$ and $\mathcal Q_2$ without causing any confusion.


%\subsection{Resolvents and limiting law}
%
%In this paper, we will study the eigenvalue statistics of $\mathcal Q_{1}$ and $\mathcal Q_2$ through their {\it{resolvents}} (or  {\it{Green's functions}}). It is equivalent to study the matrices
%\be\label{Qtilde}
%\wt{\mathcal Q}_1(X):=\Sig^{1/2} U^{*}XBX^\topU\Sig^{1/2}, \quad \wt{\mathcal Q}_2(X):=\wt\Sig^{1/2}V^\topX^\top A X V\wt \Sig^{1/2}.
%\ee
%In this paper, we shall denote the upper half complex plane and the right half real line by
%$$\mathbb C_+:=\{z\in \mathbb C: \im z>0\}, \quad \mathbb R_+:=[0,\infty).$$ %\quad  \mathbb R_*:=(0,\infty).$$
%
%\begin{definition}[Resolvents]\label{resol_not}
%For $z = E+ \ii \eta \in \mathbb C_+,$ we define the resolvents for $\wt {\mathcal Q}_{1,2}$ as
%\begin{equation}\label{def_green}
%\mathcal G_1(X,z):=\left(\wt{\mathcal Q}_1(X) -z\right)^{-1} , \ \ \ \mathcal G_2 (X,z):=\left(\wt{\mathcal Q}_2(X)-z\right)^{-1} .
%\end{equation}
% We denote the ESD $\rho^{(n)}$ of $\wt {\mathcal Q}_{1}$ and its Stieltjes transform as
%\be\label{defn_m}
%\rho\equiv \rho^{(n)} := \frac{1}{n} \sum_{i=1}^n \delta_{\lambda_i(\wt{\mathcal Q}_1)},\quad m(z)\equiv m^{(n)}(z):=\int \frac{1}{x-z}\rho_{1}^{(n)}(\dd x)=\frac{1}{n} \mathrm{Tr} \, \mathcal G_1(z).
%\ee
%We also introduce the following quantities:
%$$m_1(z)\equiv m_1^{(n)}(z):= \frac{1}{N}\sum_{i=1}^n\sigma_i (\mathcal G_1(z) )_{ii},\quad m_2(z)\equiv m_2^{(n)}(x):=\frac{1}{N}\sum_{\mu=1}^N \wt\sigma_\mu (\mathcal G_2(z) )_{\mu\mu}. $$
%
%%, \ \ \rho_{2}^{(n)} := \frac{1}{N} \sum_{i=1}^N \delta_{\lambda_i(\mathcal Q_2)}.$$
%%Then the Stieltjes transforms of $\rho_{1}$ is given by
%%\begin{align*}
%%& m_1^{(n)}(z):=\int \frac{1}{x-z}\rho_{1}^{(n)}(\dd x)=\frac{1}{n} \mathrm{Tr} \, \mathcal G_1(z).
%%%& m_2^{(n)}(z):=\int \frac{1}{x-z}\rho_{2}^{(n)}(\dd x)=\frac{1}{N} \mathrm{Tr} \, \mathcal G_2(z). %\label{ST_m12}
%%\end{align*}
%%and
%%\begin{equation}
%%m_2^{(n)}(z):=\int \frac{1}{x-z}d\rho_{2}^{M}(x)=\frac{1}{N}\sum_{i=1}^N (\mathcal G_2)_{ii}(z)=\frac{1}{N} \mathrm{Tr} \, \mathcal G_2(z). \label{ST_m2}
%%\end{equation}
%%Similarly, we can also define $m_1(z)\equiv m_1^{(M)}(z):= M^{-1}\mathrm{Tr} \, \mathcal G_1(z)$.
%\end{definition}


We now describe the asymptotic limit of $G(z)$ and $\cal G(z)$ as $n\to \infty$. First we define the deterministic limits of $(m_1(z), m_{2}(z))$, denoted by $(M_{1}(z),M_{2}(z))$
%\HZ{This $M_{1}, M_{2}$ notation is awkward; consider changing it to ${M}_1(z), {M}_2(z)$ (or  better, say $a_1(z), a_2(z)$? if they correspond to $a_1, a_2$ when $z=0$)},
as the unique solution to the following system of equations
\begin{equation}\label{selfomega}
\begin{split}
& \frac1{M_{1}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{\lambda_i^2}{  z+\lambda_i^2 r_1 M_{1} +r_2 M_{2}  } - 1 ,\  \ \frac1{M_{2}} = \frac{\gamma_n}p\sum_{i=1}^p \frac{1 }{  z+\lambda_i^2 r_1 M_{1} +  r_2 M_{2}  }- 1 ,
\end{split}
\ee
such that $(M_{1}(z), M_{2}(z))\in \C_+^2$ for $z\in \C_+$, where, for simplicity, we  introduced the following ratios
\be\label{ratios}
 \gamma_n :=\frac{p}{n}=\frac{1}{\rho_1+\rho_2},\quad r_1 :=\frac{n_1}{n}=\frac{\rho_1}{\rho_1+\rho_2},\quad r_2 :=\frac{n_2}{n}=\frac{\rho_2}{\rho_1+\rho_2}.
\ee
We then define the matrix limit of $G(z)$ as
\be \label{defn_piw}
\Pi(z) := \begin{pmatrix} -(z+r_1 M_{1}(z)\Lambda^2  +  r_2 M_{2}(z))^{-1} & 0 & 0 \\ 0 &  M_{1}(z)\id_{n_1} & 0 \\ 0 & 0 & M_{2}(z)\id_{n_2}  \end{pmatrix}.\ee
In particular, the matrix limit of $\cal G(z)$ is given by $-(z+r_1 M_{1}\Lambda^2 + r_2 M_{2})^{-1}$.

\vspace{5pt}
\noindent{\bf Proof overview:}
\HZ{Divide into several parts like sec 7 to improve readability.}
Now we give a heuristic derivation of the matrix limit when the entries of $Z^{(1)}$ and $Z^{(2)}$ are i.i.d. Gaussian. Note that in this case,
%However, notice that if the entries of $Z^{(1)}\equiv (Z^{(1)})^{\text{Gauss}}$ and $Z^{(2)}\equiv (Z^{(2)})^{\text{Gauss}}$ are i.i.d. Gaussian, then
by the rotational invariance of the multivariate Gaussian distribution we have
\be\label{eq in Gauss} Z^{(1)} U\Lambda \stackrel{d}{=} Z^{(1)} \Lambda, \quad Z^{(2)} V \stackrel{d}{=} Z^{(2)},\ee
where ``$\stackrel{d}{=}$" means ``equal in distribution". Hence it suffices to consider the following resolvent
 \begin{equation} \nonumber
   G(z)= \left( {\begin{array}{*{20}c}
   { -z\id_{p} } & n^{-1/2}\Lambda (Z^{(1)})^{\top} & n^{-1/2} (Z^{(2)})^\top  \\
   {n^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1}} & 0 \\
   {n^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2}}
   \end{array}} \right)^{-1}.
 \end{equation}
Using Schur complement formula (see \alert{equation} (\ref{resolvent2}) \HZ{check other usage of equation ref}), we have that %for $i \in \cal I_0$, $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
\begin{align}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu,\nu\in \mathcal I_1} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu} - \frac{1}{n} \sum_{\mu,\nu\in \mathcal I_2} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu} -2 \frac{\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} z_{\mu i}z_{\nu i}G^{\left( i \right)}_{\mu\nu},  \ \ \text{for }\  i \in \cal I_0 , \label{0self_Gii}\\
\frac{1}{{G_{\mu\mu} }}&=  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}\lambda_i \lambda_j z_{\mu i}z_{\mu j} G^{\left(\mu\right)}_{ij}, \quad \frac{1}{{G_{\nu\nu} }}=  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}  z_{\nu i}z_{\nu j}  G^{\left(\nu\right)}_{ij},  \ \ \text{for }\  \mu \in \cal I_1, \ \nu\in \cal I_2, \label{0self_Gmu1}
\end{align}
where we recall the notations in equation \eqref{labelZ}. For the right-hand side of equation \eqref{0self_Gii}, $G^{(i)}$ is independent of the entries $z_{\mu i}$ and $z_{\nu i}$. Hence by the concentration inequalities in Lemma \ref{largedeviation}, we have that the  right-hand side of equation \eqref{0self_Gii} concentrates around the partial expectation over the entries $\{z_{\mu i}: \mu \in \cal I_1\cup \cal I_2\}$, i.e., with overwhelming probability,
\begin{align*}
\frac{1}{{G_{ii} }}&=  - z - \frac{\lambda_i^2}{n} \sum_{\mu \in \mathcal I_1}  G^{\left( i \right)}_{\mu\mu} - \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} +\oo(1)= - z - \lambda_i^2 r_1 m_1^{(i)}(z)-  r_2m_2^{(i)}(z)+\oo(1),
\end{align*}
where we used the definition of $m_1^{(i)}$ and $m_2^{(i)}$ in equation \eqref{defm} with $G$ replaced by $G^{(i)}$. Intuitively, since we have only removed only one column and one row out of the $(p+n)$ columns and rows in $H$, $m_1^{(i)}$ and $m_2^{(i)}$ should be close to the original $m_1$ and $m_2$. Hence we obtain from the above equation that
\begin{align}\label{1self_Gii}
 G_{ii}  = -\frac{1}{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}.
\end{align}
Similarly, we can obtain from equation \eqref{0self_Gmu1} that for $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
\be\label{1self_Gmu} G_{\mu \mu }=-\frac{1}{1+\gamma_n m_0 + \oo(1)},\quad G_{\nu\nu}=-\frac1{1+\gamma_n m+\oo(1)},\ee
with overwhelming probability. Taking average we obtain that
\be\label{2self_Gmu} m_1= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu\mu}=-\frac{1}{1+\gamma_n m_0 + \oo(1)},\quad m_2=\frac{1}{n_2}\sum_{\nu \in \cal I_2}G_{\nu\nu}=-\frac{1}{1+\gamma_n m + \oo(1)},
\ee
with overwhelming probability. Together with the definition of $m$ and $m_0$ in equation \eqref{defm}, the two equations in equation \eqref{2self_Gmu} give that
\be\label{3self_Gmu}  \frac1{m_1}= -1- \frac{\gamma_n}{p} \sum_{i=1}^p \lambda_i^2 G_{ii}+ \oo(1),\quad \frac1{m_2}=-1-\frac{\gamma_n}{p} \sum_{i=1}^p G_{ii}  + \oo(1),\ee
with overwhelming probability. Plugging equation \eqref{1self_Gii} into equation \eqref{3self_Gmu}, we obtain that
\begin{align*}
& \frac1{m_1}= -1+ \frac{\gamma_n}{p} \sum_{i=1}^p \frac{\lambda_i^2 }{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}+ \oo(1),\\
& \frac1{m_2}=-1+\frac{\gamma_n}{p} \sum_{i=1}^p \frac{1}{ z +\lambda_i^2 r_1  m_1(z) + r_2 m_2(z)+\oo(1)}  + \oo(1),
\end{align*}
with overwhelming probability, which give the approximate self-consistent equations for $(m_1,m_2)$. Compare them to the deterministic self-consistent equations in equation \eqref{selfomega}, one can observe that we should have $(m_1,m_2) =(M_{1}, M_{2})+\oo(1)$ with overwhelming probability. Inserting this approximate identity into equation \eqref{1self_Gii}-equation \eqref{2self_Gmu}, we see that for  $i \in \cal I_0$, $\mu \in \cal I_1$ and $\nu\in \cal I_2$,
$$G_{ii}=-(z +\lambda_i^2 r_1  M_{1} + r_2 M_{2}+\oo(1))^{1/2},\quad G_{\mu\mu}=M_{1}+\oo(1),\quad G_{\nu\nu}=M_{2}+\oo(1),$$
with overwhelming probability. These explain the diagonal entries of $\Pi$ in equation \eqref{defn_piw}. For the off-diagonal entries, they are close to zero due to concentration. For example, for $i\ne j\in \cal I_1$, by Schur complement formula (see (\ref{resolvent3})), we have
$$G_{ij}=-G_{ii}\left(\frac{\lambda_i}{n^{1/2}}\sum_{\mu \in \cal I_1} z_{\mu i} G^{(i)}_{\mu j} + \frac{1}{n^{1/2}}\sum_{\mu \in \cal I_2} z_{\mu i} G^{(i)}_{\mu j} \right).$$
Using Lemma \ref{largedeviation}, we can show that $n^{-1/2}\sum_{\mu \in \cal I_1} z_{\mu i} G^{(i)}_{\mu j}$ and $n^{-1/2}\sum_{\mu \in \cal I_2} z_{\mu i} G^{(i)}_{\mu j}$ are both close to zero. The other off-diagonal entries can be bounded in the same way.

The above arguments are the core of the main proof. To have a rigorous proof, we need to estimate each error carefully, and extend the Gaussian case to the more general case where the entries of $Z^{(1)}$ and $Z^{(2)}$ only satisfy certain moment assumptions. These will make the real argument rather tedious, but the methods we used are standard in the random matrix literature \cite{erdos2017dynamical,Anisotropic}.

\fi


%\subsubsection{The Local Laws}\label{sec locallaw1}
%In the following proof, we choose a sufficiently small constants $c_0>0$ such that Lemma \ref{lem_mbehaviorw} and Lemma \ref{lem_stabw} hold. Then


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please use \tableofcontents for articles %%
%% with 50 pages and more                   %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tableofcontents


\bibliographystyle{imsart-number}
\bibliography{rf,ref_mtl}

\end{document}
