\section{Discussion and conclusions}\label{sec_conclude}

This work studied generalization properties of a widely used hard parameter sharing approach for multi-task learning.
We provided sharp bias-variance tradeoffs of HPS in high-dimensional linear regression.
Using these results, we analyzed how varying sample sizes and covariate shifts impact HPS, and rigorously explained several empirical phenomena such as negative transfer and covariate shift related to these dataset properties.
We validated our theory and conducted further studies on text classification tasks.
We describe several open questions for future work.
%First, it would be interesting to tighten our estimate in Corollary \ref{cor_MTL_loss}, which would extend the observation in Figure \ref{fig_size} to small $n_1$.
%Second, it would be interesting to extend our result to classification problems such as logistic regression.
First, our result in Corollary \ref{cor_MTL_loss} involves an error term that scales down with $n_1$.
Tightening this error bound requires showing the limit of $\normFro{({Z^{(1)}}^{\top} Z^{(1)} + {Z^{(2)}}^{\top} Z^{(2)})^{-1} {Z^{(1)}}^{\top} Z^{(1)}}^2$ for two isotropic sample covariance matrices.
This requires studying the asymptotic singular values distribution of the non-symmetric matrix $({Z^{(1)}}^{\top} Z^{(1)})^{-1}{Z^{(2)}}^{\top} Z^{(2)}+\id$, which is still an open problem in random matrix theory.
%FY: $+\id$ is very important and makes the problem very hard; otherwise the problem can be solved with current RMT methods.
The eigenvalue distribution of this matrix, which has been obtained in \cite{Fmatrix}, might be helpful towards resolving this problem.
%but its singular values will follow a different distribution since the matrix is not symmetric.
 %might require new techniques beyond the current ones in random matrix theory .%\HZ{to add}.
Second, it would be interesting to extend our results to classification problems.
Several recent work have made remarkable progress for logistic regression in the high-dimensional setting, e.g. \cite{sur2019modern}.
An interesting question is to study logistic regression in a multiple-sample setting.


%\begin{supplement}
%\textbf{Supplement to ``High-dimensional Asymptotics of Transferring from Different Domains in Linear Regression"}.
%In the Internet Appendix \cite{MTL_suppl}, we provide the proofs of the technical results in Sections \ref{sec_HPS}-\ref{sec_diff}, including Lemma \ref{lem_HPS_loss}, Theorem \ref{thm_many_tasks}, Theorem \ref{cor_MTL_loss}, Proposition \ref{lem_hat_v}, Theorem \ref{thm_main_RMT} and Proposition \ref{prop_main_RMT}.
%and give the technical proofs of the main theorems, Theorems \ref{thm_regularbm}, \ref{thm_twgram}, \ref{thm_twgraph} and  \ref{thm_twgraph_sparse}.
%\end{supplement}
