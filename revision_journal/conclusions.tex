\section{Conclusions}\label{sec_conclude}

Distribution shift is a fundamental challenge in applying transfer learning.
This work formulated a theoretical setup in which questions related to the transfer effect can be formally analyzed.
The setup can reproduce several interesting phenomena in the context of transfer learning.
Precise asympotics for the risk of a two-layer linear neural network are shown under various kinds of distribution shift.
Such analysis has led to a number of theoretical insights and practical implications for applying transfer learning to text classification tasks.

%We describe several open questions for future work.
%First, it would be interesting to tighten our estimate in Corollary \ref{cor_MTL_loss}, which would extend the observation in Figure \ref{fig_size} to small $n_1$.
%Second, it would be interesting to extend our result to classification problems such as logistic regression.
It is a very interesting question to derive the asymptotic limit under general covariate and model shift.
%This requires showing the limit of $\normFro{\big(({Z^{(1)}})^{\top} Z^{(1)} + {Z^{(2)}}^{\top} Z^{(2)}\big)^{-1} {Z^{(1)}}^{\top} Z^{(1)}}^2$ for two sample covariance matrices.
We remark that likely such a result will require studying the asymptotic distribution of the singular values of asymmetric matrices, which is technically challenging.
%FY: $+\id$ is very important and makes the problem very hard; otherwise the problem can be solved with current RMT methods.
%The eigenvalue distribution of this matrix, which has been obtained in \cite{Fmatrix}, might be helpful towards resolving this problem.
%but its singular values will follow a different distribution since the matrix is not symmetric.
 %might require new techniques beyond the current ones in random matrix theory .%\HZ{to add}.
Another interesting question is to study the impact of distribution shift in other settings such as logistic regression \cite{sur2019modern}.

%\begin{supplement}
%\textbf{Supplement to ``High-dimensional Asymptotics of Transferring from Different Domains in Linear Regression"}.
%In the Internet Appendix \cite{MTL_suppl}, we provide the proofs of the technical results in Sections \ref{sec_HPS}-\ref{sec_diff}, including Lemma \ref{lem_HPS_loss}, Theorem \ref{thm_many_tasks}, Theorem \ref{cor_MTL_loss}, Proposition \ref{lem_hat_v}, Theorem \ref{thm_main_RMT} and Proposition \ref{prop_main_RMT}.
%and give the technical proofs of the main theorems, Theorems \ref{thm_regularbm}, \ref{thm_twgram}, \ref{thm_twgraph} and  \ref{thm_twgraph_sparse}.
%\end{supplement}
