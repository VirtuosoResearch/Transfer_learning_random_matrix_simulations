\section{Proof of Theorem \ref{thm_main_RMT}}\label{appendix RMT}




We begin with a warm up analysis when the entries of $Z^{(1)}$ and $Z^{(2)}$ are drawn i.i.d. from an isotropic Gaussian distribution.
By the rotational invariance of the multivariate Gaussian distribution, we have that the entries of $Z^{(1)} U$ and $Z^{(2)}V$ also follow an isotropic Gaussian distribution.
Hence it suffices to consider the following resolvent
 \begin{equation} \label{resolv Gauss1}
   G(z)= \left( {\begin{array}{*{20}c}
   { -z\id_{p\times p} } & (n_1+n_2)^{-1/2}\Lambda (Z^{(1)})^\top & (n_1+n_2)^{-1/2} (Z^{(2)})^\top  \\
   {(n_1+n_2)^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1\times n_1}} & 0 \\
   {(n_1+n_2)^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2\times n_2}}
   \end{array}} \right)^{-1}.
 \end{equation}
We show how to derive the matrix limit $\Gi(z)$ and the self-consistent equation system \eqref{selfomega_a}.
We first introduce several useful notations. We define $n:=n_1+n_2$ and the following index sets
$$\cal I_0:=\llbracket 1,p\rrbracket, \quad  \cal I_1:=\llbracket p+1,p+n_1\rrbracket, \quad \cal I_2:=\llbracket p+n_1+1,p+n_1+n_2\rrbracket ,\quad \cal I:=\cal I_0\cup \cal I_1\cup \cal I_2  .$$
We will study the following partial traces of the resolve $G(z)$:
\be\label{defm}
\begin{split}
m(z) :=\frac1p\sum_{i\in \cal I_0} G_{ii}(z) ,\quad & m_0(z):=\frac1p\sum_{i\in \cal I_0} \lambda_i^2 G_{ii}(z),\\
 m_1(z):= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu\mu}(z) ,\quad & m_2(z):= \frac{1}{n_2}\sum_{\nu\in \cal I_2}G_{\nu\nu}(z).
\end{split}
\ee
To deal with the matrix inverse, we consider the following resolvent minors of $G(z)$.
\begin{definition}[Resolvent minors]\label{defn_Minor}
	Let $X \in \real^{(p + n_1 + n_2)\times (p + n_1 + n_2)}$ and $i = 1, 2, \dots, p + n_1 + n_2$.
	The minor of $X$ after removing the $i$-th row and column of $X$ is denoted by $X^{(i)} := [X_{a_1a_2}:a_1, a_2 \in \mathcal I\setminus \{i\}]$ as a square matrix with dimension $p + n_1 + n_2 - 1$.
	For the indices of $X^{(i)}$, we use $X^{(i)}_{a_1 a_2}$ to denote $ X_{a_1 a_2}$ when $a_1$ and $a_2$ are both not equal to $i$, and $X^{(i)}_{a_1 a_2}=0$ when $a_1 = i$ or $a_2 = i$.
	The resolvent minor of $G(z)$ after removing the $i$-th row and column is defined as
	\begin{align*}
		G^{(i)}(z) := \left[ \left( {\begin{array}{*{20}c}
		  { -z\id_{p\times p} } & n^{-1/2}\Lambda (Z^{(1)})^\top & n^{-1/2} (Z^{(2)})^\top  \\
      {n^{-1/2} Z^{(1)} \Lambda  } & {-\id_{n_1\times n_1}} & 0 \\
			{n^{-1/2} Z^{(2)}} & 0 & {-\id_{n_2\times n_2}}
    \end{array}} \right)^{(i)}\right]^{-1}.
	\end{align*}
\end{definition}
As a remark, we define the partial traces $m^{(i)}(z)$, $m_0^{(i)}(z)$, $m_1^{(i)}(z)$, and $m_2^{(i)}(z)$ by replacing $G(z)$ with $G^{(i)}(z)$ in equation \eqref{defm}.

\paragraph{Self-consistent equations.}
We briefly describe the ideas for deriving the system of self-consistent equations \eqref{selfomega_a}.
A complete proof can be found in Lemma \ref{lemm_selfcons_weak}.
We show that with high probability, the following equations hold approximately:
\be\label{approximate m1m2}
\begin{split}
& m_1^{-1}(z) = -1+ \frac{1}{n} \sum_{i=1}^p \frac{\lambda_i^2 }{ z +\lambda_i^2 \frac{n_1}{n_1+ n_2} m_1(z) +  \frac{n_2}{n_1+n_2}m_2(z)+\oo(1)}+ \oo(1),\\
& m_2^{-1}(z) = -1+\frac{1}{n} \sum_{i=1}^p \frac{1}{ z +\lambda_i^2 \frac{n_1}{n_1 + n_2} m_1(z) +  \frac{n_2}{n_1+n_2}m_2(z)+\oo(1)}  + \oo(1).
\end{split}
\ee
With algebraic calculations, it is not hard to verify that these equations reduce to the self-consistent equations that we stated in equation \eqref{selfomega_a} up to a small error $\oo(1)$.
More precisely, we have that $m_1(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_1} a_1(z) $ and $m_2(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_2} a_2(z)$.

The core idea is to study $G(z)$ using the Schur complement formula.
First, we consider the diagonal entries of $G(z)$ for each block in $\cal I_0$, $\cal I_1$, and $\cal I_2$.
For any $i$ in $\cal I_0$, any $\mu$ in $\cal I_1$, and any $\nu$ in $\cal I_2$, we have that
\begin{align*}
	&G_{ii}^{-1}(z) = -z - \frac{\lambda_i^2}{n} \sum_{\mu,\nu\in \mathcal I_1} Z^{(1)}_{\mu i}Z^{(1)}_{\nu i}G^{\left( i \right)}_{\mu\nu}(z) - \frac{1}{n} \sum_{\mu,\nu\in \mathcal I_2} Z^{(2)}_{\mu i}Z^{(2)}_{\nu i} G^{\left( i \right)}_{\mu\nu}(z) -\frac{2\lambda_i}{n} \sum_{\mu\in \cal I_1,\nu\in \mathcal I_2} Z^{(1)}_{\mu i}Z^{(2)}_{\nu i}G^{\left( i \right)}_{\mu\nu}(z) \\
	&G_{\mu\mu}^{-1}(z) =  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}\lambda_i \lambda_j Z^{(1)}_{\mu i}Z^{(1)}_{\mu j} G^{\left(\mu\right)}_{ij}(z) \\
	&G_{\nu\nu}^{-1}(z) =  - 1 - \frac{1}{n} \sum_{i,j\in \mathcal I_0}  Z^{(2)}_{\nu i}Z^{(2)}_{\nu j}  G^{\left(\nu\right)}_{ij}(z).
\end{align*}
For the first equation, we expand the Schur complement formula $G_{ii}^{-1}(z) = -z - H_i G^{(i)}(z) H_{i}^\top$, where $H_i$ is the $i$-th row of $H$ with the $(i,i)$-th entry removed.
The second and third equations follow by similar calculations.


Next, we apply standard concentration bounds to simplify the above results.
For $G^{-1}_{i i}(z)$, recall that the resolvent minor $G^{(i)}$ is defined such that it is independent of the $i$-th row and column of $Z^{(1)}$ and $Z^{(2)}$.
Hence by standard concentration inequalities, we have that the cross terms are approximately zero.
As shown in Lemma \ref{lemm_selfcons_weak}, we have that with high probability the following holds
\begin{align*}
	G_{ii}^{-1}(z) &= -z - \frac{\lambda_i^2}{n} \sum_{\mu \in \mathcal I_1}  G^{\left( i \right)}_{\mu\mu} - \frac{1}{n} \sum_{\mu\in \mathcal I_2} G^{\left( i \right)}_{\mu\mu} +\oo(1) \\
	&= - z - \frac{\lambda_i^2 \cdot n_1}{n_1 + n_2} m_1^{(i)}(z)-  \frac{n_2}{n_1 + n_2} m_2^{(i)}(z)+\oo(1),
\end{align*}
by our definition of the partial traces $m_1^{(i)}(z)$ and $m_2^{(i)}(z)$. %
Since we have removed only one column and one row from $H(z)$, $m_1^{(i)}(z)$ and $m_2^{(i)}(z)$ should be approximately equal to $m_1(z)$ and $m_2(z)$.
Hence we obtain that
\begin{align}\label{1self_Gii}
	G_{ii}(z)  = -\left(z + \frac{\lambda_i^2 \cdot n_1}{n_1 + n_2} m_1(z) +  \frac{n_2}{n_1 + n_2} m_2(z) + \oo(1)\right)^{-1}.
\end{align}
For the other two blocks $\cal I_1$ and $\cal I_2$, using similar ideas we obtain the following equations with high probability:
\begin{align*}
	G_{\mu\mu}(z) &= -\left(1+\frac{p}{n_1 + n_2} m_0(z) + \oo(1)\right)^{-1},\\ 
	G_{\nu\nu}(z) &= -\left(1+\frac{p}{n_1 + n_2} m(z) + \oo(1)\right)^{-1}.
\end{align*}
By averaging the above results over $\mu \in \cal I_1$ and $\nu \in \cal I_2$, we obtain that with high probability
\begin{align*}
	m_1(z) &= \frac{1}{n_1}\sum_{\mu \in \cal I_1}G_{\mu\mu}(z) = -\left(1+\frac{p}{n_1 + n_2} m_0(z) + \oo(1)\right)^{-1} ,\\
	m_2(z) &= \frac{1}{n_2}\sum_{\nu \in \cal I_2}G_{\nu\nu}(z) = -\left(1+\frac{p}{n_1 + n_2} m(z) + \oo(1)\right)^{-1}.
\end{align*}
Furthermore, we obtain that for $\mu \in \cal I_1$ and $\nu\in \cal I_2$, with high probability
$G_{\mu\mu}(z) = m_1(z) +\oo(1)$ and $G_{\nu\nu}(z) = m_2+\oo(1)$.
In other words, both block matrices within $\cal I_1$ and $\cal I_2$ are approximately a scaling of the identity matrix.
The above results for $m_1(z)$ and $m_2(z)$ imply that
\begin{align*}
	m_1^{-1}(z) &= -1- \frac{1}{n } \sum_{i=1}^p \lambda_i^2 G_{ii}(z)+ \oo(1),\\ 
	 m_2^{-1}(z) &= -1- \frac{1}{n } \sum_{i=1}^p G_{ii}(z)  + \oo(1),
\end{align*}
where we used the definitions of $m(z)$ and $m_0(z)$.
By applying equation \eqref{1self_Gii} for $G_{i i}(z)$ to these two equations, we obtain the system of self-consistent equations \eqref{approximate m1m2}.
In Lemma \ref{lem_stabw}, we show that the self-consistent equations are stable, that is, a small perturbation of the equations leads to a small perturbation of the solution.

\paragraph{Matrix limit.}
Finally, we derive the matrix limit $\Gi(z)$.
We have shown that $m_1(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_1} a_1(z) $ and $m_2(z)$ is approximately equal to $-\frac{n_1 + n_2}{n_2} a_2(z)$ because we know that \eqref{approximate m1m2} holds.
Inserting $m_1(z)$ and $m_2(z)$ into equation \eqref{1self_Gii}, we get that for $i$ in $\cal I_0$,
$G_{ii}(z) = (-z +\lambda_i^2 a_{1}(z) + a_2(z)+\oo(1))^{-1}$ with high probability.
For $\mu$ in $\cal I_1$ and $\nu$ in $\cal I_2$, by $G_{\mu\mu}(z) = m_1(z) +\oo(1)$ and $G_{\nu\nu}(z) = m_2+\oo(1)$, we have that $G_{\mu \mu}(z) = -\frac{n_1 + n_2}{n_1}a_{1}(z)+\oo(1)$ and $G_{\nu\nu}(z) = -\frac{n_1 + n_2}{n_2}a_{2}(z)+\oo(1)$ with high probability.
Hence we have derived the diagonal entries of $\Gi(z)$.
In Lemma \ref{Z_lemma}, we show that the off-diagonal entries are close to zero.
For example, for $i\ne j\in \cal I_0$, by Schur complement, we have that
$$G_{ij}(z) = -G_{ii}(z)\cdot {n^{-1/2}}\Big({\lambda_i}\sum_{\mu \in \cal I_1} Z^{(1)}_{\mu i} G^{(i)}_{\mu j}(z) + \sum_{\mu \in \cal I_2} Z^{(2)}_{\mu i} G^{(i)}_{\mu j}(z) \Big).$$
Using standard concentration inequalities, we can show that $\sum_{\mu \in \cal I_1} Z^{(1)}_{\mu i} G^{(i)}_{\mu j}(z)$ and $\sum_{\mu \in \cal I_2} Z^{(2)}_{\mu i} G^{(i)}_{\mu j}(z)$ are both close to zero.
The other off-diagonal entries are bounded similarly.


\paragraph{Notations.}
We introduce several useful notations for the proof of Theorem \ref{thm_main_RMT}.
We say that an event $\Xi$ holds with overwhelming probability if for any constant $D>0$, $\mathbb P(\Xi)\ge 1- n^{-D}$ for large enough $n$.
Moreover, we say $\Xi$ holds with overwhelming probability in an event $\Omega$ if for any constant $D>0$, $\mathbb P(\Omega\setminus \Xi)\le n^{-D}$ for large enough $n$.
The following notion of stochastic domination, which was first introduced in \citet{Average_fluc}, is commonly used in the study of random matrices.

\begin{definition}[Stochastic domination]\label{stoch_domination}
Let $\xi\equiv \xi^{(n)}$ and $\zeta\equiv \zeta^{(n)}$ be two $n$-dependent random variables.
We say that $\xi$ is stochastically dominated by $\zeta$, denoted by $\xi\prec \zeta$ or $\xi=\OO_\prec(\zeta)$, if for any small constant $\e > 0$ and any large constant $D > 0$, there exists a function $n_0(\e, D)$ such that for all $n > n_0(\e, D)$,
\[ \bbP\left(|\xi| >n^\e |\zeta|\right)\le n^{-D}. \]
In case $\xi(u)$ and $\zeta(u)$ is a function of $u$ supported in $\cal U$, then we say $\xi(u)$ is stochastically dominated by $\zeta(u)$ uniformly in $\cal U$ if %
\[ \sup_{u\in \cal U}\bbP\left(|\xi(u)|>n^\e |\zeta(u)|\right)\le n^{-D}. \]
\end{definition}

We make several remarks.
First, since we allow an $n^\e$ factor in stochastic domination, we can ignore $\log$ factors without loss of generality since $(\log n)^C\prec 1$ for any constant $C>0$.
Second, given a random variable $\xi$ whose moments exist up to any order, we have that $|\xi|\prec 1$.
This is because by Markov's inequality, let $k$ be larger than $D/ \e$, then we have that
$$ \P(|\xi|\ge n^{\e})\le n^{-k\e}{\E |\xi|^k}\le n^{-D}.$$
As a special case, this implies that a Gaussian random variable $\xi$ with unit variance satisfies that $|\xi|\prec 1$.

The following fact collects several basic properties that are often used in the proof. Roughly speaking, it shows that  the stochastic domination ``$\prec$" behaves in the same way as  ``$<$" in some sense.

\begin{fact}[Lemma 3.2 in \citet{isotropic}]\label{lem_stodomin}
Let $\xi$ and $\zeta$ be two families of nonnegative random variables depending on some parameters $u\in \cal U$ or $v\in \cal V$.
\begin{itemize}
\item[(i)] Suppose that $\xi (u,v)\prec \zeta(u,v)$ uniformly in $u\in \cal U$ and $v\in \cal V$. If $|\cal V|\le n^C$ for some constant $C>0$, then $\sum_{v\in \cal V} \xi(u,v) \prec \sum_{v\in \cal V} \zeta(u,v)$ uniformly in $u$.

\item[(ii)] If $\xi_1 (u)\prec \zeta_1(u)$ and $\xi_2 (u)\prec  \zeta_2(u)$ uniformly in $u\in \cal U$, then $\xi_1(u)\xi_2(u) \prec \zeta_1(u) \zeta_2(u)$ uniformly in $u\in \cal U$.

\item[(iii)] Suppose that $\Psi(u)\ge n^{-C}$ is a family of deterministic parameters, and $\xi(u)$ satisfies $\mathbb E\xi(u)^2 \le n^C$. If $\xi(u)\prec \Psi(u)$ uniformly in $u$, then we also have $\mathbb E\xi(u) \prec \Psi(u)$ uniformly in $u$.
\end{itemize}
\end{fact}


Next, we introduce the bounded support assumption for a random matrix.
We say that a random matrix $Z \in\real^{n \times p}$ satisfies the {\it{bounded support condition}} with $Q$ or $Z$ has support $Q$ if
\begin{equation}
	\max_{1\le i \le n, 1 \le j \le p}\vert Z_{i j} \vert \prec Q. \label{eq_support}
\end{equation}
As shown in the example above, if the entries of $Z$ have finite moments up to any order, then $Z$ has bounded support $1$.
More generally, if the entries of $Z$ have finite $\varphi$-th moment, then using Markov's inequality and a simple union bound we get that %
\begin{align}
	\P\left(\max_{1\le i\le n, 1\le j \le p}|Z_{i  j}|\ge (\log n) n^{\frac{2}{\varphi}}\right) &\le \sum_{i=1}^n \sum_{j=1}^p \P\left(|Z_{i j}|\ge (\log n) n^{\frac{2}{\varphi}}\right)  \nonumber\\
	&\le \sum_{i=1}^n \sum_{j=1}^p \frac{C(\varphi)}{\left[(\log n) n^{\frac{2}{\varphi}}\right]^\varphi} = \OO((\log n)^{-\varphi}).\label{Ptrunc}
	\end{align}
In other words, $Z$ has bounded support $Q=n^{\frac{2}{\varphi}}$ with high probability.

The following resolvent identities are important tools for our proof. Recall that the resolvent minors have been defined in Definition \ref{defn_Minor}, and matrix $\AF$ is given in equation \eqref{defn AF}. %
\begin{lemma}\label{lemm_resolvent}
We have the following resolvent identities.
\begin{itemize}
	\item[(i)] For $i\in \mathcal I_1$ and $\mu\in \mathcal I_1\cup \cal I_2$, we have
		\begin{equation}
			\frac{1}{G_{ii}} =  - z - \left( {\AF G^{\left( i \right)} \AF^\top} \right)_{ii} ,\quad  \frac{1}{{G_{\mu \mu } }} =  - 1  - \left( {\AF^\top  G^{\left( \mu  \right)} \AF} \right)_{\mu \mu }.\label{resolvent2}
		\end{equation}
	\item[(ii)] For $i\in \mathcal I_1$, $\mu \in \mathcal I_1\cup \cal I_2$, $a\in \cal I\setminus \{i\}$ and $b\in \cal I\setminus \{ \mu\}$, we have
		\begin{equation}
			G_{ia}   = -G_{ii}  \left( \AF G^{\left( {i} \right)} \right)_{ia},\quad  G_{\mu b }  = - G_{\mu \mu }  \left( \AF^\top  G^{\left( {\mu } \right)}  \right)_{\mu b }. \label{resolvent3}
		\end{equation}
 \item[(iii)] For $a \in \mathcal I$ and $a_1,a_2 \in \mathcal I \setminus \{a\}$, we have
		\begin{equation}
			G_{a_1a_2}^{\left( a \right)}  = G_{a_1a_2}  - \frac{G_{a_1a} G_{aa_2}}{G_{aa}}.
			\label{resolvent8}
		\end{equation}
\end{itemize}
\end{lemma}
The above result can be proved using Schur's complement formula, cf. \citet[Lemma 4.4]{Anisotropic}.

The following lemma gives sharp concentration bounds for linear and quadratic forms of bounded random variables. We recall that the stochastic domination ``$\prec$" has been defined in Definition \ref{stoch_domination}.
\begin{lemma}[Lemma 3.8 of \cite{EKYY1} and Theorem B.1 of \cite{Delocal}]\label{largedeviation}
Let $(x_i)$, $(y_j)$ be independent families of centered and independent random variables, and $(A_i)$, $(B_{ij})$ be families of deterministic complex numbers. Suppose the entries $x_i$ and $y_j$ have variance at most $1$, and satisfy the bounded support condition (\ref{eq_support}) for a deterministic parameter $Q$. %
Then we have the following results:
\begin{align}
\Big| \sum_{i=1}^n A_i x_i \Big\vert \prec Q \max_{i} \vert A_i \vert+ \Big(\sum_i |A_i|^2 \Big)^{1/2} , \quad &\Big\vert  \sum_{i,j=1}^n x_i B_{ij} y_j \Big\vert \prec Q^2 B_d  + Q n^{1/2}B_o +  \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}},\label{eq largedev10}  \\
\Big\vert  \sum_{i=1}^n (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec Q n^{1/2}B_d   , \quad &\Big\vert  \sum_{1\le i\ne j\le n} \bar x_i B_{ij} x_j \Big\vert  \prec Qn^{1/2}B_o +  \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}} ,\label{eq largedev20}
\end{align}
where we denote $B_d:=\max_{i} |B_{ii} |$ and $B_o:= \max_{i\ne j} |B_{ij}|.$ Moreover, if the moments of $ x_i$ and $ y_j$ exist up to any order, then we have the following stronger results:
\begin{align}
\Big\vert \sum_i A_i x_i \Big\vert \prec  \Big(\sum_i |A_i|^2 \Big)^{1/2} , \quad  & \Big\vert \sum_{i,j} x_i B_{ij} y_j \Big\vert \prec  \Big(\sum_{i, j} |B_{ij}|^2\Big)^{{1}/{2}}, \label{eq largedev1} \\
 \Big\vert  \sum_{i} (|x_i|^2-\mathbb E|x_i|^2) B_{ii}  \Big\vert  \prec  \Big( \sum_i |B_{ii} |^2\Big)^{1/2}  ,\quad & \Big\vert  \sum_{i\ne j} \bar x_i B_{ij} x_j \Big\vert  \prec \Big(\sum_{i\ne j} |B_{ij}|^2\Big)^{{1}/{2}}.\label{eq largedev2}
\end{align}
\end{lemma}
