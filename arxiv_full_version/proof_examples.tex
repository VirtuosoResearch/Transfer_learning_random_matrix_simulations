\section{Proof of Corollary \ref{cor_MTL_loss}}\label{app_iso_cov}

We follow a similar logic to the proof of Theorem \ref{thm_many_tasks}.
We first characterize the global minimizer of $f(A, B)$ in the random-effect model.
Based on the characterization, we reduce the prediction loss of hard parameter sharing to the bias-variance asymptotic limits.
Finally, we prove Corollary \ref{cor_MTL_loss} based on these limiting estimates.
We set up several notations.
In the two-task case, the optimization objective $f(A, B)$ is equal to
	\begin{align}
		f(A, B) =   \bignorm{X^{(1)} B A_1 - Y^{(1)}}^2+ \bignorm{X^{(2)} B A_2 - Y^{(2)}}^2, \label{eq_mtl_2task_cov}
	\end{align}
	where $B \in \real^{p}$ and $A = [A_1, A_2]\in \real^{2}$ because the width of $B$ is one.
Without loss of generality, we assume that $A_1$ and $A_2$ are both nonzero.
Otherwise, the problem reduces to STL. % we add a tiny amount of perturbation $\delta$ to them and the result remains the same.
Using the local optimality condition $\frac{\partial f}{\partial B} = 0$, we obtain that $\hat{B}$ satisfies the following
	\begin{align}
		\hat{B} \define  \left[A_1^2 (X^{(1)})^{\top}X^{(1)} + A_2^2 (X^{(2)})^{\top}X^{(2)}\right]^{-1} \left[A_1 (X^{(1))})^{\top}Y^{(1)} + A_2 (X^{(2)})^{\top}Y^{(2)}\right]. \label{eq_Bhat_2task} %\\
		%&= (B^\star A ^{\top}) (A A^{\top})^{-1} + (X^{\top}X)^{-1}X^{\top}   \bigbrace{\sum_{j=1}^t \varepsilon_i A_i^{\top}} (A  A^{\top})^{-1}.
	\end{align}
We denote $\hat \Sigma(x)= x^2 (X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}$.
Applying $\hat B$ to equation \eqref{eq_mtl_2task_cov}, we obtain an objective that only depends on $x:=A_1/A_2$ as follows %\HZ{$A$ has been used to denote the output layers. Could you replace $A$ with another symbol (say $x$)?}
 \begin{align}
		 g(x) \define & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
			& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2 \nonumber\\
		  & + \left\| X^{(2)} \hat \Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (x\beta^{(1)}-x^2\beta^{(2)})+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)\epsilon^{(2)} \right. \nonumber\\
		  &\left. + x X^{(2)}\hat \Sigma(x)^{-1} (X^{(1)})^\top \epsilon^{(1)} \right\|^2. \label{eq_mtl_A12}
	\end{align}
We have that the conditional expectation of $g(x)$ over $\epsilon^{(1)}$ and $\epsilon^{(2)}$ is
\begin{align*}
		&\exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(x) \mid X_1, X_2 ,\beta^{(1)},\beta^{(2)}} \\
%		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  \\
%		&+ \sigma^2\tr\left[ \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2+ \left(X^{(2)}\hat\Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2 \right]\\
%&+ 2 x^2 \sigma^2 \tr\left[ \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right]  \nonumber\\
		=& (\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)}) +\sigma^2(n_1+n_2-p).
\end{align*}
The calculation is tedious but rather straightforward, so we leave the details to the reader.
In the random-effect model, recall that the entries of $(\beta^{(1)}- x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $p^{-1}[(x-1)^2\kappa^2 +(1+x^2) d^2]$.
Hence, by further taking expectation over $\beta^{(1)}$ and $\beta^{(2)}$, we obtain
\begin{align}
	&\ex{g(x) \mid X_1, X_2} \nonumber\\
%&=[(x-1)^2\kappa^2 + (x^2+1)d^2/2] \cdot p^{-1}\tr\left[  (X^{(2)})^\top X^{(2)} \hat\Sigma(x)^{-1}(X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)}\right] \nonumber\\
%&+ A^2[(x-1)^2\kappa^2 + (x^2+1)d^2/2] \cdot p^{-1}\tr\left[  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)}\right] +\sigma^2(n_1+n_2-p) \nonumber\\
	=& [(x-1)^2\kappa^2 + (x^2+1)d^2]\cdot {p^{-1}} \tr\left[ (X^{(1)})^\top X^{(1)} \hat \Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right]+\sigma^2(n_1+n_2-p), \label{EgA}
\end{align}

%\HZ{could you align the Eq so that they look better on page?}
%\begin{align}
%\exarg{\epsilon^{(1)}, \epsilon^{(2)}}{g(x) \mid X_1, X_2 ,\beta^{(1)},\beta^{(2)}}
%&= \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\right\|^2 \nonumber\\
%&+x^2 \left\| X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\right\|^2  \nonumber\\
%& + \exarg{\epsilon^{(1)}}{(\epsilon^{(1)})^\top \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2\epsilon^{(1)}} \nonumber\\
%&+\exarg{ \epsilon^{(2)}}{(\epsilon^{(2)})^\top \left(X^{(2)}\hat \Sigma(x)^{-1} (X^{(2)})^\top-\id_{n_2\times n_2}\right)^2\epsilon^{(2)}}   \nonumber\\
%&+  x^2 \exarg{ \epsilon^{(2)}}{ (\epsilon^{(2)} )^\top X^{(2)}\hat \Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)}} \nonumber\\
%& +x^2 \exarg{ \epsilon^{(1)}} { (\epsilon^{(1)})^\top X^{(1)}\hat \Sigma(x)^{-1}  (X^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top \epsilon^{(1)}  }. \label{Eg(x)}
%\end{align}
%Using the following identity
%\begin{align*}
%(X^{(1)})^\top X^{(1)} \hat \Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} & =\left( x^2 [(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1} \right)^{-1} \\
%&= (X^{(2)})^\top X^{(2)} \hat \Sigma(x)^{-1}(X^{(1)})^\top X^{(1)},
%\end{align*}
%we can simplify that
%\begin{align*}
% & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\right\|^2  +x^2 \left\| X^{(2)} \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\right\|^2 \\
%=&(\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1} (X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)}  (\beta^{(1)}-x\beta^{(2)})\\
%&+(\beta^{(1)}-x\beta^{(2)})^\top x^2 (X^{(1)})^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)} )^\top X^{(2)}  \hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} (\beta^{(1)}-x\beta^{(2)})\\
%&=(\beta^{(1)}-x\beta^{(2)})^\top\left[ x^2 (X^{(1)})^\top X^{(1)}+(X^{(2)})^\top X^{(2)}  \right]\hat\Sigma(x)^{-1} (X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})\\
%=&(\beta^{(1)}-x\beta^{(2)})^\top(X^{(1)} )^\top X^{(1)}  \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)}).
%\end{align*}
%Using this identity, we can simplify equation \eqref{Eg(x)}  to


\paragraph{Part $1$: characterizing the global minimum of $f(A, B)$.}
Let $\hat{x}$ denote the global minimizer of $g(x)$.
We show that in the setting of Corollary \ref{cor_MTL_loss}, $\hat x$ is close to 1.
This gives us the global minimum of $f(A, B)$, since $\hat{B}$ is given by $\hat{x}$ using local optimality conditions.
First, we show that $g(x)$ and its expectation are close using standard concentration bounds.
\begin{claim}\label{claim_largedev1}
 In the setting of Corollary \ref{cor_MTL_loss}, for any $x$, we have that with high probability
$$\left| g(x) - \ex{g(x) \mid X^{(1)}, X^{(2)} }\right| \le p^{1/2+c} \left(\sigma^2 + \kappa^2+d^2 \right).$$
\end{claim}
\begin{proof}
 There are two terms in $g(A)$ from equation \eqref{eq_mtl_A12}.
 We will focus on dealing with the concentration error of the first term. The second term is similar to the first and we omit the details.
 For the first term, we expand into several equations under various situations involving the random noise and the random-effect model.
 \begin{align}
 & \left\| X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)})+ \left(x^2 X^{(1)}\hat \Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)} \right. \nonumber\\
		& \left. + x X^{(1)}\hat \Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)} \right\|^2=h_1(x) + h_2(x) +h_3(x) + 2h_4(x) + 2h_5(x) + 2h_6(x), \label{expand_6}
 \end{align}
 where
 \begin{align*}
&h_1(x):=  (\beta^{(1)}-x\beta^{(2)})^\top (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (\beta^{(1)}-x\beta^{(2)})  ,\\
& h_2(x) := {(\epsilon^{(1)})^\top \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)^2\epsilon^{(1)}}  ,\\
& h_3(x):=  x^2 { (\epsilon^{(2)} )^\top X^{(2)} \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)}\hat\Sigma(x)^{-1} (X^{(2)})^\top \epsilon^{(2)}} , \\
& h_4(x):=  (\epsilon^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}), \\
&h_5(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} (x\beta^{(2)}-\beta^{(1)}),\\
&h_6(x):=x (\epsilon^{(2)})^\top X^{(2)}\hat\Sigma(x)^{-1} (X^{(1)})^\top\left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right)\epsilon^{(1)}.
\end{align*}
Next, we estimate each term using Lemma \ref{largedeviation} for random variables with bounded moment up to any order.
We first state several facts that will be commonly used in the proof.
By Fact \ref{fact_minv} (ii), we have that w.h.p. the operator norm of $X^{(1)}$ and $X^{(2)}$ are both bounded by $O(\sqrt{n})$.
Furthermore, the operator norm of $\hat\Sigma(x)^{-1}$ is bounded by $(x^2 + 1)^{-1} O(n_1 + n_2) = (x^2 + 1)^{-1} O(p)$.
 % \be\label{op_X12}
%\|X^{(1)}\|\le \sqrt{(\sqrt{n_1} + \sqrt{p})^2 + n_1 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p}, \quad \|X^{(2)}\|\le \sqrt{(\sqrt{n_2} + \sqrt{p})^2 + n_2 \cdot p^{-c_{\varphi}}}\lesssim \sqrt{p},
% \ee
% and
% \be\label{op_Sig1}
%\| \hat\Sigma(x)^{-1}\|\le \frac1{x^2[(\sqrt{n_1} - \sqrt{p})^2 - n_1 \cdot p^{-c_{\varphi}}]+[(\sqrt{n_2} - \sqrt{p})^2 - n_2 \cdot p^{-c_{\varphi}}] }\lesssim \frac{1}{(x^2+1)p},
% \ee
%where we used that $3\le n_1/p\le \tau^{-1}$ and $3\le n_2/p\le \tau^{-1}$ for a small constant $\tau>0$.

For $h_1(x)$, using Lemma \ref{largedeviation} and the fact that the entries of $(\beta^{(1)}-x\beta^{(2)})\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variance $b = p^{-1}((x-1)^2\kappa^2 + (x^2+1)d^2)$, we obtain the following estimate w.h.p.
\begin{align}
	&\left|h_1(x) - \exarg{\beta^{(1)},\beta^{(2)}}{h_1(x) \mid X_1, X_2}\right|\nonumber \\
\le& p^{c}\cdot p^{-1} b \cdot \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F \nonumber\\
	\le& p^{c}\cdot p^{-1} b \cdot p^{1/2} \left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| \nonumber \\
	\lesssim& p^{1/2+c}\cdot \frac{b}{(x^2+1)^2}\lesssim p^{1/2+c}(\kappa^2 + d^2).\label{eq_hA1}
\end{align}
In the third step we use the operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)^{-1}$. % as follows
%\begin{align*}
%	\left\| (X^{(2)})^\top X^{(2)}  \hat\Sigma(x)^{-1}  (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\| & \le \left\| (X^{(2)})^\top X^{(2)}\right\|^2 \cdot \left\| (X^{(1)})^\top X^{(1)}\right\|\cdot \left\| \hat\Sigma(x)^{-1} \right\|^2 \\
%  & \lesssim \frac{p}{(x^2+1)^2}.
%\end{align*}

For $h_2(x)$ and $h_3(x)$, since the entries of $\epsilon^{(1)},\epsilon^{(2)}\in \R^p$ are i.i.d. Gaussian random variables with mean zero and variances $\sigma^2$, using Lemma \ref{largedeviation}, we obtain w.h.p.
\be\label{eq_hA23}\left|h_2(x) - \exarg{\epsilon^{(1)}}{h_2(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2, \quad \left|h_3(x) - \exarg{\epsilon^{(2)}}{h_3(x) \mid X_1, X_2}\right| \lesssim p^{1/2+c}\sigma^2.\ee
For $h_4(x)$, using Lemma \ref{largedeviation}, we obtain w.h.p.:
\begin{align}
			\left|h_4(x)\right|
	\le& p^{c}\cdot \sigma \cdot \sqrt{b / p}\left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|_F\nonumber \\
	\le& p^{c}\cdot \sigma \sqrt{b / p} \cdot p^{1/2} \left\| \left(x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top-\id_{n_1\times n_1}\right) X^{(1)} \hat\Sigma(x)^{-1} (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
	\le&  p^{c}\cdot \sigma \sqrt{b} \cdot \left\|  x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top - \id_{n_1\times n_1} \right\| \cdot \left\|X^{(1)}\right\| \cdot \left\|\hat\Sigma(x)^{-1}\right\| \cdot \left\| (X^{(2)})^\top X^{(2)} \right\|  \nonumber\\
	\lesssim& p^{1/2+c}\frac{\sigma \sqrt{b}}{x^2+1} \lesssim p^{1/2+c}(\sigma^2 + \kappa^2 + d^2).\label{eq_hA4}
\end{align}
Above, in the fourth step we use the operator norm of $x^2 X^{(1)}\hat\Sigma(x)^{-1} (X^{(1)})^\top - \id$ being at most one and the operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)^{-1}$.
In the last step we use AM-GM inequality. Using the same argument, we can show that
$\left|h_5(x)\right| \le p^{1/2+c}(\sigma^2 + \kappa^2 + d^2)$ and $\left|h_6(x)\right| \le p^{1/2+c}\sigma^2$.
Combining the concentration error bound for $h_1(x), h_2(x), \dots, h_6(x)$, we complete the proof.
The second term of $g(A)$ can be dealt in similar ways and we omit the details.
\end{proof}

Next, we show that the global minimizer $\hat x$ of $g(x)$ is close to 1.
\begin{claim}\label{lem_hat_v}
%Suppose the assumptions of Lemma \ref{prop_model_shift_tight} hold. Assume that $ \kappa^2 \sim pd^2 \sim \sigma^2$ are of the same order.
	Let $c$ be a sufficiently small fixed constant.
	In the setting of Corollary \ref{cor_MTL_loss}, we have that with high probability,
%There exists a constant $C>0$ such that
	\be\label{hatw_add1}|\hat x -1|\le  \frac{2d^2}{\kappa^2} + p^{-1/4+c}.
	\ee
\end{claim}

\begin{proof}
Corresponding to equation \eqref{EgA}, we define the function
\begin{align*}
	h(x)  =& [(x-1)^2\kappa^2 + (x^2+1)d^2] \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)}\right] \\
				=& [(1-x^{-1})^2\kappa^2 + (1+x^{-2})d^2] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].
\end{align*}
Let $x^{\star}$ denote the global minimizer of $h(x)$.
Our proof involves two steps.
First, we  show that $\abs{x^{\star} - 1} \le d^2 / \kappa^2$.
Second, we use Claim \ref{claim_largedev1} to show that the global minimizer of $g(x)$ and $h(x)$ are close to each other.
%Using Lemma \ref{largedeviation} again, we can simplify equation \eqref{revise_eq_val_mtl} as $\val(v)= N_2h(v) \cdot  \left( 1+\OO(p^{-1/2+\e})\right)$, where the function $ h$ is defined as
%	%We define the function
%	\begin{align}
%		h(v) =& \frac{\rho_1}{\rho_2}\left[d^2 +\left( v-1\right)^2\kappa^2\right]\cdot \tr\left[( v^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%		& +  v^2\left[d^2 +\left( v-1\right)^2\kappa^2\right]\cdot \tr\left[( v^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%		& + \left(\frac{\rho_1}{\rho_2} v^2 + 1\right)\sigma^2 \cdot \bigtr{(v^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} }. \nonumber
%	\end{align}
%	%\cor Under the setting of Lemma \ref{prop_model_shift_tight},
%	Furthermore, the validation loss in equation \eqref{approxvalid} reduces to
%	\be\label{boundv-w}
%		g(v)=\left[N_2 h(v) + (N_1+N_2)\sigma^2\right]\cdot \left( 1+\OO(p^{-(1-\e_0)/2+\e})\right)\quad \text{w.h.p.}
%	\ee
%	for any constant $\e>0$. %\nc Thus for the following discussions, it suffices to focus on the behavior of $h (v)$.
%Let $\hat w$ be the minimizer of $h(v)$. Our proof consists of the following two steps.
%\squishlist
%	\item First, we show that $\hat{w}$ is close to $1$.
%	\item Second, with equation \eqref{boundv-w} we show that $\hat v$ is close to $\hat w$. Then we plug $\hat{v}$ into $\te(\hat{\beta}_2^{\MTL})$ to get equation \eqref{simple1}.
%\squishend
%%For the minimizer $\hat w$ of $\val(w)$, we have a similar result as in Proposition \ref{thm_cov_shift}.
%For the first step, we will prove the following result.
%To be consistent with the notation $\hat w$, we change the name of the argument to $w$ in the proof.

For the first step, it is easy to observe that $h(x)< h(-x)$ for any positive $x$.
Hence the minimum of $h(x)$ is achieved when $x$ is positive.
%$h(1)=\OO(pd^2 + \sigma^2)$ and $ h(w)\gtrsim p\kappa^2 \gg h(1)$ for $w\ge 2$ or $w\le 1/2$.  Hence it suffices to consider the case $1/2\le w\le 2$.
Next, we consider the case where $x\ge 1$.
Notice that the following function always increases when $x$ increases in the positive orthant:
$$\tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]$$
By taking the derivative of $h(x)$, we obtain that for any $x > 1+d^2/\kappa^2$,
\begin{align}\label{h'(A)1}
h'(x) \ge \left[2(1-x^{-1})\frac{\kappa^2}{x^2} - 2\frac{d^2}{x^3}\right] \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] > 0,
\end{align}
Finally, we consider the case where $x \le 1$. Notice that the following function always decreases when $x$ decreases from $1$:
$$\tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} +  [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right].$$
Hence, by taking derivative of $h(x)$, we obtain that for any $x\le 1-d^2/\kappa^2$,
\begin{align}\label{h'(A)2}
	h'(x)\le [-2(1-x)\kappa^2 +2 xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right] < 0.
\end{align}
In summary, the global minimizer of $h(x)$ lies within $1-d^2/\kappa^2$ and $1+d^2/\kappa^2$.

%Taking derivative of $h(w)$, we obtain that
%\begin{align}
%	h'(w) \le& \frac{\rho_1}{\rho_2} \left[2\left( w-1\right) \kappa^2\right]\cdot \tr\left[( w^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_2^{\top}X_2)^2\right] \nonumber \\
%	& +  \left[2wd^2 +2w\left( w-1\right)(2w-1)\kappa^2\right]\cdot \tr\left[( w^2X_1^{\top}X_1 +X_2^{\top}X_2)^{-2} (X_1^{\top}X_1)^2\right] \nonumber\\
%	& + \frac{\rho_1}{\rho_2}(2 w\sigma^2) \cdot \bigtr{(w^2X_1^{\top}X_1 + X_2^{\top}X_2)^{-2} X_2^\top X_2  }= \frac{\rho_1}{\rho_2} \bigtr{(w^2X_1^{\top}X_1  + X_2^{\top}X_2)^{-1} \cal B} , \nonumber
%\end{align}
%where the matrix $\cal B$ is
%$$\cal B= 2\left( w-1\right) \kappa^2  (X_2^{\top}X_2)^2+\frac{\rho_2}{\rho_1}\left[2wd^2 +2w\left( w-1\right)(2w-1)\kappa^2\right](X_1^{\top}X_1)^2 + 2 w\sigma^2 X_2^\top X_2 .$$
%Using Fact \ref{lem_minv}, we get that $\cal B$ is upper bounded as
%\begin{align*}
%\cal B \preceq - 2(1-w)\kappa^2 n_2^2 (\al_-(\rho_2) -\oo(1))^2 +2w d^2 n_1n_2 (\al_+(\rho_1) +\oo(1))^2 + 2w\sigma^2 n_2 (\al_+(\rho_2)+\oo(1)) \prec 0,
%\end{align*}
%as long as
%$$w< w_2:=1 -   \frac{d^2}{\kappa^2}\frac{\rho_1(\al_+(\rho_1) +\oo(1))^2}{\rho_2 \al_-^2(\rho_2) } -  \frac{\sigma^2}{n_2\kappa^2}\frac{\al_{+}(\rho_2)+\oo(1)}{\al_{-}^2(\rho_2)} .$$
%Hence $h'(w)<0$ on $[0,w_2)$, %i.e. $h(w)$ is strictly decreasing for $w<w_2$. This
%which gives $\hat w\ge w_2$.

%In sum, we have shown that $w_2\le w\le w_1$. Together with
%$$\max\{|w_1 -1|, |w_2 -1|\} =\OO\left(\frac{d^2}{\kappa^2} + \frac{\sigma^2}{p\kappa^2}\right),$$
%we conclude equation \eqref{hatw_add1}.


%For the rest of this section, we choose parameters that satisfy the following relations: \be\label{choiceofpara}
%pd^2 \sim \sigma^2 \sim 1,\quad p^{-1+c_0} \sigma^2 \le \kappa^2  \le p^{-\e_0-c_0}\sigma^2 ,
%\ee
%for some small constant $c_0>0$.

For the second step, using Claim \ref{claim_largedev1}, we have that $g(x)$ and $h(x)$ differ by at most $p^{1/2 + \e}(\sigma^2 + \kappa^2 + d^2)$.
Therefore, our goal reduces to showing that if $\hat{x}$ deviates too far from $1 \pm d^2 / \kappa^2$, it is no longer a global minimum of $g(x)$.
%we first argue that $\hat{x}$ is $O(1)$. Hence we can apply Claim \ref{claim_largedev1} to bound the concentration error of $g(x)$.
%Using Fact \ref{fact_minv} (ii), we can obtain that with high probability,
%\begin{align}
%p(x^2+1)^{-1}&\lesssim \left( x^2 \left[(\sqrt{n_2}-\sqrt{p})^{2}-n_2p^{-c_\varphi}\right]^{-1} +\left[(\sqrt{n_1}-\sqrt{p})^{2}-n_1p^{-c_\varphi}\right]^{-1} \right)^{-1} \nonumber\\
%&\preceq (X^{(1)})^\top X^{(1)} \hat\Sigma(x)^{-1}(X^{(2)})^\top X^{(2)} =\left( x^2 [(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1} \right)^{-1} \nonumber\\
%&\preceq \left( x^2 \left[(\sqrt{n_2}+\sqrt{p})^{2}+n_2p^{-c_\varphi}\right]^{-1} +\left[(\sqrt{n_1}+\sqrt{p})^{2}+n_1p^{-c_\varphi}\right]^{-1} \right)^{-1}\lesssim p(x^2+1)^{-1}, \label{op_uplow}
%\end{align}
%where we also used that $3\le n_1/p \le \tau^{-1}$ and $3\le  n_2/p \le \tau^{-1}$ for a small constant $\tau>0$. Then we get that
%\be\label{eq_h1}h(1)= 2d^2 \cdot p^{-1} \tr\left[ (X^{(1)})^\top X^{(1)} \hat\Sigma(1)^{-1}(X^{(2)})^\top X^{(2)}\right]\lesssim pd^2 .\ee
%On the other hand, if $|x-1|\ge \delta$ for a small constant $\delta>0$, then we have
%\begin{align}\label{eq_hA}
%h(x)\gtrsim (1+x^2)\kappa^2 \cdot p\left( x^2 +1\right)^{-1} =p\kappa^2 ,
%\end{align}
%where in the first step we used that $|x-1|^2\gtrsim 1+x^2$ for $|x-1|\ge \delta$ and equation \eqref{op_uplow}.
%and in the second step we used equation \eqref{choiceofpara0} \HZ{I'm not sure which condition we use? put it here}.
%\begin{align}
%g(x)&\ge  h(x) +\sigma^2(n_1+n_2-p) -p^{-1/2+c}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right) \nonumber\\
%&\ge h(1) +\sigma^2(n_1+n_2-p) + p^{-1/2+c}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right) \ge g(1), \label{gA>g1}
%\end{align}
%under conditions $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$. %\eqref{choiceofpara0} \HZ{again, I don't see which condition we use here?}.
%This gives that
%Hence it remains to consider the region $|A-1|\le \delta$ for a small enough constant $\delta>0$.
We prove by contradiction.
First, suppose that $\hat{x} \ge 1 + 2d^2 /\kappa^2 + p^{-1/2 + \e}$.
For any $x \ge 1+3d^2/(2\kappa^2)$, we can lower bound the derivative of $h(x)$ using equation \eqref{h'(A)1} as follows
	\[ h'(x) \ge \frac{2(x-1)\kappa^2 - 2d^2}{x^3} \cdot p^{-1} \tr\left[\left([(X^{(2)})^\top X^{(2)}]^{-1} + x^{-2} [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]
	\gtrsim p\kappa^2\cdot \frac{x-1}{x (1 + x^2)}. \]
Therefore, the difference between $h(x)$ and $h(1)$ is at least the following
\begin{align*}
		h(x)-h(1) \ge h(x) - h\left(1+\frac{3d^2}{2\kappa^2}\right)
	\ge \int_{1+\frac{3d^2}{2\kappa^2}}^x h'(x)\dd x \gtrsim p\kappa^2 \cdot \int_{1 + \frac{3d^2}{2\kappa^2}}^x \frac{x-1}{x(1 + x^2)} \dd x.
\end{align*}
When $\hat{x}$ is sufficiently far from $1$ (e.g. $2d^2 / \kappa^2 + p^{-1/4+\e}$), one can verify that $h(x) - h(1)$ is at least $\OO(p\frac{d^4}{\kappa^2} + p^{1/2+2c} \kappa^2) > \OO(p^{1/2 + c} (\sigma^2 + \kappa^2 + d^2))$, under the condition that $\sigma^2 =\OO( \kappa^2)$ and $d^2 =\oo({\kappa^2})$.
On the other hand, by triangle inequality and Claim \ref{claim_largedev1} we have that
\begin{align*}
	h(\hat{x}) - h(1) = g(\hat{x}) - g(1) + (h(\hat{x}) - g(\hat{x})) + (g(1) - h(1))
	\le \OO\left(p^{1/2+c}\left(\sigma^2 +\kappa^2+d^2 \right)\right).
\end{align*}
Hence, we have arrived at a contradition.


%Then if we choose the constant $c$ in Claim \ref{claim_largedev1} such that $e_1<2e_2$, we have
%$$ h(x)-h(1)\gtrsim  p^{-1/2+2e_2}\cdot p\kappa^2 \gg p^{-1/2+e_1}\cdot p\left(\sigma^2 +\kappa^2+d^2 \right),$$ where we use the assumption that $\sigma^2$ and $d^2$ are both at most $\OO(\kappa^2)$, which implies the inequality \eqref{gA>g1}.
Second, suppose that $\hat{x} \le 1 - 2d^2/ \kappa^2 - p^{-1/2 + \e}$.
Using equation \eqref{h'(A)2}, we obtain that for any $x \le 1 - 3d^2/(2\kappa^2)$,
	\[ -h'(x)\ge  [2(1-x)\kappa^2 - 2xd^2] \cdot p^{-1} \tr\left[\left(x^2[(X^{(2)})^\top X^{(2)}]^{-1} + [(X^{(1)})^\top X^{(1)}]^{-1}\right)^{-1}\right]  \gtrsim p\kappa^2 \cdot \frac{(1 - x)x^2}{1 + x^2}. \]
Using a similar argument to the first case, we get that the difference between $h(x)$ and $h(1)$ is at least the integral of the abvoe derivative.
%under equation \eqref{choiceofpara0} \HZ{I don't see which condition we want to use here?},
This implies that $\hat{x}$ cannot be too far from one.
Hence we have completed the proof.
%Next we prove the following estimate on the optimizer $\hat x$: with high probability,
%%\begin{lemma}
%%For the isotropic model, we have
%\be\label{hatv_add1}
%|\hat v - 1|= \OO\left(\cal E\right), \quad \cal E:=\frac{d^2}{\kappa^2} + \frac{\sigma^2}{p\kappa^2} + p^{-1/2 + \e_0 /2+ 2\e}.
%\ee
%%\end{lemma}
%%\begin{proof}
% In fact, from the proof of Claim \ref{lem_hat_v} above, one can check that if $C\cal E \le |w-1| \le 2C\cal E$ for a large enough constant $C>1$, then $|h'(w)|\gtrsim \sigma^2$. Moreover, under equation \eqref{choiceofpara} we have
%$$h(w) =\OO(\sigma^2),\quad \text{for}\quad   |w-1|\le 2C\cal E.$$
%Thus we obtain that for $|w-1|\ge 2C\cal E$,
%$$\left|h(w) - h(\hat w)\right|\ge |h(w)-\min\{h(w_1),h(w_2)\}|\gtrsim \sigma^2 \cal E \gtrsim \cal E \cdot h(\hat w),$$
%which leads to $g(w) > g(\hat w)$ with high probability by equation \eqref{boundv-w}. Thus $w$ cannot be a minimizer of $g(v)$, and we must have $|\hat v - 1|\le 2C\cal E$. %Together with equation \eqref{hatw_add1}, we conclude equation \eqref{hatv_add1}.
%%\end{proof}
%
%{\cor require $\kappa^2\gg d^2 + p^{-1/2+c}\sigma^2$}
\end{proof}

\paragraph{Part $2$: a reduction to the bias and variance limits.}
Recall that the hard parameter sharing estimator $\hat{\beta}_2^{\MTL}$ is equal to $\hat{B} \hat{A}_2$.
Using the local optimality condition for $\hat{B}$, we obtain the predication loss of HPS:
\begin{align}
L(\hat{\beta}_2^{\MTL}) &=\left\|(\Sigma^{(2)})^{1/2} \left( \hat B \hat A_2 - \beta^{(2)}\right)\right\| \nonumber\\
&=  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x\beta^{(1)}-\hat x^2\beta^{(2)})+ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2.\label{Lbeta_HPS}
\end{align}
Using Lemma \ref{lem_hat_v} and the concentration estimates in Lemma \ref{largedeviation}, we simplify $L(\hat{\beta}_2^{\MTL})$ as follows.

\begin{claim}\label{claim_reduction}
Recall that $\hat \Sigma(1)$ is equal to $\hat \Sigma$ (cf. equation \eqref{def hatsig}).
In the setting of Claim \ref{cor_MTL_loss}, we have the following estimate w.h.p.
\begin{align*}
&\left|L(\hat{\beta}_2^{\MTL}) - \frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \bigtr{{\hat \Sigma^{-1}  }}\right| \nonumber\\
\lesssim&  \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2).
\end{align*}
\end{claim}
\begin{proof}
Our proof is divided into two steps. First, using Lemma \ref{largedeviation}, we show that
\be\label{claim_largedev2} \left| L(\hat{\beta}_2^{\MTL})- \cal L(\hat x)\right| \le p^{-1/2+c} \left(\sigma^2 +\kappa^2 +d^2 \right),
\ee
where $\cal L(\hat x)$ is defined as
\begin{align*}
	\resizebox{0.97\hsize}{!}{%
	$\cal L(\hat x)	:=  \hat x^2\left[(\hat x-1)^2 \kappa^2 +  (\hat x^2+1)d^2 \right] \cdot p^{-1}\bigtr{ (X^{(1)})^{\top}X^{(1)} \hat\Sigma(\hat x)^{-1} \Sigma^{(2)} \hat\Sigma(\hat x)^{-1} (X^{(1)})^{\top}X^{(1)} }
	 +\sigma^2\cdot \bigtr{\Sigma^{(2)}\hat \Sigma(\hat x)^{-1}}.$
	}%
\end{align*}
% \end{claim}
%We are now ready to finish the proof of Claim \ref{claim_reduction}.
Next, we further simplify $\cal L(\hat x)$ since $\hat{x}$ is close to one and $\Sigma^{(1)},\Sigma^{(2)}$ are both isotropic
\begin{align}
	&\left|\cal L(\hat x)- \frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \bigtr{\hat \Sigma^{-1}}\right| \nonumber\\
\lesssim & \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2). \label{simple12}
\end{align}
Combining equation \eqref{claim_largedev2} and \eqref{simple12}, we obtain the desired claim.
We prove these two equations one by one as follows.

First, we prove equation \eqref{simple12}.
We can bound the left hand side of equation \eqref{simple12} as
\begin{align*}
	&\left|\cal L(\hat x)-\frac{2d^2}{p}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] -\sigma^2  \tr\left({\hat\Sigma^{-1}  }\right)\right| \\
	\lesssim &\left(|\hat x-1|^2 \kappa^2 + |\hat x-1|d^2\right)\cdot p^{-1}\bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2 } \\
	&+ \frac{d^2}{p}\left|\tr\left[\left(\hat\Sigma(\hat x)^{-2}-\hat\Sigma^{-2}\right) \left((X^{(1)})^\top X^{(1)} \right)^2\right]\right|+ \sigma^2  \left|\bigtr{\hat\Sigma(\hat x)^{-1}-\hat\Sigma^{-1}  }\right|.
\end{align*}
We deal with the trace terms in the above equation one by one.
Using Claim \ref{lem_hat_v} and operator norm bound of $X^{(1)}$, $X^{(2)}$, and $\hat{\Sigma}(x)$, we have that w.h.p.
\begin{align}
	\|\hat\Sigma^{-1}-\hat\Sigma(\hat x)^{-1}\| &\le |\hat x^2-1| \cdot \|\hat\Sigma^{-1}\| \cdot \| (X^{(1)})^\top X^{(1)}\| \cdot \|\hat\Sigma(\hat x)^{-1}\|  \lesssim p^{-1}\left(\frac{d^2}{\kappa^2} + p^{-1/4+c}\right).\label{est111}
\end{align}
Using similar arguments, we get that w.h.p.
\begin{align}\label{est222}
&\left\|\left(\hat\Sigma^{-2}-\hat\Sigma(\hat x)^{-2}\right)\left((X^{(1)})^\top X^{(1)} \right)^2\right\| \lesssim  \frac{d^2}{\kappa^2} + p^{-1/4+c},
\end{align}
and
\begin{align}
& \bigtr{ \hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}  \le p \bignorm{\hat\Sigma^{-2} \left((X^{(1)})^{\top}X^{(1)}\right)^2}\lesssim p.\label{est333}
\end{align}
Applying the above results \eqref{est111}, \eqref{est222}, and \eqref{est333} to the bound of $\cL(\hat{x})$ above, we have shown that equation \eqref{simple12} holds.

Second, we prove equation \eqref{claim_largedev2}.
The proof is very similar to Claim \ref{claim_largedev1}.
The key difference is that $\hat x$ correlates with $\epsilon^{(1)}$, $\epsilon^{(2)}$, $\beta^{(1)}$, and $\beta^{(2)}$.
Nevertheless, Lemma \ref{largedeviation} still applies for any arbitrary $\hat x$.
We describe a proof sketch and omit the details.
Recall that $\beta_0$ is the shared component of $\beta^{(1)}$ and $\beta^{(2)}$ with i.i.d. Gaussian entries of mean zero and variance $p^{-1}\kappa^2$.
The task-specific components, denoted by $\wt\beta^{(1)}$ and $\wt\beta^{(2)}$, consist of i.i.d. Gaussian random variables with mean zero and variance $p^{-1} d^2$.
We write $L(\hat{\beta}_2^{\MTL}) $ from equation \eqref{Lbeta_HPS} as:
\begin{align}
L(\hat{\beta}_2^{\MTL})  =&  \left\| (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1} \left[(X^{(1)})^\top X^{(1)} (\hat x -\hat x^2)\beta_0+(X^{(1)})^\top X^{(1)} \hat x\wt \beta^{(1)} - (X^{(1)})^\top X^{(1)}  \hat x^2\wt \beta^{(2)} \right] \right. \nonumber\\
&\left. + (\Sigma^{(2)})^{1/2}\hat \Sigma(\hat x)^{-1}\left[ (X^{(2)})^\top \epsilon^{(2)} + \hat x   (X^{(1)})^\top \epsilon^{(1)} \right]\right\|^2. \label{expand_15}
\end{align}
Similar to the analysis of $g(x)$, we expand $L(\hat{\beta}_2^{\MTL})$ into the sum of 15 terms, and bound the concentration error of each term similar to $h_1(x), \dots, h_6(x)$.
For example, for the leading term 
\[\hat x^2 (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)},\] 
using Lemma \ref{largedeviation} and the operator norm bounds, we obtain the following estimate w.h.p.
\begin{align*}
&  \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}\wt\beta^{(1)} - \frac{d^2}{p}\bigtr{(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)}}\right| \\
&\le p^{-1+c}d^2 \cdot \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|_F \\
&\le p^{-1/2+c}d^2 \cdot  \left\| (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(1)})^\top X^{(1)} \right\|  \lesssim p^{-1/2+c}d^2.
\end{align*} 
For the cross term $\hat x (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}$, using Lemma \ref{largedeviation} and the operator norm bounds, we obtain the following estimate w.h.p.
\begin{align*}
& \left| (\wt \beta^{(1)})^\top (X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \epsilon^{(2)}\right|  \\
  \le & p^c \cdot \sigma\sqrt{p^{-1}d^2} \cdot \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \|_F \\
 \lesssim & p^{c}\cdot \sigma d \cdot  \|(X^{(1)})^\top X^{(1)} \hat \Sigma(\hat x)^{-1}  \Sigma^{(2)} \hat \Sigma(\hat x)^{-1}  (X^{(2)})^\top  \| \\
 \lesssim & p^{-1/2+c}\sigma d\le p^{-1/2+c}(\sigma^2+ d^2).
\end{align*}
The rest of the terms in the expansion of $L(\hat{\beta}_2^{\MTL})$ can be dealt with similarly, and we omit the details.
 \end{proof}
  
\paragraph{Part $3$: applying the bias-variance limits.}
Finally, we are ready to complete the proof of Corollary \ref{cor_MTL_loss}.
We derive the variance term $\sigma^2  \tr[\hat\Sigma^{-1}]$ and the bias term $\frac{2d^2}{p} \tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right]$ using our random matrix theory results.

\begin{proof}[Proof of Corollary \ref{cor_MTL_loss}]
For the variance term, using equation \eqref{lem_cov_shift_eq}, we obtain that
\be\label{eq_var111}\tr[\hat\Sigma^{-1}] = \tr\left[ \left((X^{(1)})^\top X^{(1)}  + (X^{(2)})^\top X^{(2)}\right)^{-1}\right]=\bigtr{\frac{(a_1 +a_2)^{-1}\id_{p\times p}}{n_{1}+n_2} }+\OO(p^{-c_\varphi})\ee
with high probability. Solving equation \eqref{eq_a12extra} with $\lambda_i\equiv 1$, $1\le i\le p$, we get that  
	\begin{align}
		 a_1 = \frac{n_1(n_1 + n_2 - p)}{(n_1 + n_2)^2} ,\quad
		& a_2 = \frac{n_2(n_1 + n_2 - p)}{(n_1 +n_2)^2} . \label{simplesovlea12}
			\end{align}
Applying the above to equation \eqref{eq_var111}, we obtain that with high probability
\be\label{eq_var112}\tr[\hat\Sigma^{-1}]  = \frac{p}{n_1+n_2} \cdot \frac{n_1+n_2}{n_1+n_2-p}+\OO(p^{-c_\varphi})=  \frac{p}{n_1+n_2-p}+\OO(p^{-c_\varphi}).\ee
For the bias term, since the spectrum of $(X^{(1)})^{\top} X^{(1)}$ is tightly concentrated by Fact \ref{fact_minv}, we have that
\begin{align}
 \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi})}{p} \bigtr{\hat \Sigma^{-2}}  &\le {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] \label{eq_bias111}\\
 &\le \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi})}{p} \bigtr{\hat\Sigma^{-2}}.\nonumber
\end{align}
%To obtain this inequality, we used
%$$ (\sqrt{n_1}-\sqrt{p})^4 \cdot (1-p^{-c_\varphi}) \preceq \left((X^{(1)})^\top X^{(1)} \right)^2 \preceq (\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi}) \quad \text{with high probability},$$
%by Fact \ref{fact_minv} (ii), and the fact that for the product of two PSD matrices, its trace is always nonnegative.
Using the bias limit \eqref{lem_cov_derv_eq} with $\Sigma^{(1)}=\Sigma^{(2)}=\Lambda=V=\id_{p\times p}$, and $w = e_i$ (the $i$-th coordinate vector), we have w.h.p. (via a union bound)
\begin{align*}%\label{eq_bias112}
 e_i^{\top}\hat\Sigma^{-2} e_i = \frac{1}{(n_1+n_2)^2} \left[\frac{a_3+a_4+1}{(a_1+a_2)^2} +\OO(p^{-c_\varphi})\right], \text{ for all } i = 1, 2, \dots, p.
\end{align*}
We solve the self-consistent equations \eqref{eq_a34extra} given $a_1, a_2$, and obtain
\begin{align*}
		a_3 = \frac{p\cdot n_1}{(n_1 +n_2)(n_1 +n_2 - p)}, \quad
		&  a_4 = \frac{p\cdot n_2}{(n_1 + n_2)(n_1 + n_2 - p)}. %\label{simplesovlea34}
\end{align*}
Applying $a_3, a_4$ to the equation above, we obtain
\begin{align*}%\label{eq_bias113}
 e_i^{\top}\hat\Sigma^{-2}e_i =  \frac{1}{(n_1+n_2)^2} \left[  \frac{ (n_1+n_2)^3 }{(n_1+n_2-p)^3} +\OO(p^{-c_\varphi})\right], \text{ for all } i = 1, 2, \dots, p.
\end{align*}
%In fact, we need this estimate to hold simultaneously for all $1\le i \le p$ with high probability. For this purpose, we shall use equation \eqref{apply derivlocal} to get that
%\begin{align*}%\label{eq_bias113}
%\left| \left(\hat\Sigma^{-2}\right)_{ii} - \frac{1}{(n_1+n_2)^2}\cdot \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\prec \frac{p^{-\frac{\varphi-4}{2\varphi}}}{(n_1+n_2)^2}
%\end{align*}
%on a high probability event that does not depend on $i$. Then using Fact \ref{lem_stodomin} (i), we obtain that
%\begin{align*}
%\left| \sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} - \frac{p}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\prec \frac{p^{1-\frac{\varphi-4}{2\varphi}}}{(n_1+n_2)^2}
%\end{align*}
%with high probability, which by Definition \ref{stoch_domination} gives that
%\begin{align}\label{eq_bias113}
%\left| p^{-1}\sum_{i=1}^p \left(\hat\Sigma^{-2}\right)_{ii} - \frac{1}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\right|\le \frac{p^{-c_\varphi}}{(n_1+n_2)^2}
%\end{align}
%with high probability.
Applying the above result to equation \eqref{eq_bias111}, we get the desired result for the bias term.
%\begin{align*}
%		& \frac{(\sqrt{n_1}-\sqrt{p})^4 \cdot (1- p^{-c_\varphi}) - n_1^2 \cdot (1+\OO(p^{-c_\varphi}))}{(n_1+n_2)^2}\cdot   \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}\nonumber \\
%		\le &  {p}^{-1}\tr\left[\hat\Sigma^{-2} \left((X^{(1)})^\top X^{(1)} \right)^2\right] - \frac{n_1^2}{(n_1+n_2)^2}\cdot  \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3} \nonumber\\
%		\le &  \frac{(\sqrt{n_1}+\sqrt{p})^4 \cdot (1+ p^{-c_\varphi}) - n_1^2 \cdot (1+\OO(p^{-c_\varphi}))}{(n_1+n_2)^2}\cdot   \frac{(n_1+n_2)^3}{(n_1+n_2-p)^3}.
%\end{align*}
Combining the bias and variance estimates, we get that
\begin{align*}
	 \left|L(\hat{\beta}_2^{\MTL})  -  \frac{2d^2 n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3} - \frac{\sigma^2 p}{n_1+n_2-p}   \right|&
	  \le   \left[\left( 1+\sqrt{\frac{p}{n_1}}\right)^4-1\right] \cdot \frac{2d^2n_1^2 (n_1+n_2)}{(n_1 + n_2 - p)^3} \\
	&\hspace{-2cm}+\OO\left( p^{-c_\varphi} (\sigma^2 + d^2)+ \frac{d^4 + \sigma^2 d^2}{\kappa^2}+p^{-1/2+2c}\kappa^2 +  p^{-1/4+c} (\sigma^2 +d^2) \right).
\end{align*}
Since $\sigma^2 \lesssim  \kappa^2$ and $d^2 \le p^{-\e}{\kappa^2}$ by our assumption, we obtain the desired result.
The proof is complete.
\end{proof}
