\section{Missing Proof of Theorem \ref{thm_many_tasks}}\label{app_proof_error_same_cov}


	
Next we present the proof of Claim \ref{claim_opt_dist}, Claim \ref{lem_exp_opt}, and Claim \ref{claim_pred_err}. 
	\begin{proof}[Proof of Claim \ref{lem_exp_opt}]
	To facilitate the analysis, we consider the following matrix notations.
	Denote
		%\[ \cE A^{\top} := {\sum_{k=1}^t \varepsilon^{(k)} A_k^{\top}}, \]
\[\cE  :=[\epsilon^{(1)},\epsilon^{(2)},\cdots, \epsilon^{(t)}],  \quad \text{ and } \quad \cW \define X(X^{\top} X)^{-1} X^{\top} \cE A^{\top} (AA^{\top})^{+}. \]
	For any $j = 1,2,\dots, t$, let
	\begin{align*}
		H_j &\define B^{\star} A^{\top} (AA^{\top})^{+} A_j - \beta^{(j)}, \quad \text{ and } \quad E_j \define \cW A_j - \varepsilon^{(j)}.
	\end{align*}
	Then we can write the function $g(A)$ conveniently as
	\[ g(A) = \sum_{j=1}^t \bignorm{X H_j + E_j}^2. \]
	We will divide $g(A)$ into three parts.
	For simplicity, we will use matrix notations in the proof, %since they are more compact.
	that is, stacking $[H_j]_j$ gives matrix $B^{\star} A^{\top} (AA^{\top}) A - B^{\star}$,
	and stacking $[E_j]_j$ gives $\cW A - \cE$.

	\paragraph{Part 1:} The first part is the square of $XH_j$,
	\begin{align}\label{eq_gA_p1}
		\sum_{j=1}^t \bignorm{X H_j}^2
		= \bignormFro{X (B^{\star} A^{\top} (AA^{\top}) A - B^{\star})}^2
		= \bignormFro{X (B^{\star} U_A U_A^{\top} - B^{\star})}^2,
	\end{align}
	where $U_{A} U_{A}^{\top} \in\real^{t\times t}$ denotes the subspace projection $ {A}^{\top} ( {A}{A}^{\top})^{+}  {A}$.
	Taking expectation of equation \eqref{eq_gA_p1} over $X$, we get
		\[ \sum_{j=1}^t \bignorm{X H_j}^2=n\bignorm{\Sigma^{1/2}(B^{\star} U_A U_A^{\top} - B^{\star})}^2. \]

	\paragraph{Part 2:} The second part is the cross term, which is equal to the following using the matrix notations:
	\begin{align}\label{eq_gA_p2}
		\sum_{j=1}^t\inner{XH_j}{E_j} = \inner{X(B^{\star} U_A U_A^{\top} - B^{\star})}{\cW A - \cE}
		= - \inner{X (B^{\star} U_A U_A^{\top} - B^{\star})}{\cE},
	\end{align}
	which is zero in expectation over $\cE$.

	\paragraph{Part 3:} The last part is the square of $E_j$:
	\begin{align}\label{eq_gA_p3}
		\sum_{j=1}^t \norm{E_j}^2 &= \bignormFro{\cW A - \cE}^2
		= \bignormFro{\cE}^2 - \inner{\cW A}{\cE},
	\end{align}
	where in the second step we use $\norm{\cW A}^2 = \inner{\cW A}{\cE}$ by algebraic calculation.
	Hence, it suffices to show that the expectation of equation \eqref{eq_gA_p3} is equal to $\sigma^2 (n\cdot t - p\cdot r)$.
	First, we have that $\ex{\|\cE\|_F^2} = \sigma^2 \cdot n \cdot t$.
%	Conditional on $X$, we have that
%	\begin{align*}
%		\exarg{\cE}{\norm{E_j}^2}
%		= \exarg{\cE}{\sum_{j=1}^t \bigbrace{\bignorm{\cW A_j}^2 - 2\inner{\cW A_j}{\varepsilon^{(j)}} }} + \sigma^2 \cdot n \cdot t.
%	\end{align*}
%	We consider both terms in the above equation one by one.
	Second, we show that
	%	\begin{align*}
	%		\inner{\cW A}{\cE} = \sigma^2 \cdot r \cdot \bigtr{U_X U_X^{\top}} = \sigma^2 \cdot r \cdot p,
	%	\end{align*}
	%because $U_X$ has rank $p$ by Fact \ref{lem_minv}.
	\begin{align*}
		\exarg{\cE}{ \inner{\cW A}{\cE}} =\exarg{\cE}{\bigtr{\cal E^\top U_X U_X^\top \cal E U_AU_A^\top} }= p\sigma^2 \cdot \bigtr{ U_AU_A^\top} =  p \sigma^2 \cdot r,
	\end{align*}
	where $U_XU_X^\top = X(X^{\top} X)^{-1} X^{\top}$.
	The first step follows by applying the definition of $\cW$.
	The last step is because $U_AU_A^\top$ has rank $r$.
	Hence, it suffices to show the second step is correct.
	For any $1\le i, j \le t$, let $\delta_{i, j} = 1$ if $i = j$, and $0$ otherwise.
	%\begin{align}
	%	\sum_{j=1}^t \norm{\cW A_j}^2 = \sum_{j=1}^t \bigtr{\cW A_j A_j^{\top} \cW^{\top}} = \bigtr{\cW AA^{\top} \cW^{\top}} = \bigtr{U_X U_X^{\top} \cE A^{\top} (AA^{\top})^{-1} A \cE^{\top}}. \label{eq_proof_same_cov_1}
	%\end{align}
	%We observe that
	Because $\varepsilon^{(i)}$ and $\varepsilon^{(j)}$ are pairwise independent, we have that
	\begin{align*}
			 \exarg{\cE}{(\cE^{\top}U_XU_X^\top \cE)_{ij} }
		&= \exarg{\cE}{{\varepsilon^{(i)}}^\top U_X U_X^\top  \varepsilon^{(j)} } = \sigma^2 \cdot \tr\left[U_XU_X^\top \right] \cdot \delta_{ij} = p\sigma^2 \cdot \delta_{ij}.
	\end{align*}
	The last step uses the fact that $\tr[U_XU_X^\top] = p$.
	Hence, the second step is correct.
	%Above, the second step used the fact that $\E(\varepsilon^{(i)}_k \varepsilon^{(i)}_l )=\sigma^2\cdot \delta_{ij}\delta_{kl}$ for $1\le k,l \le n$, and the third step used the fact that $\tr(U_XU_X^\top) =\tr(U_X^\top U_X) =p$.
	%Next, note that
	%\[ \sum_{j=1}^t \inner{\cW A_j}{\varepsilon^{(j)}} = \bigtr{\cW A\cE^{\top}}, \]
	%which is equal to equation \eqref{eq_proof_same_cov_1} by the definition of $\cW$.

	Combining the three parts, the proof is complete.
	\end{proof}
	
	\iffalse
	Second, notice that
	%\begin{align}
	%	\sum_{j=1}^t \norm{\cW A_j}^2 = \sum_{j=1}^t \bigtr{\cW A_j A_j^{\top} \cW^{\top}} = \bigtr{\cW AA^{\top} \cW^{\top}} = \bigtr{U_X U_X^{\top} \cE A^{\top} (AA^{\top})^{-1} A \cE^{\top}}. \label{eq_proof_same_cov_1}
	%\end{align}
	%We observe that
	\begin{align*}
			\exarg{\cE}{\cE A^{\top} (AA^{\top})^{+} A \cE^{\top}}
		&= \exarg{\cE}{\sum_{j=1}^t \sum_{k=1}^t \varepsilon^{(j)} A_j^{\top} (AA^{\top})^{+} A_k {\varepsilon^{(k)}}^{\top}} \\
		&= \exarg{\cE}{\sum_{j=1}^t \varepsilon^{(j)} A_j^{\top} (AA^{\top})^{+} A_j {\varepsilon^{(j)}}^{\top}} \\
		&= \sigma^2 \cdot r \cdot \id_{n\times n}.
	\end{align*}
	In the above derivation, the second step used the fact that for any $j\neq k$, $\varepsilon^{(j)}$ and $\varepsilon^{(k)}$ are pairwise independent.
	The third step used the fact that $\sum_{j=1}^t A_j^{\top} (AA^{\top})^{+} A_j = \bigtr{\id_{r\times r}} = r$, and $\ex{\varepsilon^{(j)} {\varepsilon^{(j)}}^{\top}} = \sigma^2 \cdot \id_{n\times n}$.
	Therefore, we have that
	\begin{align*}
		\exarg{\cE}{ \inner{\cW A}{\cE}} = \sigma^2 \cdot r \cdot \bigtr{X(X^{\top} X)^{-1} X^{\top}} =  \sigma^2 \cdot r \cdot \bigtr{ X^{\top}X(X^{\top} X)^{-1}}  = \sigma^2 \cdot r \cdot p.
	\end{align*}
   because $X^\top X$ is a $p\times p$ matrix.	
	%Next, note that
	%\[ \sum_{j=1}^t \inner{\cW A_j}{\varepsilon^{(j)}} = \bigtr{\cW A\cE^{\top}}, \]
	%which is equal to equation \eqref{eq_proof_same_cov_1} by the definition of $\cW$.
	Hence the proof is complete.
\fi

	\begin{proof}[Proof of Claim \ref{claim_opt_dist}]
	Corresponding to the right-hand side of \eqref{eq_gA}, we define the function
	\be\label{same_hA}h(A):= n \bignormFro{\Sigma^{1/2} B^{\star} \bigbrace{A^{\top} (AA^{\top})^{+} A - \id_{t\times t}}}^2 + \sigma^2 (n\cdot t - p \cdot r).\ee
	Let $\e$ be a fixed constant that is sufficiently small.
	Let $c_{\infty}$ be any fixed value within $(0, 1/2 - \e)$.
	To show that $U_{\hat{A}}U_{\hat{A}}^\top$ is close to $A^\star {A^\star}^\top$, we first show that $g(A)$ is close to $h(A)$ as follows:
	\begin{align}\label{eq_gA_err}
		\bigabs{g(A) - h(A)} \lesssim  n^{-c_{\varphi}} \cdot n \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id_{t\times t})}_F^2 + n^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
	\end{align}
%	\begin{align}\label{eq_gA_err}
%		\bigabs{g(A) - \exarg{\cE, X}{g(A)}} \lesssim p^{-c_{\varphi}} \cdot n \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id)}^2 + p^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
%	\end{align}
	We consider the concentration error of each part of $g(A)$.

	For equation \eqref{eq_gA_p1}, % $g_0(\cal W)= \sum_{j=1}^t\left\|Z v_j \right\|^2$.
%	$$g_0(\cal W)= \sum_{j=1}^t\left\|Z v_j \right\|^2,\quad v_j:= \Sigma^{1/2}\left(B^{\star} \cW^{\top} (\cW\cW^{\top})^{-1} W_j -  \beta_j\right).$$
%	note that $X H_j = Z \Sigma^{1/2} H_j \in \real^n$ is a random vector with i.i.d. entries of mean zero, variance $\|\Sigma^{1/2}H_j\|^2$, and finite $\varphi$-th moment by Assumption \eqref{assume_rm}.
%	Hence by the law of large numbers, we have that with high probability	
	applying Corollary \ref{cor_largedeviation} to $X H_j = Z \Sigma^{1/2} H_j$, we obtain that 
	${\left\|Z \Sigma^{1/2} H_j \right\|^2 } = { n \|\Sigma^{1/2} H_j\|^2} \cdot (1 + \OO(n^{-c_\varphi}))$ with high probability. 
	This implies that
	\begin{align}
		\abs{\sum_{j=1}^t \|X H_j\|^2 -  \sum_{j=1}^t n\norm{\Sigma^{1/2} H_j}^2} \lesssim   n^{-c_{\varphi}} \cdot n  \bignorm{\Sigma^{1/2} B^{\star} (U_{A}U_A^{\top} - \id_{t\times t})}^2. \label{eq_gA_err1}
	\end{align}

	For equation \eqref{eq_gA_p2}, 
	%using Lemma \ref{largedeviation} in Appendix \ref{app_tool} and the fact that all moments of $\varepsilon_i$ exist by Assumption \eqref{assmAhigh2}, 
	using Corollary \ref{cor_calE}, we obtain the following with high probability:
	\begin{align}
		|\inner{XB^{\star} (U_AU_A^{\top} - \id_{t\times t})}{\cE}| &\le n^\e \cdot \sigma \cdot \bignormFro{XB^{\star} (U_AU_A^{\top} - \id_{t\times t})} \nonumber \\
		&\le n^{\e} \cdot \sigma \cdot \norm{Z} \cdot \bignormFro{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id_{t\times t})} \nonumber \\
		&\lesssim n^{\e + 1/2} \cdot \sigma \cdot \bignormFro{\Sigma^{1/2} B^{\star}(U_AU_A^{\top} -\id_{t\times t})} \label{eq_gA_err2}
%		&\le  n^{c_\infty +c} \cdot \bignorm{\Sigma^{1/2} B^{\star} (U_AU_A^{\top} - \id_{t\times t})}^2 + n^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
	\end{align}
	In the second step, we use the fact that $X=Z\Sigma^{1/2}$.
	In the third step, we use Fact \ref{fact_minv}(ii) to bound the operator norm of $Z$ by $\OO(\sqrt{n})$.
	By the AM-GM inequality, equation \eqref{eq_gA_err2} is bounded by the right-hand side of \eqref{eq_gA_err}.

	For equation \eqref{eq_gA_p3}, using Corollary \ref{cor_calE},
	%Lemma \ref{largedeviation} and the fact that all moments of $\varepsilon_i$ exist, 
	we obtain that with high probability,
	\begin{align}
		\abs{\normFro{\cE}^2 - \sigma^2 \cdot n \cdot t}=\abs{\tr\left[ \cal E^\top \id_{n\times n}\cal E\right] - \sigma^2 \cdot n \cdot t} &\le n^{c} \cdot \sigma^2 \| \id_{n\times n}\|_F
		= n^{1/2+c} \cdot \sigma^2. \label{eq_gA_err3}
	\end{align}
	For the inner product between $\cW A$ and $\cE$, we have that with high probability,
	\begin{align}
		\bigabs{\inner{\cW A}{\cE} - \sigma^2\cdot p \cdot r } &= \bigabs{\bigtr{\left(\cE^\top U_X U_X^{\top}\cE  - p\sigma^2 \cdot  \id_{t\times t} \right)  U_AU_A^{\top} }} \nonumber  \\
		&\le \bignormFro{U_{A} U_A^{\top}} \cdot \bignorm{\cE^\top U_X U_X^{\top} \cE - p \sigma^2 \cdot  \id_{t\times t}} \nonumber \\
		&\le \sqrt{r} \cdot n^{\e}\cdot \sigma^2\cdot \|U_X U_X^\top\|_F \nonumber \\
		&\le \sqrt{r} \cdot n^{1/2+c} \cdot \sigma^2. \label{eq_gA_err4} %  \le  n^{-c_{\infty}} \cdot \sigma^2 \cdot n \cdot t.
	\end{align}
	Here in the third step, we apply equation \eqref{vcalA2} to $\bignorm{\cE^\top U_X U_X^{\top} \cE - p \sigma^2 \cdot  \id_{t\times t}}$ and use that $ \bignormFro{U_{A} U_A^{\top}}=\sqrt{r}$ because $U_A$ has rank $r$. In the fourth step, we use $\|U_X U_X^\top\|_F=\sqrt{p}$ because $U_X$ has rank $p$.

	Combining the concentration error estimate for all three parts, we obtain equation \eqref{eq_gA_err}.

	\bigskip
	Next, we use equation \eqref{eq_gA_err} to prove the claim.
	Using triangle inequality, we upper bound the gap between $h(A^{\star})$ and $h(\hat{A})$:  
	\begin{align}
		h(\hat{A})- h(A^{\star})   &\le \bigabs{g(A^{\star}) - h(A^{\star})} + (g(\hat A) - g({A}^{\star})) + \big|g(\hat{A}) - h(\hat{A})\big| \nonumber \\
		&\le \bigabs{g(A^{\star}) - h(A^{\star})}  + \big|g(\hat{A}) - h(\hat{A})\big|\nonumber \\
		&\lesssim n^{-c_\varphi}\cdot n \bignormFro{\Sigma^{1/2} B^{\star}}^2 + n^{-c_\infty}\cdot \sigma^2 \cdot n \cdot t. \label{eq_g_gap}
	\end{align}
	The second step used the fact that $\hat A$ is the global minimizer of $g(\cdot)$, so that $g(\hat A) \le g({A}^\star)$.
	The third step used equation \eqref{eq_gA_err} and the fact that the spectral norm of $U_A U_A^{\top} - \id_{t\times t}$ is at most one. 
	%With \eqref{eq_g_gap}, we can derive an upper bound on $\bignormFro{A^{\star} {A^{\star}}^{\top}- U_{\hat{A}} U_{\hat{A}}^{\top}}$ as follows. 
	Using equation \eqref{same_hA}, we can verify that
	$$h(\hat A)-h(A^\star) = n \bigtr{{B^\star}^\top\Sigma B^\star ( A^\star {A^\star}^\top -U_{\hat A}U^\top_{\hat A})} .$$
	Let $\lambda_1\ge\lambda_2 \ge \cdots\ge \lambda_t$ be the eigenvalues of ${B^\star}^\top\Sigma B^\star$.
	Let $v_i$ be the corresponding eigenvector of $\lambda_i$.
	Then, we have $A^\star {A^\star}^\top =\sum_{i=1}^r v_i v_i^\top$, and
	\begin{align}
	h(\hat A)-h(A^\star) = n \sum_{i=1}^r \lambda_i - n\sum_{i=1}^t \lambda_i \| U^\top_{\hat A} v_i\|^2 &= n\sum_{i=1}^r \lambda_i\left(1 -  \| U^\top_{\hat A} v_i\|^2\right)-n\sum_{i=r+1}^t \lambda_i \| U^\top_{\hat A} v_i\|^2 \nonumber\\
	&\ge  n(\lambda_r-\lambda_{r+1}) \sum_{i=r+1}^t \| U^\top_{\hat A} v_i\|^2 , \label{bdd_A-A}
	\end{align}
	where we use $\sum_{i=1}^r \left(1 -  \| U^\top_{\hat A} v_i\|^2\right) = r- \sum_{i=1}^r  \| U^\top_{\hat A} v_i\|^2 =\sum_{i=r+1}^t \| U^\top_{\hat A} v_i\|^2  $ in the last step.
	On the other hand, we have
	\begin{align*}
	\| A^\star {A^\star}^\top -U_{\hat A}U^\top_{\hat A}\|_F^2
	&= 2r - 2\inner{A^{\star}{A^{\star}}^{\top}}{U_{\hat{A}} {U_{\hat{A}}}^{\top}} \\
	&= 2 \sum_{i=r+1}^t \| U^\top_{\hat A} v_i\|^2.
	\end{align*}
	Thus from equation \eqref{eq_g_gap} and  \eqref{bdd_A-A}, we obtain that
	$$\| A^\star {A^\star}^\top -U_{\hat A}U^\top_{\hat A}\|_F^2  =2 \sum_{i=r+1}^t \| U^\top_{\hat A} v_i\|^2 \lesssim \frac{n^{-c_\varphi}\cdot  \bignormFro{\Sigma^{1/2} B^{\star}}^2 + n^{-c_\infty}\cdot \sigma^2 t}{\lambda_r- \lambda_{r+1}}  .$$
	Hence the proof is complete.
	\end{proof}
	
	\iffalse
	We simplify equation \eqref{eq_mtl_output_layer} as
	\begin{align*}
		g(A)  &= \normFro{X (X^{\top}X)^{-1} X^{\top} Y U_A U_A^{\top} - Y}^2 \\
					&= \normFro{Y}^2 - \inner{U_A U_A^{\top}}{Y^{\top} X (X^{\top} X)^{-1} X^{\top} Y}.
	\end{align*}
	We denote $H = Y^{\top} X (X^{\top} X)^{-1} X^{\top} Y$, which appears in the above equation.
	Since $\hat{A}$ is a global minimum of $g(A)$, we have that $g(A^{\star}) \ge g(\hat{A})$.
	Hence, we have
	\begin{align}
		g(A^{\star}) - g(\hat{A}) &= \abs{g(A^{\star}) - g(\hat{A})} = \abs{\inner{H}{A^{\star} {A^{\star}}^{\top} - U_{\hat{A}} U_{\hat{A}}^{\top}}} \nonumber \\
												&\ge \lambda_{\min}(H) \cdot \normFro{A^{\star} {A^{\star}}^{\top} - U_{\hat{A}} U_{\hat{A}}^{\top}}. \label{eq_g_gap1}
	\end{align}
	Recall that $Y = X B^{\star} + \cE$ in matrix notation.
	We bound the minimum singular value of $H$ as follows
	\begin{align}
		\lambda_{\min}(H) &= \lambda_{\min}({B^{\star}}^{\top} X^{\top} X B^{\star} + \cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}X^{\top}X B^{\star}) + \lambda_{\min}(\cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}\Sigma B^{\star}) \cdot \lambda_{\min}(Z^{\top}Z) + \lambda_{\min}(\cE^{\top} X (X^{\top} X)^{-1} X^{\top} \cE) \nonumber \\
		&\ge \lambda_{\min}({B^{\star}}^{\top}\Sigma B^{\star}) \cdot ((\sqrt n - \sqrt p)^2 - n \cdot p^{-c}) + \sigma^2 \cdot p \cdot (1 - p^{-1/2 + \e}). \label{eq_g_gap2}
	\end{align}
	The second equation uses the fact that the minimum singular value of the sum of two PSD matrices is greater than the sum of the minimum singular value of each matrix.
	The third equation uses the fact that $X = Z \Sigma^{1/2}$ and Fact \ref{fact_proof_gA} in Appendix \ref{app_tool}.
	The last equation uses Fact \ref{fact_minv} (ii) for the minimum singular value of $Z^{\top}Z$ and Lemma \ref{largedeviation} for $\cE\cE^{\top}$.

	Combining equation \eqref{eq_g_gap}, \eqref{eq_g_gap1}, and \eqref{eq_g_gap2}, we conclude that
	\begin{align*}
		\bignormFro{A^{\star} {A^{\star}}^{\top}- U_{\hat{A}} U_{\hat{A}}^{\top}}
		&\le \frac{p^{-c} \cdot n \bignormFro{\Sigma^{1/2} B^{\star}}^2 + p^{-1/2 + c} \cdot \sigma^2 nt} {\lambda_{\min}(H)} \\
		&\le \frac{p^{-c} \rho \normFro{\Sigma^{1/2} B^{\star}}^2 + p^{-1/2 + \e} \cdot \sigma^2 \rho t}{\lambda_{\min}^2(\Sigma^{1/2} B^{\star}) ((\sqrt{\rho} - 1)^2 - \rho \cdot p^{-c}) + \sigma^2 (1 - p^{-1/2 + \e})}
	\end{align*}
	Hence for large enough $p$, we obtain that Claim \ref{claim_opt_dist} holds.
	\fi
	

	\begin{proof}[Proof of Claim \ref{claim_pred_err}]
	%We apply similar arguments from Claim \ref{lem_exp_opt} and \ref{claim_opt_dist} for this proof.
	The proof is similar to that of equation \eqref{eq_gA_err}.
	The prediction loss of hard parameter sharing for task $i$ is equal to
	\begin{align*}
		L(\hat{\beta}_i^{\MTL}) &= \bignorm{\Sigma^{1/2} (\hat{B} \hat{A}_i - \beta^{(i)})}^2 \\
		&= \bignorm{\Sigma^{1/2} ((X^{\top} X)^{-1} X^{\top} Y \hat{A}^{\top} (\hat{A} \hat{A}^{\top})^{+} \hat{A}_i - \beta^{(i)})}^2 \\
		&= \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta^{(i)} + R_i)}^2,
	\end{align*}
	where we denote $R_i = (X^{\top} X)^{-1} X^{\top} \cE \hat{a}_i$.
	We divide the prediction loss into three parts.

	\paragraph{Part 1:} The first part is the bias term:
	$\norm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta^{(i)})}^2 = L(B^{\star} \hat{a}_i)$.

	\paragraph{Part 2:} The second part is the cross term, whose expectation over $\cE$ is zero.
	Let $b = B^{\star} \hat{a}_i - \beta^{(i)}$ for simplicity.
	Using Corollary \ref{cor_calE}, the concentration error can be bounded as
	\begin{align*}
		\bigabs{\inner{\Sigma^{1/2}b}{\Sigma^{1/2}R_i}}
		&= \bigabs{\inner{X (X^{\top}X)^{-1} \Sigma b \hat{a}_i^{\top}}{\cE}} \\
		&\le \sum_{j=1}^t \abs{\hat{a}_i(j)} \cdot \bigabs{\inner{X (X^{\top} X)^{-1} \Sigma b}{\varepsilon^{(j)}}} \\
		&\le \sum_{j=1}^t \abs{\hat{a}_i(j)} \cdot n^{\e}\sigma \bignorm{X (X^{\top}X)^{-1} \Sigma b} \\
		&\le \sqrt{t} \norm{\hat{a}_i} \cdot n^{\e} \sigma \bignorm{X (X^{\top} X)^{-1} \Sigma b}.
	\end{align*}
%	\begin{align*}
%		\bigabs{\inner{\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)}{\Sigma^{1/2}R_i}} &= \bigabs{\inner{\hat a_i^\top X(X^\top X)^{-1}\Sigma (B^{\star} \hat{a}_i - \beta_i)}{\cal E}} \\
%		&\le n^{c}\sigma \cdot \bignormFro{\hat a_i^\top X(X^\top X)^{-1}\Sigma (B^{\star} \hat{a}_i - \beta_i)}\\
%		&\le n^{c}\sigma \cdot\|\hat a_i\| \cdot  \|\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)\| \cdot \| (Z^\top Z)^{-1} \|^{1/2} \\
%		&\lesssim n^{-1/2+c}\sigma \cdot\|\hat a_i\| \cdot \bignorm{\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)} \\
%		&\le n^{-1/2+c}\sigma^2  \cdot\|\hat a_i\|^2 + n^{-1/2+c} L(B^{\star} \hat{a}_i), 
%	\end{align*}
	In the first step, we plug in the definition of $R_i$ and re-arrange terms.
	In the second step, we use $\hat{a}_i(j)$ to denote the $j$-th coordinate of $\hat{a}_i$.
	In the third step, we use equation \eqref{vcalE}.
	In the last step, we use $\sum_j |\hat a_i(j)|\le \sqrt{t}\|\hat a_i\|$ by Cauchy-Schwarz inequality.
	Finally, we have
	\begin{align*}
		\bignormFro{ X(X^\top X)^{-1}\Sigma b} &=\left[ {b}^\top \Sigma (X^\top X)^{-1}X^\top X(X^\top X)^{-1}\Sigma b \right]^{1/2} \\
		&\le  \|\Sigma^{1/2} b \|  \cdot \left\| \Sigma^{1/2}(X^\top X)^{-1}\Sigma ^{1/2} \right\|^{1/2}  =   \|\Sigma^{1/2} b \|  \cdot \left\| (Z^\top Z)^{-1}\right\|^{1/2} \\
		&\le n^{-1/2} \cdot \norm{\Sigma^{1/2} b}.
	\end{align*}
%	\begin{align*}
%	\bignormFro{\hat a_i^\top X(X^\top X)^{-1}\Sigma (B^{\star} \hat{a}_i - \beta_i)} &=\|\hat a_i\|\cdot \left[ (B^{\star} \hat{a}_i - \beta_i)^\top \Sigma (X^\top X)^{-1}X^\top X(X^\top X)^{-1}\Sigma (B^{\star} \hat{a}_i - \beta_i) \right]^{1/2} \\
%	&\le \|\hat a_i\|\cdot  \|\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)\|  \cdot \left\| \Sigma^{1/2}(X^\top X)^{-1}\Sigma ^{1/2} \right\|^{1/2} \\
%	&= \|\hat a_i\|\cdot  \|\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)\|  \cdot \left\| (Z^\top Z)^{-1}\right\|^{1/2}
%	\end{align*}
	Above, we use $X=Z\Sigma^{1/2}$. In the last step, we use Fact \ref{fact_minv}(ii) to bound the operator norm of $Z^\top Z$ by $\OO(n^{-1})$. %, and apply the AM-GM inequality.
	One can see that the concentration error from this part is upper bounded by the result in Claim \ref{claim_pred_err}.
	
	\iffalse
	Next, we use the fact that the spectral norm of $\cE$ is at most $\sigma \cdot p^{\e}$.
	Hence, the spectral norm of $\Sigma^{1/2} E_i$ is at most
		\[ \sigma \cdot p^{\e} \cdot \norm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}} \cdot \norm{\hat{a}_i}. \]
	Therefore, the cross term is bounded by $\sigma \cdot p^{\e}$ times
	\begin{align*}
			& \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i)} \cdot \norm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}} \cdot \norm{\hat{a}_i} \\
		\le & \bignorm{\Sigma^{1/2} (B^{\star} \hat{a}_i - \beta_i)}^2 + \norm{\hat{a}_i}^2 \cdot \bignorm{\Sigma^{1/2} (X^{\top} X)^{-1} X^{\top}}^2 \tag{by Cauchy-Shwartz inequality} \\
		\le & \bignorm{\Sigma^{1/2}(B^{\star} \hat{a}_i - \beta_i)} + \norm{\hat{a}_i}^2 \cdot \bigtr{\Sigma (X^{\top} X)^{-1}}.
	\end{align*}
	\fi

	\paragraph{Part 3:} The final part is the squared term of $R_i$. We rewrite it as
	\begin{align}
	\|\Sigma^{1/2}R_i\|^2 &= \left\|\sum_{j=1}^t \hat{a}_i(j) \Sigma^{1/2}(X^{\top} X)^{-1} X^{\top} \epsilon^{(j)}\right\|^2 \nonumber\\
	& = \sum_{1\le j , k \le t}\hat a_i(j) \hat a_i(k)  {\epsilon^{(j)}}^\top X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} {\epsilon^{(k)}}.\label{E_i^2}
	\end{align}	
%	If $j\ne k$, using Corollary \ref{cor_calE} we obtain that 
%	\begin{align}
%	\left|{\epsilon^{(j)}}^\top X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} {\epsilon^{(k)}} \right| &\le \sigma^2 \cdot \bignormFro{X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} } \nonumber\\
%	& =\sigma^2 \cdot \bignormFro{\Sigma^{1/2}(X^{\top} X)^{-1}\Sigma^{1/2}} \nonumber\\
%	&  \le \sigma^2 \cdot p^{1/2} \cdot \bignorm{(Z^{\top} Z)^{-1}} \lesssim \sigma^2 \cdot n^{-1/2+c} \label{jnek}
%	\end{align}
%	with high probability for any small constant $c>0$. On the other hand, if $j=k$, 
	First, for any $1 \le j, k \le t$, the expectation is
	$$\exarg{\cal E}{{\epsilon^{(j)}}^\top X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} \epsilon^{(k)}}=\delta_{jk}\cdot \sigma^2\tr\left[\Sigma(X^\top X)^{-1}\right] .$$
	Second, using equation \eqref{vcalA2}, the concentration error is at most
	\begin{align}
	& \left|{\epsilon^{(j)}}^\top X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} {\epsilon^{(k)}} -\delta_{jk}\cdot \sigma^2\tr\left[\Sigma(X^\top X)^{-1}\right]\right|\nonumber \\
	\le &n^c\cdot \sigma^2  \bignormFro{X(X^{\top} X)^{-1}\Sigma (X^{\top} X)^{-1} X^{\top} }   = n^c\cdot \sigma^2  \bignormFro{\Sigma^{1/2}(X^{\top} X)^{-1}\Sigma^{1/2}} \nonumber\\
	  \le &\sigma^2 \cdot p^{1/2} \cdot  \bignorm{(Z^{\top} Z)^{-1}}^{1/2} \lesssim \sigma^2 \cdot n^{-1/2+c}. \label{jeqk}
	\end{align}
	Above, we used Fact \ref{fact_minv}(ii) in the last step to bound the operator norm of $(Z^{\top} Z)^{-1}$.
	Plugging equation \eqref{jeqk} into equation \eqref{E_i^2}, we obtain that 
	\begin{align*}
		\bigabs{\bignorm{\Sigma^{1/2} R_i}^2 -  \sigma^2 \|\hat a_i\|^2 \cdot\tr\left[\Sigma(X^\top X)^{-1}\right]} \lesssim  \sigma^2 \cdot n^{-1/2+c}  \sum_{1\le j,k\le t}|\hat a_i(j)||\hat a_i(k)| =  n^{-1/2+c} \sigma^2 \cdot  \|\hat a_i\|^2.
	\end{align*}
%	One can verify that the expectation of $\norm{\Sigma^{1/2} E_i}^2$ is equal to $\sigma^2 \norm{\hat{a}_i}^2 \cdot \tr[\Sigma (X^{\top} X)^{-1}]$.
%	Next, we bound the concentration error.
%	Let $A = X (X^{\top} X)^{-1} \Sigma (X^{\top} X)^{-1} X^{\top}$.
%	We have that
%	\begin{align*}
%		\bigabs{\bignorm{\Sigma^{1/2} E_i}^2 - \exarg{\cE}{\bignorm{\Sigma^{1/2} E_i}^2}}
%		&= \bigabs{\inner{A}{\cE \hat{a}_i \hat{a}_i^{\top} \cE^{\top} - \exarg{\cE}{\cE \hat{a}_i \hat{a}_i^{\top} \cE}}}.
%	\end{align*}
%	In the above equation, we note that the trace of $A$ is precisely the trace of $\Sigma (X^{\top} X)^{-1}$.
%	On the other hand, the concentration error of the spectral norm of $\cE \hat{a}_i \hat{a}_i^{\top} \cE^{\top}$ is at most $\sigma^2 \e $.

	Finally, combining the three parts together, we complete the proof.
\end{proof}
